#!/bin/bash

: ${WORKING_DIR:=$MOUNT_PATH/kubernetes/install_k8s}
source "$WORKING_DIR"/config

release=$2
CMD=$1

export ADMIN_ID=admin
export ADMIN_PASSWORD=admin

getOAuth0Config(){
 IFS='' read -r -d '' OAUTH <<"EOF"
export OIDC_ISSUE_URL=https://skmaji.auth0.com/
export OIDC_CLIENT_ID=C3UHISO3z60iF1JLG8L7VPUSWOASrJfO
export OIDC_USERNAME_CLAIM=sub
export OIDC_GROUPS_CLAIM=http://localhost:8080/claims/groups
export AUTH0_DOMAIN=skmaji.auth0.com
export APP_HOST=kube.gokcloud.com
export JWKS_URL=$OIDC_ISSUE_URL/.well-known/jwks.json
EOF
echo "$OAUTH"
}

getKeycloakConfig(){
  IFS='' read -r -d '' OAUTH <<"EOF"
export OIDC_ISSUE_URL=https://keycloak.gokcloud.com/realms/GokDevelopers
export OIDC_CLIENT_ID=gok-developers-client
export OIDC_USERNAME_CLAIM=sub
export OIDC_GROUPS_CLAIM=groups
export REALM=GokDevelopers
export AUTH0_DOMAIN=keycloak.gokcloud.com
export APP_HOST=kube.gokcloud.com
export JWKS_URL=$OIDC_ISSUE_URL/protocol/openid-connect/certs
EOF
echo "$OAUTH"
}

cat <<EOF  > ${MOUNT_PATH}/root_config
#dns, http, selfsigned
export LETS_ENCRYPT_PROD_URL=https://acme-v02.api.letsencrypt.org/directory
export LETS_ENCRYPT_STAGING_URL=https://acme-staging-v02.api.letsencrypt.org/directory
export CERTMANAGER_CHALANGE_TYPE=selfsigned
#staging, prod
export LETS_ENCRYPT_ENV=staging
export REGISTRY=registry
export KEYCLOAK=keycloak
export SPINNAKER=spinnaker
export DEFAULT_SUBDOMAIN=kube
export GROUP_NAME=$GOK_ROOT_DOMAIN
#ldap, oidc
export AUTHENTICATION_METHOD=oidc
export IDENTITY_PROVIDER=${IDENTITY_PROVIDER}
`
case ${IDENTITY_PROVIDER} in
  "oauth0")
    echo "$(getOAuth0Config)"
    ;;
  "keycloak")
    echo "$(getKeycloakConfig)"
    ;;
  *)
    echo "Unsupported identity provider: ${IDENTITY_PROVIDER}"
    ;;
esac
`
EOF

source ${MOUNT_PATH}/root_config

rootDomain(){
  echo "$GOK_ROOT_DOMAIN"
}

sedRootDomain(){
  rootDomain | sed 's/\./-/g'
}

registrySubdomain(){
  echo "$REGISTRY"
}

defaultSubdomain(){
  echo "$DEFAULT_SUBDOMAIN"
}

keycloakSubdomain(){
  echo "$KEYCLOAK"
}

fullDefaultUrl(){
  echo "${DEFAULT_SUBDOMAIN}.${GOK_ROOT_DOMAIN}"
}

fullRegistryUrl(){
  echo "${REGISTRY}.${GOK_ROOT_DOMAIN}"
}

fullKeycloakUrl(){
  echo "${KEYCLOAK}.${GOK_ROOT_DOMAIN}"
}

fullSpinnakerUrl(){
  echo "${SPINNAKER}.${GOK_ROOT_DOMAIN}"
}

echoSuccess(){
  echo -e "\e[32m$1\e[0m"
}

echoFailed(){
  echo -e "\e[31m$1\e[0m"
}

echoWarning(){
  echo -e "\e[32m$1\e[0m"
}

replaceEnvVariable(){
  wget -O- $1 | envsubst
}

createApp1() {
  cat <<EOF | kubectl apply -f -
apiVersion: apps/v1
kind: Deployment
metadata:
  name: app1
  namespace: default
spec:
  replicas: 1
  selector:
    matchLabels:
      app: app1
  template:
    metadata:
      labels:
        app: app1
    spec:
      containers:
      - name: app1
        image: dockersamples/static-site
        env:
        - name: AUTHOR
          value: app1
        ports:
        - containerPort: 80
---
apiVersion: v1
kind: Service
metadata:
  name: appsvc1
  namespace: default
spec:
  ports:
  - port: 80
    protocol: TCP
    targetPort: 80
  selector:
    app: app1
---
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  annotations:
    nginx.ingress.kubernetes.io/rewrite-target: /
    kubernetes.io/ingress.class: "nginx"
  name: app-ingress
  namespace: default
spec:
  rules:
  - host: $(fullDefaultUrl)
    http:
      paths:
      - backend:
          service:
            name: appsvc1
            port:
              number: 80
        path: /app1
        pathType: Prefix
EOF
}

kcurl(){
  cat <<EOF | kubectl apply -f -
apiVersion: v1
kind: Pod
metadata:
  name: curl
  namespace: default
spec:
  containers:
  - name: main
    image: curlimages/curl
    command: ["sleep", "9999999"]
EOF
  echo "Commands"
  echo "checkCurl https://kubernetes"
}

checkCurl(){
  kubectl exec -i -t curl -n default -- curl -kv "$@"
}

checkCMWebhook(){
  kubectl exec -i -t curl -n default -- curl -kv \
      --cacert <(kubectl -n cert-manager get secret cert-manager-webhook-ca -ojsonpath='{.data.ca\.crt}' | base64 -d) \
      https://cert-manager-webhook.cert-manager.svc:443/validate 2>&1 -d@- <<'EOF' | sed '/^* /d; /bytes data]$/d; s/> //; s/< //'
{"kind":"AdmissionReview","apiVersion":"admission.k8s.io/v1","request":{"requestKind":{"group":"cert-manager.io","version":"v1","kind":"Certificate"},"requestResource":{"group":"cert-manager.io","version":"v1","resource":"certificates"},"name":"foo","namespace":"default","operation":"CREATE","object":{"apiVersion":"cert-manager.io/v1","kind":"Certificate","spec":{"dnsNames":["foo"],"issuerRef":{"group":"cert-manager.io","kind":"Issuer","name":"letsencrypt"},"secretName":"foo","usages":["digital signature"]}}}}
EOF
}

join(){
  kubeadm token create --print-join-command
}

dnsUtils(){
  cat <<EOF | kubectl apply -f -
apiVersion: v1
kind: Pod
metadata:
  name: dnsutils
  namespace: default
spec:
  containers:
  - name: dnsutils
    image: k8s.gcr.io/e2e-test-images/jessie-dnsutils:1.3
    command:
      - sleep
      - "3600"
    imagePullPolicy: IfNotPresent
  restartPolicy: Always
EOF
  echo "Commands"
  echo "checkDns kubernetes.default.svc.cloud.uat"

}

istioInst(){
  helm repo add istio https://istio-release.storage.googleapis.com/charts
  helm repo update
  helm install istio-base istio/base -n istio-system \
    --create-namespace \
    --set defaultRevision=default

  helm install istiod istio/istiod -n istio-system --wait
  helm ls -n istio-system

}


checkDns(){
  kubectl exec -i -t dnsutils -n default -- nslookup "$@"
}

getpod() {
  pod=$(kubectl get po -l app.kubernetes.io/name="$release" 2>/dev/null | awk "/${release}/" | awk '{print $1}' | head -n 1)
  echo "$pod"
}

updateSys() {
  apt-get update
}

setupDockerRegistry(){
  if [[ -n $TRACE ]]; then
    set -x
  fi

  EXPORTDIR=$MOUNT_PATH

  if [ "$(hostname)" == 'master.cloud.com' ]; then
      mkdir -p "$EXPORTDIR"/certs
      mkdir -p /mnt/registry
      rm -rf /mnt/registry/config.yml
      wget https://github.com/sumitmaji/kubernetes/raw/master/install_k8s/registry/config.yml -P /mnt/registry/
      pushd "$EXPORTDIR" || exit
      USERNAME=master.cloud.com
      FILENAME=registry
      CERTIFICATE_KEY_NAME=$USERNAME
      rm /etc/docker/certs.d/master.cloud.com:5000/${CERTIFICATE_KEY_NAME}.crt
      rm /root/certs/${CERTIFICATE_KEY_NAME}.crt
      rm /root/certs/${CERTIFICATE_KEY_NAME}.key
      if [ -f /etc/docker/certs.d/master.cloud.com:5000/${CERTIFICATE_KEY_NAME}.crt ]
      then
        echo "The file is present, not creating it!!!!!"
      else
        createCertificate -i "${MASTER_HOST_IP}" -h master.cloud.com -t server -f registry
      fi

      docker stop registry
      docker rm registry

      docker run -d \
        --restart=always \
        --name registry \
        -v $EXPORTDIR/certs:/root/certs \
        -e REGISTRY_HTTP_ADDR=0.0.0.0:5000 \
        -e REGISTRY_HTTP_TLS_CERTIFICATE=/root/certs/${CERTIFICATE_KEY_NAME}.crt \
        -e REGISTRY_HTTP_TLS_KEY=/root/certs/${CERTIFICATE_KEY_NAME}.key \
        -v /mnt/registry:/var/lib/registry \
        -v /mnt/registry/config.yml:/etc/docker/registry/config.yml \
        -p 5000:5000 \
        registry:latest

      mkdir -p /etc/docker/certs.d/master.cloud.com:5000

      cp "$EXPORTDIR"/certs/${CERTIFICATE_KEY_NAME}.crt /etc/docker/certs.d/master.cloud.com:5000/${CERTIFICATE_KEY_NAME}.crt

      popd || exit
  else
      mkdir -p /etc/docker/certs.d/master.cloud.com:5000
      cp "$EXPORTDIR"/certs/${USERNAME}.crt /etc/docker/certs.d/master.cloud.com:5000/${USERNAME}.crt
  fi
}

installDeps() {
  #Install network tools
  apt-get install net-tools
  apt-get install jq -y

  #Installing python
  apt-get install python3 -y
  apt-get install python3-pip -y

}

ingressUnInst() {
  output=$(kubectl get po -n ingress-nginx -l app.kubernetes.io/component=controller -o json | jq '.items | length')
  if [ "$output" == "1" ]; then
    helm uninstall ingress-nginx -n ingress-nginx
    kubectl delete ns ingress-nginx
  fi
}

ingressInst() {
  helm repo add ingress-nginx https://kubernetes.github.io/ingress-nginx
  helm repo update
  helm install \
    ingress-nginx ingress-nginx/ingress-nginx \
    --namespace ingress-nginx \
    --create-namespace \
    --set controller.service.nodePorts.http=80 \
    --set controller.service.nodePorts.https=443 \
    --set controller.service.type=NodePort \
    --set defaultBackend.enabled=true
  #    -f charts/values.yaml

  waitForServiceAvailable ingress-nginx
}

dockrInst() {

  echo "Installing docker"
  apt-get install -y \
    apt-transport-https \
    ca-certificates \
    curl \
    software-properties-common
  curl -fsSL https://download.docker.com/linux/ubuntu/gpg | apt-key add -
  sudo install -m 0755 -d /etc/apt/keyrings
  sudo curl -fsSL https://download.docker.com/linux/ubuntu/gpg -o /etc/apt/keyrings/docker.asc
  sudo chmod a+r /etc/apt/keyrings/docker.asc

  # Add the repository to Apt sources:
  echo \
    "deb [arch=$(dpkg --print-architecture) signed-by=/etc/apt/keyrings/docker.asc] https://download.docker.com/linux/ubuntu \
    $(. /etc/os-release && echo "$VERSION_CODENAME") stable" | \
    sudo tee /etc/apt/sources.list.d/docker.list > /dev/null
  sudo apt-get update

  apt-get update && apt-get install -y \
    docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin

  # Create required directories
  sudo mkdir -p /etc/systemd/system/docker.service.d

  # Create daemon json config file
  sudo tee /etc/docker/daemon.json <<EOF
{
  "exec-opts": ["native.cgroupdriver=systemd"],
  "log-driver": "json-file",
  "log-opts": {
    "max-size": "100m"
  },
  "storage-driver": "overlay2"
}
EOF

  # Start and enable Services
  sudo systemctl daemon-reload
  sudo systemctl restart docker
  sudo systemctl enable docker

  # Start and enable containerd services
  sudo systemctl enable containerd
  sudo systemctl start containerd
  sudo rm /etc/containerd/config.toml
  sudo systemctl restart containerd
}

customDns() {
  #Adding custom dns server
  cat <<EOF | kubectl apply -f -
apiVersion: v1
data:
  Corefile: |
    .:53 {
        errors
        health {
           lameduck 5s
        }
        ready
        kubernetes cloud.uat in-addr.arpa ip6.arpa {
           pods insecure
           fallthrough in-addr.arpa ip6.arpa
           ttl 30
        }
        prometheus :9153
        forward . /etc/resolv.conf {
           max_concurrent 1000
        }
        cache 30
        loop
        reload
        loadbalance
    }
    cloud.com:53 {
        errors
        cache 30
        forward . ${MASTER_HOST_IP}
    }
kind: ConfigMap
metadata:
  name: coredns
  namespace: kube-system
EOF

  kubectl delete pod --namespace kube-system -l k8s-app=kube-dns
}

taintNode() {
  echo "Going to taint node for scheduling in master"

  # Define a function to get IP address
  getIP() {
    ifconfig eth1 2>/dev/null | awk '/inet / {print $2}' | sed 's/addr://'
  }

  # Get the IP address
  IP=$(getIP)

  # If IP is empty, try another network interface
  if [ -z "$IP" ]; then
    IP=$(ifconfig enp0s3 2>/dev/null | awk '/inet / {print $2}' | sed 's/addr://')
  fi

  # If IP is still empty, print an error message and exit
  if [ -z "$IP" ]; then
    echo "Error: Could not determine IP address"
    return 1
  fi

  # Get the node name
  JSONPATH="{.items[?(@.status.addresses[0].address == \"${IP}\")].metadata.name}"
  NODE_NAME="$(kubectl get nodes -o jsonpath="$JSONPATH")"

  # If node name is empty, print an error message and exit
  if [ -z "$NODE_NAME" ]; then
    echo "Error: Could not determine node name"
    return 1
  fi

  # Taint the node
  kubectl taint node "${NODE_NAME}" node-role.kubernetes.io/control-plane:NoSchedule-
}

k8sInst() {
  # Enable kernel modules
  sudo modprobe overlay
  sudo modprobe br_netfilter

  # Add some settings to sysctl
  sudo tee /etc/sysctl.d/kubernetes.conf <<EOF
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
net.ipv4.ip_forward = 1
EOF

  # Reload sysctl
  sudo sysctl --system
  sudo tee /etc/modules-load.d/containerd.conf <<EOF
overlay
br_netfilter
EOF
  sysctl --system
  mkdir -p /etc/containerd
  containerd config default>/etc/containerd/config.toml

  #https://github.com/containerd/containerd/blob/main/docs/cri/config.md#cgroup-driver
  sed -i 's/SystemdCgroup = false/SystemdCgroup = true/g' /etc/containerd/config.toml
  systemctl restart containerd
  systemctl enable containerd


  echo "Installing Kubernetes"
  apt-get update && apt-get install -y apt-transport-https
  sudo apt-get update
  # apt-transport-https may be a dummy package; if so, you can skip that package
  sudo apt-get install -y apt-transport-https ca-certificates curl

  # If the folder `/etc/apt/keyrings` does not exist, it should be created before the curl command, read the note below.
  # sudo mkdir -p -m 755 /etc/apt/keyrings
  curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.30/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg
  sudo chmod 644 /etc/apt/keyrings/kubernetes-apt-keyring.gpg # allow unprivileged APT programs to read this keyring
  # This overwrites any existing configuration in /etc/apt/sources.list.d/kubernetes.list
  echo 'deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.30/deb/ /' | sudo tee /etc/apt/sources.list.d/kubernetes.list
  sudo chmod 644 /etc/apt/sources.list.d/kubernetes.list   # helps tools such as command-not-found to work correctly

  sudo apt-get update
  sudo apt-get install -y kubectl kubeadm kubelet

  if [ $1 == "kubernetes" ]; then
    kubeadm version
    kubeadm config images pull

    sudo sed -i '/ swap / s/^\(.*\)$/#\1/g' /etc/fstab
    sudo swapoff -a

    sudo systemctl enable kubelet

    envsubst <"$WORKING_DIR"/cluster-config-master.yaml >"$WORKING_DIR"/config.yaml
    kubeadm init --config="$WORKING_DIR"/config.yaml --upload-certs
    export KUBECONFIG=/etc/kubernetes/admin.conf

    mkdir -p "$HOME"/.kube
    sudo cp -i /etc/kubernetes/admin.conf "$HOME"/.kube/config

    # shellcheck disable=SC2181
    if [ $? -ne 0 ]; then
      echo "Kubectl command execution failed, please check!!!!!"
      exit 1
    fi
  elif [ $1 == "kubernetes-worker" ]; then
    echo "kubectl join ...."
    echo "kubectl label node node01 node-role.kubernetes.io/worker=worker"
  fi

}

calicoInst(){
  kubectl create -f https://raw.githubusercontent.com/projectcalico/calico/v3.25.1/manifests/calico.yaml

  if [ $? -ne 0 ]; then
    echo "Calico installation failed, please check!!!!!"
    exit 1
  fi
}

helmInst() {
  #Installing helm
  curl https://baltocdn.com/helm/signing.asc | apt-key add - &&
    apt-get install apt-transport-https --yes &&
    echo "deb https://baltocdn.com/helm/stable/debian/ all main" | tee /etc/apt/sources.list.d/helm-stable-debian.list &&
    apt-get update &&
    apt-get install helm &&
    helm version --short &&
    helm repo add stable https://charts.helm.sh/stable

}

certmanagerInst() {
  helm repo add jetstack https://charts.jetstack.io
  helm repo update
  #kubectl apply -f https://github.com/cert-manager/cert-manager/releases/download/v1.14.5/cert-manager.crds.yaml

  #--set serviceAccount.automountServiceAccountToken=false \
  #--set webhook.timeoutSeconds=30
  #--set startupapicheck.timeout=10m
  # --debug
  helm install \
    cert-manager jetstack/cert-manager \
    --namespace cert-manager \
    --create-namespace \
    --set installCRDs=true \
    --set global.leaderElection.namespace=cert-manager \
    --version v1.14.5 \
    --values https://github.com/sumitmaji/kubernetes/raw/master/install_k8s/cert-manager/values.yaml
}

subDomain(){
  if [ -z $1 ]; then
    echo "$(defaultSubdomain)"
  else
    echo "$1"
  fi
}

certificateRequestForNs() {
  NS=$1
  SUBDOMAIN=$(subDomain $2)
  cat <<EOF | kubectl apply -f -
apiVersion: cert-manager.io/v1
kind: Certificate
metadata:
  name: ${SUBDOMAIN}-$(sedRootDomain)-tls
  namespace: ${NS}
spec:
  secretName: ${SUBDOMAIN}-$(sedRootDomain)
  issuerRef:
    name: $(getClusterIssuerName)
    kind: ClusterIssuer
  commonName: ${SUBDOMAIN}.$(rootDomain)
  dnsNames:
    - ${SUBDOMAIN}.$(rootDomain)
  issuerRef:
    name: $(getClusterIssuerName)
    kind: ClusterIssuer
EOF
  echo "Certificate request for NS $NS created, executing below command to know current status"
  echo "kubectl get Issuers,ClusterIssuers,Certificates,CertificateRequests,Orders,Challenges --all-namespaces"
  kubectl get Issuers,ClusterIssuers,Certificates,CertificateRequests,Orders,Challenges --all-namespaces
  kubectl --timeout=10s -n ${NS} wait --for=condition=Ready certificates.cert-manager.io "${SUBDOMAIN}"-"$(sedRootDomain)"-tls
}

getLetsEncryptUrl(){
  [[ $(getLetsEncEnv) == 'prod' ]] && echo "https://acme-v02.api.letsencrypt.org/directory " || echo "https://acme-staging-v02.api.letsencrypt.org/directory"
}

getLetsEncEnv(){
  echo "${LETS_ENCRYPT_ENV}"
}

getClusterIssuerName(){
  case "$CERTMANAGER_CHALANGE_TYPE" in
   'dns') echo "letsencrypt-$(getLetsEncEnv)" ;;
   'http') echo "letsencrypt-$(getLetsEncEnv)" ;;
   'selfsigned') echo "gokselfsign-ca-cluster-issuer" ;;
  esac
}

setupCertiIssuers() {

if [ $CERTMANAGER_CHALANGE_TYPE == 'dns' ]; then
  cat <<EOF | kubectl apply -f -
apiVersion: cert-manager.io/v1
kind: ClusterIssuer
metadata:
  name: $(getClusterIssuerName)
spec:
  acme:
    email: majisumitkumar@gmail.com
    server: $(getLetsEncryptUrl)
    privateKeySecretRef:
      name: letsencrypt-$(getLetsEncEnv)
    solvers:
    - dns01:
        webhook:
          config:
            apiKeySecretRef:
              name: godaddy-api-key-secret
              key: api-key
            production: true
            ttl: 600
          groupName: acme.mycompany.com
          solverName: godaddy
      selector:
       dnsNames:
       - '$(defaultSubdomain).$(rootDomain)'
       - '*.$(rootDomain)'
EOF
  godaddyWebhook
elif [ $CERTMANAGER_CHALANGE_TYPE == 'http' ]; then
    

  cat <<EOF | kubectl apply -f -
apiVersion: cert-manager.io/v1
kind: ClusterIssuer
metadata:
  name: $(getClusterIssuerName)
spec:
  acme:
    email: majisumitkumar@gmail.com
    server: $(getLetsEncryptUrl)
    #preferredChain: "(STAGING) Pretend Pear X1"
    privateKeySecretRef:
      name: letsencrypt-$(getLetsEncEnv)
    solvers:
    - http01:
        ingress:
          ingressClassName: nginx
EOF

elif [ $CERTMANAGER_CHALANGE_TYPE == 'selfsigned' ]; then
  # https://medium.com/geekculture/a-simple-ca-setup-with-kubernetes-cert-manager-bc8ccbd9c2
  # https://gist.github.com/jakexks/c1de8238cbee247333f8c274dc0d6f0f
  # Create self signed cluster issuer:
  echo "Creating self-signed cluster-issuer..."
  until cat <<EOYAML | kubectl apply -f -
apiVersion: cert-manager.io/v1
kind: ClusterIssuer
metadata:
  name: selfsigned-cluster-issuer
spec:
  selfSigned: {}
EOYAML
  do sleep 1; done
  kubectl --timeout=10s wait --for=condition=Ready clusterissuers.cert-manager.io selfsigned-cluster-issuer

  # Create CA certificate. If you want to use it as a ClusterIssuer the secret must be in the cert-manager namespace:
  echo "Creating self-signed certificate..."
  cat <<EOYAML | kubectl apply -f -
apiVersion: cert-manager.io/v1
kind: Certificate
metadata:
  name: gokselfsign-ca
  namespace: cert-manager
spec:
  isCA: true
  commonName: gokselfsign-ca
  secretName: gokselfsign-ca
  subject:
    organizations:
      - GOK Inc.
    organizationalUnits:
      - Widgets
  privateKey:
    algorithm: ECDSA
    size: 256
  issuerRef:
    name: selfsigned-cluster-issuer
    kind: ClusterIssuer
    group: cert-manager.io
EOYAML
  kubectl --timeout=10s -n cert-manager wait --for=condition=Ready certificates.cert-manager.io gokselfsign-ca

  # Create clusterissuer
  echo "Creating CA cluster issuer..."
  cat <<EOYAML | kubectl apply -f -
apiVersion: cert-manager.io/v1
kind: ClusterIssuer
metadata:
  name: $(getClusterIssuerName)
spec:
  ca:
    secretName: gokselfsign-ca
EOYAML
  kubectl --timeout=10s wait --for=condition=Ready clusterissuers.cert-manager.io "$(getClusterIssuerName)"

  # Add the self signed issuer(ca) certificate to authorized certificates
  kubectl get secrets -n cert-manager gokselfsign-ca -o json | jq -r '.data["tls.crt"]' | base64 -d > /usr/local/share/ca-certificates/issuer.crt
  update-ca-certificates
  #Need to restart the container
fi


  cat <<EOF | kubectl apply -f -
apiVersion: cert-manager.io/v1
kind: Certificate
metadata:
  name: $(sedRootDomain)-tls
  namespace: default
spec:
  secretName: $(sedRootDomain)
  issuerRef:
    name: $(getClusterIssuerName)
    kind: ClusterIssuer
  commonName: $(defaultSubdomain).$(rootDomain)
  dnsNames:
    - $(defaultSubdomain).$(rootDomain)
  issuerRef:
    name: $(getClusterIssuerName)
    kind: ClusterIssuer
EOF

echoWarning "Selfsigned certificate is added to root ca, please reboot the system for it to effect"
}

certManagerReset() {
  kubectl delete Issuers,ClusterIssuers,Certificates,CertificateRequests,Orders,Challenges --all --all-namespaces
  helm --namespace cert-manager delete cert-manager
  kubectl delete namespace cert-manager
  #kubectl delete -f https://github.com/cert-manager/cert-manager/releases/download/v1.8.2/cert-manager.crds.yaml

  if [[ $CERTMANAGER_CHALANGE_TYPE == 'dns' ]]; then godaddyWebhook; fi
}

#Godday api calls are disabled, hence going to remove this call.
godaddyWebhook() {
  replaceEnvVariable  https://github.com/sumitmaji/kubernetes/raw/master/install_k8s/godaddy-cert-webhook/webhook-all.yml | kubectl create -f - --validate=false
#  kubectl apply -f https://github.com/sumitmaji/kubernetes/raw/master/install_k8s/godaddy-cert-webhook/webhook-all.yml --validate=false
  echo "Provide godaddy apikey and secret <API_KEY:SECRET>"
  # shellcheck disable=SC2162
  read API_KEY

  cat <<EOF | kubectl apply -f -
apiVersion: v1
kind: Secret
metadata:
  name: godaddy-api-key-secret
  namespace: cert-manager
type: Opaque
stringData:
  api-key: ${API_KEY}
EOF

  cat <<EOF | kubectl apply -f -
apiVersion: cert-manager.io/v1
kind: ClusterIssuer
metadata:
  name: letsencrypt-prod
spec:
  acme:
    email: majisumitkumar@gmail.com
    server: https://acme-v02.api.letsencrypt.org/directory
    privateKeySecretRef:
      name: letsencrypt-prod
    solvers:
    - dns01:
        webhook:
          config:
            apiKeySecretRef:
              name: godaddy-api-key-secret
              key: api-key
            production: true
            ttl: 600
          groupName: $(rootDomain)
          solverName: godaddy
      selector:
       dnsNames:
       - '$(defaultSubdomain).$(rootDomain)'
       - '*.$(rootDomain)'
EOF

  cat <<EOF | kubectl apply -f -
apiVersion: cert-manager.io/v1
kind: Certificate
metadata:
  name: $(sedRootDomain)-tls
  namespace: default
spec:
  secretName: $(sedRootDomain)
  issuerRef:
    name: letsencrypt-prod
    kind: ClusterIssuer
  commonName: $(defaultSubdomain).$(rootDomain)
  dnsNames:
    - $(defaultSubdomain).$(rootDomain)
  issuerRef:
    name: letsencrypt-prod
    kind: ClusterIssuer
EOF

}

godaddyWebhookReset() {
  kubectl delete -f https://github.com/sumitmaji/kubernetes/raw/master/install_k8s/godaddy-cert-webhook/webhook-all.yml
  kubectl delete secret godaddy-api-key-secret -n cert-manager
}

haInst() {
  docker stop master-proxy
  docker rm master-proxy
  cat <<EOF >/opt/haproxy.cfg
global
        log 127.0.0.1 local0
        log 127.0.0.1 local1 notice
        maxconn 4096
        maxpipes 1024
        daemon
defaults
        log global
        mode tcp
        option tcplog
        option dontlognull
        option redispatch
        option http-server-close
        retries 3
        timeout connect 5000
        timeout client 50000
        timeout server 50000
        frontend default_frontend
        bind *:$HA_PROXY_PORT
        default_backend master-cluster
backend master-cluster
$(#Install master nodes
    IFS=','
    counter=0
    cluster=""
    for worker in $API_SERVERS; do
      oifs=$IFS
      IFS=':'
      read -r ip node <<<"$worker"
      if [ -z "$cluster" ]; then
        cluster="$ip:6443"
      else
        cluster="$cluster,http://$ip:4001"
      fi
      counter=$((counter + 1))
      IFS=$oifs
      echo "        server master-$counter ${cluster} check"
      cluster=""
    done
    unset IFS
  )
EOF

  docker run -d --name master-proxy \
    -v /opt/haproxy.cfg:/usr/local/etc/haproxy/haproxy.cfg:ro \
    --net=host haproxy
}

startKubelet() {
  systemctl stop kubelet
  systemctl start kubelet
}

startHa() {
  docker stop master-proxy
  docker rm master-proxy
  docker run -d --name master-proxy \
    -v /opt/haproxy.cfg:/usr/local/etc/haproxy/haproxy.cfg:ro \
    --net=host haproxy
}

disableSwap() {
  sudo sed -i '/ swap / s/^\(.*\)$/#\1/g' /etc/fstab
  sudo swapoff -a
}

hostSecret() {
  openssl genrsa -out ${APP_HOST}.key 4096
  openssl req -new -key ${APP_HOST}.key -out ${APP_HOST}.csr -subj "/CN=${APP_HOST}" \
    -addext "subjectAltName = DNS:${APP_HOST}"
  openssl x509 -req -in ${APP_HOST}.csr -CA /etc/kubernetes/pki/ca.crt -CAkey /etc/kubernetes/pki/ca.key -CAcreateserial -out ${APP_HOST}.crt -days 7200

  kubectl create secret tls appingress-certificate --key ${APP_HOST}.key --cert ${APP_HOST}.crt -n default
}

dashboardInst() {
  helm repo add kubernetes-dashboard https://kubernetes.github.io/dashboard/
  kubectl delete ns kubernetes-dashboard
  helm install kubernetes-dashboard \
    kubernetes-dashboard/kubernetes-dashboard \
    --namespace kubernetes-dashboard \
    --create-namespace \
    -f https://github.com/sumitmaji/kubernetes/raw/master/install_k8s/dashboard/values2.yaml
}

prometheusGrafanaReset() {
  helm -n monitoring delete monitoring
  kubectl delete ns monitoring
  helm -n db delete postgres
  kubectl delete ns db
}

emptyLocalFsStorage() {
  local service=$1
  local pvName=$2
  local scName=$3
  local volumePath=$4
  local namespace=$5

  if [[ -n $namespace ]]; then
    kubectl delete pvc --all -n $namespace
  fi

  kubectl delete pv $pvName
  kubectl delete sc $scName
  rm -rf $volumePath
}

createLocalStorageClassAndPV() {
  local storageClassName=$1
  local pvName=$2
  local volumePath=$3

  cat << EOF | kubectl apply -f -
kind: StorageClass
apiVersion: storage.k8s.io/v1
metadata:
  name: ${storageClassName}
provisioner: kubernetes.io/no-provisioner
volumeBindingMode: WaitForFirstConsumer
EOF

  mkdir -p "${volumePath}"
  chmod 777 "${volumePath}"

  cat << EOF | kubectl apply -f -
apiVersion: v1
kind: PersistentVolume
metadata:
  name: ${pvName}
spec:
  capacity:
    storage: 10Gi
  accessModes:
  - ReadWriteOnce
  persistentVolumeReclaimPolicy: Retain
  storageClassName: ${storageClassName}
  local:
    path: ${volumePath}
  nodeAffinity:
    required:
      nodeSelectorTerms:
      - matchExpressions:
        - key: kubernetes.io/hostname
          operator: In
          values:
          - master.cloud.com
EOF
}


prometheusGrafanaResetv2(){
  helm -n monitoring delete prometheus
  helm -n monitoring delete grafana
  kubectl delete ns monitoring

}

prometheusGrafanaInstv2(){
  CLIENT_SECRET=$(python3 -c "
import getpass
client_secret = getpass.getpass('Please enter your client secret: ')
print(client_secret)
")
  CLIENT_ID=gok-developers-client
  URL=$(fullDefaultUrl)
  OAUTH2_HOST=$(fullKeycloakUrl)
  REALM=GokDevelopers
  kubectl create ns monitoring
  kubectl create secret generic kube-prometheus-stack-grafana-oauth \
    --from-literal GF_AUTH_KEYCLOAK_CLIENT_ID="${CLIENT_ID}" \
    --from-literal GF_AUTH_KEYCLOAK_CLIENT_SECRET="${CLIENT_SECRET}" \
    --from-literal=OAUTH_AUTH_URL="https://${OAUTH2_HOST}/realms/${REALM}/protocol/openid-connect/auth" \
    --from-literal=OAUTH_TOKEN_URL="https://${OAUTH2_HOST}/realms/${REALM}/protocol/openid-connect/token" \
    --from-literal=OAUTH_API_URL="https://${OAUTH2_HOST}/realms/${REALM}/protocol/openid-connect/userinfo" \
    --from-literal=GRAFANA_DOMAIN="${URL}" \
    --from-literal=GRAFANA_ROOT_URL="https://${URL}/grafana" \
    --namespace monitoring

  kubectl create configmap -n monitoring env-data \
    --from-literal=OAUTH_AUTH_URL="https://${OAUTH2_HOST}/realms/${REALM}/protocol/openid-connect/auth" \
    --from-literal=OAUTH_TOKEN_URL="https://${OAUTH2_HOST}/realms/${REALM}/protocol/openid-connect/token" \
    --from-literal=OAUTH_API_URL="https://${OAUTH2_HOST}/realms/${REALM}/protocol/openid-connect/userinfo" \
    --from-literal=GRAFANA_DOMAIN="${URL}" \
    --from-literal=GRAFANA_ROOT_URL="https://${URL}/grafana"

  kubectl get secrets -n cert-manager gokselfsign-ca -o json | jq -r '.data["tls.crt"]' | base64 -d > ~/issuer.crt
  kubectl get secrets -n keycloak keycloak-gokcloud-com -o json | jq -r '.data["tls.crt"]' | base64 -d > ~/keycloak.crt
  kubectl create configmap certs-configmap -n monitoring --from-file=/root/issuer.crt --from-file=/root/keycloak.crt
  rm /root/issuer.crt
  rm /root/keycloak.crt
  helm repo add prometheus-community https://prometheus-community.github.io/helm-charts
  helm repo add grafana https://grafana.github.io/helm-charts
  helm repo update
  helm install prometheus prometheus-community/prometheus \
    --set server.extraFlags=\{web.enable-lifecycle,web.route-prefix=/,web.external-url=prometheus\} \
    --values https://github.com/sumitmaji/kubernetes/raw/master/install_k8s/prometheus-grafana/prometheus-values.yaml \
    --namespace monitoring \
    --create-namespace

  helm install grafana grafana/grafana \
    --set grafana.ini.server.root_url=https://$URL/grafana \
    --values https://github.com/sumitmaji/kubernetes/raw/master/install_k8s/prometheus-grafana/grafana-values.yaml \
    --namespace monitoring

  kubectl -n kube-system get cm kube-proxy -o yaml | sed 's/metricsBindAddress: ""/metricsBindAddress: 0.0.0.0:10249/' | kubectl apply -f -

  kubectl -n kube-system patch ds kube-proxy -p \
      '{"spec":{"template":{"metadata":{"labels":{"updateTime":"'`date +'%s'`'"}}}}}'

}

prometheusGrafanaInst() {

  helm repo add prometheus-community \
    https://prometheus-community.github.io/helm-charts
  helm repo update
  helm install monitoring \
    prometheus-community/kube-prometheus-stack \
    --values https://github.com/sumitmaji/kubernetes/raw/master/install_k8s/prometheus-grafana/values.yaml \
    --version 39.6.0 \
    --namespace monitoring \
    --create-namespace

#  kubectl -n kube-system get cm kube-proxy-config -o yaml | sed \
#    's/metricsBindAddress: 127.0.0.1:10249/metricsBindAddress: 0.0.0.0:10249' \
#    kubectl apply -f -

  kubectl -n kube-system get cm kube-proxy -o yaml | sed 's/metricsBindAddress: ""/metricsBindAddress: 0.0.0.0:10249/' | kubectl apply -f -

  kubectl -n kube-system patch ds kube-proxy -p \
    '{"spec":{"template":{"metadata":{"labels":{"updateTime":"'`date +'%s'`'"}}}}}'

  helm repo add bitnami https://charts.bitnami.com/bitnami
  helm repo update
  helm install postgres \
    bitnami/postgresql \
    --values https://github.com/sumitmaji/kubernetes/raw/master/install_k8s/prometheus-grafana/postgres-values.yaml \
    --version 11.7.1 \
    --namespace db \
    --create-namespace

}

dashboardReset() {
  helm uninstall kubernetes-dashboard -n kubernetes-dashboard
  helm delete ns kubernetes-dashboard
}

adminRole() {
  WC=$(kubectl get clusterrolebinding cloud-cluster-admin 2>/dev/null | wc -l)
  if [ "$WC" == "0" ]; then

    cat <<EOF | kubectl apply -f -
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: cloud-cluster-admin
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: cluster-admin
subjects:
- apiGroup: rbac.authorization.k8s.io
  kind: Group
  name: cloud:masters
EOF
  fi

}

#Creating user role developers which would allow users authenticated with oauth and
#having developers role to connect with cluster
oauthDev(){
  cat <<EOF | kubectl create -f -
kind: Role
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  namespace: default
  name: pod-reader
rules:
- apiGroups: [""]
  resources: ["pods"]
  verbs: ["get", "watch", "list", "create", "update", "patch", "delete"]
- apiGroups: ["extensions", "apps"]
  resources: ["deployments"]
  verbs: ["get", "list", "watch", "create", "update", "patch", "delete"]
---
kind: RoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: read-pods
  namespace: default
subjects:
- kind: Group
  name: developers
  apiGroup: rbac.authorization.k8s.io
roleRef:
  kind: Role
  name: pod-reader
  apiGroup: rbac.authorization.k8s.io
EOF
}

#Creating user role administrators which would allow users authenticated with oauth and
#having administrators role to connect with cluster
oauthAdmin(){
  cat <<EOF | kubectl create -f -
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: oauth-cluster-admin
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: cluster-admin
subjects:
- apiGroup: rbac.authorization.k8s.io
  kind: Group
  name: administrators
EOF
}

oauthUser(){
  kubectl config set-cluster cloud.com --certificate-authority=/etc/kubernetes/pki/ca.crt --embed-certs=true --server="$(apiUrl)" --kubeconfig=/root/oauth.conf
  kubectl config --kubeconfig=/root/oauth.conf set-context oauthuser@cloud.com --cluster=cloud.com --user=oauthuser
  kubectl config --kubeconfig=/root/oauth.conf use-context oauthuser@cloud.com

  echoSuccess "OAuth kubeconfig file create in /root/oauth.conf"
  echoSuccess "Use below command to use oauth.conf"
  echoSuccess "kubectl --kubeconfig=/root/oauth.conf --token=__USER_TOKEN__ rest of command"
  echoSuccess "alias kctl='kubectl --kubeconfig=/root/oauth.conf --token=\${__USER_TOKEN__}'"
  echoSuccess "alias kcd='kctl config set-context \$(kctl config current-context) --namespace'"
}

apiUrl(){
  kubectl config view -o json | jq -r '.clusters[] | .cluster.server'
}

opensearchReset(){
  helm uninstall opensearch -n opensearch
  emptyLocalFsStorage "Opensearch" "opensearch-pv" "opensearch-storage" "/data/volumes/pv5" "opensearch"
  kubectl delete ns opensearch
}

opensearchDashReset(){
  helm uninstall opensearch-dashboard -n opensearch
}

opensearchDashInst(){
  helm repo add openSearch https://opensearch-project.github.io/helm-charts/
  helm repo update
  helm install opensearch-dashboard \
      --namespace opensearch \
      --values https://github.com/sumitmaji/kubernetes/raw/master/install_k8s/opensearch/values-dashboard.yaml \
      openSearch/opensearch-dashboards

  gok patch ingress opensearch-dashboard-opensearch-dashboards opensearch letsencrypt opensearch
  echo "Waiting for dashboard service to be up!!!!"
    kubectl --timeout=120s wait --for=condition=Ready pods --all --namespace opensearch
    [[ $? -eq 0 ]] && echoSuccess "Opensearch dashboard service now up!" || echoFailed "Opensearch dashboard service timed-out, plaese check!!"
}

opensearchInst(){
  helm repo add openSearch https://opensearch-project.github.io/helm-charts/
  helm repo update
  kubectl create ns opensearch
  kubectl create secret generic opensearch-password \
      --from-literal=OPENSEARCH_INITIAL_ADMIN_PASSWORD=Sumit@7656 -n opensearch
  helm install opensearch \
    --namespace opensearch \
    --create-namespace \
    --values https://github.com/sumitmaji/kubernetes/raw/master/install_k8s/opensearch/values.yaml \
    openSearch/opensearch

  createLocalStorageClassAndPV "opensearch-storage" "opensearch-pv" "/data/volumes/pv5"
  echo "Waiting for services to be up!!!!"
  kubectl --timeout=120s wait --for=condition=Ready pods --all --namespace opensearch
  [[ $? -eq 0 ]] && echoSuccess "Opensearch services are now up!" || echoFailed "Opensearch services timed-out, plaese check!!"
  checkCurl curl -XGET https://opensearch-cluster-master.opensearch.svc:9200 -u 'admin:Sumit@7656' --insecure
  #gok patch ingress opensearch opensearch letsencrypt opensearch
}

fluentdReset(){
  helm uninstall fluentd -n fluentd
  kubectl delete ns fluentd
}

fluentdInst(){
  helm repo add fluent https://fluent.github.io/helm-charts
  helm repo update
  helm install fluentd \
        --namespace fluentd \
        --create-namespace \
        --values $MOUNT_PATH/kubernetes/install_k8s/fluentd/values.yaml \
        fluent/fluentd

  gok patch ingress fluentd fluentd letsencrypt fluentd
}


resetDockerRegistry(){
  helm uninstall registry -n registry

  emptyLocalFsStorage "Registry" "registry-pv" "registry-storage" "/data/volumes/pv4" "registry"
  kubectl delete ns registry
}

# https://itnext.io/error-x509-certificate-signed-by-unknown-authority-error-is-returned-f0e5d436f467
# Generate User & Password
genRegistyrPassword(){
  export REGISTRY_USER=sumit
  export REGISTRY_PASS=sumit
  export DESTINATION_FOLDER=./registry-creds

  # Backup credentials to local files (in case you'll forget them later on)
  mkdir -p ${DESTINATION_FOLDER}
  echo ${REGISTRY_USER} > ${DESTINATION_FOLDER}/registry-user.txt
  echo ${REGISTRY_PASS} > ${DESTINATION_FOLDER}/registry-pass.txt

  docker run --entrypoint htpasswd registry:2.7.0 \
      -Bbn ${REGISTRY_USER} ${REGISTRY_PASS} \
      > ${DESTINATION_FOLDER}/htpasswd

  unset REGISTRY_USER REGISTRY_PASS DESTINATION_FOLDER
}

# https://blog.zachinachshon.com/docker-registry/
# Verify access to the registry
verifyRegistyrInst(){
  export DESTINATION_FOLDER=./registry-creds
  export USER=$(cat ${DESTINATION_FOLDER}/registry-user.txt)
  export PASSWORD=$(cat ${DESTINATION_FOLDER}/registry-pass.txt)

  curl -kiv -H \
    "Authorization: Basic $(echo -n "${USER}:${PASSWORD}" | base64)" \
    https://"$(registrySubdomain).$(rootDomain)"/v2/_catalog

  wget --no-check-certificate --header \
    "Authorization: Basic $(echo -n "${USER}:${PASSWORD}" | base64)" \
    https://"$(registrySubdomain).$(rootDomain)"/v2/_catalog

  unset USER PASSWORD
}

# https://itnext.io/error-x509-certificate-signed-by-unknown-authority-error-is-returned-f0e5d436f467
dockerRegistryInst(){
  genRegistyrPassword
  helm repo add twuni https://helm.twun.io
  export DESTINATION_FOLDER=./registry-creds
  helm upgrade --install registry \
      --namespace registry \
      --create-namespace \
      --set replicaCount=1 \
      --set persistence.enabled=true \
      --set persistence.size=10Gi \
      --set persistence.deleteEnabled=true \
      --set persistence.storageClass=registry-storage \
      --set secrets.htpasswd="$(cat ${DESTINATION_FOLDER}/htpasswd)" \
      --values https://github.com/sumitmaji/kubernetes/raw/master/install_k8s/registry/values.yaml \
      twuni/docker-registry

}

# It is used to generate both client/server certificate using
# Kubernetes ca certificate in /etc/kubernetes/pki/ca.crt
createCertificate(){
  if [[ -n $TRACE ]]; then
    set -x
  fi

  : ${INSTALL_PATH:=$MOUNT_PATH/kubernetes/install_k8s}

  while [[ $# -gt 0 ]]
  do
  key="$1"
  case $key in
   -i|--ip)
   NODE_IP="$2"
   shift
   shift
   ;;
   -h|--host)
   HOSTNAME="$2"
   shift
   shift
   ;;
   -f|--file)
   FILENAME="$2"
   shift
   shift
   ;;
   -t|--type)
   TYPE="$2"
   shift
   shift
   ;;
  esac
  done

  if [ -z "$NODE_IP" ]
  then
  	echo "Please provide node ip"
  	exit 0
  fi
  if [ -z "$HOSTNAME" ]
  then
          echo "Please provide node hostname"
          exit 0
  fi
  if [ -z "$FILENAME" ]
  then
          echo "Please provide node filename"
          exit 0
  fi
  if [ -z "$TYPE" ]
  then
          echo "Please provide file type"
          exit 0
  fi

  : ${COUNTRY:=IN}
  : ${STATE:=UP}
  : ${LOCALITY:=GN}
  : ${ORGANIZATION:=CloudInc}
  : ${ORGU:=IT}
  : ${EMAIL:=cloudinc.gmail.com}
  : ${COMMONNAME:=kube-system}

  mkdir -p $MOUNT_PATH/certs
  pushd $MOUNT_PATH/certs

  if [ $TYPE == 'server' ]
  then
   keyUsage='extendedKeyUsage = clientAuth,serverAuth'
   #HOSTNAME="${HOSTNAME}-${FILENAME}" # Need to see why I did that
  else
   keyUsage='extendedKeyUsage = clientAuth'
   FILENAME="${FILENAME}-client"
   #HOSTNAME="${HOSTNAME}-$FILENAME"
  fi

  cat <<EOF | sudo tee ${FILENAME}-openssl.cnf
[req]
req_extensions = v3_req
distinguished_name = req_distinguished_name
[req_distinguished_name]
[ v3_req ]
basicConstraints = CA:FALSE
keyUsage = nonRepudiation, digitalSignature, keyEncipherment
$keyUsage
`
if [ $TYPE == 'server' ]
then
echo "subjectAltName = IP:$NODE_IP, DNS:$HOSTNAME"
fi`
EOF

  #Create a private key
  openssl genrsa -out $HOSTNAME.key 2048

  #Create CSR for the node
  openssl req -new -key $HOSTNAME.key \
    -subj "/CN=$NODE_IP" \
    -subj "/C=$COUNTRY/ST=$STATE/L=$LOCALITY/O=$ORGANIZATION/OU=$ORGU/CN=$HOSTNAME/emailAddress=$EMAIL" \
    -out $HOSTNAME.csr -config ${FILENAME}-openssl.cnf

  #Create a self signed certificate
  openssl x509 -req -in $HOSTNAME.csr -CA /etc/kubernetes/pki/ca.crt -CAkey /etc/kubernetes/pki/ca.key -CAcreateserial \
    -out $HOSTNAME.crt -days 10000 -extensions v3_req -extfile ${FILENAME}-openssl.cnf

  #Copy ca.crt to crt
  cat /etc/kubernetes/pki/ca.crt >> $HOSTNAME.crt

  #Verify a Private Key Matches a Certificate
  openssl x509 -noout -text -in $HOSTNAME.crt

  popd
}

# It is used to generate client certificate
# using kubectl command
createClientCertificate(){
  USERNAME=$1
  GROUPNAME=$2
  echo + Creating private key: ${USERNAME}.key
  openssl genrsa -out ${USERNAME}.key 4096

  echo + Creating signing request: ${USERNAME}-csr
  openssl req -new -key ${USERNAME}.key -out ${USERNAME}.csr -subj "/CN=${USERNAME}/O=${GROUPNAME}" \
          -addext "subjectAltName = DNS:${USERNAME}"

  WC=$(kubectl get csr ${USERNAME}-csr 2>/dev/null | wc -l)
  if [ "$WC" != "0" ]; then
    kubectl delete csr ${USERNAME}-csr
  fi
  echo + Creating signing request in kubernetes
  cat <<EOF | kubectl create -f -
apiVersion: certificates.k8s.io/v1
kind: CertificateSigningRequest
metadata:
  name: ${USERNAME}-csr
spec:
  groups:
    - system:authenticated
    - ${GROUPNAME}
  request: $(cat ${USERNAME}.csr | base64 | tr -d '\n')
  signerName: kubernetes.io/kube-apiserver-client
  usages:
    - digital signature
    - key encipherment
    - client auth
EOF

  kubectl certificate approve ${USERNAME}-csr

  kubectl get csr ${USERNAME}-csr -o jsonpath='{.status.certificate}' | base64 -d >${USERNAME}.crt
}

# The method creates certificate for user having admin role and generate kube config file
# for login to api server
createKubeConfig() {
  USERNAME=$1
  : ${IP:=$(ifconfig eth0 2>/dev/null | awk '/inet / {print $2}' | sed 's/addr://')}
  if [ -z "$IP" ]; then
    : ${IP:=$(ifconfig enp0s3 2>/dev/null | awk '/inet / {print $2}' | sed 's/addr://')}
  fi
  adminRole
  echo + Creating private key: ${USERNAME}.key
  openssl genrsa -out ${USERNAME}.key 4096

  echo + Creating signing request: ${USERNAME}.csr
  openssl req -new -key ${USERNAME}.key -out ${USERNAME}.csr -subj "/CN=${USERNAME}/O=cloud:masters"
  WC=$(kubectl get csr ${USERNAME}-csr 2>/dev/null | wc -l)
  if [ "$WC" != "0" ]; then
    kubectl delete csr ${USERNAME}-csr
  fi
  echo + Creating signing request in kubernetes
  cat <<EOF | kubectl create -f -
apiVersion: certificates.k8s.io/v1
kind: CertificateSigningRequest
metadata:
  name: ${USERNAME}-csr
spec:
  groups:
    - system:authenticated
    - cloud:masters
  request: $(cat ${USERNAME}.csr | base64 | tr -d '\n')
  signerName: kubernetes.io/kube-apiserver-client
  usages:
    - digital signature
    - key encipherment
    - client auth
EOF

  kubectl certificate approve ${USERNAME}-csr

  kubectl get csr ${USERNAME}-csr -o jsonpath='{.status.certificate}' | base64 -d >${USERNAME}.crt

  echo "======Kubeconfig file user ${USERNAME}.conf generated"

  kubectl config set-cluster cloud.com --certificate-authority=/etc/kubernetes/pki/ca.crt --embed-certs=true --server=https://${IP}:6443 --kubeconfig=/root/${USERNAME}.conf
  kubectl config set-credentials ${USERNAME} --client-key=${USERNAME}.key --client-certificate=${USERNAME}.crt --embed-certs=true --kubeconfig=/root/${USERNAME}.conf
  kubectl config --kubeconfig=/root/${USERNAME}.conf set-context ${USERNAME}@cloud.com --cluster=cloud.com --user=${USERNAME}
  kubectl config --kubeconfig=/root/${USERNAME}.conf use-context ${USERNAME}@cloud.com

  rm ${USERNAME}.key ${USERNAME}.csr ${USERNAME}.crt

}

patchLdapSecure() {
  kubectl patch ing "$NAME" --patch "$(
    cat <<EOF
metadata:
  annotations:
    nginx.ingress.kubernetes.io/auth-signin: https://$(defaultSubdomain).$(rootDomain)/authenticate
    nginx.ingress.kubernetes.io/auth-url: https://$(defaultSubdomain).$(rootDomain)/check
EOF
  )" -n "$NS"
}

patchLetsEncrypt() {
  NAME=$1
  NS=$2
  SUBDOMAIN=$(subDomain $3)

  kubectl patch ing "$NAME" --patch "$(
    cat <<EOF
metadata:
  annotations:
    #certmanager.k8s.io/cluster-issuer: $(getClusterIssuerName)
    cert-manager.io/cluster-issuer: $(getClusterIssuerName)
    nginx.ingress.kubernetes.io/ssl-redirect: "true"
spec:
  tls:
    - hosts:
        - ${SUBDOMAIN}.$(rootDomain)
      secretName: ${SUBDOMAIN}-$(sedRootDomain)
EOF
  )" -n "$NS"
  kubectl patch ing "$NAME" --type=json -p='[{"op": "replace", "path": "/spec/rules/0/host", "value":"'$SUBDOMAIN'.'$(rootDomain)'"}]' -n "$NS"

  kubectl --timeout=10s -n ${NS} wait --for=condition=Ready certificates.cert-manager.io ${SUBDOMAIN}-$(sedRootDomain)
}

helmTemplateDebug(){
  CHART_NAME=$1
  CHART_REPO=$2
  helm install $1 $2  \
    --dry-run --debug | less
}

keycloakReset(){
  helm uninstall keycloak -n keycloak
  kubectl delete pvc --all -n keycloak

  emptyLocalFsStorage "Keycloak" "keycloak-pv" "keycloak-storage" "/data/volumes/pv3"
  kubectl delete ns keycloak
}

keycloakInst(){

  kubectl create ns keycloak
  kubectl create secret generic keycloak-secrets \
    --from-literal=KEYCLOAK_LOG_LEVEL="TRACE" \
    --from-literal=KEYCLOAK_ADMIN="${ADMIN_ID}" \
    --from-literal=KEYCLOAK_ADMIN_PASSWORD="${ADMIN_PASSWORD}" -n keycloak

  helm install keycloak oci://registry-1.docker.io/bitnamicharts/keycloak \
        --namespace keycloak \
        --create-namespace \
        --values $MOUNT_PATH/kubernetes/install_k8s/keycloak/values2.yaml
}

installRegistryWithCertMgr(){
  createLocalStorageClassAndPV "registry-storage" "registry-pv" "/data/volumes/pv4"
  dockerRegistryInst
  # No need to generate the certificate.
  # The certificate and secret will be directory issued the ingress annotation cert-manager.io/cluster-issuer
  # gok create certificate registry registry
  gok patch ingress registry-docker-registry registry letsencrypt $(registrySubdomain)

  # Let docker trust the self-signed certificates
  # https://itnext.io/error-x509-certificate-signed-by-unknown-authority-error-is-returned-f0e5d436f467
  #BEGIN
  rm /etc/docker/certs.d/$(registrySubdomain).$(rootDomain)/ca.crt
  mkdir -p /etc/docker/certs.d/$(registrySubdomain).$(rootDomain)/
  kubectl get secret $(registrySubdomain)-$(sedRootDomain) -n registry -o jsonpath="{['data']['tls\.crt']}" | base64 --decode > /etc/docker/certs.d/$(registrySubdomain).$(rootDomain)/ca.crt
  systemctl restart docker
  #END

  # Restarting docker stops the haproxy, need to start it again
  gok start proxy
  docker login $(registrySubdomain).$(rootDomain)
  [[ $? -eq 0 ]] && echoSuccess "OK" || echoFailed "FAILED"
  openssl s_client -connect $(registrySubdomain).$(rootDomain):443 -showcerts </dev/null | grep 'Verify return code: 0 (ok)'
  [[ $? -eq 0 ]] && echoSuccess "Registry certificate verification succeeded" || echoFailed "Registry certificate verification failed"
}

installKeycloakWithCertMgr(){
  keycloakInst

  createLocalStorageClassAndPV "keycloak-storage" "keycloak-pv" "/data/volumes/pv3"
  # No need to generate the certificate.
  # The certificate and secret will be directory issued the ingress annotation cert-manager.io/cluster-issuer
  #gok create certificate keycloak keycloak
  gok patch ingress keycloak keycloak letsencrypt $(keycloakSubdomain)

  echo "Waiting for services to be up!!!!"
  kubectl --timeout=180s wait --for=condition=Ready pods --all --namespace keycloak
  [[ $? -eq 0 ]] && echoSuccess "Keycloak services are now up!\nAccess it using https://$(keycloakSubdomain).$(rootDomain)/" || echoFailed "Keycloak services timed-out, plaese check!!"
}

installPrometheusGrafanaWithCertMgr(){

  createLocalStorageClassAndPV "prometheus-storage" "prometheus-pv" "/data/volumes/pv1"
  createLocalStorageClassAndPV "alertmanager-storage" "alertmanager-pv" "/data/volumes/pv2"
  prometheusGrafanaInstv2

  # No need to generate the certificate.
  # The certificate and secret will be directory issued the ingress annotation cert-manager.io/cluster-issuer
  #gok create certificate monitoring kube
  #No need for prometheus to be externalized, removing the ingress as well.
  gok patch ingress prometheus-server monitoring letsencrypt $(defaultSubdomain)
  gok patch ingress grafana monitoring letsencrypt $(defaultSubdomain)

  echo "Waiting for services to be up!!!!"
  kubectl --timeout=120s wait --for=condition=Ready pods --all --namespace monitoring
  [[ $? -eq 0 ]] && echoSuccess "Prometheus and Grafana services are now up!\nAccess it using \nprometheus: https://$(defaultSubdomain).$(rootDomain)/prometheus\ngrafana: https://$(defaultSubdomain).$(rootDomain)/grafana" || echoFailed "Keycloak services timed-out, plaese check!!"
}

installDashboardwithCertManager(){
  dashboardInst
  # No need to generate the certificate.
  # The certificate and secret will be directory issued the ingress annotation cert-manager.io/cluster-issuer
  #gok create certificate kubernetes-dashboard kube
  gok patch ingress kubernetes-dashboard kubernetes-dashboard letsencrypt $(defaultSubdomain)

  echo "Waiting for services to be up!!!!"
  kubectl --timeout=120s wait --for=condition=Ready pods --all --namespace kubernetes-dashboard
  [[ $? -eq 0 ]] && echoSuccess "Dashboard services are now up!\nAccess it using https://$(defaultSubdomain).$(rootDomain)/dashboard/" || echoFailed "Keycloak services timed-out, plaese check!!"
}

oauth2ProxyReset(){
  helm uninstall oauth2proxy -n oauth2

  emptyLocalFsStorage "OAuth2" "oauth-pv" "oauth-storage" "/data/volumes/pv6" "oauth2"
  kubectl delete ns oauth2
}

oauth2ProxyInst(){
  client_secret=$(python3 -c "
import getpass
client_secret = getpass.getpass('Please enter your client secret: ')
print(client_secret)
")
  OAUTH2_PROXY_CLIENT_ID="gok-developers-client"
  REALM="GokDevelopers"
  OAUTH2_HOST="$(fullKeycloakUrl)"
  OAUTH2_OIDC_ISSUE_URL="https://$(fullKeycloakUrl)/realms/GokDevelopers/protocol/openid-connect/auth"

  createLocalStorageClassAndPV "oauth-storage" "oauth-pv" "/data/volumes/pv6"
  kubectl create ns oauth2
  kubectl create secret generic oauth2-proxy-secrets --from-literal=clientSecret="${client_secret}" -n oauth2
  kubectl create configmap oauth2-proxy-config \
      --from-literal=clientID="${OAUTH2_PROXY_CLIENT_ID}" \
      --from-literal=oidcIssuerUrl="${OAUTH2_OIDC_ISSUE_URL}" \
      --from-literal=redirectUrl=kube*. \
      -n oauth2
  helm install oauth2proxy oci://registry-1.docker.io/bitnamicharts/oauth2-proxy \
    --namespace oauth2 \
    --set "extraArgs[0]=--provider=keycloak" \
    --set "extraArgs[1]=--login-url=https://${OAUTH2_HOST}/realms/${REALM}/protocol/openid-connect/auth" \
    --set "extraArgs[2]=--redeem-url=https://${OAUTH2_HOST}/realms/${REALM}/protocol/openid-connect/token" \
    --set "extraArgs[3]=--profile-url=https://${OAUTH2_HOST}/realms/${REALM}/protocol/openid-connect/userinfo" \
    --set "extraArgs[4]=--validate-url=https://${OAUTH2_HOST}/realms/${REALM}/protocol/openid-connect/userinfo" \
    --set "extraArgs[5]=--keycloak-group=groups" \
    --set "extraArgs[6]=--scope=openid email profile groups" \
    --values https://github.com/sumitmaji/kubernetes/raw/master/install_k8s/oauth2-proxy/values.yaml \
    --create-namespace

    waitForServiceAvailable oauth2
}

waitForServiceAvailable(){
  NS=$1
  echo "Waiting for services to be up!!!!"
  kubectl --timeout=120s wait --for=condition=Ready pods --all --namespace "$NS"
  [[ $? -eq 0 ]] && echoSuccess "Service is now up!" || echoFailed "Service timed-out, please check!!"
}

decodeSecret(){
  SECRET=$1
  NS=$2
  kubectl get secret -n $NS $SECRET -o json | jq -r '.data."tls.crt"' | base64 -d | openssl x509 -noout -text
}

spinnakerInst(){
  CLIENT_SECRET=$(python3 -c "
import getpass
client_secret = getpass.getpass('Please enter your client secret: ')
print(client_secret)
")

  useradd -m -g root spinnaker
  mkdir -p /home/spinnaker/.hal && chmod -R 777 /home/spinnaker/.hal
  mkdir -p /home/spinnaker/.kube/ && cp ~/.kube/config /home/spinnaker/.kube/config && chmod 755 /home/spinnaker/.kube/config
  docker run --name halyard -v /home/spinnaker/.hal:/home/spinnaker/.hal -v /home/spinnaker/.kube/config:/home/spinnaker/.kube/config \
    -d gcr.io/spinnaker-marketplace/halyard:stable

  echo "Waiting for the service to be up for 30 Seconds"
  sleep 30
  docker exec -it halyard kubectl cluster-info
  docker exec -it halyard hal config provider kubernetes enable
  docker exec -it halyard hal config provider kubernetes account add gok-k8s --provider-version v2 \
    --context $(kubectl config current-context)
  docker exec -it halyard hal config features edit --artifacts true
  docker exec -it halyard hal config deploy edit --type distributed --account-name gok-k8s
  kubectl create ns spinnaker
  helm install minio --namespace spinnaker --set accessKey="myaccesskey" --set secretKey="mysecretkey" \
    --set persistence.enabled=false stable/minio
  docker exec -it halyard bash -c 'mkdir /home/spinnaker/.hal/default/profiles'
  docker exec -it halyard echo "spinnaker.s3.versioning: false" > /home/spinnaker/.hal/default/profiles/front50-local.yml

  cat <<EOF > /home/spinnaker/.hal/default/profiles/front50-local.yml
kubernetes:
  volumes:
  - id: internal-trust-store
    mountPath: /etc/ssl/certs/java
    type: secret
EOF

  ##https://github.com/spinnaker/spinnaker/issues/6498
  cat <<EOF > /home/spinnaker/.hal/permission.yml
users:
 - username: skmaji1
   roles:
   - admin
EOF

  cat <<EOF > /home/spinnaker/.hal/default/profiles/spinnaker-local.yml
logging:
  level:
    # Enable debug logging by changing level to DEBUG
    root: TRACE  # default
EOF

  #https://stackoverflow.com/questions/78087469/how-to-apply-custom-profiles-setting-to-spinnaker-to-make-it-deploy-with-one-com
  cat <<EOF > /home/spinnaker/.hal/default/profiles/fiat-local.yml
fiat.restrictApplicationCreation: true #Allows to restrict permissions
auth.permissions.provider.application: aggregate
auth.permissions.source.application.prefix: #Allows to work with
  enabled: true                          # applications prefixes
  prefixes:
  - prefix: "*" # All applications
    permissions:
      READ:
      - "administrators"
      - "admin"
      WRITE:
      - "administrators"
      - "admin"
      EXECUTE:
      - "administrators"
      - "admin"
      CREATE:
      - "administrators"
      - "admin"
EOF

  docker exec -it halyard hal config storage s3 edit --endpoint http://minio:9000 --access-key-id "myaccesskey" --secret-access-key "mysecretkey"
  docker exec -it halyard hal config storage s3 edit --path-style-access true
  docker exec -it halyard hal config storage edit --type s3
  docker exec -it halyard hal version list
  docker exec -it halyard hal config version edit --version 1.34.2

  #https://spinnaker.io/docs/reference/halyard/custom/
  cat <<EOF > /home/spinnaker/.hal/default/service-settings/gate.yml
kubernetes:
  volumes:
  - id: internal-trust-store
    mountPath: /etc/ssl/certs/java
    type: secret
EOF
  cat <<EOF > /home/spinnaker/.hal/default/profiles/gate-local.yml
kubernetes:
  volumes:
  - id: internal-trust-store
    mountPath: /etc/ssl/certs
    type: secret
server:
  tomcat:
    protocolHeader: X-Forwarded-Proto
    remoteIpHeader: X-Forwarded-For
    internalProxies: .*
    httpsServerPort: X-Forwarded-Port
security:
  oauth2:
    enabled: true
    client:
      clientId: gok-developers-client
      clientSecret: $CLIENT_SECRET
      userAuthorizationUri: https://keycloak.gokcloud.com/realms/GokDevelopers/protocol/openid-connect/auth
      accessTokenUri: https://keycloak.gokcloud.com/realms/GokDevelopers/protocol/openid-connect/token
      scope: openid,email,profile,groups
      preEstablishedRedirectUri: https://spin-gate.gokcloud.com/login
      clientAuthenticationScheme: form
    resource:
      userInfoUri: https://keycloak.gokcloud.com/realms/GokDevelopers/protocol/openid-connect/userinfo
    userInfoMapping:
      email: email
      firstName: given_name
      lastName: family_name
      username: preferred_username
      roles: groups
EOF

  cat <<EOF | kubectl apply -f -
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  annotations:
    kubernetes.io/ingress.allow-http: "false"
    nginx.ingress.kubernetes.io/ssl-redirect: "true"
    ingress.kubernetes.io/ssl-passthrough: "true"
    nginx.ingress.kubernetes.io/backend-protocol: "HTTP"
  name: spin-deck
  namespace: spinnaker
spec:
  ingressClassName: nginx
  rules:
  - host: spinnaker.cloud.com
    http:
      paths:
      - backend:
          service:
            name: spin-deck
            port:
              number: 9000
        path: /
        pathType: ImplementationSpecific
  tls:
    - secretName: appingress-certificate
      hosts:
        - spinnaker.cloud.com
---
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  annotations:
    kubernetes.io/ingress.allow-http: "false"
    nginx.ingress.kubernetes.io/ssl-redirect: "true"
    ingress.kubernetes.io/ssl-passthrough: "true"
    nginx.ingress.kubernetes.io/backend-protocol: "HTTP"
  name: spin-gate
  namespace: spinnaker
spec:
  ingressClassName: nginx
  rules:
  - host: spin-gate.cloud.com
    http:
      paths:
      - backend:
          service:
            name: spin-gate
            port:
              number: 8084
        path: /
        pathType: ImplementationSpecific
  tls:
    - secretName: appingress-certificate
      hosts:
        - spin-gate.cloud.com
EOF

  gok patch ingress spin-deck spinnaker letsencrypt spinnaker
  gok patch ingress spin-gate spinnaker letsencrypt spin-gate

  docker exec -it halyard hal config security ui edit --override-base-url "https://spinnaker.gokcloud.com"
  docker exec -it halyard hal config security api edit --override-base-url "https://spin-gate.gokcloud.com"
  docker exec -it halyard hal config security api edit --cors-access-pattern "https://spinnaker.gokcloud.com"

  #Authentication
  docker exec -it halyard hal config security authn oauth2 edit --client-id gok-developers-client --client-secret $CLIENT_SECRET --provider OTHER --pre-established-redirect-uri https://spin-gate.gokcloud.com/login
  docker exec -it halyard hal config security authn oauth2 enable

  #Authorization
  #Roles will be fetched from keycloak
  #https://github.com/spinnaker/spinnaker/issues/6498
  #docker exec -it halyard hal config security authz file edit --file-path /home/spinnaker/.hal/permission.yml
  docker exec -it halyard hal config security authz enable

  docker exec -it halyard hal config artifact github account add sumitmaji

  kubectl get secrets -n cert-manager gokselfsign-ca -o json | jq -r '.data["tls.crt"]' | base64 -d > /home/spinnaker/.hal/issuer.crt
  kubectl get secrets -n keycloak keycloak-gokcloud-com -o json | jq -r '.data["tls.crt"]' | base64 -d > /home/spinnaker/.hal/keycloak.crt

  docker exec -it halyard  cp /etc/ssl/certs/java/cacerts /home/spinnaker/.hal/
  chmod +w /home/spinnaker/.hal/cacerts
  #password is changeid
  docker exec -it halyard keytool -import -noprompt -trustcacerts -alias ca -file /home/spinnaker/.hal/issuer.crt -keystore /home/spinnaker/.hal/cacerts
  docker exec -it halyard keytool -import -noprompt -trustcacerts -alias keycloak -file /home/spinnaker/.hal/keycloak.crt -keystore /home/spinnaker/.hal/cacerts
  kubectl create secret generic -n spinnaker internal-trust-store \
  	--from-file /home/spinnaker/.hal/cacerts

  docker exec -it halyard hal deploy apply
}

getUserInfo(){
  curl https://keycloak.gokcloud.com/realms/GokDevelopers/protocol/openid-connect/auth?client_id=gok-developers-client&redirect_uri=https://localhost/login&response_type=code&scope=openid%20email%20profile&state=eLUwT2

  TOKEN=$(curl --location --request POST 'https://keycloak.gokcloud.com/realms/GokDevelopers/protocol/openid-connect/token' \
     --header 'Content-Type: application/x-www-form-urlencoded' \
     --data-urlencode 'grant_type=authorization_code' \
     --data-urlencode 'client_id=gok-developers-client' \
     --data-urlencode 'client_secret=sC1hHm9c2qHjwYtfumcQnEwyH7NOqkaV' \
     --data-urlencode 'redirect_uri=https://localhost/login' \
     --data-urlencode 'scope=roles email profile openid' \
     --data-urlencode 'code=2a34daeb-4bba-48fb-9531-69f55104ab62.69c4c286-f2c9-4455-8368-7a8723ad60cf.f79ef798-e122-403f-8626-ad3793bf3f44' | jq -r '.["access_token"]')

  curl -kv 'https://keycloak.gokcloud.com/realms/GokDevelopers/protocol/openid-connect/userinfo' \
  --header 'Content-Type: application/x-www-form-urlencoded' \
  --header "Authorization: Bearer $TOKEN"

}


spinnakerReset(){
  kubectl delete all --all -n spinnaker
  kubectl delete ns spinnaker
  docker stop halyard
  docker rm halyard
  rm -rf /home/spinnaker/.hal
  rm -rf /home/spinnaker/.kube
}

patchLocalTls() {
  NAME=$1
  NS=$2
  kubectl patch ing "$NAME" --patch "$(
    cat <<EOF
spec:
  tls:
    - hosts:
        - $APP_HOST
      secretName: appingress-certificate
EOF
  )" -n "$NS"
  kubectl patch ing "$NAME" --type=json -p='[{"op": "replace", "path": "/spec/rules/0/host", "value":"master.cloud.com"}]' -n "$NS"
  kubectl patch ing "$NAME" --type=json -p='[{"op": "add", "path": "/metadata/annotations", "value":{"nginx.ingress.kubernetes.io/rewrite-target": "/", "kubernetes.io/ingress.class": "nginx"}}]' -n "$NS"
}

help(){
  IFS='' read -r -d '' HELP <<"EOF"
gok install kubernetes => Installing Kubernetes
gok install ingress => Installing Ingress
gok install dashboard => Installing Dashboard
gok install cert-manager => Installing Cert-Manager
gok install keycloak => Installing Keycloak
gok install registry => Install registry
python3 keycloak-setup.py all => Setup keycloak
helmTemplateDebug $char_name $char_repo = Debug helm template
apiUrl => view url for kubernetes api server
oauthUser => Configure Kubeconfig for OAuth
oauthDev => Create developers role for oauth
oauthAdmin => Create administrators role for oauth

kcd $namespace => switch namespace
k => short form for kubectl
current => Show current namespace
pods => show pods in current namespace
secrets => show secrets in current namespace
viewcert => View certificate in secret in current namespace
decode => Base64 decode content in secret in current namespace
ns => Show all namespaces
edit $resouce => Edit resource in current namespace
logs => View logs of pod in current namespace
bash => Get terminal in pod
all => Show all resource in current namespace
kctl => Execute kubectl command with oauth authentication, create dev and admin role
EOF
echo "$HELP"
}

function bashCmd() {
  pod=$(getpod)
  echo "Opening terminal on $pod"
  kubectl exec -it "$pod" -- /bin/bash
}

function helpCmd() {
    echo "Available commands:"
    echo "1. gok install <component>: Install a component. Supported components are docker, cert-manager, monitoring, dashboard, keycloak, ingress, registry, fluentd, opensearch, spinnaker, oauth2, kubernetes-worker, and kubernetes."
    echo "2. gok start <component>: Start a component. Supported components are kubernetes, proxy, and kubelet."
    echo "3. gok reset <component>: Reset a component. Supported components are kubernetes, monitoring, dashboard, keycloak, ingress, registry, fluentd, opensearch, spinnaker, oauth2, and cert-manager."
    echo "4. gok deploy <component>: Deploy a component. Currently, only app1 is supported."
    echo "5. gok patch <resource> <name> <namespace> <options> <subdomain>: Patch resources. Currently, only ingress resource is supported. The options can be letsencrypt, ldap, or localtls."
    echo "6. gok create <resource> <name> <additional>: Create resources. Currently, secret and certificate resources are supported. For secret, only apphost is supported. For certificate, it creates a certificate request for a namespace."
    echo "7. gok bash: Open a terminal in a pod."
    echo "8. gok desc: Describe a pod."
    echo "9. gok logs: Show the logs of a pod."
    echo "10. gok status: Show the status of a helm release."
    echo "11. gok taint-node: Taint a node."
    echo "12. gok help: Show this help document."
}

function descCmd() {
  pod=$(getpod)
  echo "Describing pod $pod"
  kubectl describe po "$pod"
}

function logsCmd() {
  pod=$(getpod)
  echo "Viewing logs of pod $pod"
  kubectl logs "$pod"
}

function statusCmd() {
  helm status "$release"
}

function taintNodeCmd() {
  taintNode
}

function installCmd() {
  updateSys
  installDeps
  COMPONENT=$1
  if [ "$COMPONENT" == "docker" ]; then
    dockrInst
  elif [ "$COMPONENT" == "cert-manager" ]; then
    certmanagerInst
    setupCertiIssuers
  elif [ "$COMPONENT" == "monitoring" ]; then
    installPrometheusGrafanaWithCertMgr
  elif [ "$COMPONENT" == "dashboard" ]; then
    installDashboardwithCertManager
  elif [ "$COMPONENT" == "keycloak" ]; then
    installKeycloakWithCertMgr
  elif [ "$COMPONENT" == "ingress" ]; then
    ingressInst
  elif [ "$COMPONENT" == "registry" ]; then
    installRegistryWithCertMgr
  elif [ "$COMPONENT" == "fluentd" ]; then
    fluentdInst
  elif [ "$COMPONENT" == "opensearch" ]; then
    opensearchInst
    opensearchDashInst
  elif [ "$COMPONENT" == "spinnaker" ]; then
    spinnakerInst
  elif [ "$COMPONENT" == "oauth2" ]; then
    oauth2ProxyInst
  elif [ "$COMPONENT" == "kubernetes-worker" ]; then
    dockrInst
    k8sInst "kubernetes-worker"
  elif [ "$COMPONENT" == "kubernetes" ]; then
    dockrInst
    haInst
    k8sInst "kubernetes"
    helmInst
    taintNode
    calicoInst
    waitForServiceAvailable kube-system
    dnsUtils
    kcurl
    echo "source $MOUNT_PATH/kubernetes/install_k8s/util" >> /etc/bash.bashrc
    echo "source $MOUNT_PATH/kubernetes/install_k8s/gok" >> /etc/bash.bashrc
    ln -s $MOUNT_PATH/kubernetes/install_k8s/gok /bin/gok
    source /etc/bash.bashrc
    figlet "Please wait for 1 minute"
  fi
}

function startCmd() {
  COMPONENT=$1
  if [ "$COMPONENT" == "kubernetes" ]; then
    disableSwap
    startHa
    startKubelet
  elif [ "$COMPONENT" == "proxy" ]; then
    startHa
  elif [ "$COMPONET" == "kubelet" ]; then
    startKubelet
  fi
}

function resetCmd() {
  COMPONENT=$1
  if [ "$COMPONENT" == "kubernetes" ]; then
    kubeadm reset <<EOF
y
EOF
  elif [ "$COMPONENT" == "monitoring" ]; then
    prometheusGrafanaResetv2

    emptyLocalFsStorage "Monitoring" "prometheus-pv" "prometheus-storage" "/data/volumes/pv1"
    emptyLocalFsStorage "Monitoring" "alertmanager-pv" "alertmanager-storage" "/data/volumes/pv2"
  elif [ "$COMPONENT" == "dashboard" ]; then
    dashboardReset
  elif [ "$COMPONENT" == "keycloak" ]; then
    keycloakReset
  elif [ "$COMPONENT" == "ingress" ]; then
    ingressUnInst
  elif [ "$COMPONENT" == "registry" ]; then
    resetDockerRegistry
  elif [ "$COMPONENT" == "fluentd" ]; then
    fluentdReset
  elif [ "$COMPONENT" == "opensearch" ]; then
    opensearchDashReset
    opensearchReset
  elif [ "$COMPONENT" == "spinnaker" ]; then
    spinnakerReset
  elif [ "$COMPONENT" == "oauth2" ]; then
    oauth2ProxyReset
  elif [ "$COMPONENT" == "cert-manager" ]; then
    certManagerReset
  fi
}

function deployCmd() {
  COMPONENT=$2
  if [ "$COMPONENT" == "app1" ]; then
    createApp1
  fi
}

function patchCmd() {
  RESOURCE=$1
  NAME=$2
  NS=$3
  OPTIONS=$4
  SUBDOMAIN=$5
  if [ "$RESOURCE" == "ingress" ]; then
    if [ "$OPTIONS" == "letsencrypt" ]; then
      patchLetsEncrypt "$NAME" "$NS" "$SUBDOMAIN"
    elif [ "$OPTIONS" == "ldap" ]; then
      patchLdapSecure "$NAME" "$NS"
    elif [ "$OPTIONS" == "localtls" ]; then
      patchLocalTls "$NAME" "$NS"
    fi
  fi
}

function createCmd() {
  RESOURCE=$1
  NAME=$2
  ADDITIONAL=$3
  if [ "$RESOURCE" == "secret" ]; then
    if [ "$NAME" == "apphost" ]; then
      hostSecret
    fi
  elif [ "$RESOURCE" == "certificate" ]; then
    certificateRequestForNs "$NAME" "$ADDITIONAL"
  elif [ "$RESOURCE" == "kubeconfig" ]; then
    createKubeConfig "$NAME"
  fi
}

# Call the appropriate function based on the command
if [ "$CMD" == "bash" ]; then
  bashCmd
elif [ "$CMD" == "help" ]; then
  helpCmd
elif [ "$CMD" == "desc" ]; then
  descCmd
elif [ "$CMD" == "logs" ]; then
  logsCmd
elif [ "$CMD" == "status" ]; then
  statusCmd
elif [ "$CMD" == "taint-node" ]; then
  taintNodeCmd
elif [ "$CMD" == "install" ]; then
  installCmd "$2"
elif [ "$CMD" == "start" ]; then
  startCmd "$2"
elif [ "$CMD" == "reset" ]; then
  resetCmd "$2"
elif [ "$CMD" == "deploy" ]; then
  deployCmd "$2"
elif [ "$CMD" == "patch" ]; then
  patchCmd "$2" "$3" "$4" "$5" "$6"
elif [ "$CMD" == "create" ]; then
  createCmd "$2" "$3" "$4"
fi