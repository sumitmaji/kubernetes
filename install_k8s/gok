#!/bin/bash

: ${WORKING_DIR:=$MOUNT_PATH/kubernetes/install_k8s}
source "$WORKING_DIR"/config

release=$2
CMD=$1

getOAuth0Config(){
 IFS='' read -r -d '' OAUTH <<"EOF"
export OIDC_ISSUE_URL=https://skmaji.auth0.com/
export OIDC_CLIENT_ID=C3UHISO3z60iF1JLG8L7VPUSWOASrJfO
export OIDC_USERNAME_CLAIM=sub
export OIDC_GROUPS_CLAIM=http://localhost:8080/claims/groups
export AUTH0_DOMAIN=skmaji.auth0.com
export APP_HOST=kube.gokcloud.com
export JWKS_URL=$OIDC_ISSUE_URL/.well-known/jwks.json
EOF
echo "$OAUTH"
}

getKeycloakConfig(){
  IFS='' read -r -d '' OAUTH <<"EOF"
export OIDC_ISSUE_URL=https://keycloak.gokcloud.com/realms/GokDevelopers
export OIDC_CLIENT_ID=gok-developers-client
export OIDC_USERNAME_CLAIM=sub
export OIDC_GROUPS_CLAIM=groups
export REALM=GokDevelopers
export AUTH0_DOMAIN=keycloak.gokcloud.com
export APP_HOST=kube.gokcloud.com
export JWKS_URL=$OIDC_ISSUE_URL/protocol/openid-connect/certs
EOF
echo "$OAUTH"
}

cat <<EOF  > ${MOUNT_PATH}/root_config
export LETS_ENCRYPT_PROD_URL=https://acme-v02.api.letsencrypt.org/directory
export LETS_ENCRYPT_STAGING_URL=https://acme-staging-v02.api.letsencrypt.org/directory
#dns, http, selfsigned
export CERTMANAGER_CHALANGE_TYPE=selfsigned
#staging, prod
export LETS_ENCRYPT_ENV=staging
export REGISTRY=registry
export KEYCLOAK=keycloak
export SPINNAKER=spinnaker
export VAULT=vault
export JUPYTERHUB=jupyterhub
export ARGOCD=argocd
export DEFAULT_SUBDOMAIN=kube
export GROUP_NAME=$GOK_ROOT_DOMAIN
#ldap, oidc
export AUTHENTICATION_METHOD=oidc
export IDENTITY_PROVIDER=${IDENTITY_PROVIDER}
`
case ${IDENTITY_PROVIDER} in
  "oauth0")
    echo "$(getOAuth0Config)"
    ;;
  "keycloak")
    echo "$(getKeycloakConfig)"
    ;;
  *)
    echo "Unsupported identity provider: ${IDENTITY_PROVIDER}"
    ;;
esac
`
EOF

source ${MOUNT_PATH}/root_config

rootDomain(){
  echo "$GOK_ROOT_DOMAIN"
}

sedRootDomain(){
  rootDomain | sed 's/\./-/g'
}

registrySubdomain(){
  echo "$REGISTRY"
}

defaultSubdomain(){
  echo "$DEFAULT_SUBDOMAIN"
}

keycloakSubdomain(){
  echo "$KEYCLOAK"
}

argocdSubdomain(){
  echo "$ARGOCD"
}

jupyterHubSubdomain(){
  echo "$JUPYTERHUB"
}

fullDefaultUrl(){
  echo "${DEFAULT_SUBDOMAIN}.${GOK_ROOT_DOMAIN}"
}

fullRegistryUrl(){
  echo "${REGISTRY}.${GOK_ROOT_DOMAIN}"
}

fullKeycloakUrl(){
  echo "${KEYCLOAK}.${GOK_ROOT_DOMAIN}"
}

fullVaultUrl(){
  echo "${VAULT}.${GOK_ROOT_DOMAIN}"
}

fullSpinnakerUrl(){
  echo "${SPINNAKER}.${GOK_ROOT_DOMAIN}"
}

echoSuccess(){
  echo -e "\e[32m$1\e[0m"
}

echoFailed(){
  echo -e "\e[31m$1\e[0m"
}

echoWarning(){
  echo -e "\e[32m$1\e[0m"
}

replaceEnvVariable(){
  wget -O- $1 | envsubst
}

promptUserInput(){
 MSG=$1
 DEFAULT=$2
id=$(python3 -c "
import sys
sys.stderr.write('${MSG}')
id=input()
print(id)
")
output=${id:-$DEFAULT}
echo $output
}

promptSecret(){
  MSG=$1
  secret=$(python3 -c "
import getpass
secret = getpass.getpass('${MSG}')
print(secret)
")
  echo $secret
}

dataFromSecret(){
  NAME=$1
  NS=$2
  KEY=$3
  kubectl get secret $NAME -n $NS -o jsonpath="{['data']['$KEY']}" | base64 --decode
}

createApp1() {
  cat <<EOF | kubectl apply -f -
apiVersion: apps/v1
kind: Deployment
metadata:
  name: app1
  namespace: default
spec:
  replicas: 1
  selector:
    matchLabels:
      app: app1
  template:
    metadata:
      labels:
        app: app1
    spec:
      containers:
      - name: app1
        image: dockersamples/static-site
        env:
        - name: AUTHOR
          value: app1
        ports:
        - containerPort: 80
---
apiVersion: v1
kind: Service
metadata:
  name: appsvc1
  namespace: default
spec:
  ports:
  - port: 80
    protocol: TCP
    targetPort: 80
  selector:
    app: app1
---
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  annotations:
    nginx.ingress.kubernetes.io/rewrite-target: /
    kubernetes.io/ingress.class: "nginx"
  name: app-ingress
  namespace: default
spec:
  rules:
  - host: $(fullDefaultUrl)
    http:
      paths:
      - backend:
          service:
            name: appsvc1
            port:
              number: 80
        path: /app1
        pathType: Prefix
EOF
}

#This deploys a pod that has curl installed
kcurl(){
  cat <<EOF | kubectl apply -f -
apiVersion: v1
kind: Pod
metadata:
  name: curl
  namespace: default
  labels:
    app: curl
spec:
  containers:
  - name: main
    image: curlimages/curl
    command: ["sleep", "9999999"]
EOF
  echo "Commands"
  echo "checkCurl https://kubernetes"
}

checkCurl(){
  kubectl exec -i -t curl -n default -- curl -kv "$@"
}

checkCMWebhook(){
  kubectl exec -i -t curl -n default -- curl -kv \
      --cacert <(kubectl -n cert-manager get secret cert-manager-webhook-ca -ojsonpath='{.data.ca\.crt}' | base64 -d) \
      https://cert-manager-webhook.cert-manager.svc:443/validate 2>&1 -d@- <<'EOF' | sed '/^* /d; /bytes data]$/d; s/> //; s/< //'
{"kind":"AdmissionReview","apiVersion":"admission.k8s.io/v1","request":{"requestKind":{"group":"cert-manager.io","version":"v1","kind":"Certificate"},"requestResource":{"group":"cert-manager.io","version":"v1","resource":"certificates"},"name":"foo","namespace":"default","operation":"CREATE","object":{"apiVersion":"cert-manager.io/v1","kind":"Certificate","spec":{"dnsNames":["foo"],"issuerRef":{"group":"cert-manager.io","kind":"Issuer","name":"letsencrypt"},"secretName":"foo","usages":["digital signature"]}}}}
EOF
}

#This gives token to join a new node to kubernetes cluster
join(){
  kubeadm token create --print-join-command
}

dnsUtils(){
  cat <<EOF | kubectl apply -f -
apiVersion: v1
kind: Pod
metadata:
  name: dnsutils
  namespace: default
spec:
  containers:
  - name: dnsutils
    image: k8s.gcr.io/e2e-test-images/jessie-dnsutils:1.3
    command:
      - sleep
      - "3600"
    imagePullPolicy: IfNotPresent
  restartPolicy: Always
EOF
  echo "Commands"
  echo "checkDns kubernetes.default.svc.cloud.uat"

}

istioInst(){
  helm repo add istio https://istio-release.storage.googleapis.com/charts
  helm repo update
  helm install istio-base istio/base -n istio-system \
    --create-namespace \
    --set defaultRevision=default

  helm install istiod istio/istiod -n istio-system --wait
  helm ls -n istio-system

}

enableIstio(){
  NAMESPACE=$1
  kubectl label namespace $NAMESPACE istio-injection=enabled
}

istioReset(){
  helm -n istio-system delete istiod
  helm -n istio-system delete istio-base
  kubectl delete ns istio-system
}

checkDns(){
  kubectl exec -i -t dnsutils -n default -- nslookup "$@"
}

getpod() {
  pod=$(kubectl get po -l app.kubernetes.io/name="$release" 2>/dev/null | awk "/${release}/" | awk '{print $1}' | head -n 1)
  echo "$pod"
}

updateSys() {
  apt-get update
}

setupDockerRegistry(){
  if [[ -n $TRACE ]]; then
    set -x
  fi

  EXPORTDIR=$MOUNT_PATH

  if [ "$(hostname)" == 'master.cloud.com' ]; then
      mkdir -p "$EXPORTDIR"/certs
      mkdir -p /mnt/registry
      rm -rf /mnt/registry/config.yml
      wget https://github.com/sumitmaji/kubernetes/raw/master/install_k8s/registry/config.yml -P /mnt/registry/
      pushd "$EXPORTDIR" || exit
      USERNAME=master.cloud.com
      FILENAME=registry
      CERTIFICATE_KEY_NAME=$USERNAME
      rm /etc/docker/certs.d/master.cloud.com:5000/${CERTIFICATE_KEY_NAME}.crt
      rm /root/certs/${CERTIFICATE_KEY_NAME}.crt
      rm /root/certs/${CERTIFICATE_KEY_NAME}.key
      if [ -f /etc/docker/certs.d/master.cloud.com:5000/${CERTIFICATE_KEY_NAME}.crt ]
      then
        echo "The file is present, not creating it!!!!!"
      else
        createCertificate -i "${MASTER_HOST_IP}" -h master.cloud.com -t server -f registry
      fi

      docker stop registry
      docker rm registry

      docker run -d \
        --restart=always \
        --name registry \
        -v $EXPORTDIR/certs:/root/certs \
        -e REGISTRY_HTTP_ADDR=0.0.0.0:5000 \
        -e REGISTRY_HTTP_TLS_CERTIFICATE=/root/certs/${CERTIFICATE_KEY_NAME}.crt \
        -e REGISTRY_HTTP_TLS_KEY=/root/certs/${CERTIFICATE_KEY_NAME}.key \
        -v /mnt/registry:/var/lib/registry \
        -v /mnt/registry/config.yml:/etc/docker/registry/config.yml \
        -p 5000:5000 \
        registry:latest

      mkdir -p /etc/docker/certs.d/master.cloud.com:5000

      cp "$EXPORTDIR"/certs/${CERTIFICATE_KEY_NAME}.crt /etc/docker/certs.d/master.cloud.com:5000/${CERTIFICATE_KEY_NAME}.crt

      popd || exit
  else
      mkdir -p /etc/docker/certs.d/master.cloud.com:5000
      cp "$EXPORTDIR"/certs/${USERNAME}.crt /etc/docker/certs.d/master.cloud.com:5000/${USERNAME}.crt
  fi
}

installDeps() {
  #Install network tools
  apt-get install net-tools
  apt-get install jq -y

  #Installing python
  apt-get install python3 -y
  apt-get install python3-pip -y

}

ingressUnInst() {
  output=$(kubectl get po -n ingress-nginx -l app.kubernetes.io/component=controller -o json | jq '.items | length')
  if [ "$output" == "1" ]; then
    helm uninstall ingress-nginx -n ingress-nginx
    kubectl delete ns ingress-nginx
  fi
}

patchNginxConfig() {
  echo "Patching nginx-configuration ConfigMap to enable debug logging..."

  # Patch the ConfigMap
  kubectl patch configmap ingress-nginx-controller -n ingress-nginx --type merge --patch "$(
    cat <<EOF
data:
  enable-vts-status: "true"
  log-format-upstream: '{"time": "\$time_iso8601", "remote_addr": "\$remote_addr", "x-forwarded-for": "\$http_x_forwarded_for", "request_id": "\$request_id", "remote_user": "\$remote_user", "bytes_sent": "\$bytes_sent", "request_time": "\$request_time", "status": "\$status", "vhost": "\$host", "request_proto": "\$server_protocol", "path": "\$uri", "request_query": "\$args", "request_length": "\$request_length", "duration": "\$request_time", "method": "\$request_method", "http_referrer": "\$http_referer", "http_user_agent": "\$http_user_agent", "upstream_addr": "\$upstream_addr", "upstream_status": "\$upstream_status", "upstream_response_length": "\$upstream_response_length", "upstream_response_time": "\$upstream_response_time", "upstream_cache_status": "\$upstream_cache_status", "authorization": "\$http_authorization", "request_body": "\$request_body"}'
EOF
  )"

  # Restart the ingress-nginx-controller deployment
  echo "Restarting ingress-nginx-controller deployment..."
  kubectl rollout restart deployment ingress-nginx-controller -n ingress-nginx

  echo "Nginx configuration patched and ingress-nginx-controller restarted successfully!"
}

ingressInst() {
  helm repo add ingress-nginx https://kubernetes.github.io/ingress-nginx
  helm repo update
  helm install \
    ingress-nginx ingress-nginx/ingress-nginx --version 4.12.1 \
    --namespace ingress-nginx \
    --create-namespace \
    --set controller.service.nodePorts.http=80 \
    --set controller.service.nodePorts.https=443 \
    --set controller.service.type=NodePort \
    --set defaultBackend.enabled=true
  #    -f charts/values.yaml

  waitForServiceAvailable ingress-nginx
}

resetChart(){
  echo "Resetting ChartMuseum installation..."
  helm uninstall chartmuseum --namespace chartmuseum
  emptyLocalFsStorage "ChartMuseum" "chart-pv" "chart-storage" "/data/volumes/chart-storage"
  kubectl delete namespace chartmuseum
  echo "ChartMuseum has been reset successfully."
}

baseInst(){
  echo "Installing base image"
  pushd "$MOUNT_PATH"/kubernetes/install_k8s/base || exit
  find . -type f -name "*.sh" -exec chmod +x {} \;
  ./build.sh
  popd || exit
  echoSuccess "Base image installed successfully!"
}


chartInst() {
  # Step 1: Create a namespace for ChartMuseum
  kubectl create namespace chartmuseum || echo "Namespace chartmuseum already exists"

  # Step 2: Install ChartMuseum using Helm
  helm repo add chartmuseum https://chartmuseum.github.io/charts
  helm repo update
  
  createLocalStorageClassAndPV "chart-storage" "chart-pv" "/data/volumes/chart-storage"

  # Install ChartMuseum with persistent storage
  helm install chartmuseum chartmuseum/chartmuseum \
    --namespace chartmuseum \
    --set persistence.enabled=true \
    --set persistence.size=10Gi \
    --set persistence.storageClass="chart-storage" \
    --set env.open.STORAGE="local" \
    --set env.open.STORAGE_LOCAL_ROOTDIR="/charts" \
    --values "$MOUNT_PATH"/kubernetes/install_k8s/chart-registry/values.yaml

  gok patch ingress chartmuseum chartmuseum letsencrypt chart

  # Step 4: Verify ChartMuseum Installation
  echo "Waiting for ChartMuseum to be ready..."
  kubectl --namespace chartmuseum wait --for=condition=Ready pods --all --timeout=120s
  if [[ $? -eq 0 ]]; then
    echo "ChartMuseum is now up and running!"
    echo "Access it at: https://chartmuseum.gokcloud.com"
  else
    echo "ChartMuseum setup timed out. Please check the logs."
    return 1
  fi

  helm plugin install https://github.com/chartmuseum/helm-push

  # Step 5: Add the ChartMuseum Repository to Helm
  helm repo add gok https://chart.gokcloud.com --username sumit --password abcdef
  helm repo update
  #helm cm-push ldap-0.1.0.tgz gok
  echo "Helm repository added successfully!"
}

dockrInst() {

  echo "Installing docker"
  apt-get install -y \
    apt-transport-https \
    ca-certificates \
    curl \
    software-properties-common
  curl -fsSL https://download.docker.com/linux/ubuntu/gpg | apt-key add -
  sudo install -m 0755 -d /etc/apt/keyrings
  sudo curl -fsSL https://download.docker.com/linux/ubuntu/gpg -o /etc/apt/keyrings/docker.asc
  sudo chmod a+r /etc/apt/keyrings/docker.asc

  # Add the repository to Apt sources:
  echo \
    "deb [arch=$(dpkg --print-architecture) signed-by=/etc/apt/keyrings/docker.asc] https://download.docker.com/linux/ubuntu \
    $(. /etc/os-release && echo "$VERSION_CODENAME") stable" | \
    sudo tee /etc/apt/sources.list.d/docker.list > /dev/null
  sudo apt-get update

  apt-get update && apt-get install -y \
    docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin

  # Create required directories
  sudo mkdir -p /etc/systemd/system/docker.service.d

  # Create daemon json config file
  sudo tee /etc/docker/daemon.json <<EOF
{
  "exec-opts": ["native.cgroupdriver=systemd"],
  "log-driver": "json-file",
  "log-opts": {
    "max-size": "100m"
  },
  "storage-driver": "overlay2"
}
EOF

  # Start and enable Services
  sudo systemctl daemon-reload
  sudo systemctl restart docker
  sudo systemctl enable docker

  # Start and enable containerd services
  sudo systemctl enable containerd
  sudo systemctl start containerd
  sudo rm /etc/containerd/config.toml
  sudo systemctl restart containerd
}

customDns() {
  echo "Going to add custom dns server"
  #Adding custom dns server
  cat <<EOF | kubectl apply -f -
apiVersion: v1
data:
  Corefile: |
    .:53 {
        errors
        health {
           lameduck 5s
        }
        ready
        kubernetes cloud.uat in-addr.arpa ip6.arpa {
           pods insecure
           fallthrough in-addr.arpa ip6.arpa
           ttl 30
        }
        prometheus :9153
        forward . /etc/resolv.conf {
           max_concurrent 1000
        }
        cache 30
        loop
        reload
        loadbalance
    }
    cloud.com:53 {
        errors
        cache 30
        forward . ${MASTER_HOST_IP}
    }
    gokcloud.com:53 {
        errors
        cache 30
        forward . ${MASTER_HOST_IP}
    }
kind: ConfigMap
metadata:
  name: coredns
  namespace: kube-system
EOF

  kubectl delete pod --namespace kube-system -l k8s-app=kube-dns
}

taintNode() {
  echo "Going to taint node for scheduling in master"

  # Define a function to get IP address
  getIP() {
    ifconfig eth1 2>/dev/null | awk '/inet / {print $2}' | sed 's/addr://'
  }

  # Get the IP address
  IP=$(getIP)

  # If IP is empty, try another network interface
  if [ -z "$IP" ]; then
    IP=$(ifconfig enp1s0.100 2>/dev/null | awk '/inet / {print $2}' | sed 's/addr://')
  fi

  # If IP is still empty, print an error message and exit
  if [ -z "$IP" ]; then
    echo "Error: Could not determine IP address"
    return 1
  fi

  # Get the node name
  JSONPATH="{.items[?(@.status.addresses[0].address == \"${IP}\")].metadata.name}"
  NODE_NAME="$(kubectl get nodes -o jsonpath="$JSONPATH")"

  # If node name is empty, print an error message and exit
  if [ -z "$NODE_NAME" ]; then
    echo "Error: Could not determine node name"
    return 1
  fi

  # Taint the node
  kubectl taint node "${NODE_NAME}" node-role.kubernetes.io/control-plane:NoSchedule-
}

k8sInst() {
  # Enable kernel modules
  sudo modprobe overlay
  sudo modprobe br_netfilter

  # Add some settings to sysctl
  sudo tee /etc/sysctl.d/kubernetes.conf <<EOF
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
net.ipv4.ip_forward = 1
EOF

  # Reload sysctl
  sudo sysctl --system
  sudo tee /etc/modules-load.d/containerd.conf <<EOF
overlay
br_netfilter
EOF
  sysctl --system
  mkdir -p /etc/containerd
  containerd config default>/etc/containerd/config.toml

  #https://github.com/containerd/containerd/blob/main/docs/cri/config.md#cgroup-driver
  sed -i 's/SystemdCgroup = false/SystemdCgroup = true/g' /etc/containerd/config.toml
  systemctl restart containerd
  systemctl enable containerd


  echo "Installing Kubernetes"
  apt-get update && apt-get install -y apt-transport-https
  sudo apt-get update
  # apt-transport-https may be a dummy package; if so, you can skip that package
  sudo apt-get install -y apt-transport-https ca-certificates curl

  # If the folder `/etc/apt/keyrings` does not exist, it should be created before the curl command, read the note below.
  # sudo mkdir -p -m 755 /etc/apt/keyrings
  curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.32/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg
  sudo chmod 644 /etc/apt/keyrings/kubernetes-apt-keyring.gpg # allow unprivileged APT programs to read this keyring
  # This overwrites any existing configuration in /etc/apt/sources.list.d/kubernetes.list
  echo 'deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.32/deb/ /' | sudo tee /etc/apt/sources.list.d/kubernetes.list
  sudo chmod 644 /etc/apt/sources.list.d/kubernetes.list   # helps tools such as command-not-found to work correctly

  sudo apt-get update
  sudo apt-get install -y kubectl kubeadm kubelet

  if [ $1 == "kubernetes" ]; then
    kubeadm version
    kubeadm config images pull

    sudo sed -i '/ swap / s/^\(.*\)$/#\1/g' /etc/fstab
    sudo swapoff -a

    sudo systemctl enable kubelet

    envsubst <"$WORKING_DIR"/cluster-config-master.yaml >"$WORKING_DIR"/config.yaml
    kubeadm init --config="$WORKING_DIR"/config.yaml --upload-certs
    export KUBECONFIG=/etc/kubernetes/admin.conf

    mkdir -p "$HOME"/.kube
    sudo cp -i /etc/kubernetes/admin.conf "$HOME"/.kube/config

    # shellcheck disable=SC2181
    if [ $? -ne 0 ]; then
      echo "Kubectl command execution failed, please check!!!!!"
      exit 1
    fi
  elif [ $1 == "kubernetes-worker" ]; then
    cp /export/certs/issuer.crt /usr/local/share/ca-certificates/issuer.crt
    update-ca-certificates
    echo "kubectl join ...."
    echo "kubectl label node node01 node-role.kubernetes.io/worker=worker"
    echo "reboot the vm to make the changes ca certificates"
  fi

}

calicoInst(){
  kubectl create -f https://raw.githubusercontent.com/projectcalico/calico/v3.25.1/manifests/calico.yaml

  if [ $? -ne 0 ]; then
    echo "Calico installation failed, please check!!!!!"
    exit 1
  fi
}

helmInst() {
  #Installing helm
  curl https://baltocdn.com/helm/signing.asc | apt-key add - &&
    apt-get install apt-transport-https --yes &&
    echo "deb https://baltocdn.com/helm/stable/debian/ all main" | tee /etc/apt/sources.list.d/helm-stable-debian.list &&
    apt-get update &&
    apt-get install helm &&
    helm version --short
}

certmanagerInst() {
  helm repo add jetstack https://charts.jetstack.io
  helm repo update
  #kubectl apply -f https://github.com/cert-manager/cert-manager/releases/download/v1.14.5/cert-manager.crds.yaml

  #--set serviceAccount.automountServiceAccountToken=false \
  #--set webhook.timeoutSeconds=30
  #--set startupapicheck.timeout=10m
  # --debug
  helm install \
    cert-manager jetstack/cert-manager \
    --namespace cert-manager \
    --create-namespace \
    --set installCRDs=true \
    --set global.leaderElection.namespace=cert-manager \
    --version v1.14.5 \
    --values https://github.com/sumitmaji/kubernetes/raw/master/install_k8s/cert-manager/values.yaml
}

subDomain(){
  if [ -z $1 ]; then
    echo "$(defaultSubdomain)"
  else
    echo "$1"
  fi
}

certificateRequestForNs() {
  NS=$1
  SUBDOMAIN=$(subDomain $2)
  cat <<EOF | kubectl apply -f -
apiVersion: cert-manager.io/v1
kind: Certificate
metadata:
  name: ${SUBDOMAIN}-$(sedRootDomain)-tls
  namespace: ${NS}
spec:
  secretName: ${SUBDOMAIN}-$(sedRootDomain)
  issuerRef:
    name: $(getClusterIssuerName)
    kind: ClusterIssuer
  commonName: ${SUBDOMAIN}.$(rootDomain)
  dnsNames:
    - ${SUBDOMAIN}.$(rootDomain)
  issuerRef:
    name: $(getClusterIssuerName)
    kind: ClusterIssuer
EOF
  echo "Certificate request for NS $NS created, executing below command to know current status"
  echo "kubectl get Issuers,ClusterIssuers,Certificates,CertificateRequests,Orders,Challenges --all-namespaces"
  kubectl get Issuers,ClusterIssuers,Certificates,CertificateRequests,Orders,Challenges --all-namespaces
  kubectl --timeout=10s -n ${NS} wait --for=condition=Ready certificates.cert-manager.io "${SUBDOMAIN}"-"$(sedRootDomain)"-tls
}

getLetsEncEnv(){
  echo "${LETS_ENCRYPT_ENV}"
}

getLetsEncryptUrl(){
  [[ $(getLetsEncEnv) == 'prod' ]] && echo "https://acme-v02.api.letsencrypt.org/directory " || echo "https://acme-staging-v02.api.letsencrypt.org/directory"
}

isProd(){
  [[ $(getLetsEncEnv) == 'prod' ]] && echo "true" || echo "false"
}

getClusterIssuerName(){
  case "$CERTMANAGER_CHALANGE_TYPE" in
   'dns') echo "letsencrypt-$(getLetsEncEnv)" ;;
   'http') echo "letsencrypt-$(getLetsEncEnv)" ;;
   'selfsigned') echo "gokselfsign-ca-cluster-issuer" ;;
  esac
}

#Godday api calls are disabled, hence going to remove this call.
godaddyWebhook() {
  replaceEnvVariable  https://github.com/sumitmaji/kubernetes/raw/master/install_k8s/godaddy-cert-webhook/webhook-all.yml | kubectl create -f - --validate=false
  echo "Provide godaddy apikey and secret <API_KEY:SECRET>"
  API_KEY=$(promptSecret "Provide godaddy apikey and secret <API_KEY:SECRET>")

  cat <<EOF | kubectl apply -f -
apiVersion: v1
kind: Secret
metadata:
  name: godaddy-api-key-secret
  namespace: cert-manager
type: Opaque
stringData:
  api-key: ${API_KEY}
EOF

  cat <<EOF | kubectl apply -f -
apiVersion: cert-manager.io/v1
kind: ClusterIssuer
metadata:
  name: $(getClusterIssuerName)
spec:
  acme:
    email: majisumitkumar@gmail.com
    server: $(getLetsEncryptUrl)
    privateKeySecretRef:
      name: letsencrypt-$(getLetsEncEnv)
    solvers:
    - dns01:
        webhook:
          config:
            apiKeySecretRef:
              name: godaddy-api-key-secret
              key: api-key
            production: $(isProd)
            ttl: 600
          groupName: $(rootDomain)
          solverName: godaddy
      selector:
       dnsNames:
       - '$(defaultSubdomain).$(rootDomain)'
       - '*.$(rootDomain)'
EOF

  cat <<EOF | kubectl apply -f -
apiVersion: cert-manager.io/v1
kind: Certificate
metadata:
  name: $(sedRootDomain)-tls
  namespace: default
spec:
  secretName: $(sedRootDomain)
  issuerRef:
    name: letsencrypt-prod
    kind: ClusterIssuer
  commonName: $(defaultSubdomain).$(rootDomain)
  dnsNames:
    - $(defaultSubdomain).$(rootDomain)
  issuerRef:
    name: letsencrypt-prod
    kind: ClusterIssuer
EOF

}



addLetsEncryptStagingCertificates(){
  wget https://letsencrypt.org/certs/staging/letsencrypt-stg-root-x1.pem
  sudo cp letsencrypt-stg-root-x1.pem /usr/local/share/ca-certificates/
  sudo update-ca-certificates
  echo "Added letsencrypt staging certificates, please reboot the system for it to effect"
}


godaddyWebhookReset() {
  kubectl delete -f https://github.com/sumitmaji/kubernetes/raw/master/install_k8s/godaddy-cert-webhook/webhook-all.yml
  kubectl delete secret godaddy-api-key-secret -n cert-manager
}

setupCertiIssuers() {

if [ $CERTMANAGER_CHALANGE_TYPE == 'dns' ]; then
  cat <<EOF | kubectl apply -f -
apiVersion: cert-manager.io/v1
kind: ClusterIssuer
metadata:
  name: $(getClusterIssuerName)
spec:
  acme:
    email: majisumitkumar@gmail.com
    server: $(getLetsEncryptUrl)
    privateKeySecretRef:
      name: letsencrypt-$(getLetsEncEnv)
    solvers:
    - dns01:
        webhook:
          config:
            apiKeySecretRef:
              name: godaddy-api-key-secret
              key: api-key
            production: $(isProd)
            ttl: 600
          groupName: $(rootDomain)
          solverName: godaddy
      selector:
       dnsNames:
       - '$(defaultSubdomain).$(rootDomain)'
       - '*.$(rootDomain)'
EOF
  godaddyWebhook
  addLetsEncryptStagingCertificates
elif [ $CERTMANAGER_CHALANGE_TYPE == 'http' ]; then
    

  cat <<EOF | kubectl apply -f -
apiVersion: cert-manager.io/v1
kind: ClusterIssuer
metadata:
  name: $(getClusterIssuerName)
spec:
  acme:
    email: majisumitkumar@gmail.com
    server: $(getLetsEncryptUrl)
    #preferredChain: "(STAGING) Pretend Pear X1"
    privateKeySecretRef:
      name: letsencrypt-$(getLetsEncEnv)
    solvers:
    - http01:
        ingress:
          ingressClassName: nginx
EOF
  addLetsEncryptStagingCertificates
elif [ $CERTMANAGER_CHALANGE_TYPE == 'selfsigned' ]; then
  # https://medium.com/geekculture/a-simple-ca-setup-with-kubernetes-cert-manager-bc8ccbd9c2
  # https://gist.github.com/jakexks/c1de8238cbee247333f8c274dc0d6f0f
  # Create self signed cluster issuer:
  echo "Creating self-signed cluster-issuer..."
  until cat <<EOYAML | kubectl apply -f -
apiVersion: cert-manager.io/v1
kind: ClusterIssuer
metadata:
  name: selfsigned-cluster-issuer
spec:
  selfSigned: {}
EOYAML
  do sleep 1; done
  kubectl --timeout=10s wait --for=condition=Ready clusterissuers.cert-manager.io selfsigned-cluster-issuer

  # Create CA certificate. If you want to use it as a ClusterIssuer the secret must be in the cert-manager namespace:
  echo "Creating self-signed certificate..."
  cat <<EOYAML | kubectl apply -f -
apiVersion: cert-manager.io/v1
kind: Certificate
metadata:
  name: gokselfsign-ca
  namespace: cert-manager
spec:
  isCA: true
  commonName: gokselfsign-ca
  secretName: gokselfsign-ca
  subject:
    organizations:
      - GOK Inc.
    organizationalUnits:
      - Widgets
  privateKey:
    algorithm: ECDSA
    size: 256
  issuerRef:
    name: selfsigned-cluster-issuer
    kind: ClusterIssuer
    group: cert-manager.io
EOYAML
  kubectl --timeout=10s -n cert-manager wait --for=condition=Ready certificates.cert-manager.io gokselfsign-ca

  # Create clusterissuer
  echo "Creating CA cluster issuer..."
  cat <<EOYAML | kubectl apply -f -
apiVersion: cert-manager.io/v1
kind: ClusterIssuer
metadata:
  name: $(getClusterIssuerName)
spec:
  ca:
    secretName: gokselfsign-ca
EOYAML
  kubectl --timeout=10s wait --for=condition=Ready clusterissuers.cert-manager.io "$(getClusterIssuerName)"

  # Add the self signed issuer(ca) certificate to authorized certificates
  kubectl get secrets -n cert-manager gokselfsign-ca -o json | jq -r '.data["tls.crt"]' | base64 -d > /usr/local/share/ca-certificates/issuer.crt

  # Add the issuer.crt to export directory so that the worker nodes can add the same to their trusted certificates.
  mkdir -p /export/certs
  cp /usr/local/share/ca-certificates/issuer.crt /export/certs/issuer.crt
  update-ca-certificates
  #Need to restart the container
fi


  cat <<EOF | kubectl apply -f -
apiVersion: cert-manager.io/v1
kind: Certificate
metadata:
  name: $(sedRootDomain)-tls
  namespace: default
spec:
  secretName: $(sedRootDomain)
  issuerRef:
    name: $(getClusterIssuerName)
    kind: ClusterIssuer
  commonName: $(defaultSubdomain).$(rootDomain)
  dnsNames:
    - $(defaultSubdomain).$(rootDomain)
  issuerRef:
    name: $(getClusterIssuerName)
    kind: ClusterIssuer
EOF

echoWarning "Selfsigned certificate is added to root ca, please reboot the system for it to effect"
}

certManagerReset() {
  kubectl delete Issuers,ClusterIssuers,Certificates,CertificateRequests,Orders,Challenges --all --all-namespaces
  if [[ $CERTMANAGER_CHALANGE_TYPE == 'dns' ]]; then godaddyWebhookReset; fi
  helm --namespace cert-manager delete cert-manager
  kubectl delete namespace cert-manager
}

haInst() {
  docker stop master-proxy
  docker rm master-proxy
  cat <<EOF >/opt/haproxy.cfg
global
        log 127.0.0.1 local0
        log 127.0.0.1 local1 notice
        maxconn 4096
        maxpipes 1024
        daemon
defaults
        log global
        mode tcp
        option tcplog
        option dontlognull
        option redispatch
        option http-server-close
        retries 3
        timeout connect 5000
        timeout client 50000
        timeout server 50000
        frontend default_frontend
        bind *:$HA_PROXY_PORT
        default_backend master-cluster
backend master-cluster
$(#Install master nodes
    IFS=','
    counter=0
    cluster=""
    for worker in $API_SERVERS; do
      oifs=$IFS
      IFS=':'
      read -r ip node <<<"$worker"
      if [ -z "$cluster" ]; then
        cluster="$ip:6443"
      else
        cluster="$cluster,http://$ip:4001"
      fi
      counter=$((counter + 1))
      IFS=$oifs
      echo "        server master-$counter ${cluster} check"
      cluster=""
    done
    unset IFS
  )
EOF

  docker run -d --name master-proxy \
    -v /opt/haproxy.cfg:/usr/local/etc/haproxy/haproxy.cfg:ro \
    --net=host haproxy
}

startKubelet() {
  systemctl stop kubelet
  systemctl start kubelet
}

startHa() {
  docker stop master-proxy
  docker rm master-proxy
  docker run -d --name master-proxy \
    -v /opt/haproxy.cfg:/usr/local/etc/haproxy/haproxy.cfg:ro \
    --net=host haproxy
}

disableSwap() {
  sudo sed -i '/ swap / s/^\(.*\)$/#\1/g' /etc/fstab
  sudo swapoff -a
}

hostSecret() {
  openssl genrsa -out ${APP_HOST}.key 4096
  openssl req -new -key ${APP_HOST}.key -out ${APP_HOST}.csr -subj "/CN=${APP_HOST}" \
    -addext "subjectAltName = DNS:${APP_HOST}"
  openssl x509 -req -in ${APP_HOST}.csr -CA /etc/kubernetes/pki/ca.crt -CAkey /etc/kubernetes/pki/ca.key -CAcreateserial -out ${APP_HOST}.crt -days 7200

  kubectl create secret tls appingress-certificate --key ${APP_HOST}.key --cert ${APP_HOST}.crt -n default
}

dashboardInst() {
  helm repo add kubernetes-dashboard https://kubernetes.github.io/dashboard/
  kubectl delete ns kubernetes-dashboard
  helm install kubernetes-dashboard \
    kubernetes-dashboard/kubernetes-dashboard \
    --namespace kubernetes-dashboard \
    --create-namespace \
    -f "${MOUNT_PATH}"/kubernetes/install_k8s/dashboard/values2.yaml
}

prometheusGrafanaReset() {
  helm -n monitoring delete monitoring
  kubectl delete ns monitoring
  helm -n db delete postgres
  kubectl delete ns db
}

emptyLocalFsStorage() {
  local service=$1
  local pvName=$2
  local scName=$3
  local volumePath=$4
  local namespace=$5

  if [[ -n $namespace ]]; then
    kubectl delete pvc --all -n $namespace
  fi

  kubectl delete pv $pvName
  kubectl delete sc $scName
  rm -rf $volumePath
}

createLocalStorageClassAndPV() {
  local storageClassName=$1
  local pvName=$2
  local volumePath=$3

  cat << EOF | kubectl apply -f -
kind: StorageClass
apiVersion: storage.k8s.io/v1
metadata:
  name: ${storageClassName}
provisioner: kubernetes.io/no-provisioner
volumeBindingMode: WaitForFirstConsumer
EOF

  mkdir -p "${volumePath}"
  chmod 777 "${volumePath}"

  cat << EOF | kubectl apply -f -
apiVersion: v1
kind: PersistentVolume
metadata:
  name: ${pvName}
spec:
  capacity:
    storage: 10Gi
  accessModes:
  - ReadWriteOnce
  persistentVolumeReclaimPolicy: Retain
  storageClassName: ${storageClassName}
  local:
    path: ${volumePath}
  nodeAffinity:
    required:
      nodeSelectorTerms:
      - matchExpressions:
        - key: kubernetes.io/hostname
          operator: In
          values:
          - master.cloud.com
EOF
}

prometheusGrafanaResetv2(){
  helm -n monitoring delete prometheus
  helm -n monitoring delete grafana
  kubectl delete ns monitoring

}

prometheusGrafanaInstv2(){
  CLIENT_SECRET=$(dataFromSecret oauth-secrets kube-system OIDC_CLIENT_SECRET)
  CLIENT_ID=$(dataFromSecret oauth-secrets kube-system OIDC_CLIENT_ID)
  URL=$(fullDefaultUrl)
  OAUTH2_HOST=$(fullKeycloakUrl)
  REALM=$(dataFromSecret oauth-secrets kube-system OIDC_REALM)

  kubectl create ns monitoring
  kubectl create secret generic kube-prometheus-stack-grafana-oauth \
    --from-literal GF_AUTH_KEYCLOAK_CLIENT_ID="${CLIENT_ID}" \
    --from-literal GF_AUTH_KEYCLOAK_CLIENT_SECRET="${CLIENT_SECRET}" \
    --from-literal=OAUTH_AUTH_URL="https://${OAUTH2_HOST}/realms/${REALM}/protocol/openid-connect/auth" \
    --from-literal=OAUTH_TOKEN_URL="https://${OAUTH2_HOST}/realms/${REALM}/protocol/openid-connect/token" \
    --from-literal=OAUTH_API_URL="https://${OAUTH2_HOST}/realms/${REALM}/protocol/openid-connect/userinfo" \
    --from-literal=GRAFANA_DOMAIN="${URL}" \
    --from-literal=GRAFANA_ROOT_URL="https://${URL}/grafana" \
    --namespace monitoring

  kubectl create configmap -n monitoring env-data \
    --from-literal=OAUTH_AUTH_URL="https://${OAUTH2_HOST}/realms/${REALM}/protocol/openid-connect/auth" \
    --from-literal=OAUTH_TOKEN_URL="https://${OAUTH2_HOST}/realms/${REALM}/protocol/openid-connect/token" \
    --from-literal=OAUTH_API_URL="https://${OAUTH2_HOST}/realms/${REALM}/protocol/openid-connect/userinfo" \
    --from-literal=GRAFANA_DOMAIN="${URL}" \
    --from-literal=GRAFANA_ROOT_URL="https://${URL}/grafana"

  kubectl get secrets -n cert-manager gokselfsign-ca -o json | jq -r '.data["tls.crt"]' | base64 -d > ~/issuer.crt
  kubectl get secrets -n keycloak keycloak-gokcloud-com -o json | jq -r '.data["tls.crt"]' | base64 -d > ~/keycloak.crt
  kubectl create configmap certs-configmap -n monitoring --from-file=/root/issuer.crt --from-file=/root/keycloak.crt
  rm /root/issuer.crt
  rm /root/keycloak.crt
  helm repo add prometheus-community https://prometheus-community.github.io/helm-charts
  helm repo add grafana https://grafana.github.io/helm-charts
  helm repo update
  helm install prometheus prometheus-community/prometheus \
    --set server.extraFlags=\{web.enable-lifecycle,web.route-prefix=/,web.external-url=prometheus\} \
    --values $MOUNT_PATH/kubernetes/install_k8s/prometheus-grafana/prometheus-values.yaml \
    --namespace monitoring \
    --create-namespace

  helm install grafana grafana/grafana \
    --set grafana.ini.server.root_url=https://$URL/grafana \
    --values $MOUNT_PATH/kubernetes/install_k8s/prometheus-grafana/grafana-values.yaml \
    --namespace monitoring

  kubectl -n kube-system get cm kube-proxy -o yaml | sed 's/metricsBindAddress: ""/metricsBindAddress: 0.0.0.0:10249/' | kubectl apply -f -

  kubectl -n kube-system patch ds kube-proxy -p \
      '{"spec":{"template":{"metadata":{"labels":{"updateTime":"'`date +'%s'`'"}}}}}'

}

prometheusGrafanaInst() {

  helm repo add prometheus-community \
    https://prometheus-community.github.io/helm-charts
  helm repo update
  helm install monitoring \
    prometheus-community/kube-prometheus-stack \
    --values https://github.com/sumitmaji/kubernetes/raw/master/install_k8s/prometheus-grafana/values.yaml \
    --version 39.6.0 \
    --namespace monitoring \
    --create-namespace

#  kubectl -n kube-system get cm kube-proxy-config -o yaml | sed \
#    's/metricsBindAddress: 127.0.0.1:10249/metricsBindAddress: 0.0.0.0:10249' \
#    kubectl apply -f -

  kubectl -n kube-system get cm kube-proxy -o yaml | sed 's/metricsBindAddress: ""/metricsBindAddress: 0.0.0.0:10249/' | kubectl apply -f -

  kubectl -n kube-system patch ds kube-proxy -p \
    '{"spec":{"template":{"metadata":{"labels":{"updateTime":"'`date +'%s'`'"}}}}}'

  helm repo add bitnami https://charts.bitnami.com/bitnami
  helm repo update
  helm install postgres \
    bitnami/postgresql \
    --values https://github.com/sumitmaji/kubernetes/raw/master/install_k8s/prometheus-grafana/postgres-values.yaml \
    --version 11.7.1 \
    --namespace db \
    --create-namespace

}

dashboardReset() {
  helm uninstall kubernetes-dashboard -n kubernetes-dashboard
  kubectl delete ns kubernetes-dashboard
}

#This creates a cluster-role-binding for admin user
adminRole() {
  WC=$(kubectl get clusterrolebinding cloud-cluster-admin 2>/dev/null | wc -l)
  if [ "$WC" == "0" ]; then

    cat <<EOF | kubectl apply -f -
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: cloud-cluster-admin
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: cluster-admin
subjects:
- apiGroup: rbac.authorization.k8s.io
  kind: Group
  name: cloud:masters
EOF
  fi

}

#Creating user role developers which would allow users authenticated with oauth and
#having developers role to connect with cluster
oauthDev(){
  cat <<EOF | kubectl create -f -
kind: Role
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  namespace: default
  name: pod-reader
rules:
- apiGroups: [""]
  resources: ["pods"]
  verbs: ["get", "watch", "list", "create", "update", "patch", "delete"]
- apiGroups: ["extensions", "apps"]
  resources: ["deployments"]
  verbs: ["get", "list", "watch", "create", "update", "patch", "delete"]
---
kind: RoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: read-pods
  namespace: default
subjects:
- kind: Group
  name: developers
  apiGroup: rbac.authorization.k8s.io
roleRef:
  kind: Role
  name: pod-reader
  apiGroup: rbac.authorization.k8s.io
EOF
}

# Run kubectl command on pod
runKubectlOnPod(){

  # The container will run will priveledges that are provided in the service account
  cat <<EOF | kubectl apply -f -
apiVersion: v1
kind: ServiceAccount
metadata:
  name: internal-deployer
  namespace: default
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: internal-deployer
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: cluster-admin
subjects:
  - kind: ServiceAccount
    name: internal-deployer
    namespace: default
EOF
  # Execute the container as job as it will execute the kubectl command and exit
  cat <<EOF | kubectl apply -f -
apiVersion: batch/v1
kind: Job
metadata:
  name: kubectl-executor
  namespace: default
spec:
  template:
    metadata:
      name: kubectl-executor
    spec:
      # The service account that will be used to run the container
      serviceAccountName: internal-deployer
      containers:
      - name: kubectl
        image: bitnami/kubectl:latest
        args: ["cluster-info"]
      restartPolicy: Never
EOF
}

#Creating user role administrators which would allow users authenticated with oauth and
#having administrators role to connect with cluster
oauthAdmin(){
  cat <<EOF | kubectl create -f -
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: oauth-cluster-admin
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: cluster-admin
subjects:
- apiGroup: rbac.authorization.k8s.io
  kind: Group
  name: administrators
EOF
}

#Create kubeconfig file for oauth user used with kube-login
oauthUser(){
  kubectl config set-cluster cloud.com --certificate-authority=/etc/kubernetes/pki/ca.crt --embed-certs=true --server="$(apiUrl)" --kubeconfig=/root/oauth.conf
  kubectl config --kubeconfig=/root/oauth.conf set-context oauthuser@cloud.com --cluster=cloud.com --user=oauthuser
  kubectl config --kubeconfig=/root/oauth.conf use-context oauthuser@cloud.com

  echoSuccess "OAuth kubeconfig file create in /root/oauth.conf"
  echoSuccess "Use below command to use oauth.conf"
  echoSuccess "kubectl --kubeconfig=/root/oauth.conf --token=__USER_TOKEN__ rest of command"
  echoSuccess "alias kctl='kubectl --kubeconfig=/root/oauth.conf --token=\${__USER_TOKEN__}'"
  echoSuccess "alias kcd='kctl config set-context \$(kctl config current-context) --namespace'"
}

#This gives api server url
apiUrl(){
  kubectl config view -o json | jq -r '.clusters[] | .cluster.server'
}

opensearchReset(){
  helm uninstall opensearch -n opensearch
  emptyLocalFsStorage "Opensearch" "opensearch-pv" "opensearch-storage" "/data/volumes/pv5" "opensearch"
  kubectl delete ns opensearch
}

jenkinsReset(){
  helm uninstall jenkins -n jenkins
  emptyLocalFsStorage "Jenkins" "jenkins-pv" "jenkins-storage" "/data/volumes/jenkins"
  kubectl delete ns jenkins
}

opensearchDashReset(){
  helm uninstall opensearch-dashboard -n opensearch
}

opensearchDashInst(){
  helm repo add openSearch https://opensearch-project.github.io/helm-charts/
  helm repo update
  helm install opensearch-dashboard \
      --namespace opensearch \
      --values https://github.com/sumitmaji/kubernetes/raw/master/install_k8s/opensearch/values-dashboard.yaml \
      openSearch/opensearch-dashboards

  gok patch ingress opensearch-dashboard-opensearch-dashboards opensearch letsencrypt opensearch
  echo "Waiting for dashboard service to be up!!!!"
    kubectl --timeout=240s wait --for=condition=Ready pods --all --namespace opensearch
    [[ $? -eq 0 ]] && echoSuccess "Opensearch dashboard service now up!\n You can access the service using https://opensearch.gokcloud.com" || echoFailed "Opensearch dashboard service timed-out, plaese check!!"
}


callJenkinsApi(){

  # Jenkins API URL
  JENKINS_URL="https://jenkins.gokcloud.com/api/xml"
  QUERY_PARAMS="tree=jobs%5Bname%2ClastBuild%5Bactions%5BfailCount%2CskipCount%2CtotalCount%2CurlName%5D%2Cduration%2Cnumber%2Ctimestamp%2Cresult%2Cbuilding%2Curl%5D%2Cjobs%5Bname%2ClastBuild%5Bactions%5BfailCount%2CskipCount%2CtotalCount%2CurlName%5D%2Cduration%2Cnumber%2Ctimestamp%2Cresult%2Cbuilding%2Curl%5D%2Cjobs%5Bname%2ClastBuild%5Bactions%5BfailCount%2CskipCount%2CtotalCount%2CurlName%5D%2Cduration%2Cnumber%2Ctimestamp%2Cresult%2Cbuilding%2Curl%5D%2Cjobs%5Bname%2ClastBuild%5Bactions%5BfailCount%2CskipCount%2CtotalCount%2CurlName%5D%2Cduration%2Cnumber%2Ctimestamp%2Cresult%2Cbuilding%2Curl%5D%2Cjobs%5Bname%2ClastBuild%5Bactions%5BfailCount%2CskipCount%2CtotalCount%2CurlName%5D%2Cduration%2Cnumber%2Ctimestamp%2Cresult%2Cbuilding%2Curl%5D%2Cjobs%5Bname%2ClastBuild%5Bactions%5BfailCount%2CskipCount%2CtotalCount%2CurlName%5D%2Cduration%2Cnumber%2Ctimestamp%2Cresult%2Cbuilding%2Curl%5D%2Cjobs%5Bname%2ClastBuild%5Bactions%5BfailCount%2CskipCount%2CtotalCount%2CurlName%5D%2Cduration%2Cnumber%2Ctimestamp%2Cresult%2Cbuilding%2Curl%5D%2Cjobs%5Bname%2ClastBuild%5Bactions%5BfailCount%2CskipCount%2CtotalCount%2CurlName%5D%2Cduration%2Cnumber%2Ctimestamp%2Cresult%2Cbuilding%2Curl%5D%2Cjobs%5Bname%2ClastBuild%5Bactions%5BfailCount%2CskipCount%2CtotalCount%2CurlName%5D%2Cduration%2Cnumber%2Ctimestamp%2Cresult%2Cbuilding%2Curl%5D%5D%5D%5D%5D%5D%5D%5D%5D%5D"
  EXCLUDE_PARAMS="exclude=%2F*%2F*%2F*%2Faction%5Bnot%28totalCount%29%5D"

  # Jenkins credentials
  JENKINS_USER=$(promptUserInput "Enter Jenkins Username (skmaji1): " "skmaji1")
  JENKINS_API_TOKEN=$(promptSecret "Enter Jenkins API Token(Check README.md for steps): ")

  # Make the API call
  response=$(curl -s -u "$JENKINS_USER:$JENKINS_API_TOKEN" "$JENKINS_URL?$QUERY_PARAMS&$EXCLUDE_PARAMS")

  # Check if the response is empty
  if [ -z "$response" ]; then
    echo "Failed to fetch data from Jenkins API. Please check your credentials or URL."
    return 1
  fi

  # Print the response
  echo "Jenkins API Response:"
  echo "$response"
}

createExampleJenkinsPipeline() {
  # Prompt for Jenkins username and API token
  JENKINS_USER=$(promptUserInput "Enter Jenkins Username (skmaji1): " "skmaji1")
  JENKINS_API_TOKEN=$(promptSecret "Enter Jenkins API Token(Check README.md for steps): ")

  # Prompt for Jenkins URL and pipeline details
  JENKINS_URL=$(promptUserInput "Enter Jenkins URL (https://jenkins.gokcloud.com): " "https://jenkins.gokcloud.com")
  PIPELINE_NAME=$(promptUserInput "Enter Pipeline Name(Kaniko-Pipeline): " "Kaniko-Pipeline")

  pushd "$MOUNT_PATH/kubernetes/install_k8s/jenkins"
  # Make the API call to create the pipeline
  RESPONSE=$(curl -s -o /dev/null -w "%{http_code}" -X POST "$JENKINS_URL/createItem?name=$PIPELINE_NAME" \
    --user "$JENKINS_USER:$JENKINS_API_TOKEN" \
    -H "Content-Type: application/xml" \
    --data-binary @pipeline-config.xml)

  popd
  # Check the response
  if [ "$RESPONSE" -eq 200 ]; then
    echoSuccess "Pipeline '$PIPELINE_NAME' created successfully!"
  else
    echoFailed "Failed to create pipeline. HTTP Status Code: $RESPONSE"
  fi
}


createJenkinsPipeline() {
  # Prompt for Jenkins username and API token
  JENKINS_USER=$(promptUserInput "Enter Jenkins Username(skmaji1): " "skmaji1")
  JENKINS_API_TOKEN=$(promptSecret "Enter Jenkins API Token: ")

  # Prompt for Jenkins URL and pipeline details
  JENKINS_URL=$(promptUserInput "Enter Jenkins URL (e.g., https://jenkins.gokcloud.com): " "https://jenkins.gokcloud.com")
  PIPELINE_NAME=$(promptUserInput "Enter Pipeline Name: " "Git-Pipeline")
  GIT_REPO_URL=$(promptUserInput "Enter Git Repository URL (e.g., https://github.com/your-username/your-repo.git): ")
  GIT_BRANCH=$(promptUserInput "Enter Git Branch (e.g., main): " "main")
  CREDENTIALS_ID=$(promptUserInput "Enter Jenkins Credentials ID: ")

  # Create the pipeline configuration XML
  cat <<EOF > pipeline-config.xml
<flow-definition plugin="workflow-job">
  <description>Pipeline created automatically to refer to Jenkinsfile in Git</description>
  <keepDependencies>false</keepDependencies>
  <definition class="org.jenkinsci.plugins.workflow.cps.CpsScmFlowDefinition" plugin="workflow-cps">
    <scm class="hudson.plugins.git.GitSCM" plugin="git">
      <configVersion>2</configVersion>
      <userRemoteConfigs>
        <hudson.plugins.git.UserRemoteConfig>
          <url>${GIT_REPO_URL}</url>
          <credentialsId>${CREDENTIALS_ID}</credentialsId>
        </hudson.plugins.git.UserRemoteConfig>
      </userRemoteConfigs>
      <branches>
        <hudson.plugins.git.BranchSpec>
          <name>*/${GIT_BRANCH}</name>
        </hudson.plugins.git.BranchSpec>
      </branches>
    </scm>
    <scriptPath>Jenkinsfile</scriptPath>
    <lightweight>true</lightweight>
  </definition>
  <triggers/>
</flow-definition>
EOF

  # Make the API call to create the pipeline
  RESPONSE=$(curl -s -o /dev/null -w "%{http_code}" -X POST "$JENKINS_URL/createItem?name=$PIPELINE_NAME" \
    --user "$JENKINS_USER:$JENKINS_API_TOKEN" \
    -H "Content-Type: application/xml" \
    --data-binary @pipeline-config.xml)

  # Check the response
  if [ "$RESPONSE" -eq 200 ]; then
    echoSuccess "Pipeline '$PIPELINE_NAME' created successfully!"
  else
    echoFailed "Failed to create pipeline. HTTP Status Code: $RESPONSE"
  fi

  # Clean up the temporary XML file
  rm -f pipeline-config.xml
}

jenkinsInst() {
  helm repo add jenkins https://charts.jenkins.io
  helm repo update
  kubectl create ns jenkins

  kubectl create configmap jenkins-logging-config \
  --from-file=${MOUNT_PATH}/kubernetes/install_k8s/jenkins/logging.properties \
  -n jenkins

  ADMIN_PASSWORD=$(promptSecret "Enter Jenkins Admin Password: ")
  kubectl create secret generic jenkins-admin-password \
    --from-literal=jenkins-admin-password="${ADMIN_PASSWORD}" -n jenkins

  CLIENT_SECRET=$(dataFromSecret oauth-secrets kube-system OIDC_CLIENT_SECRET)
  CLIENT_ID=$(dataFromSecret oauth-secrets kube-system OIDC_CLIENT_ID)
  keycloakUrl="https://$(fullKeycloakUrl)"
  REALM=$(dataFromSecret oauth-secrets kube-system OAUTH_REALM)


  kubectl create secret generic oic-auth \
  --from-literal=clientID="${CLIENT_ID}" \
  --from-literal=clientSecret="${CLIENT_SECRET}" \
  --from-literal=keycloakUrl=${keycloakUrl} \
  --from-literal=realm=${REALM} \
  --namespace jenkins
  DOCKER_BUILD_ENABLED=true
  if [ "$DOCKER_BUILD_ENABLED" == "true" ]; then
    # kubectl create secret generic kaniko-docker-config \
    #   --from-file=/root/.docker/config.json \
    #   -n jenkins

    kubectl create secret generic docker-credentials \
    --from-file=.dockerconfigjson=/root/.docker/config.json \
    --type=kubernetes.io/dockerconfigjson \
    -n jenkins

    kubectl create secret generic registry-credentials \
    --from-file=config.json=/root/.docker/config.json \
    -n jenkins
  fi

  helm install jenkins \
    --namespace jenkins \
    --set dockerBuildEnabled=$DOCKER_BUILD_ENABLED \
    --values $MOUNT_PATH/kubernetes/install_k8s/jenkins/values-mod.yaml \
    jenkins/jenkins

  createLocalStorageClassAndPV "jenkins-storage" "jenkins-pv" "/data/volumes/jenkins"
  gok patch ingress jenkins jenkins letsencrypt jenkins
  echo "Waiting for Jenkins services to be up!!!!"
  kubectl --timeout=120s wait --for=condition=Ready pods --all --namespace jenkins
  [[ $? -eq 0 ]] && echoSuccess "Jenkins services are now up!" || echoFailed "Jenkins services timed-out, please check!!"
  checkCurl curl -XGET http://jenkins.jenkins.svc:8080 -u "admin:${ADMIN_PASSWORD}" --insecure
}

opensearchInst(){
  helm repo add openSearch https://opensearch-project.github.io/helm-charts/
  helm repo update
  kubectl create ns opensearch
  ADMIN_PASSWORD=$(promptSecret "Enter Admin Password: ")
  kubectl create secret generic opensearch-password \
        --from-literal=OPENSEARCH_INITIAL_ADMIN_PASSWORD="${ADMIN_PASSWORD}" -n opensearch
  helm install opensearch \
    --namespace opensearch \
    --create-namespace \
    --values $MOUNT_PATH/kubernetes/install_k8s/opensearch/values.yaml \
    openSearch/opensearch

  createLocalStorageClassAndPV "opensearch-storage" "opensearch-pv" "/data/volumes/pv5"
  echo "Waiting for services to be up!!!!"
  kubectl --timeout=120s wait --for=condition=Ready pods --all --namespace opensearch
  [[ $? -eq 0 ]] && echoSuccess "Opensearch services are now up!" || echoFailed "Opensearch services timed-out, plaese check!!"
  checkCurl curl -XGET https://opensearch-cluster-master.opensearch.svc:9200 -u "admin:${ADMIN_PASSWORD}" --insecure
  #gok patch ingress opensearch opensearch letsencrypt opensearch
  opensearchSecret $ADMIN_PASSWORD
}

fluentdReset(){
  helm uninstall fluentd -n fluentd
  kubectl delete ns fluentd
}

fluentdInst(){
  helm repo add fluent https://fluent.github.io/helm-charts
  helm repo update
  kubectl create ns fluentd

  # Workaround for now before permanent solution is identified, Begin
  ADMIN_PASSWORD=$(kubectl get secret opensearch-secrets -n kube-system -o=jsonpath='{.data.OPENSEARCH_INITIAL_ADMIN_PASSWORD}' | base64 -d)
  cat $MOUNT_PATH/kubernetes/install_k8s/fluentd/values.yaml | \
    sed "s|__PASSWORD__|${ADMIN_PASSWORD}|g" > $MOUNT_PATH/kubernetes/install_k8s/fluentd/values_temp.yaml
  # End

  helm install fluentd \
        --namespace fluentd \
        --create-namespace \
        --values $MOUNT_PATH/kubernetes/install_k8s/fluentd/values_temp.yaml \
        fluent/fluentd
  rm $MOUNT_PATH/kubernetes/install_k8s/fluentd/values_temp.yaml
  gok patch ingress fluentd fluentd letsencrypt fluentd
}

resetDockerRegistry(){
  helm uninstall registry -n registry

  emptyLocalFsStorage "Registry" "registry-pv" "registry-storage" "/data/volumes/pv4" "registry"
  kubectl delete ns registry
}

# createDevWorkspace and deleteDevWorkspace methods to prompt for namespace, username, workspace name, and manifest file, then call apply_devworkspace.py for create/delete actions.
createDevWorkspace() {
  echo "=== Create Che DevWorkspace ==="
  NAMESPACE=$(promptUserInput "Enter namespace: " "che-user")
  USERNAME=$(promptUserInput "Enter username: " "user1")
  WORKSPACE=$(promptUserInput "Enter workspace name: " "devworkspace1")
  MANIFEST_FILE=$(promptUserInput "Enter devworkspace manifest file path: " "devworkspace.yaml")
  export CHE_USER_NAMESPACE="$NAMESPACE"
  export CHE_USER_NAME="$USERNAME"
  export CHE_WORKSPACE_NAME="$WORKSPACE"
  export DW_FILE="$MANIFEST_FILE"
  export DW_DELETE="false"
  pushd "$MOUNT_PATH/kubernetes/install_k8s/eclipseche" || exit
  if ! python3 -c "import kubernetes" &>/dev/null; then
    apt-get install -y python3-kubernetes
  else
    echo "python3-kubernetes is already installed."
  fi

  if ! python3 -c "import yaml" &>/dev/null; then
    apt-get install -y python3-yaml
  else
    echo "python3-yaml is already installed."
  fi
  apt get install -y python3-kubernetes
  apt get install -y python3-yaml
  python3 "$WORKING_DIR/eclipseche/apply_devworkspace.py"
  popd || exit
}

deleteDevWorkspace() {
  echo "=== Delete Che DevWorkspace ==="
  NAMESPACE=$(promptUserInput "Enter namespace: " "che-user")
  USERNAME=$(promptUserInput "Enter username: " "user1")
  WORKSPACE=$(promptUserInput "Enter workspace name: " "devworkspace1")
  MANIFEST_FILE=$(promptUserInput "Enter devworkspace manifest file path: " "devworkspace.yaml")
  export CHE_USER_NAMESPACE="$NAMESPACE"
  export CHE_USER_NAME="$USERNAME"
  export CHE_WORKSPACE_NAME="$WORKSPACE"
  export DW_FILE="$MANIFEST_FILE"
  export DW_DELETE="true"
  pushd "$MOUNT_PATH/kubernetes/install_k8s/eclipseche" || exit
  python3 "$WORKING_DIR/eclipseche/apply_devworkspace.py"
  popd || exit
}

# https://itnext.io/error-x509-certificate-signed-by-unknown-authority-error-is-returned-f0e5d436f467
# Generate User & Password
genRegistryPassword(){
  export REGISTRY_USER="$1"
  export REGISTRY_PASS="$2"
  export DESTINATION_FOLDER=./registry-creds

  # Backup credentials to local files (in case you'll forget them later on)
  mkdir -p ${DESTINATION_FOLDER}
  echo ${REGISTRY_USER} > ${DESTINATION_FOLDER}/registry-user.txt
  echo ${REGISTRY_PASS} > ${DESTINATION_FOLDER}/registry-pass.txt

  docker run --entrypoint htpasswd registry:2.7.0 \
      -Bbn ${REGISTRY_USER} ${REGISTRY_PASS} \
      > ${DESTINATION_FOLDER}/htpasswd

  unset REGISTRY_USER REGISTRY_PASS DESTINATION_FOLDER
}

imagePullSecrets(){
  # Create a secret for the registry
  : "${DOCKER_USERNAME:=$(promptUserInput "Please enter docker user id: ")}"
  : "${DOCKER_PASSWORD:=$(promptSecret "Please enter your docker password: ")}"
  DOCKER_EMAIL="skmaji@outlook.com"

  # Delete the secret if it already exists
  kubectl get secret regcred -n kube-system 2>/dev/null && kubectl delete secret regcred -n kube-system
  kubectl create secret docker-registry regcred \
    --docker-server=$(fullRegistryUrl) \
    --docker-username=$DOCKER_USERNAME \
    --docker-password=$DOCKER_PASSWORD \
    --docker-email=$DOCKER_EMAIL -n kube-system

  genRegistryPassword $DOCKER_USERNAME $DOCKER_PASSWORD
}

# https://blog.zachinachshon.com/docker-registry/
# Verify access to the registry
verifyRegistryInst(){
  export DESTINATION_FOLDER=./registry-creds
  export USER=$(cat ${DESTINATION_FOLDER}/registry-user.txt)
  export PASSWORD=$(cat ${DESTINATION_FOLDER}/registry-pass.txt)

  curl -kiv -H \
    "Authorization: Basic $(echo -n "${USER}:${PASSWORD}" | base64)" \
    https://"$(registrySubdomain).$(rootDomain)"/v2/_catalog

  wget --no-check-certificate --header \
    "Authorization: Basic $(echo -n "${USER}:${PASSWORD}" | base64)" \
    https://"$(registrySubdomain).$(rootDomain)"/v2/_catalog

  unset USER PASSWORD
}

# https://itnext.io/error-x509-certificate-signed-by-unknown-authority-error-is-returned-f0e5d436f467
dockerRegistryInst(){
  imagePullSecrets
  helm repo add twuni https://helm.twun.io
  export DESTINATION_FOLDER=./registry-creds
  helm upgrade --install registry \
      --namespace registry \
      --create-namespace \
      --set replicaCount=1 \
      --set persistence.enabled=true \
      --set persistence.size=10Gi \
      --set persistence.deleteEnabled=true \
      --set persistence.storageClass=registry-storage \
      --set secrets.htpasswd="$(cat ${DESTINATION_FOLDER}/htpasswd)" \
      --values https://github.com/sumitmaji/kubernetes/raw/master/install_k8s/registry/values.yaml \
      twuni/docker-registry

}

# It is used to generate both client/server certificate using
# Kubernetes ca certificate in /etc/kubernetes/pki/ca.crt
createCertificate(){
  if [[ -n $TRACE ]]; then
    set -x
  fi

  : ${INSTALL_PATH:=$MOUNT_PATH/kubernetes/install_k8s}

  while [[ $# -gt 0 ]]
  do
  key="$1"
  case $key in
   -i|--ip)
   NODE_IP="$2"
   shift
   shift
   ;;
   -h|--host)
   HOSTNAME="$2"
   shift
   shift
   ;;
   -f|--file)
   FILENAME="$2"
   shift
   shift
   ;;
   -t|--type)
   TYPE="$2"
   shift
   shift
   ;;
  esac
  done

  if [ -z "$NODE_IP" ]
  then
  	echo "Please provide node ip"
  	exit 0
  fi
  if [ -z "$HOSTNAME" ]
  then
          echo "Please provide node hostname"
          exit 0
  fi
  if [ -z "$FILENAME" ]
  then
          echo "Please provide node filename"
          exit 0
  fi
  if [ -z "$TYPE" ]
  then
          echo "Please provide file type"
          exit 0
  fi

  : ${COUNTRY:=IN}
  : ${STATE:=UP}
  : ${LOCALITY:=GN}
  : ${ORGANIZATION:=CloudInc}
  : ${ORGU:=IT}
  : ${EMAIL:=cloudinc.gmail.com}
  : ${COMMONNAME:=kube-system}

  mkdir -p $MOUNT_PATH/certs
  pushd $MOUNT_PATH/certs

  if [ $TYPE == 'server' ]
  then
   keyUsage='extendedKeyUsage = clientAuth,serverAuth'
   #HOSTNAME="${HOSTNAME}-${FILENAME}" # Need to see why I did that
  else
   keyUsage='extendedKeyUsage = clientAuth'
   FILENAME="${FILENAME}-client"
   #HOSTNAME="${HOSTNAME}-$FILENAME"
  fi

  cat <<EOF | sudo tee ${FILENAME}-openssl.cnf
[req]
req_extensions = v3_req
distinguished_name = req_distinguished_name
[req_distinguished_name]
[ v3_req ]
basicConstraints = CA:FALSE
keyUsage = nonRepudiation, digitalSignature, keyEncipherment
$keyUsage
`
if [ $TYPE == 'server' ]
then
echo "subjectAltName = IP:$NODE_IP, DNS:$HOSTNAME"
fi`
EOF

  #Create a private key
  openssl genrsa -out $HOSTNAME.key 2048

  #Create CSR for the node
  openssl req -new -key $HOSTNAME.key \
    -subj "/CN=$NODE_IP" \
    -subj "/C=$COUNTRY/ST=$STATE/L=$LOCALITY/O=$ORGANIZATION/OU=$ORGU/CN=$HOSTNAME/emailAddress=$EMAIL" \
    -out $HOSTNAME.csr -config ${FILENAME}-openssl.cnf

  #Create a self signed certificate
  openssl x509 -req -in $HOSTNAME.csr -CA /etc/kubernetes/pki/ca.crt -CAkey /etc/kubernetes/pki/ca.key -CAcreateserial \
    -out $HOSTNAME.crt -days 10000 -extensions v3_req -extfile ${FILENAME}-openssl.cnf

  #Copy ca.crt to crt
  cat /etc/kubernetes/pki/ca.crt >> $HOSTNAME.crt

  #Verify a Private Key Matches a Certificate
  openssl x509 -noout -text -in $HOSTNAME.crt

  popd
}

# It is used to generate client certificate
# using kubectl command
createClientCertificate(){
  USERNAME=$1
  GROUPNAME=$2
  echo + Creating private key: ${USERNAME}.key
  openssl genrsa -out ${USERNAME}.key 4096

  echo + Creating signing request: ${USERNAME}-csr
  openssl req -new -key ${USERNAME}.key -out ${USERNAME}.csr -subj "/CN=${USERNAME}/O=${GROUPNAME}" \
          -addext "subjectAltName = DNS:${USERNAME}"

  WC=$(kubectl get csr ${USERNAME}-csr 2>/dev/null | wc -l)
  if [ "$WC" != "0" ]; then
    kubectl delete csr ${USERNAME}-csr
  fi
  echo + Creating signing request in kubernetes
  cat <<EOF | kubectl create -f -
apiVersion: certificates.k8s.io/v1
kind: CertificateSigningRequest
metadata:
  name: ${USERNAME}-csr
spec:
  groups:
    - system:authenticated
    - ${GROUPNAME}
  request: $(cat ${USERNAME}.csr | base64 | tr -d '\n')
  signerName: kubernetes.io/kube-apiserver-client
  usages:
    - digital signature
    - key encipherment
    - client auth
EOF

  kubectl certificate approve ${USERNAME}-csr

  kubectl get csr ${USERNAME}-csr -o jsonpath='{.status.certificate}' | base64 -d >${USERNAME}.crt
}

# The method creates certificate for user having admin role and generate kube config file
# for login to api server
createKubeConfig() {
  USERNAME=$1
  : ${IP:=$(ifconfig eth0 2>/dev/null | awk '/inet / {print $2}' | sed 's/addr://')}
  if [ -z "$IP" ]; then
    : ${IP:=$(ifconfig enp0s3 2>/dev/null | awk '/inet / {print $2}' | sed 's/addr://')}
  fi
  adminRole
  echo + Creating private key: ${USERNAME}.key
  openssl genrsa -out ${USERNAME}.key 4096

  echo + Creating signing request: ${USERNAME}.csr
  openssl req -new -key ${USERNAME}.key -out ${USERNAME}.csr -subj "/CN=${USERNAME}/O=cloud:masters"
  WC=$(kubectl get csr ${USERNAME}-csr 2>/dev/null | wc -l)
  if [ "$WC" != "0" ]; then
    kubectl delete csr ${USERNAME}-csr
  fi
  echo + Creating signing request in kubernetes
  cat <<EOF | kubectl create -f -
apiVersion: certificates.k8s.io/v1
kind: CertificateSigningRequest
metadata:
  name: ${USERNAME}-csr
spec:
  groups:
    - system:authenticated
    - cloud:masters
  request: $(cat ${USERNAME}.csr | base64 | tr -d '\n')
  signerName: kubernetes.io/kube-apiserver-client
  usages:
    - digital signature
    - key encipherment
    - client auth
EOF

  kubectl certificate approve ${USERNAME}-csr

  kubectl get csr ${USERNAME}-csr -o jsonpath='{.status.certificate}' | base64 -d >${USERNAME}.crt

  echo "======Kubeconfig file user ${USERNAME}.conf generated"

  kubectl config set-cluster cloud.com --certificate-authority=/etc/kubernetes/pki/ca.crt --embed-certs=true --server=https://${IP}:6443 --kubeconfig=/root/${USERNAME}.conf
  kubectl config set-credentials ${USERNAME} --client-key=${USERNAME}.key --client-certificate=${USERNAME}.crt --embed-certs=true --kubeconfig=/root/${USERNAME}.conf
  kubectl config --kubeconfig=/root/${USERNAME}.conf set-context ${USERNAME}@cloud.com --cluster=cloud.com --user=${USERNAME}
  kubectl config --kubeconfig=/root/${USERNAME}.conf use-context ${USERNAME}@cloud.com

  rm ${USERNAME}.key ${USERNAME}.csr ${USERNAME}.crt

}

patchLdapSecure() {
  kubectl patch ing "$NAME" --patch "$(
    cat <<EOF
metadata:
  annotations:
    nginx.ingress.kubernetes.io/auth-signin: https://$(defaultSubdomain).$(rootDomain)/authenticate
    nginx.ingress.kubernetes.io/auth-url: https://$(defaultSubdomain).$(rootDomain)/check
EOF
  )" -n "$NS"
}

patchOauth2Secure() {
  NAME=$1
  NS=$2
  RD=$3
  kubectl patch ing "$NAME" --patch "$(
    cat <<EOF
metadata:
  annotations:
    nginx.ingress.kubernetes.io/auth-signin: https://$(defaultSubdomain).$(rootDomain)/oauth2/start?rd=${RD}
    nginx.ingress.kubernetes.io/auth-url: https://$(defaultSubdomain).$(rootDomain)/oauth2/auth
    nginx.ingress.kubernetes.io/auth-response-headers: Authorization
EOF
  )" -n "$NS"
}


patchLetsEncrypt() {
  NAME=$1
  NS=$2
  SUBDOMAIN=$(subDomain $3)

  kubectl patch ing "$NAME" --patch "$(
    cat <<EOF
metadata:
  annotations:
    #certmanager.k8s.io/cluster-issuer: $(getClusterIssuerName)
    cert-manager.io/cluster-issuer: $(getClusterIssuerName)
    nginx.ingress.kubernetes.io/ssl-redirect: "true"
spec:
  tls:
    - hosts:
        - ${SUBDOMAIN}.$(rootDomain)
      secretName: ${SUBDOMAIN}-$(sedRootDomain)
EOF
  )" -n "$NS"
  kubectl patch ing "$NAME" --type=json -p='[{"op": "replace", "path": "/spec/rules/0/host", "value":"'$SUBDOMAIN'.'$(rootDomain)'"}]' -n "$NS"

  kubectl --timeout=120s -n ${NS} wait --for=condition=Ready certificates.cert-manager.io ${SUBDOMAIN}-$(sedRootDomain)
}

# Is is used see the values that would be generated by helm
helmShowValues(){
  CHART_NAME=$1
  helm get values $CHART_NAME -a
}

# Is is used to see all the resources that would be generated by helm
helmShowAll(){
  CHART_NAME=$1
  helm get all $CHART_NAME
}

helmTemplate(){
  CHART_REPO=$1
  helm template $CHART_REPO -g
}

# Is is used see the template that would be generated by helm
helmShowTemplate(){
  CHART_REPO=$1
  helm install $CHART_REPO  -g \
    --dry-run
}

fetch_client_secret() {
  # Parameters
  local KEYCLOAK_URL=$1
  local REALM_NAME=$2
  local CLIENT_ID=$3
  local ADMIN_USERNAME=$4
  local ADMIN_PASSWORD=$5

  # Validate input
  if [[ -z "$KEYCLOAK_URL" || -z "$REALM_NAME" || -z "$CLIENT_ID" || -z "$ADMIN_USERNAME" || -z "$ADMIN_PASSWORD" ]]; then
    echo "Usage: fetch_client_secret <KEYCLOAK_URL> <REALM_NAME> <CLIENT_ID> <ADMIN_USERNAME> <ADMIN_PASSWORD>"
    return 1
  fi

  # Fetch admin access token
  ACCESS_TOKEN=$(curl -s -X POST https://"$KEYCLOAK_URL/realms/master/protocol/openid-connect/token" \
    -H "Content-Type: application/x-www-form-urlencoded" \
    -d "username=$ADMIN_USERNAME" \
    -d "password=$ADMIN_PASSWORD" \
    -d "grant_type=password" \
    -d "client_id=admin-cli" | jq -r '.access_token')

  if [ -z "$ACCESS_TOKEN" ]; then
    echo "Failed to fetch access token. Please check your credentials."
    return 1
  fi

  # Fetch client UUID from the realm
  CLIENT_UUID=$(curl -s -X GET https://"$KEYCLOAK_URL/admin/realms/$REALM_NAME/clients" \
    -H "Authorization: Bearer $ACCESS_TOKEN" \
    -H "Content-Type: application/json" | jq -r ".[] | select(.clientId==\"$CLIENT_ID\") | .id")

  if [ -z "$CLIENT_UUID" ]; then
    echo "Client '$CLIENT_ID' not found in realm '$REALM_NAME'."
    return 1
  fi

  # Fetch client secret
  CLIENT_SECRET=$(curl -s -X GET https://"$KEYCLOAK_URL/admin/realms/$REALM_NAME/clients/$CLIENT_UUID/client-secret" \
    -H "Authorization: Bearer $ACCESS_TOKEN" \
    -H "Content-Type: application/json" | jq -r '.value')

  if [ -z "$CLIENT_SECRET" ]; then
    echo "Failed to fetch client secret for client '$CLIENT_ID'."
    return 1
  fi

  # Return the client secret
  echo "$CLIENT_SECRET"
}

oauth2Secret(){
  CLIENT_ID=$(kubectl get secret keycloak-secrets -n keycloak -o jsonpath="{['data']['CLIENT_ID']}" | base64 --decode)
  REALM=$(kubectl get secret keycloak-secrets -n keycloak -o jsonpath="{['data']['OAUTH_REALM']}" | base64 --decode)
  KEYCLOAK_URL=$(fullKeycloakUrl)
  ADMIN_USERNAME=$(kubectl get secret keycloak-secrets -n keycloak -o jsonpath="{['data']['KEYCLOAK_ADMIN']}" | base64 --decode)
  ADMIN_PASSWORD=$(kubectl get secret keycloak -n keycloak -o jsonpath="{.data.admin-password}" | base64 --decode)

  client_secret=$(fetch_client_secret "$KEYCLOAK_URL" "$REALM" "$CLIENT_ID" "$ADMIN_USERNAME" "$ADMIN_PASSWORD")
  : "${ACTIVE_PROFILE:=$(promptUserInput "Enter Active Profile (keycloak): " "keycloak")}"
  : "${OIDC_ISSUE_URL:=$(promptUserInput "Enter OIDC Issue URL (https://keycloak.gokcloud.com/realms/$REALM): " "https://keycloak.gokcloud.com/realms/$REALM")}"
  : "${OIDC_USERNAME_CLAIM:=$(promptUserInput "Enter OIDC Username Claim (${OIDC_USERNAME_CLAIM}): " "${OIDC_USERNAME_CLAIM}")}"
  : "${OIDC_GROUPS_CLAIM:=$(promptUserInput "Enter OIDC Groups Claim (${OIDC_GROUPS_CLAIM}): " "${OIDC_GROUPS_CLAIM}")}"
  : "${AUTH0_DOMAIN:=$(promptUserInput "Enter Auth0 Domain (${AUTH0_DOMAIN}): " "${AUTH0_DOMAIN}")}"
  : "${APP_HOST:=$(promptUserInput "Enter App Host (${APP_HOST}): " "${APP_HOST}")}"
  : "${JWKS_URL:=$(promptUserInput "Enter JWKS URL (${JWKS_URL}): " "${JWKS_URL}")}"
  OAUTH_SERVER_URI="https://$(fullKeycloakUrl)"

  # If secret already exists then delete it
  kubectl get secret oauth-secrets -n kube-system 2>/dev/null && kubectl delete secret oauth-secrets -n kube-system
  kubectl create secret generic oauth-secrets \
    --from-literal=OAUTH_REALM="${REALM}" \
    --from-literal=ACTIVE_PROFILE="${ACTIVE_PROFILE}" \
    --from-literal=OIDC_CLIENT_ID="${CLIENT_ID}" \
    --from-literal=OIDC_ISSUE_URL="${OIDC_ISSUE_URL}" \
    --from-literal=OIDC_USERNAME_CLAIM="${OIDC_USERNAME_CLAIM}" \
    --from-literal=OIDC_GROUPS_CLAIM="${OIDC_GROUPS_CLAIM}" \
    --from-literal=AUTH0_DOMAIN="${AUTH0_DOMAIN}" \
    --from-literal=APP_HOST="${APP_HOST}" \
    --from-literal=JWKS_URL="${JWKS_URL}" \
    --from-literal=OAUTH_SERVER_URI="${OAUTH_SERVER_URI}" \
    --from-literal=OIDC_CLIENT_SECRET="${client_secret}" -n kube-system
}

opensearchSecret(){
  ADMIN_PASSWORD=$1
  # If secret already exists then delete it
  kubectl get secret opensearch-secrets -n kube-system 2>/dev/null && kubectl delete secret opensearch-secrets -n kube-system
  kubectl create secret generic opensearch-secrets \
    --from-literal=OPENSEARCH_INITIAL_ADMIN_PASSWORD="${ADMIN_PASSWORD}" -n kube-system
}

csiDriverInstall(){
  helm repo add secrets-store-csi-driver https://kubernetes-sigs.github.io/secrets-store-csi-driver/charts 
  helm repo update
  helm install csi-secrets-store secrets-store-csi-driver/secrets-store-csi-driver --namespace kube-system 

}

csiDriverUnInstall(){
  helm uninstall csi-secrets-store --namespace kube-system
}

vaultLogin(){
  ROOT_TOKEN=$(kubectl get secret vault-init-keys -n vault -o json | jq -r '.data["vault-init.json"]' | base64 -d | jq -r '.root_token')
  kubectl exec -it vault-0 -n vault -- vault login ${ROOT_TOKEN}
}

cleanExampleSecretStoreInVault(){
  kubectl delete pod vault-secret-pod -n default
  kubectl delete secretproviderclass vault-secret-provider -n default
  kubectl delete secret my-k8s-secret -n default >/dev/null 2>&1

  vaultLogin
  kubectl exec -it vault-0 -n vault -- vault kv delete secret/my-secret
  kubectl exec -it vault-0 -n vault -- vault delete auth/kubernetes/role/my-role
  kubectl exec -it vault-0 -n vault -- vault policy delete my-policy  
}

cleanDockerRegistrySecretStoreInVault(){
  kubectl delete pod docker-registry-secret-pod -n default
  kubectl delete secretproviderclass docker-registry-secret-provider -n default
  
}

verifyVault(){
  pushd $MOUNT_PATH/kubernetes/install_k8s/vault
  chmod +x *.sh
  ./verification.sh
  popd
}

verifyCertManater(){
  pushd $MOUNT_PATH/kubernetes/install_k8s/scripts
  chmod +x *.sh
  ./verify_cert_manager.sh
  popd
}

debugVault(){
  pushd $MOUNT_PATH/kubernetes/install_k8s/vault
  chmod +x *.sh
  ./debug-vault.sh
  popd
}

createVaultSecretStore() {
  # Initialize variables
  local secret_path=""
  local role_name=""
  local policy_name=""
  local secret_provider_class_name=""
  local prefix=""
  local namespace="default"
  local kv_pairs=()
  local secret_name=""

  # Parse command-line arguments
  while [[ $# -gt 0 ]]; do
    case $1 in
      -p|--secret-path)
        secret_path="$2"
        shift 2
        ;;
      -r|--role-name)
        role_name="$2"
        shift 2
        ;;
      -l|--policy-name)
        policy_name="$2"
        shift 2
        ;;
      -s|--secret-provider-class-name)
        secret_provider_class_name="$2"
        shift 2
        ;;
      -x|--prefix)
        prefix="$2"
        shift 2
        ;;
      -n|--namespace)
        namespace="$2"
        shift 2
        ;;
      -k|--key-value)
        kv_pairs+=("$2")
        shift 2
        ;;
      --secret-name)
        secret_name="$2"
        shift 2
        ;;
      *)
        echo "Unknown option: $1"
        echo "Usage: createVaultSecretStore -p <secret_path> -r <role_name> -l <policy_name> -s <secret_provider_class_name> -x <prefix> -n <namespace> -k <key=value> [--secret-name <secret_name>]"
        return 1
        ;;
    esac
  done

  # Validate required arguments
  if [[ -z "$secret_path" || -z "$role_name" || -z "$policy_name" || -z "$secret_provider_class_name" || -z "$prefix" || ${#kv_pairs[@]} -eq 0 ]]; then
    echo "Usage: createVaultSecretStore -p <secret_path> -r <role_name> -l <policy_name> -s <secret_provider_class_name> -x <prefix> -n <namespace> -k <key=value> [--secret-name <secret_name>]"
    return 1
  fi

  # Default secret name if not provided
  if [[ -z "$secret_name" ]]; then
    secret_name="${secret_provider_class_name}-secret"
  fi

  echo "SECRET_PATH: $secret_path"
  echo "ROLE_NAME: $role_name"
  echo "POLICY_NAME: $policy_name"
  echo "SECRET_PROVIDER_CLASS_NAME: $secret_provider_class_name"
  echo "PREFIX: $prefix"
  echo "NAMESPACE: $namespace"
  echo "KV_PAIRS: ${kv_pairs[*]}"
  echo "SECRET_NAME: $secret_name"

  vaultLogin

  # Create the secret in Vault with multiple key-value pairs
  if kubectl exec -it vault-0 -n vault -- vault kv get "$secret_path" >/dev/null 2>&1; then
    echo "Secret at path $secret_path already exists in Vault. Skipping creation."
  else
    local secret_command="vault kv put $secret_path"
    for kv_pair in "${kv_pairs[@]}"; do
      secret_command+=" $kv_pair"
    done
    kubectl exec -it vault-0 -n vault -- $secret_command
  fi

  # Create a policy for the secret
  if kubectl exec -it vault-0 -n vault -- vault policy read "$policy_name" >/dev/null 2>&1; then
    echo "Policy $policy_name already exists in Vault. Skipping creation."
  else
    kubectl exec -i vault-0 -n vault -- vault policy write "$policy_name" - <<EOF
path "$secret_path" {
  capabilities = ["read", "list"]
}
EOF
  fi

  # Create a role for Kubernetes authentication
  if kubectl exec -it vault-0 -n vault -- vault read auth/kubernetes/role/"$role_name" >/dev/null 2>&1; then
    echo "Role $role_name already exists in Vault. Skipping creation."
  else
    kubectl exec -it vault-0 -n vault -- vault write auth/kubernetes/role/"$role_name" \
      bound_service_account_names=* \
      bound_service_account_namespaces="$namespace" \
      policies="$policy_name" \
      ttl=24h
  fi

  # Generate the `objects` section dynamically
  local objects=""
  for kv_pair in "${kv_pairs[@]}"; do
    local key=$(echo "$kv_pair" | cut -d '=' -f 1)
    objects+="      - objectName: \"$prefix-$key\"\n"
    objects+="        objectType: \"kv\"\n"
    objects+="        secretPath: \"$secret_path\"\n"
    objects+="        secretKey: \"$key\"\n"
    objects+="        objectVersion: \"\"\n"
  done

  # Add the `secretObjects` section
  local secret_objects="  secretObjects:\n"
  secret_objects+="    - secretName: $secret_name\n"
  secret_objects+="      type: Opaque\n"
  secret_objects+="      data:\n"
  for kv_pair in "${kv_pairs[@]}"; do
    local key=$(echo "$kv_pair" | cut -d '=' -f 1)
    secret_objects+="        - objectName: \"$prefix-$key\"\n"
    secret_objects+="          key: \"$key\"\n"
  done

  # Generate the YAML for the SecretProviderClass
  local yaml=$(cat <<EOF
apiVersion: secrets-store.csi.x-k8s.io/v1
kind: SecretProviderClass
metadata:
  name: "$secret_provider_class_name"
  namespace: "$namespace"
spec:
  provider: vault
  parameters:
    vaultAddress: "https://vault.gokcloud.com"
    roleName: "$role_name"
    skipVerify: "true"
    vaultSkipTLSVerify: "true"
    objects: |
$objects
$secret_objects
EOF
)

  # Echo the generated YAML
  echo "Generated YAML:"
  printf "$yaml"

  # Apply the YAML
  printf "$yaml" | kubectl apply -f -
}

exampleSecretStoreInVaule(){
  vaultLogin
  kubectl exec -it vault-0 -n vault -- vault kv put secret/my-secret username="my-username" password="my-password"

  kubectl exec -it vault-0 -n vault -- vault write auth/kubernetes/role/my-role \
    bound_service_account_names=default \
    bound_service_account_namespaces=default \
    policies=my-policy \
    ttl=24h

  # For K/V v2 secrets engine
#   kubectl exec -i vault-0 -n vault -- vault policy write my-policy - <<EOF
# path "secret/data/my-secret" {
#   capabilities = ["read", "list"]
# }
# EOF

  # For K/V v1 secrets engine
  kubectl exec -i vault-0 -n vault -- vault policy write my-policy - <<EOF
path "secret/my-secret" {
  capabilities = ["read", "list"]
}
EOF

  cat <<EOF | kubectl apply -f -
apiVersion: secrets-store.csi.x-k8s.io/v1
kind: SecretProviderClass
metadata:
  name: vault-secret-provider
  namespace: default
spec:
  provider: vault
  parameters:
    vaultAddress: "https://vault.gokcloud.com"  
    roleName: "my-role"  
    skipVerify: "true"   
    vaultSkipTLSVerify: "true"                 
    objects: |
      - objectName: "my-username"
        objectType: "kv"
        secretPath: "secret/my-secret"
        objectVersion: ""
        secretKey: "username"
      - objectName: "my-password"
        objectType: "kv"
        secretPath: "secret/my-secret"
        objectVersion: ""
        secretKey: "password"
EOF

  cat <<EOF | kubectl apply -f -
apiVersion: v1
kind: Pod
metadata:
  name: vault-secret-pod
  namespace: default
spec:
  containers:
  - name: app
    image: busybox
    command: ["sleep", "3600"]
    volumeMounts:
    - name: secrets-store-inline
      mountPath: "/mnt/secrets-store"  # Path where the secret will be mounted
      readOnly: true
  volumes:
  - name: secrets-store-inline
    csi:
      driver: secrets-store.csi.k8s.io
      readOnly: true
      volumeAttributes:
        secretProviderClass: "vault-secret-provider"
EOF

}

dockerRegistrySecretStoreInVault(){
  vaultLogin
  kubectl exec -it vault-0 -n vault -- vault kv put secret/docker-registry \
    server="https://index.docker.io/v1/" \
    username="my-username" \
    password="my-password" \
    email="my-email@example.com"

  kubectl exec -i vault-0 -n vault -- vault write auth/kubernetes/role/docker-registry-role \
    bound_service_account_names=my-service-account \
    bound_service_account_namespaces=default \
    policies=docker-registry-policy \
    ttl=24h

  kubectl exec -i vault-0 -n vault -- vault policy write docker-registry-policy - <<EOF
path "secret/data/docker-registry" {
  capabilities = ["read"]
}
EOF


  cat <<EOF | kubectl apply -f -
apiVersion: secrets-store.csi.x-k8s.io/v1
kind: SecretProviderClass
metadata:
  name: docker-registry-secret-provider
  namespace: default
spec:
  provider: vault
  parameters:
    vaultAddress: "https://vault.vault.svc.cloud.uat"
    roleName: "docker-registry-role"
    objects: |
      - objectName: "docker-registry"
        objectType: "kv"
        objectVersion: ""
  secretObjects:          
    - secretName: docker-registry-secret
      type: kubernetes.io/dockerconfigjson
      data:
        - objectName: "docker-registry"
          key: ".dockerconfigjson"
EOF

}

vaultInstall() {
  csiDriverInstall
  # Create namespace for Vault
  kubectl create namespace vault

  # Prompt user for Vault storage configuration
  STORAGE_CLASS_NAME="vault-storage"
  PV_NAME="vault-pv"
  VOLUME_PATH="/data/volumes/vault"

  # Create local storage class and persistent volume
  createLocalStorageClassAndPV "$STORAGE_CLASS_NAME" "$PV_NAME" "$VOLUME_PATH"

  # kubectl apply -f $MOUNT_PATH/kubernetes/install_k8s/vault/vault-k8s-seal-role.yaml
  # Add HashiCorp Helm repository
  helm repo add hashicorp https://helm.releases.hashicorp.com
  helm repo update

  # Install Vault using Helm
  helm install vault hashicorp/vault \
    --namespace vault \
    --values $MOUNT_PATH/kubernetes/install_k8s/vault/values.yaml


  # Valut UI is disabled, so ingress is not created
  # gok patch ingress vault vault letsencrypt vault
  # if [[ $? -ne 0 ]]; then
  #   echoFailed "Failed to patch Vault ingress with Let's Encrypt!"
  #   return 1
  # fi
  # echoSuccess "Vault ingress patched successfully with Let's Encrypt!"
  
  echo "Vault installation started. This may take a few minutes..."

  kubectl apply -f $MOUNT_PATH/kubernetes/install_k8s/vault/vault-k8s-seal-role.yaml
  copySecret regcred default vault
  pushd $MOUNT_PATH/kubernetes/install_k8s/vault
  chmod +x *.sh
  ./ci.sh
  popd
  kubectl patch serviceaccount vault -p '{"imagePullSecrets": [{"name": "regcred"}]}' -n vault
  kubectl apply -f $MOUNT_PATH/kubernetes/install_k8s/vault/vault-init-unseal-job.yaml
  
  # Wait for the Job to complete
  if kubectl wait --for=condition=complete job.batch/vault-init-unseal -n vault --timeout=30s; then
    # Check if the Job completed successfully
    STATUS=$(kubectl get job vault-init-unseal -n vault -o jsonpath='{.status.succeeded}')
    if [[ "$STATUS" -eq 1 ]]; then
      echoSuccess "Vault unseal completed successfully."
      kubectl delete -f $MOUNT_PATH/kubernetes/install_k8s/vault/vault-init-unseal-job.yaml
    else
      echoFailed "JVault unsealob did not complete successfully."
      exit 1  # Return failure
    fi
  else
    echoSuccess "Vault unseal did not complete within the timeout."
    exit 1  # Return failure
  fi
  
  # Wait for Vault pods to be ready
  echo "Waiting for Vault services to be up!!!!"
  kubectl --timeout=120s wait --for=condition=Ready pods --all --namespace vault
  if [[ $? -eq 0 ]]; then
    echoSuccess "Vault services are now up!"
    echoSuccess "Vault installation completed!"
    echoSuccess "You can access Vault at: https://$(fullVaultUrl)"

    kubectl create clusterrolebinding vault-auth-delegator \
      --clusterrole=system:auth-delegator \
      --serviceaccount=vault:vault

    vaultLogin

    kubectl exec -it vault-0 -n vault -- vault auth enable kubernetes

    TOKEN=$(kubectl exec -it vault-0 -n vault -- cat /var/run/secrets/kubernetes.io/serviceaccount/token)

    kubectl exec -it vault-0 -n vault -- vault write auth/kubernetes/config \
      token_reviewer_jwt="$TOKEN" \
      kubernetes_host="https://11.0.0.1:6643" \
      kubernetes_ca_cert=@/var/run/secrets/kubernetes.io/serviceaccount/ca.crt
    
    kubectl exec -it vault-0 -n vault -- vault secrets enable -path=secret kv
    
  else
    echoFailed "Vault services timed-out, please check!"
  fi

}

gokAgentReset(){
  # Uninstall Gok Agent using Helm
  helm uninstall gok-agent -n gok-agent

  # Delete the Gok Agent namespace
  kubectl delete ns gok-agent
}


gokAgentInstall(){
  
  pushd ${MOUNT_PATH}/kubernetes/install_k8s/gok-agent/agent
  ./build.sh
  ./tag_push.sh
  popd

  # Create namespace for gok-agent
  kubectl create namespace gok-agent

  kubectl create configmap ca-cert --from-file=issuer.crt=/usr/local/share/ca-certificates/issuer.crt -n gok-agent

  # Install Vault using Helm
  helm install gok-agent ${MOUNT_PATH}/kubernetes/install_k8s/gok-agent/agent/chart \
    --namespace gok-agent

}

# Function to delete old Docker images
# that are not tagged as "latest"
# Usage: deleteOldDockerImages <image_name>
deleteOldDockerImages() {
  IMAGE_NAME="$1"
  if [[ -z "$IMAGE_NAME" ]]; then
    echo "Usage: deleteOldDockerImages <image_name>"
    return 1
  fi
  docker images | grep "registry.gokcloud.com/${IMAGE_NAME}" | grep -v latest | awk '{print $3}' | xargs -r docker rmi
}

gokControllerReset(){
  # Uninstall Gok Controller using Helm
  helm uninstall gok-controller -n gok-controller

  # Delete the Gok Controller namespace
  kubectl delete ns gok-controller
}


gokControllerInstall(){
  pushd ${MOUNT_PATH}/kubernetes/install_k8s/gok-agent/controller
  ./build.sh
  ./tag_push.sh
  popd

  # Create namespace for gok-controller
  kubectl create namespace gok-controller
  
  createVaultSecretStore \
  -p "secret/web-controller" \
  -r "web-controller" \
  -l "web-controller-policy" \
  -s "web-controller-provider" \
  -x "web-controller" \
  -n "gok-controller" \
  -k "api-token=adfasdfasdfasdfasd"
  
  kubectl create configmap ca-cert --from-file=issuer.crt=/usr/local/share/ca-certificates/issuer.crt -n gok-controller

  # Install Vault using Helm
  helm install gok-controller ${MOUNT_PATH}/kubernetes/install_k8s/gok-agent/controller/chart \
    --namespace gok-controller

  # Patch the ingress with Let's Encrypt
  gok patch ingress web-controller gok-controller letsencrypt controller
  patchControllerWithOauth
  if [[ $? -ne 0 ]]; then
    echoFailed "Failed to patch Gok Controller ingress with Let's Encrypt!"
    return 1
  fi
  echoSuccess "Gok Controller ingress patched successfully with Let's Encrypt!"
  echoSuccess "Access Gok Controller at: https://controller.$(rootDomain)"
}

vaultReset() {
  # Uninstall Vault using Helm
  helm uninstall vault -n vault

  # Delete all PersistentVolumeClaims in the Vault namespace
  kubectl delete pvc --all -n vault

  # Clean up the local persistent volume and storage class
  emptyLocalFsStorage "Vault" "vault-pv" "vault-storage" "/data/volumes/vault"

  # Delete the Vault namespace
  kubectl delete ns vault

  csiDriverUnInstall

  kubectl delete clusterrolebinding vault-auth-delegator

  # Provide feedback to the user
  echo "Vault has been reset successfully!"
}


keycloakReset(){
  helm uninstall keycloak -n keycloak
  kubectl delete pvc --all -n keycloak

  emptyLocalFsStorage "Keycloak" "keycloak-pv" "keycloak-storage" "/data/volumes/pv3"
  kubectl delete ns keycloak
  kubectl get secret oauth-secrets >/dev/null 2>&1

  # vaultLogin
  # kubectl delete secretProviderClass keycloak-admin-secret-provider -n keycloak >/dev/null 2>&1
  # kubectl exec -it vault-0 -n vault -- vault kv delete secret/keycloak/admin
  # kubectl exec -it vault-0 -n vault -- vault delete auth/kubernetes/role/keycloak-admin-role
  # kubectl exec -it vault-0 -n vault -- vault policy delete keycloak-admin-policy

  # kubectl delete secretProviderClass keycloak-postgresql-secret-provider -n keycloak >/dev/null 2>&1
  # kubectl exec -it vault-0 -n vault -- vault kv delete secret/keycloak/postgresql
  # kubectl exec -it vault-0 -n vault -- vault delete auth/kubernetes/role/keycloak-postgresql-role
  # kubectl exec -it vault-0 -n vault -- vault policy delete keycloak-postgresql-policy
  # kubectl delete secret keycloak-secrets -n keycloak >/dev/null 2>&1
  # kubectl delete secret keycloak-postgresql -n keycloak >/dev/null 2>&1
  # kubectl delete secret keycloak-secrets -n kube-system >/dev/null 2>&1
  # kubectl delete secret keycloak-postgresql -n kube-system >/dev/null 2>&1
  # kubectl delete secret keycloak-secrets -n default >/dev/null 2>&1
  # kubectl delete secret keycloak-postgresql -n default >/dev/null 2>&1

}

keycloakInst(){
  #Admin user is "user" and password is fetched from secret keycloak with key "admin-password"
  #Hence removing the below 2 lines
  #keycloak_adminid=$(promptUserInput "Please enter keycloak admin id (admin): " "admin")
  #keycloak_adminpwd=$(promptSecret "Please enter keycloak admin pwd: ")
  kubectl create ns keycloak
  : "${KEYCLOAK_ADMIN_USERNAME:=$(promptUserInput "Please enter keycloak admin username (admin): " "admin")}"
  : "${KEYCLOAK_ADMIN_PASSWORD:=$(promptSecret "Please enter keycloak admin password: ")}"
  
  # createVaultSecretStore \
  # -p "secret/keycloak/admin" \
  # -r "keycloak-admin-role" \
  # -l "keycloak-admin-policy" \
  # -s "keycloak-admin-secret-provider" \
  # -x "keycloak" \
  # -n "keycloak" \
  # -k "adminUser=${KEYCLOAK_ADMIN_USERNAME}" \
  # -k "adminPassword=${KEYCLOAK_ADMIN_PASSWORD}"
  
  : "${POSTGRESQL_USERNAME:=$(promptUserInput "Please enter postgresql username (postgres): " "postgres")}"
  : "${POSTGRESQL_PASSWORD:=$(promptSecret "Please enter postgresql password: ")}"

  # createVaultSecretStore \
  # -p "secret/keycloak/postgresql" \
  # -r "keycloak-postgresql-role" \
  # -l "keycloak-postgresql-policy" \
  # -s "keycloak-postgresql-secret-provider" \
  # -x "keycloak" \
  # -n "keycloak" \
  # -k "username=${POSTGRESQL_USERNAME}" \
  # -k "password=${POSTGRESQL_PASSWORD}"
  
  : "${OIDC_CLIENT_ID:=$(promptUserInput "Please enter OIDC client id (${OIDC_CLIENT_ID}): " "${OIDC_CLIENT_ID}")}"
  : "${REALM:=$(promptUserInput "Please enter realm name (${REALM}): " "${REALM}")}"

  kubectl create ns keycloak
  kubectl create secret generic keycloak-secrets \
    --from-literal=KEYCLOAK_LOG_LEVEL="TRACE" \
    --from-literal=KEYCLOAK_ADMIN="${KEYCLOAK_ADMIN_USERNAME}" \
    --from-literal=CLIENT_ID="${OIDC_CLIENT_ID}" \
    --from-literal=OAUTH_REALM="${REALM}"  -n keycloak \
    --from-literal=KEYCLOAK_ADMIN_PASSWORD="${KEYCLOAK_ADMIN_PASSWORD}" -n keycloak

  kubectl create secret generic keycloak-postgresql \
    --from-literal=username="${POSTGRESQL_USERNAME}" \
    --from-literal=password="${POSTGRESQL_PASSWORD}" \
    --from-literal=postgres-password="${POSTGRESQL_PASSWORD}" -n keycloak

  hostname="$(subDomain 'keycloak').$(rootDomain)"

  helm install keycloak oci://registry-1.docker.io/bitnamicharts/keycloak \
        --namespace keycloak \
        --set ingress.hostname="$hostname" \
        --set auth.adminUser="$KEYCLOAK_ADMIN_USERNAME" \
        --set auth.adminPassword="$KEYCLOAK_ADMIN_PASSWORD" \
        --values "${MOUNT_PATH}"/kubernetes/install_k8s/keycloak/values2.yaml
}

addPolicyToSyncSecrets(){
  cat <<EOF | kubectl apply -f -  
apiVersion: kyverno.io/v1
kind: ClusterPolicy
metadata:
  name: sync-regcred
spec:
  rules:
    - name: sync-regcred-secret
      match:
        any:
          - resources:
              kinds:
                - Namespace
      generate:
        apiVersion: v1
        kind: Secret
        name: regcred
        namespace: "{{request.object.metadata.name}}"
        synchronize: true
        clone:
          namespace: kube-system
          name: regcred
    - name: patch-serviceaccount
      match:
        any:
          - resources:
              kinds:
                - ServiceAccount
              namespaces:
                - "*"
      mutate:
        patchStrategicMerge:
          spec:
            imagePullSecrets:
              - name: regcred
EOF
}

installRegistryWithCertMgr(){
  createLocalStorageClassAndPV "registry-storage" "registry-pv" "/data/volumes/pv4"
  dockerRegistryInst
  # No need to generate the certificate.
  # The certificate and secret will be directory issued the ingress annotation cert-manager.io/cluster-issuer
  # gok create certificate registry registry
  gok patch ingress registry-docker-registry registry letsencrypt $(registrySubdomain)

  # Let docker trust the self-signed certificates
  # https://itnext.io/error-x509-certificate-signed-by-unknown-authority-error-is-returned-f0e5d436f467
  #BEGIN
  rm /etc/docker/certs.d/$(registrySubdomain).$(rootDomain)/ca.crt
  mkdir -p /etc/docker/certs.d/$(registrySubdomain).$(rootDomain)/
  kubectl get secret $(registrySubdomain)-$(sedRootDomain) -n registry -o jsonpath="{['data']['tls\.crt']}" | base64 --decode > /etc/docker/certs.d/$(registrySubdomain).$(rootDomain)/ca.crt
  kubectl get secret $(registrySubdomain)-$(sedRootDomain) -n registry -o jsonpath="{['data']['ca\.crt']}" | base64 --decode >> /etc/docker/certs.d/$(registrySubdomain).$(rootDomain)/ca.crt
  systemctl restart docker
  #END

  # Restarting docker stops the haproxy, need to start it again
  gok start proxy
  docker login $(registrySubdomain).$(rootDomain)
  [[ $? -eq 0 ]] && echoSuccess "OK" || echoFailed "FAILED"
  openssl s_client -connect $(registrySubdomain).$(rootDomain):443 -showcerts </dev/null | grep 'Verify return code: 0 (ok)'
  [[ $? -eq 0 ]] && echoSuccess "Registry certificate verification succeeded" || echoFailed "Registry certificate verification failed"
  addPolicyToSyncSecrets
}



create_sub_scope() {
  # Parameters
  local KEYCLOAK_URL=$1
  local REALM_NAME=$2
  local ADMIN_USERNAME=$3
  local ADMIN_PASSWORD=$4
  local CLIENT_ID=$5

  # Validate input
  if [[ -z "$KEYCLOAK_URL" || -z "$REALM_NAME" || -z "$ADMIN_USERNAME" || -z "$ADMIN_PASSWORD" || -z "$CLIENT_ID" ]]; then
    echo "Usage: create_sub_scope <KEYCLOAK_URL> <REALM_NAME> <ADMIN_USERNAME> <ADMIN_PASSWORD> <CLIENT_ID>"
    return 1
  fi

  # Fetch admin access token
  ACCESS_TOKEN=$(curl -s -X POST "$KEYCLOAK_URL/realms/master/protocol/openid-connect/token" \
    -H "Content-Type: application/x-www-form-urlencoded" \
    -d "username=$ADMIN_USERNAME" \
    -d "password=$ADMIN_PASSWORD" \
    -d "grant_type=password" \
    -d "client_id=admin-cli" | jq -r '.access_token')

  if [ -z "$ACCESS_TOKEN" ]; then
    echo "Failed to fetch access token. Please check your credentials."
    return 1
  fi

  # Create the `sub` client scope
  CLIENT_SCOPE_ID=$(curl -s -X POST "$KEYCLOAK_URL/admin/realms/$REALM_NAME/client-scopes" \
    -H "Authorization: Bearer $ACCESS_TOKEN" \
    -H "Content-Type: application/json" \
    -d '{
          "name": "sub",
          "description": "Custom scope for the sub claim",
          "protocol": "openid-connect"
        }' | jq -r '.id')

  if [ -z "$CLIENT_SCOPE_ID" ]; then
    echo "Failed to create client scope 'sub'. It may already exist."
    CLIENT_SCOPE_ID=$(curl -s -X GET "$KEYCLOAK_URL/admin/realms/$REALM_NAME/client-scopes" \
      -H "Authorization: Bearer $ACCESS_TOKEN" \
      -H "Content-Type: application/json" | jq -r '.[] | select(.name=="sub") | .id')
  fi

  echo "Client Scope ID for 'sub': $CLIENT_SCOPE_ID"

  # Add a protocol mapper for the `sub` claim
  curl -s -X POST "$KEYCLOAK_URL/admin/realms/$REALM_NAME/client-scopes/$CLIENT_SCOPE_ID/protocol-mappers/models" \
    -H "Authorization: Bearer $ACCESS_TOKEN" \
    -H "Content-Type: application/json" \
    -d '{
          "name": "sub-mapper",
          "protocol": "openid-connect",
          "protocolMapper": "oidc-usermodel-property-mapper",
          "consentRequired": false,
          "config": {
            "user.attribute": "sub",
            "claim.name": "sub",
            "jsonType.label": "String",
            "id.token.claim": "true",
            "access.token.claim": "true",
            "userinfo.token.claim": "true"
          }
        }'

  echo "Protocol mapper for 'sub' added successfully."

  # Fetch client UUID
  CLIENT_UUID=$(curl -s -X GET "$KEYCLOAK_URL/admin/realms/$REALM_NAME/clients" \
    -H "Authorization: Bearer $ACCESS_TOKEN" \
    -H "Content-Type: application/json" | jq -r ".[] | select(.clientId==\"$CLIENT_ID\") | .id")

  if [ -z "$CLIENT_UUID" ]; then
    echo "Client '$CLIENT_ID' not found in realm '$REALM_NAME'."
    return 1
  fi

  # Assign the `sub` scope to the client
  curl -s -X PUT "$KEYCLOAK_URL/admin/realms/$REALM_NAME/clients/$CLIENT_UUID/optional-client-scopes/$CLIENT_SCOPE_ID" \
    -H "Authorization: Bearer $ACCESS_TOKEN" \
    -H "Content-Type: application/json"

  echo "Client scope 'sub' assigned to client '$CLIENT_ID'."

  # Verify the configuration
  echo "Verifying the configuration..."
  curl -s -X GET "$KEYCLOAK_URL/admin/realms/$REALM_NAME/clients/$CLIENT_UUID" \
    -H "Authorization: Bearer $ACCESS_TOKEN" \
    -H "Content-Type: application/json" | jq

  echo "Automation for 'sub' scope creation completed successfully."
}

debugScope(){
  # This function is used to debug the scope of the keycloak client
  # It will fetch the client secret and print it
  KEYCLOAK_URL=$(fullKeycloakUrl)
  REALM_NAME=$(kubectl get secret keycloak-secrets -n keycloak -o jsonpath="{['data']['OAUTH_REALM']}" | base64 --decode)
  CLIENT_ID=$(kubectl get secret keycloak-secrets -n keycloak -o jsonpath="{['data']['CLIENT_ID']}" | base64 --decode)
  ADMIN_USERNAME=$(kubectl get secret keycloak-secrets -n keycloak -o jsonpath="{['data']['KEYCLOAK_ADMIN']}" | base64 --decode)
  ADMIN_PASSWORD=$(kubectl get secret keycloak -n keycloak -o jsonpath="{.data.admin-password}" | base64 --decode)

  CLIENT_SECRET=$(fetch_client_secret "$KEYCLOAK_URL" "$REALM_NAME" "$CLIENT_ID" "$ADMIN_USERNAME" "$ADMIN_PASSWORD")
  echo "Client Secret: $CLIENT_SECRET"
  if [[ -z "$CLIENT_SECRET" ]]; then
    echoFailed "Failed to fetch client secret, please check the logs!"
    return 1
  fi
  echoSuccess "Client Secret fetched successfully!"
  generateAccessToken "$KEYCLOAK_URL" "$REALM_NAME" "$CLIENT_ID" "$CLIENT_SECRET" "openid profile email"
  if [[ $? -ne 0 ]]; then
    echoFailed "Failed to generate access token, please check the logs!"
    return 1
  fi  
}

generateAccessToken() {
    local KEYCLOAK_URL=$1
    local REALM_NAME=$2
    local CLIENT_ID=$3
    local CLIENT_SECRET=$4
    local SCOPE=$5

    if [[ -z "$KEYCLOAK_URL" || -z "$REALM_NAME" || -z "$CLIENT_ID" || -z "$CLIENT_SECRET" || -z "$SCOPE" ]]; then
      echo "Usage: generateAccessToken <KEYCLOAK_URL> <REALM_NAME> <CLIENT_ID> <CLIENT_SECRET> <SCOPE>"
      return 1
    fi

    curl -s -X POST https://"$KEYCLOAK_URL/realms/$REALM_NAME/protocol/openid-connect/token" \
      -H "Content-Type: application/x-www-form-urlencoded" \
      -d "client_id=$CLIENT_ID" \
      -d "client_secret=$CLIENT_SECRET" \
      -d "grant_type=client_credentials" \
      -d "scope=$SCOPE" | jq -r '.access_token'
  }

installKeycloakWithCertMgr(){
  keycloakInst

  createLocalStorageClassAndPV "keycloak-storage" "keycloak-pv" "/data/volumes/pv3"
  # No need to generate the certificate.
  # The certificate and secret will be directory issued the ingress annotation cert-manager.io/cluster-issuer
  #gok create certificate keycloak keycloak
  gok patch ingress keycloak keycloak letsencrypt $(keycloakSubdomain)

  echo "Waiting for services to be up!!!!"
  kubectl --timeout=240s wait --for=condition=Ready pods --all --namespace keycloak
  [[ $? -eq 0 ]] && echoSuccess "Keycloak services are now up!\nAccess it using https://$(keycloakSubdomain).$(rootDomain)/" || echoFailed "Keycloak services timed-out, plaese check!!"
  echo "Wait for 10 seconds"
  sleep 10
  apt install python3-dotenv python3-requests python3-jose -y
  ADMIN_ID=$(kubectl get secret keycloak-secrets -n keycloak -o jsonpath="{['data']['KEYCLOAK_ADMIN']}" | base64 --decode)
  ADMIN_PWD=$(kubectl get secret keycloak -n keycloak -o jsonpath="{.data.admin-password}" | base64 --decode)
  CLIENT_ID=$(kubectl get secret keycloak-secrets -n keycloak -o jsonpath="{['data']['CLIENT_ID']}" | base64 --decode)
  REALM=$(kubectl get secret keycloak-secrets -n keycloak -o jsonpath="{['data']['OAUTH_REALM']}" | base64 --decode)

  python3 $MOUNT_PATH/kubernetes/install_k8s/keycloak/keycloak-client.py all $ADMIN_ID $ADMIN_PWD $CLIENT_ID $REALM

  KEYCLOAK_URL=$(fullKeycloakUrl)
  # Example usage
  create_sub_scope "https://${KEYCLOAK_URL}" "${REALM}" "${ADMIN_ID}" "${ADMIN_PWD}" "${CLIENT_ID}"
  
  oauth2Secret
  # Check if LDAP service is running
  echo "Checking if LDAP service is running..."
  LDAP_STATUS=$(kubectl get svc ldap -n ldap 2>/dev/null | grep ldap | wc -l)

  if [ "$LDAP_STATUS" -eq 0 ]; then
    echo "LDAP service is not running. Please ensure the LDAP service is up and running before proceeding."
    exit 0
  else
    echo "LDAP service is running. Proceeding with user federation creation."
  fi
  pushd $MOUNT_PATH/kubernetes/install_k8s/keycloak
  echo "Creating User Federation"
  chmod +x setup_user_federation.sh setup_group_mappers.sh
  : "${LDAP_PASSWORD:=$(promptSecret "Please enter LDAP password for admin: ")}"
  
  ./setup_user_federation.sh $ADMIN_ID $ADMIN_PWD $LDAP_PASSWORD
  if [[ $? -ne 0 ]]; then
    echoFailed "User Federation creation failed, please check the logs!"
    popd
    exit 1
  fi
  echoSuccess "User Federation created successfully!"
  echo "Creating Keycloak Groups"
  ./setup_group_mappers.sh $ADMIN_ID $ADMIN_PWD
  if [[ $? -ne 0 ]]; then
    echoFailed "Keycloak Groups creation failed, please check the logs!"
    popd
    exit 1
  fi
  echoSuccess "Keycloak Groups created successfully!"
  popd
}

installLdap(){
  pushd $MOUNT_PATH/kubernetes/install_k8s/ldap

  : "${LDAP_PASSWORD:=${1:-$(promptSecret "Please enter LDAP password for admin: ")} }"
  : "${KERBEROS_PASSWORD:=${1:-$(promptSecret "Please enter Kerberos password: ")} }"
  : "${KERBEROS_KDC_PASSWORD:=${1:-$(promptSecret "Please enter Kerberos kdc password: ")} }"
  : "${KERBEROS_ADM_PASSWORD:=${1:-$(promptSecret "Please enter Kerberos adm password: ")} }"
  
  ./run_ldap.sh $LDAP_PASSWORD $KERBEROS_PASSWORD $KERBEROS_KDC_PASSWORD $KERBEROS_ADM_PASSWORD
  if [[ $? -ne 0 ]]; then
    echoFailed "LDAP installation failed, please check the logs!"
    popd
    return 1
  fi
  gok patch ingress ldap ldap letsencrypt $(defaultSubdomain)
  popd
}

updateLdapConfig(){

  # Usage: ./update_configmap.sh <LDAP_HOSTNAME> <LDAP_ADMIN_DN> <BASE_DN>
  LDAP_HOSTNAME=$1
  BASE_DN=$2

  if [[ -z "$LDAP_HOSTNAME" || -z "$BASE_DN" ]]; then
    echo "Usage: $0 <LDAP_HOSTNAME> <BASE_DN>"
    exit 1
  fi

  # Create or update the ConfigMap
  kubectl create configmap ldap-env-config \
    --from-literal=LDAP_HOSTNAME="$LDAP_HOSTNAME" \
    --from-literal=BASE_DN="$BASE_DN" \
    --namespace=default \
    --dry-run=client -o yaml | kubectl apply -f -
}

updateUserData(){

  # Usage: ./update_user_data.sh <USERNAME> <PASSWORD> <EMAIL> <FIRST_NAME> <LAST_NAME> <GROUP_NAME>
  USERNAME=$1
  PASSWORD=$2
  EMAIL=$3
  FIRST_NAME=$4
  LAST_NAME=$5
  GROUP_NAME=$6

  # Prepare the `kubectl create configmap` command
  CONFIGMAP_CMD="kubectl create configmap ldap-user-data --namespace=default"

  # Add non-empty parameters to the ConfigMap
  [[ -n "$USERNAME" ]] && CONFIGMAP_CMD+=" --from-literal=USERNAME=\"$USERNAME\""
  [[ -n "$PASSWORD" ]] && CONFIGMAP_CMD+=" --from-literal=USER_PASSWORD=\"$PASSWORD\""
  [[ -n "$EMAIL" ]] && CONFIGMAP_CMD+=" --from-literal=EMAIL=\"$EMAIL\""
  [[ -n "$FIRST_NAME" ]] && CONFIGMAP_CMD+=" --from-literal=FIRST_NAME=\"$FIRST_NAME\""
  [[ -n "$LAST_NAME" ]] && CONFIGMAP_CMD+=" --from-literal=LAST_NAME=\"$LAST_NAME\""
  [[ -n "$GROUP_NAME" ]] && CONFIGMAP_CMD+=" --from-literal=GROUP_NAME=\"$GROUP_NAME\""

  # Add dry-run and apply options
  CONFIGMAP_CMD+=" --dry-run=client -o yaml | kubectl apply -f -"

  # Execute the command
  eval "$CONFIGMAP_CMD"
}

copySecret(){
  # Usage: ./copy_secret.sh <secret-name> <source-namespace> <target-namespace>
  SECRET_NAME=$1
  SOURCE_NAMESPACE=$2
  TARGET_NAMESPACE=$3

  if [[ -z "$SECRET_NAME" || -z "$SOURCE_NAMESPACE" || -z "$TARGET_NAMESPACE" ]]; then
    echo "Usage: $0 <secret-name> <source-namespace> <target-namespace>"
    exit 1
  fi

  # Export the secret from the source namespace
  kubectl get secret "$SECRET_NAME" -n "$SOURCE_NAMESPACE" -o yaml | \
  # Update the namespace in the YAML and apply it to the target namespace
  sed "s/namespace: $SOURCE_NAMESPACE/namespace: $TARGET_NAMESPACE/" | \
  kubectl apply -n "$TARGET_NAMESPACE" -f -
}

# createUserGroup -u skmaji -p sumit -e skm@outlook.com -f Sumit -l Maji -g hadoop -s create_ldap_user.sh
# createUserGroup -g sqa -s create_ldap_group.sh
# Function to create an LDAP user and group
createUserGroup(){

  local script_name=""
  while [[ $# -gt 0 ]]; do
    case $1 in
      -u|--username)
        local username="$2"
        shift 2
        ;;
      -p|--password)
        local password="$2"
        shift 2
        ;;
      -e|--email)
        local email="$2"
        shift 2
        ;;
      -f|--first-name)
        local first_name="$2"
        shift 2
        ;;
      -l|--last-name)
        local last_name="$2"
        shift 2
        ;;
      -g|--group-name)
        local group_name="$2"
        shift 2
        ;;
      -s|--script-name)
        script_name="$2"
        shift 2
        ;;
      *)
        echo "Unknown option: $1"
        exit 1
        ;;
    esac
  done

  if [[ -z "$script_name" ]]; then
    echo "Error: Script name is required. Use -s or --script-name to specify it."
    exit 1
  fi
  
  source $MOUNT_PATH/kubernetes/install_k8s/ldap/config/config
  
  BASE_DN=${DC}
  updateUserData "$username" "$password" "$email" "$first_name" "$last_name" "$group_name"
  copySecret ldapsecret ldap default
  updateLdapConfig "ldap://ldap.ldap.svc.cloud.uat" "$BASE_DN"

  cp ${MOUNT_PATH}/kubernetes/install_k8s/ldap/${script_name} /tmp/user_script.sh

  kubectl create configmap user-script --from-file=/tmp/user_script.sh -n default
  
  # Apply the Job
  kubectl apply -f ${MOUNT_PATH}/kubernetes/install_k8s/gokutil/job.yaml

  # Wait for the Job to complete
  kubectl wait --for=condition=complete job/gokclient-runtime-job --timeout=300s

  # Get the logs from the Job
  kubectl logs job/gokclient-runtime-job -n default


  { 
    kubectl delete job gokclient-runtime-job -n default;
    kubectl delete configmap ldap-user-data -n default; 
    kubectl delete configmap user-script -n default; 
    kubectl delete configmap ldap-env-config -n default; 
    kubectl delete secret ldapsecret -n default;
  } >> /dev/null 2>&1
}

ldapReset(){
  helm uninstall ldap -n ldap
  kubectl delete ns ldap
}

kcAdminPwd(){
  kubectl get secret keycloak -n keycloak -o jsonpath="{.data.admin-password}" | base64 --decode
}

argocdAdminPwd(){
  kubectl get secret argocd-secret -n argocd -o jsonpath='{.data.admin\.password}' | base64 -d
}

installPrometheusGrafanaWithCertMgr(){

  createLocalStorageClassAndPV "prometheus-storage" "prometheus-pv" "/data/volumes/pv1"
  createLocalStorageClassAndPV "alertmanager-storage" "alertmanager-pv" "/data/volumes/pv2"
  prometheusGrafanaInstv2

  # No need to generate the certificate.
  # The certificate and secret will be directory issued the ingress annotation cert-manager.io/cluster-issuer
  #gok create certificate monitoring kube
  #No need for prometheus to be externalized, removing the ingress as well.
  gok patch ingress prometheus-server monitoring letsencrypt $(defaultSubdomain)
  gok patch ingress grafana monitoring letsencrypt $(defaultSubdomain)

  echo "Waiting for services to be up!!!!"
  kubectl --timeout=120s wait --for=condition=Ready pods --all --namespace monitoring
  [[ $? -eq 0 ]] && echoSuccess "Prometheus and Grafana services are now up!\nAccess it using \nprometheus: https://$(defaultSubdomain).$(rootDomain)/prometheus\ngrafana: https://$(defaultSubdomain).$(rootDomain)/grafana" || echoFailed "Keycloak services timed-out, plaese check!!"
}

patchDashboardWithOauth(){
  patchOauth2Secure kubernetes-dashboard kubernetes-dashboard https://kube.gokcloud.com/dashboard/
}

patchJupyterWithOauth() {
  patchOauth2Secure jupyterhub jupyterhub https://$(defaultSubdomain).$(rootDomain)/hub/
}

patchTtydWithOauth() {
  patchOauth2Secure ttyd ttyd https://ttyd.$(rootDomain)
}

patchCloudshellWithOauth() {
  patchOauth2Secure cloudshell-cloudshell cloudshell https://$(defaultSubdomain).$(rootDomain)
}

patchControllerWithOauth() {
  patchOauth2Secure web-controller gok-controller https://controller.$(rootDomain)
}

checkAndPatchDashboard() {
  # Check if the oauth2-proxy service is running
  echo "Checking if oauth2-proxy service is running..."
  if kubectl get pods -n oauth2 -l app.kubernetes.io/name=oauth2-proxy 2>/dev/null | grep -q "Running"; then
    echo "oauth2-proxy service is running. Proceeding to patch the dashboard..."
    patchDashboardWithOauth
  else
    echoFailed "oauth2-proxy service is not running. Please ensure it is running before patching the dashboard."
  fi
}

checkAndPatchHub() {
  # Check if the oauth2-proxy service is running
  echo "Checking if oauth2-proxy service is running..."
  if kubectl get pods -n oauth2 -l app.kubernetes.io/name=oauth2-proxy 2>/dev/null | grep -q "Running"; then
    echo "oauth2-proxy service is running. Proceeding to patch JupyterHub..."
    patchJupyterWithOauth
  else
    echo "oauth2-proxy service is not running. Please ensure it is running before patching JupyterHub."
  fi
}

cloudshellReset() {
  helm uninstall cloudshell -n cloudshell
  kubectl delete ns cloudshell
  echo "cloudshell has been uninstalled and the namespace deleted."
}

cloudshellInst() {
  pushd ${MOUNT_PATH}/kubernetes/install_k8s/cloud-shell/gok
  ./build.sh
  ./tag_push.sh
  popd

  # Create namespace for cloudshell
  kubectl create namespace cloudshell || echo "Namespace cloudshell already exists"

  copySecret regcred kube-system cloudshell

  kubectl create configmap ca-cert --from-file=issuer.crt=/usr/local/share/ca-certificates/issuer.crt -n cloudshell

  helm install cloudshell ${MOUNT_PATH}/kubernetes/install_k8s/cloud-shell/gok/chart -n cloudshell

  # Patch ingress with letsencrypt and oauth if needed
  gok patch ingress cloudshell-cloudshell cloudshell letsencrypt $(defaultSubdomain)
  patchCloudshellWithOauth

  echo "Waiting for cloudshell to be ready..."
  kubectl --namespace cloudshell wait --for=condition=Ready pods --all --timeout=120s
  if [[ $? -eq 0 ]]; then
    echoSuccess "cloudshell is now up! Access it at: https://$(defaultSubdomain).$(rootDomain)/cloudshell/home"
  else
    echoFailed "cloudshell setup timed out. Please check the logs."
    return 1
  fi
}

consoleReset() {
  helm uninstall console -n console
  kubectl delete ns console
  echo "console has been uninstalled and the namespace deleted."
}

patchConsoleWithOauth() {
  patchOauth2Secure console-console console https://console.$(rootDomain)/
}

consoleInst() {
  pushd ${MOUNT_PATH}/kubernetes/install_k8s/console/app
  ./build.sh
  ./tag_push.sh
  popd


  # Create namespace for console
  kubectl create namespace console || echo "Namespace console already exists"

  # Copy image pull secret if needed
  copySecret regcred kube-system console

  kubectl create configmap ca-cert --from-file=issuer.crt=/usr/local/share/ca-certificates/issuer.crt -n console

  # Install console using Helm chart (adjust path if needed)
  helm install console ${MOUNT_PATH}/kubernetes/install_k8s/console/app/chart -n console

  # Patch ingress with letsencrypt and oauth if needed
  gok patch ingress console-console console letsencrypt console
  # Optionally patch with oauth if you use oauth2-proxy or similar
  patchConsoleWithOauth

  echo "Waiting for console to be ready..."
  kubectl --namespace console wait --for=condition=Ready pods --all --timeout=120s
  if [[ $? -eq 0 ]]; then
    echoSuccess "console is now up! Access it at: https://console.$(rootDomain)/"
  else
    echoFailed "console setup timed out. Please check the logs."
    return 1
  fi
}

installDashboardwithCertManager(){
  dashboardInst
  # No need to generate the certificate.
  # The certificate and secret will be directory issued the ingress annotation cert-manager.io/cluster-issuer
  #gok create certificate kubernetes-dashboard kube
  gok patch ingress kubernetes-dashboard kubernetes-dashboard letsencrypt $(defaultSubdomain)
  checkAndPatchDashboard
  echo "Waiting for services to be up!!!!"
  kubectl --timeout=120s wait --for=condition=Ready pods --all --namespace kubernetes-dashboard
  [[ $? -eq 0 ]] && echoSuccess "Dashboard services are now up!\nAccess it using https://$(defaultSubdomain).$(rootDomain)/dashboard/" || echoFailed "Keycloak services timed-out, plaese check!!"
}

# add_audience_mapper_to_groups_scope \
#   "https://keycloak.gokcloud.com" \
#   "GokDevelopers" \
#   "admin" \
#   "your-admin-password" \
#   "gok-developers-client"

add_audience_mapper_to_groups_scope() {
  # Required inputs
  local KEYCLOAK_URL=$1         # e.g. https://keycloak.gokcloud.com
  local REALM=$2                # e.g. GokDevelopers
  local ADMIN_USER=$3           # e.g. admin
  local ADMIN_PASS=$4           # e.g. <admin-password>
  local CLIENT_ID=$5            # e.g. gok-developers-client

  # Get admin access token
  local ADMIN_TOKEN=$(curl -s -X POST "$KEYCLOAK_URL/realms/master/protocol/openid-connect/token" \
    -d "username=$ADMIN_USER" \
    -d "password=$ADMIN_PASS" \
    -d "grant_type=password" \
    -d "client_id=admin-cli" \
    | jq -r .access_token)

  if [[ -z "$ADMIN_TOKEN" || "$ADMIN_TOKEN" == "null" ]]; then
    echo "Failed to get admin token"
    return 1
  fi

  # Get client UUID
  local CLIENT_UUID=$(curl -s -X GET "$KEYCLOAK_URL/admin/realms/$REALM/clients" \
    -H "Authorization: Bearer $ADMIN_TOKEN" \
    | jq -r ".[] | select(.clientId==\"$CLIENT_ID\") | .id")

  if [[ -z "$CLIENT_UUID" ]]; then
    echo "Client $CLIENT_ID not found"
    return 1
  fi

  # Add Audience protocol mapper to client
  curl -s -X POST "$KEYCLOAK_URL/admin/realms/$REALM/clients/$CLIENT_UUID/protocol-mappers/models" \
    -H "Authorization: Bearer $ADMIN_TOKEN" \
    -H "Content-Type: application/json" \
    -d "{
      \"name\": \"audience-groups\",
      \"protocol\": \"openid-connect\",
      \"protocolMapper\": \"oidc-audience-mapper\",
      \"consentRequired\": false,
      \"config\": {
        \"included.client.audience\": \"$CLIENT_ID\",
        \"id.token.claim\": \"true\",
        \"access.token.claim\": \"true\"
      }
    }"

  echo "Audience protocol mapper added to client $CLIENT_ID."

  # (Optional) Assign groups scope to client if not already assigned
  # Get groups client scope ID
  local GROUPS_SCOPE_ID=$(curl -s -X GET "$KEYCLOAK_URL/admin/realms/$REALM/client-scopes" \
    -H "Authorization: Bearer $ADMIN_TOKEN" \
    | jq -r '.[] | select(.name=="groups") | .id')

  if [[ -n "$GROUPS_SCOPE_ID" ]]; then
    # Assign as default client scope if not already
    curl -s -X PUT "$KEYCLOAK_URL/admin/realms/$REALM/clients/$CLIENT_UUID/default-client-scopes/$GROUPS_SCOPE_ID" \
      -H "Authorization: Bearer $ADMIN_TOKEN"
    echo "Assigned 'groups' scope to client $CLIENT_ID."
  else
    echo "No 'groups' client scope found, skipping scope assignment."
  fi
}

generate_kubeconfig() {
  read -p "Enter your Kubernetes API server URL (e.g., https://kubernetes.default.svc): " KUBE_API
  read -p "Enter your Kubernetes namespace: " KUBE_NAMESPACE
  read -p "Enter your Bearer token: " KUBE_TOKEN

  # Optional: set a context name
  CONTEXT_NAME="user-ttyd"

  cat > kubeconfig <<EOF
apiVersion: v1
kind: Config
clusters:
- cluster:
    server: ${KUBE_API}
    insecure-skip-tls-verify: true
  name: cluster
users:
- name: user
  user:
    token: ${KUBE_TOKEN}
contexts:
- context:
    cluster: cluster
    user: user
    namespace: ${KUBE_NAMESPACE}
  name: ${CONTEXT_NAME}
current-context: ${CONTEXT_NAME}
EOF

  echo "Kubeconfig written to ./kubeconfig"
  echo "To use it, run:"
  echo "  export KUBECONFIG=$(pwd)/kubeconfig"
}

get_keycloak_token() {
  read -p "Keycloak URL (default: https://keycloak.gokcloud.com/realms/GokDevelopers/protocol/openid-connect/token): " KEYCLOAK_URL
  KEYCLOAK_URL=${KEYCLOAK_URL:-https://keycloak.gokcloud.com/realms/GokDevelopers/protocol/openid-connect/token}

  read -p "Client ID (default: gok-developers-client): " CLIENT_ID
  CLIENT_ID=${CLIENT_ID:-gok-developers-client}
  read -p "Client Secret: " CLIENT_SECRET
  read -p "Username: " USERNAME
  read -s -p "Password: " PASSWORD
  echo

  TOKEN_RESPONSE=$(curl -s -X POST "$KEYCLOAK_URL" \
    -d "client_id=$CLIENT_ID" \
    -d "client_secret=$CLIENT_SECRET" \
    -d "grant_type=password" \
    -d "username=$USERNAME" \
    -d "password=$PASSWORD")

  ACCESS_TOKEN=$(echo "$TOKEN_RESPONSE" | grep -o '"access_token":"[^"]*' | grep -o '[^"]*$')
  if [ -n "$ACCESS_TOKEN" ]; then
    echo "Your access token:"
    echo "$ACCESS_TOKEN"
  else
    echo "Failed to get token. Response:"
    echo "$TOKEN_RESPONSE"
  fi
}

oauth2ProxyReset(){
  helm uninstall oauth2proxy -n oauth2

  emptyLocalFsStorage "OAuth2" "oauth-pv" "oauth-storage" "/data/volumes/pv6" "oauth2"
  kubectl delete ns oauth2
}

#export NODE_TLS_REJECT_UNAUTHORIZED=0
jupyterHubInst() {
  # Add the Helm repository for JupyterHub
  helm repo add jupyterhub https://jupyterhub.github.io/helm-chart/
  helm repo update

  # Create the storage class for JupyterHub using the generic method
  createLocalStorageClassAndPV "jupyter-storage" "jupyter-pv" "/data/volumes/jupyter"
  createLocalStorageClassAndPV "jupyter-user-storage" "jupyter-user-pv" "/data/volumes/jupyter-user"
  # Create a namespace for JupyterHub
  kubectl create namespace jupyterhub || echo "Namespace jupyterhub already exists"

  # Generate a random secret token for JupyterHub
  JUPYTERHUB_SECRET=$(openssl rand -hex 32)

  # Create a Kubernetes secret for the JupyterHub proxy
  kubectl create secret generic hub-secret \
    --from-literal=hub-secret-key="${JUPYTERHUB_SECRET}" \
    --namespace jupyterhub || echo "Secret hub-secret already exists"

  CLIENT_SECRET=$(dataFromSecret oauth-secrets kube-system OIDC_CLIENT_SECRET)
  CLIENT_ID=$(dataFromSecret oauth-secrets kube-system OIDC_CLIENT_ID)
  keycloakUrl="https://$(fullKeycloakUrl)"
  REALM=$(dataFromSecret oauth-secrets kube-system OAUTH_REALM)
  OAUTH2_HOST="$(fullKeycloakUrl)"
  ENABLE_OAUTH2="true"

  # Install JupyterHub using Helm with the provided values.yaml
  helm install jupyterhub jupyterhub/jupyterhub \
    --namespace jupyterhub \
    --set proxy.secretToken="${JUPYTERHUB_SECRET}" \
    --values "${MOUNT_PATH}/kubernetes/install_k8s/jupyter/values.yaml" \
    $(if [[ "$ENABLE_OAUTH2" == "true" ]]; then
      echo "--set hub.config.GenericOAuthenticator.client_id=${CLIENT_ID} \
      --set hub.config.GenericOAuthenticator.client_secret=${CLIENT_SECRET} \
      --set hub.config.GenericOAuthenticator.oauth_callback_url=https://$(jupyterHubSubdomain).$(rootDomain)/oauth_callback \
      --set hub.config.GenericOAuthenticator.authorize_url=https://${OAUTH2_HOST}/realms/${REALM}/protocol/openid-connect/auth \
      --set hub.config.GenericOAuthenticator.token_url=https://${OAUTH2_HOST}/realms/${REALM}/protocol/openid-connect/token \
      --set hub.config.GenericOAuthenticator.userdata_url=https://${OAUTH2_HOST}/realms/${REALM}/protocol/openid-connect/userinfo \
      --set hub.config.GenericOAuthenticator.login_service=keycloak \
      --set hub.config.GenericOAuthenticator.username_claim=preferred_username \
      --set hub.config.GenericOAuthenticator.userdata_params.state=state \
      --set hub.config.GenericOAuthenticator.allow_all=true \
      --set hub.config.GenericOAuthenticator.tls_verify=false \
      --set hub.config.GenericOAuthenticator.admin_users[0]=admin \
      --set hub.config.GenericOAuthenticator.admin_users[1]=skmaji1 \
      --set hub.config.JupyterHub.authenticator_class=generic-oauth"
    fi)

  # Wait for the JupyterHub pods to be ready
  echo "Waiting for JupyterHub services to be up..."
  kubectl --namespace jupyterhub wait --for=condition=Ready pods --all --timeout=300s

  gok patch ingress jupyterhub jupyterhub letsencrypt $(jupyterHubSubdomain)
  # checkAndPatchHub

  if [[ $? -eq 0 ]]; then
    echoSuccess "JupyterHub services are now up!"
    echoSuccess "Access it using https://master.cloud.com"
  else
    echoFailed "JupyterHub services timed out. Please check the logs!"
  fi
}

jupyterHubReset() {
  helm uninstall jupyterhub -n jupyterhub
  kubectl delete secret hub-secret -n jupyterhub
  kubectl delete pod --all -n jupyterhub
  kubectl delete pvc --all -n jupyterhub
  emptyLocalFsStorage "JupyterHub" "jupyter-pv" "jupyter-storage" "/data/volumes/jupyter"
  emptyLocalFsStorage "JupyterHub User" "jupyter-user-pv" "jupyter-user-storage" "/data/volumes/jupyter-user"
  kubectl delete ns jupyterhub
}

installKubectlClient(){
  # Install kubectl client
  if ! command -v kubectl &> /dev/null; then
    echo "kubectl not found, installing..."
    curl -LO "https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl"
    chmod +x kubectl
    sudo mv kubectl /usr/local/bin/
    echoSuccess "kubectl installed successfully!"
  else
    echo "kubectl is already installed."
  fi
}   

installDocker(){
  # Install Docker
  if ! command -v docker &> /dev/null; then
    echo "Docker not found, installing..."
    sudo apt-get update
    sudo apt-get install -y docker.io
    sudo systemctl start docker
    sudo systemctl enable docker
    echoSuccess "Docker installed successfully!"
  else
    echo "Docker is already installed."
  fi

  # Add current user to the docker group
  sudo usermod -aG docker $USER
  echoSuccess "Current user added to the docker group. Please log out and log back in for changes to take effect."
}

# Method to remove claimRef from a specific Persistent Volume
# removeClaimRefFromPV "eclipse-che-pv"
removeClaimRefFromPV() {
  local pv_name=$1

  if [ -z "$pv_name" ]; then
    echo "Error: No PV name provided."
    return 1
  fi

  # Check if the PV exists and is in the "Released" state
  pv_state=$(kubectl get pv "$pv_name" --no-headers | awk '{print $5}')
  if [ "$pv_state" != "Released" ]; then
    echo "Error: PV '$pv_name' is not in the 'Released' state. Current state: $pv_state"
    return 1
  fi

  echo "Processing PV: $pv_name"

  # Patch the PV to remove the claimRef
  kubectl patch pv "$pv_name" --type=json -p='[{"op": "remove", "path": "/spec/claimRef"}]'

  if [ $? -eq 0 ]; then
    echo "Successfully removed claimRef from PV: $pv_name"
    return 0
  else
    echo "Failed to remove claimRef from PV: $pv_name"
    return 1
  fi
}


eclipseCheInst(){

  # Fetch OIDC client secret and other required parameters from the secret
  CLIENT_SECRET=$(dataFromSecret oauth-secrets kube-system OIDC_CLIENT_SECRET)
  CLIENT_ID=$(dataFromSecret oauth-secrets kube-system OIDC_CLIENT_ID)
  REALM_NAME=$(dataFromSecret oauth-secrets kube-system OAUTH_REALM)

  createLocalStorageClassAndPV "eclipse-che-storage" "eclipse-che-pv" "/data/volumes/eclipse-che"
  
  # Install Chectl
  wget https://che-incubator.github.io/chectl/install.sh
  chmod +x install.sh
  ./install.sh
  [[ $? -ne 0 ]] && echoFailed "Chectl installation failed, please check the logs!" && return 1
  echoSuccess "Chectl installed successfully!"

  # Create a patch file for CheCluster
  cat > che-patch.yaml << EOF
kind: CheCluster
apiVersion: org.eclipse.che/v2
spec:
  devEnvironments:
    secondsOfInactivityBeforeIdling: -1
    storage:
      perUserStrategyPvcConfig:
        claimSize: 2Gi 
        storageClass: eclipse-che-storage
      pvcStrategy: per-user 
    
  networking:
    auth:
      oAuthClientName: ${CLIENT_ID}
      oAuthSecret: ${CLIENT_SECRET}
      identityProviderURL: "https://$(fullKeycloakUrl)/realms/${REALM_NAME}"
      gateway:
        oAuthProxy:
          cookieExpireSeconds: 300
  components:
    cheServer:
      extraVolumes:
        - name: keycloak-ca
          configMap:
            name: keycloak-certs
      extraVolumeMounts:
        - name: keycloak-ca
          mountPath: /etc/ssl/certs/keycloak-ca.crt
          subPath: keycloak-ca.crt
          readOnly: true
      extraProperties:
        CHE_OIDC_USERNAME__CLAIM: email
        CHE_OIDC_SKIP_CERTIFICATE_VERIFICATION: "true"
        REQUESTS_CA_BUNDLE: /etc/ssl/certs/keycloak-ca.crt
EOF

  kubectl create ns eclipse-che || echo "Namespace eclipse-che already exists"

  # Create a secret for Keycloak CA certificate
  kubectl get secret keycloak-gokcloud-com -n keycloak -o jsonpath="{.data['ca\.crt']}" | base64 -d > keycloak-ca.crt
  kubectl create configmap keycloak-certs --from-file=keycloak-ca.crt=keycloak-ca.crt -n eclipse-che
  kubectl label configmap keycloak-certs app.kubernetes.io/part-of=che.eclipse.org app.kubernetes.io/component=ca-bundle -n eclipse-che

  # Deploy Eclipse Che using chectl
  chectl server:deploy --platform k8s --domain che.gokcloud.com --che-operator-cr-patch-yaml che-patch.yaml --skip-cert-manager
  [[ $? -eq 0 ]] && echoSuccess "Eclipse Che is now installed and running!" || echoFailed "Eclipse Che installation failed, please check the logs!"
  rm -f keycloak-ca.crt che-patch.yaml install.sh
}

resetEclipseChe(){
  # Uninstall Eclipse Che
  kubectl delete deployment --all -n eclipse-che
  chectl server:delete --delete-namespace --delete-all
  [[ $? -ne 0 ]] && echoFailed "Eclipse Che uninstallation failed, please check the logs!" && return 1
  kubectl delete configmap keycloak-certs -n eclipse-che || echo "ConfigMap keycloak-certs already deleted or does not exist"
  emptyLocalFsStorage "Eclipse Che" "eclipse-che-pv" "eclipse-che-storage" "/data/volumes/eclipse-che"
  echoSuccess "Eclipse Che has been uninstalled successfully!"
}

oauth2ProxyInst(){

  createLocalStorageClassAndPV "oauth-storage" "oauth-pv" "/data/volumes/pv6"
  kubectl create ns oauth2

  OAUTH2_PROXY_CLIENT_ID=$(dataFromSecret oauth-secrets kube-system OIDC_CLIENT_ID)
  REALM=$(dataFromSecret oauth-secrets kube-system OAUTH_REALM)
  OAUTH2_HOST="$(fullKeycloakUrl)"
  OAUTH2_OIDC_ISSUE_URL="$(dataFromSecret oauth-secrets kube-system OIDC_ISSUE_URL)"

  kubectl create configmap oauth2-proxy-config \
      --from-literal=clientID="${OAUTH2_PROXY_CLIENT_ID}" \
      --from-literal=oidcIssuerUrl="${OAUTH2_OIDC_ISSUE_URL}" \
      --from-literal=redirectUrl=oauth2/callback \
      -n oauth2

  kubectl create configmap oauth2-ca-cert --from-file=issuer.crt=/usr/local/share/ca-certificates/issuer.crt -n oauth2

  helm install oauth2proxy oci://registry-1.docker.io/bitnamicharts/oauth2-proxy \
    --namespace oauth2 \
    --set "extraArgs[0]=--provider=oidc" \
    --set "extraArgs[1]=--login-url=https://${OAUTH2_HOST}/realms/${REALM}/protocol/openid-connect/auth" \
    --set "extraArgs[2]=--redeem-url=https://${OAUTH2_HOST}/realms/${REALM}/protocol/openid-connect/token" \
    --set "extraArgs[3]=--profile-url=https://${OAUTH2_HOST}/realms/${REALM}/protocol/openid-connect/userinfo" \
    --set "extraArgs[4]=--validate-url=https://${OAUTH2_HOST}/realms/${REALM}/protocol/openid-connect/userinfo" \
    --set "extraArgs[5]=--keycloak-group=administrators" \
    --set "extraArgs[6]=--keycloak-group=developers" \
    --set "extraArgs[7]=--allowed-group=administrators" \
    --set "extraArgs[8]=--allowed-group=developers" \
    --set "extraArgs[9]=--scope=openid email profile groups sub offline_access" \
    --set "extraArgs[10]=--ssl-insecure-skip-verify=false" \
    --set "extraArgs[11]=--set-authorization-header=true" \
    --set "extraArgs[12]=--whitelist-domain=.gokcloud.com" \
    --set "extraArgs[13]=--oidc-groups-claim=groups" \
    --set "extraArgs[14]=--user-id-claim=sub" \
    --set "extraArgs[15]=--cookie-domain=.gokcloud.com" \
    --set "extraArgs[16]=--cookie-secure=true" \
    --set "extraArgs[17]=--pass-access-token=true" \
    --set "extraArgs[18]=--pass-authorization-header=true" \
    --set "extraArgs[19]=--oidc-issuer-url=${OAUTH2_OIDC_ISSUE_URL}" \
    --set "extraArgs[20]=--standard-logging=true" \
    --set "extraArgs[21]=--auth-logging=true" \
    --set "extraArgs[22]=--request-logging=true" \
    --set "extraArgs[23]=--cookie-refresh=1h" \
    --set "extraArgs[24]=--cookie-expire=8h" \
    --set "extraArgs[25]=--set-xauthrequest=true" \
    --set "extraArgs[26]=--skip-jwt-bearer-tokens=true" \
    --values "${MOUNT_PATH}"/kubernetes/install_k8s/oauth2-proxy/values.yaml \
    --create-namespace
  gok patch ingress oauth2proxy-oauth2-proxy oauth2 letsencrypt $(defaultSubdomain)
  waitForServiceAvailable oauth2
}

waitForServiceAvailable(){
  NS=$1
  echo "Waiting for services to be up!!!!"
  kubectl --timeout=120s wait --for=condition=Ready pods --all --namespace "$NS"
  [[ $? -eq 0 ]] && echoSuccess "Service is now up!" || echoFailed "Service timed-out, please check!!"
}

resolveDns() {
  local domain=$1
  if [ -z "$domain" ]; then
    echo "Usage: resolveDns <domain>"
    return 1
  fi

  kubectl run -it --rm dnsutils --image=busybox:1.28 --restart=Never -- nslookup "$domain"
}

ttydReset() {
  helm uninstall ttyd -n ttyd
  kubectl delete ns ttyd
  echo "ttyd has been uninstalled and the namespace deleted."
}

ttydInst() {
  # Create namespace for ttyd
  kubectl create namespace ttyd || echo "Namespace ttyd already exists"

  # Add Helm repo if needed (ttyd uses Docker image directly, so no repo needed)
  # Prepare a minimal values.yaml if you want customization

  # Install ttyd using a local chart directory or a published chart
  helm install ttyd ${MOUNT_PATH}/kubernetes/install_k8s/ttyd/chart \
    --namespace ttyd \
    --set ingress.enabled=true \
    --set ingress.host="ttyd.$(rootDomain)"

  gok patch ingress ttyd ttyd letsencrypt ttyd
  patchTtydWithOauth

  echo "Waiting for ttyd to be ready..."
  kubectl --namespace ttyd wait --for=condition=Ready pods --all --timeout=120s
  if [[ $? -eq 0 ]]; then
    echoSuccess "ttyd is now up! Access it at: https://ttyd.$(rootDomain)"
  else
    echoFailed "ttyd setup timed out. Please check the logs."
    return 1
  fi
}


enableJenkins(){
  USERNAME=$(promptUserInput "Enter Jenkins Username (skmaji1): " "skmaji1")
  PASSWORD=$(promptSecret "Enter Jenkins Password: ")

  docker exec -it halyard hal config ci jenkins enable

  docker exec -it halyard hal config ci jenkins master add gok-jenkins-master \
    --address https://jenkins.gokcloud.com \
    --username ${USERNAME} \
    --password ${PASSWORD}

  docker exec -it halyard hal config ci jenkins master edit gok-jenkins-master --csrf true

  docker exec -it halyard hal deploy apply
}

spinnakerInst(){
  CLIENT_SECRET=$(dataFromSecret oauth-secrets kube-system OIDC_CLIENT_SECRET)
  CLIENT_ID=$(dataFromSecret oauth-secrets kube-system OIDC_CLIENT_ID)
  OIDC_ISSUE_URL=$(dataFromSecret oauth-secrets kube-system OIDC_ISSUE_URL)

  useradd -m -g root spinnaker
  mkdir -p /home/spinnaker/.hal && chmod -R 777 /home/spinnaker/.hal
  mkdir -p /home/spinnaker/.kube/ && cp ~/.kube/config /home/spinnaker/.kube/config && chmod 755 /home/spinnaker/.kube/config
  docker run --name halyard -v /home/spinnaker/.hal:/home/spinnaker/.hal -v /home/spinnaker/.kube/config:/home/spinnaker/.kube/config \
    -d gcr.io/spinnaker-marketplace/halyard:stable

  echo "Waiting for the service to be up for 30 Seconds"
  sleep 30
  docker exec -it halyard kubectl cluster-info
  docker exec -it halyard hal config provider kubernetes enable
  docker exec -it halyard hal config provider kubernetes account add gok-k8s --provider-version v2 \
    --context $(kubectl config current-context)
  docker exec -it halyard hal config features edit --artifacts true
  docker exec -it halyard hal config deploy edit --type distributed --account-name gok-k8s
  kubectl create ns spinnaker
  helm repo add stable https://charts.helm.sh/stable
  helm install minio --namespace spinnaker --set accessKey="myaccesskey" --set secretKey="mysecretkey" \
    --set persistence.enabled=false stable/minio
  docker exec -it halyard bash -c 'mkdir /home/spinnaker/.hal/default/profiles'
  docker exec -it halyard echo "spinnaker.s3.versioning: false" > /home/spinnaker/.hal/default/profiles/front50-local.yml

  cat <<EOF > /home/spinnaker/.hal/default/profiles/front50-local.yml
kubernetes:
  volumes:
  - id: internal-trust-store
    mountPath: /etc/ssl/certs/java
    type: secret
EOF

  ##https://github.com/spinnaker/spinnaker/issues/6498
  cat <<EOF > /home/spinnaker/.hal/permission.yml
users:
 - username: skmaji1
   roles:
   - admin
EOF

  cat <<EOF > /home/spinnaker/.hal/default/profiles/spinnaker-local.yml
logging:
  level:
    # Enable debug logging by changing level to DEBUG
    root: TRACE  # default
EOF

  #https://stackoverflow.com/questions/78087469/how-to-apply-custom-profiles-setting-to-spinnaker-to-make-it-deploy-with-one-com
  cat <<EOF > /home/spinnaker/.hal/default/profiles/fiat-local.yml
fiat.restrictApplicationCreation: true #Allows to restrict permissions
auth.permissions.provider.application: aggregate
auth.permissions.source.application.prefix: #Allows to work with
  enabled: true                          # applications prefixes
  prefixes:
  - prefix: "*" # All applications
    permissions:
      READ:
      - "administrators"
      - "admin"
      WRITE:
      - "administrators"
      - "admin"
      EXECUTE:
      - "administrators"
      - "admin"
      CREATE:
      - "administrators"
      - "admin"
EOF

  docker exec -it halyard hal config storage s3 edit --endpoint http://minio:9000 --access-key-id "myaccesskey" --secret-access-key "mysecretkey"
  docker exec -it halyard hal config storage s3 edit --path-style-access true
  docker exec -it halyard hal config storage edit --type s3
  docker exec -it halyard hal version list
  docker exec -it halyard hal config version edit --version 1.34.2

  cat <<EOF > /home/spinnaker/.hal/default/profiles/gate-local.yml
kubernetes:
  volumes:
  - id: internal-trust-store
    mountPath: /etc/ssl/certs
    type: secret
server:
  tomcat:
    protocolHeader: X-Forwarded-Proto
    remoteIpHeader: X-Forwarded-For
    internalProxies: .*
    httpsServerPort: X-Forwarded-Port
security:
  oauth2:
    enabled: true
    client:
      clientId: gok-developers-client
      clientSecret: $CLIENT_SECRET
      userAuthorizationUri: $OIDC_ISSUE_URL/protocol/openid-connect/auth
      accessTokenUri: $OIDC_ISSUE_URL/protocol/openid-connect/token
      scope: openid,email,profile,groups
      preEstablishedRedirectUri: https://spin-gate.gokcloud.com/login
      clientAuthenticationScheme: form
    resource:
      userInfoUri: $OIDC_ISSUE_URL/protocol/openid-connect/userinfo
    userInfoMapping:
      email: email
      firstName: given_name
      lastName: family_name
      username: preferred_username
      roles: groups
EOF

  cat <<EOF | kubectl apply -f -
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  annotations:
    kubernetes.io/ingress.allow-http: "false"
    nginx.ingress.kubernetes.io/ssl-redirect: "true"
    ingress.kubernetes.io/ssl-passthrough: "true"
    nginx.ingress.kubernetes.io/backend-protocol: "HTTP"
  name: spin-deck
  namespace: spinnaker
spec:
  ingressClassName: nginx
  rules:
  - host: spinnaker.cloud.com
    http:
      paths:
      - backend:
          service:
            name: spin-deck
            port:
              number: 9000
        path: /
        pathType: ImplementationSpecific
  tls:
    - secretName: appingress-certificate
      hosts:
        - spinnaker.cloud.com
---
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  annotations:
    kubernetes.io/ingress.allow-http: "false"
    nginx.ingress.kubernetes.io/ssl-redirect: "true"
    ingress.kubernetes.io/ssl-passthrough: "true"
    nginx.ingress.kubernetes.io/backend-protocol: "HTTP"
  name: spin-gate
  namespace: spinnaker
spec:
  ingressClassName: nginx
  rules:
  - host: spin-gate.cloud.com
    http:
      paths:
      - backend:
          service:
            name: spin-gate
            port:
              number: 8084
        path: /
        pathType: ImplementationSpecific
  tls:
    - secretName: appingress-certificate
      hosts:
        - spin-gate.cloud.com
EOF

  gok patch ingress spin-deck spinnaker letsencrypt spinnaker
  gok patch ingress spin-gate spinnaker letsencrypt spin-gate

  docker exec -it halyard hal config security ui edit --override-base-url "https://spinnaker.gokcloud.com"
  docker exec -it halyard hal config security api edit --override-base-url "https://spin-gate.gokcloud.com"
  docker exec -it halyard hal config security api edit --cors-access-pattern "https://spinnaker.gokcloud.com"

  #Authentication
  docker exec -it halyard hal config security authn oauth2 edit --client-id $CLIENT_ID --client-secret $CLIENT_SECRET --provider OTHER --pre-established-redirect-uri https://spin-gate.gokcloud.com/login
  docker exec -it halyard hal config security authn oauth2 enable

  #Authorization
  #Roles will be fetched from keycloak
  #https://github.com/spinnaker/spinnaker/issues/6498
  #docker exec -it halyard hal config security authz file edit --file-path /home/spinnaker/.hal/permission.yml
  docker exec -it halyard hal config security authz enable

  docker exec -it halyard hal config artifact github account add sumitmaji

  kubectl get secrets -n cert-manager gokselfsign-ca -o json | jq -r '.data["tls.crt"]' | base64 -d > /home/spinnaker/.hal/issuer.crt
  kubectl get secrets -n keycloak keycloak-gokcloud-com -o json | jq -r '.data["tls.crt"]' | base64 -d > /home/spinnaker/.hal/keycloak.crt
  kubectl get secrets -n jenkins jenkins-gokcloud-com -o json | jq -r '.data["tls.crt"]' | base64 -d > /home/spinnaker/.hal/jenkins.crt
  
  docker exec -it halyard  cp /etc/ssl/certs/java/cacerts /home/spinnaker/.hal/
  chmod +w /home/spinnaker/.hal/cacerts
  #password is changeit
  docker exec -it halyard keytool -import -noprompt -trustcacerts -alias ca -file /home/spinnaker/.hal/issuer.crt -keystore /home/spinnaker/.hal/cacerts
  docker exec -it halyard keytool -import -noprompt -trustcacerts -alias keycloak -file /home/spinnaker/.hal/keycloak.crt -keystore /home/spinnaker/.hal/cacerts
  docker exec -it halyard keytool -import -noprompt -trustcacerts -alias jenkins -file /home/spinnaker/.hal/jenkins.crt -keystore /home/spinnaker/.hal/cacerts

  kubectl create secret generic -n spinnaker internal-trust-store \
  	--from-file /home/spinnaker/.hal/cacerts

  #https://spinnaker.io/docs/reference/halyard/custom/
  mkdir -p /home/spinnaker/.hal/default/service-settings/
  cat <<EOF > /home/spinnaker/.hal/default/service-settings/gate.yml
kubernetes:
  volumes:
  - id: internal-trust-store
    mountPath: /etc/ssl/certs/java
    type: secret
EOF

  cat <<EOF > /home/spinnaker/.hal/default/service-settings/igor.yml
kubernetes:
  volumes:
  - id: internal-trust-store
    mountPath: /etc/ssl/certs/java
    type: secret
EOF

  docker exec -it halyard hal config artifact github enable
  docker exec -it halyard hal config artifact github account add gok-github

  docker exec -it halyard hal config artifact helm enable
  docker exec -it halyard hal config artifact helm account add gok-helm \
    --repository https://chart.gokcloud.com --username sumit --password abcdef


  docker exec -it halyard hal deploy apply
}

rabbitmqReset(){
  kubectl delete all --all -n rabbitmq
  kubectl delete ns rabbitmq
  emptyLocalFsStorage "RabbitMQ" "rabbitmq-pv" "rabbitmq-storage" "/data/volumes/rabbitmq" "rabbitmq"
}

rabbitmqInst(){
  helm repo add bitnami https://charts.bitnami.com/bitnami
  helm repo update

  kubectl create namespace rabbitmq || true
  createLocalStorageClassAndPV "rabbitmq-storage" "rabbitmq-pv" "/data/volumes/rabbitmq"
  
  helm install rabbitmq bitnami/rabbitmq \
    --namespace rabbitmq \
    --create-namespace \
    --values "${MOUNT_PATH}"/kubernetes/install_k8s/rabbitmq/values.yaml

  kubectl get secret --namespace rabbitmq rabbitmq -o jsonpath="{.data.rabbitmq-password}" | base64 --decode
  echo
}


argocdReset(){
  helm uninstall argocd -n argocd
  kubectl delete ns argocd
  # emptyLocalFsStorage "ArgoCD" "argocd-pv" "argocd-storage" "/data/volumes/argocd" "argocd"
}

argocdInst(){
  CLIENT_SECRET=$(dataFromSecret oauth-secrets kube-system OIDC_CLIENT_SECRET)
  CLIENT_ID=$(dataFromSecret oauth-secrets kube-system OIDC_CLIENT_ID)
  REALM_NAME=$(dataFromSecret oauth-secrets kube-system OAUTH_REALM)
  OAUTH2_HOST="$(fullKeycloakUrl)"
  OAUTH2_OIDC_ISSUE_URL="$(dataFromSecret oauth-secrets kube-system OIDC_ISSUE_URL)"

  kubectl create ns argocd
  helm repo add argo https://argoproj.github.io/argo-helm
  helm repo update
  helm install argocd argo/argo-cd --namespace argocd \
    --set configs.secret.extra."oidc\.keycloak\.clientSecret"="${CLIENT_SECRET}" \
    --set configs.secret.argocdServerAdminPassword="sumit" \
    --values "${MOUNT_PATH}"/kubernetes/install_k8s/argocd/values.yaml

  gok patch ingress argocd-server argocd letsencrypt $(argocdSubdomain)
 
  #kubectl create secret generic argocd-secret -n argocd \
  #--from-literal=oidc.clientId=$CLIENT_ID \
  #--from-literal=oidc.clientSecret=$CLIENT_SECRET \
  #--from-literal=oidc.issuerUrl=$OAUTH2_HOST \
  #--from-literal=oidc.scopes="openid email profile groups" \
  #--from-literal=oidc.usernameClaim=email \
  #--from-literal=oidc.groupsClaim=groups
}

getUserInfo(){
  curl https://keycloak.gokcloud.com/realms/GokDevelopers/protocol/openid-connect/auth?client_id=gok-developers-client&redirect_uri=https://localhost/login&response_type=code&scope=openid%20email%20profile&state=eLUwT2

  TOKEN=$(curl --location --request POST 'https://keycloak.gokcloud.com/realms/GokDevelopers/protocol/openid-connect/token' \
     --header 'Content-Type: application/x-www-form-urlencoded' \
     --data-urlencode 'grant_type=authorization_code' \
     --data-urlencode 'client_id=gok-developers-client' \
     --data-urlencode 'client_secret=sC1hHm9c2qHjwYtfumcQnEwyH7NOqkaV' \
     --data-urlencode 'redirect_uri=https://localhost/login' \
     --data-urlencode 'scope=roles email profile openid' \
     --data-urlencode 'code=2a34daeb-4bba-48fb-9531-69f55104ab62.69c4c286-f2c9-4455-8368-7a8723ad60cf.f79ef798-e122-403f-8626-ad3793bf3f44' | jq -r '.["access_token"]')

  curl -kv 'https://keycloak.gokcloud.com/realms/GokDevelopers/protocol/openid-connect/userinfo' \
  --header 'Content-Type: application/x-www-form-urlencoded' \
  --header "Authorization: Bearer $TOKEN"

}

spinnakerReset(){
  kubectl delete all --all -n spinnaker
  kubectl delete ns spinnaker
  docker stop halyard
  docker rm halyard
  rm -rf /home/spinnaker/.hal
  rm -rf /home/spinnaker/.kube
}

patchLocalTls() {
  NAME=$1
  NS=$2
  kubectl patch ing "$NAME" --patch "$(
    cat <<EOF
spec:
  tls:
    - hosts:
        - $APP_HOST
      secretName: appingress-certificate
EOF
  )" -n "$NS"
  kubectl patch ing "$NAME" --type=json -p='[{"op": "replace", "path": "/spec/rules/0/host", "value":"master.cloud.com"}]' -n "$NS"
  kubectl patch ing "$NAME" --type=json -p='[{"op": "add", "path": "/metadata/annotations", "value":{"nginx.ingress.kubernetes.io/rewrite-target": "/", "kubernetes.io/ingress.class": "nginx"}}]' -n "$NS"
}

#Enable debug logs for Api Server and Kubelet
debugLog(){
  kcd default
  cat << EOT | kubectl apply -f -
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: edit-debug-flags-v
rules:
- apiGroups:
  - ""
  resources:
  - nodes/proxy
  verbs:
  - update
- nonResourceURLs:
  - /debug/flags/v
  verbs:
  - put
EOT

  cat << EOT | kubectl apply -f -
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: edit-debug-flags-v
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: edit-debug-flags-v
subjects:
- kind: ServiceAccount
  name: default
  namespace: default
EOT
  TOKEN=$(kubectl create token default)

  # Change the log level to 5 (trace) for Api Server
  APISERVER=$(apiUrl)
  curl -s -X PUT -d '5' $APISERVER/debug/flags/v --header "Authorization: Bearer $TOKEN" -k

  # Change the log level to 5 (trace) for Kubelet : Currently not working
  docker exec kind-control-plane curl -s -X PUT -d '5' https://localhost:10250/debug/flags/v --header "Authorization: Bearer $TOKEN" -k

}

kubeloginInst(){
  useradd -m -g root linuxbrew
  cd /home/linuxbrew
  su - linuxbrew
  /bin/bash -c "$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)"
  echo 'eval "$(/home/linuxbrew/.linuxbrew/bin/brew shellenv)"' >> /home/linuxbrew/.profile
  eval "$(/home/linuxbrew/.linuxbrew/bin/brew shellenv)"
  brew install kubelogin
  echo 'export PATH="/home/linuxbrew/.linuxbrew/bin:$PATH"' >> /home/linuxbrew/.profile
  echo 'export PATH="/home/linuxbrew/.linuxbrew/sbin:$PATH"' >> /home/linuxbrew/.profile
  brew doctor
  brew install Azure/kubelogin/kubelogin
  # upgrade
  brew update
  brew upgrade Azure/kubelogin/kubelogin
}

kyvernoInst(){
  helm repo add kyverno https://kyverno.github.io/kyverno/
  helm repo update
  helm install kyverno --namespace kyverno kyverno/kyverno --create-namespace
  waitForServiceAvailable kyverno
  sleep 10

  #grants the cluster-admin role to two specific ServiceAccounts
cat <<EOF | kubectl apply -f -
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: kyverno:cloud-cluster-admin
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: cluster-admin
subjects:
- kind: ServiceAccount
  name: kyverno-admission-controller
  namespace: kyverno
- kind: ServiceAccount
  name: kyverno-background-controller
  namespace: kyverno
EOF


cat <<EOF | kubectl apply -f -
apiVersion: kyverno.io/v1
kind: ClusterPolicy
metadata:
  name: sync-secrets
spec:
  rules:
  - name: sync-image-pull-secret
    match:
      any:
      - resources:
          kinds:
          - Namespace
    generate:
      apiVersion: v1
      kind: Secret
      name: regcred
      namespace: "{{request.object.metadata.name}}"
      synchronize: true
      clone:
        namespace: kube-system
        name: regcred
  - name: sync-oauth-secrets
    match:
      any:
      - resources:
          kinds:
          - Namespace
    generate:
      apiVersion: v1
      kind: Secret
      name: oauth-secrets
      namespace: "{{request.object.metadata.name}}"
      synchronize: true
      clone:
        namespace: kube-system
        name: oauth-secrets
  - name: sync-opensearch-secrets
    match:
      any:
      - resources:
          kinds:
          - Namespace
    generate:
      apiVersion: v1
      kind: Secret
      name: opensearch-secrets
      namespace: "{{request.object.metadata.name}}"
      synchronize: true
      clone:
        namespace: kube-system
        name: opensearch-secrets
EOF
}

kyvernoReset(){
  helm uninstall kyverno -n kyverno
  kubectl delete ns kyverno
  kubectl delete clusterpolicy --all
}

help(){
  IFS='' read -r -d '' HELP <<"EOF"
gok install kubernetes => Installing Kubernetes
gok install ingress => Installing Ingress
gok install dashboard => Installing Dashboard
gok install cert-manager => Installing Cert-Manager
gok install keycloak => Installing Keycloak
gok install registry => Install registry
python3 keycloak-setup.py all => Setup keycloak
helmTemplateDebug $char_name $char_repo = Debug helm template
apiUrl => view url for kubernetes api server
oauthUser => Configure Kubeconfig for OAuth
oauthDev => Create developers role for oauth
oauthAdmin => Create administrators role for oauth

kcd $namespace => switch namespace
k => short form for kubectl
current => Show current namespace
pods => show pods in current namespace
secrets => show secrets in current namespace
viewcert => View certificate in secret in current namespace
decode => Base64 decode content in secret in current namespace
ns => Show all namespaces
edit $resouce => Edit resource in current namespace
logs => View logs of pod in current namespace
bash => Get terminal in pod
all => Show all resource in current namespace
kctl => Execute kubectl command with oauth authentication, create dev and admin role
EOF
echo "$HELP"
}

function bashCmd() {
  pod=$(getpod)
  echo "Opening terminal on $pod"
  kubectl exec -it "$pod" -- /bin/bash
}

function helpCmd() {
    echo "Available commands:"
    echo "1. gok install <component>: Install a component. Supported components are docker, cert-manager, monitoring, dashboard, keycloak, ingress, registry, fluentd, opensearch, spinnaker, oauth2, kubernetes-worker, and kubernetes."
    echo "2. gok start <component>: Start a component. Supported components are kubernetes, proxy, and kubelet."
    echo "3. gok reset <component>: Reset a component. Supported components are kubernetes, monitoring, dashboard, keycloak, ingress, registry, fluentd, opensearch, spinnaker, oauth2, and cert-manager."
    echo "4. gok deploy <component>: Deploy a component. Currently, only app1 is supported."
    echo "5. gok patch <resource> <name> <namespace> <options> <subdomain>: Patch resources. Currently, only ingress resource is supported. The options can be letsencrypt, ldap, or localtls."
    echo "6. gok create <resource> <name> <additional>: Create resources. Currently, secret and certificate resources are supported. For secret, only apphost is supported. For certificate, it creates a certificate request for a namespace."
    echo "7. gok bash: Open a terminal in a pod."
    echo "8. gok desc: Describe a pod."
    echo "9. gok logs: Show the logs of a pod."
    echo "10. gok status: Show the status of a helm release."
    echo "11. gok taint-node: Taint a node."
    echo "12. gok help: Show this help document."
}

function descCmd() {
  pod=$(getpod)
  echo "Describing pod $pod"
  kubectl describe po "$pod"
}

function logsCmd() {
  pod=$(getpod)
  echo "Viewing logs of pod $pod"
  kubectl logs "$pod"
}

function statusCmd() {
  helm status "$release"
}

function taintNodeCmd() {
  taintNode
}

collectUserInputs(){
  INPUT_FILE="/root/base_services_inputs"
  
  echo "# Base Services Installation Inputs" > "$INPUT_FILE"

  # Collect Docker credentials
  echo "=== Docker Registry Credentials ==="
  DOCKER_USERNAME=$(promptUserInput "Please enter docker user id: ")
  DOCKER_PASSWORD=$(promptSecret "Please enter your docker password: ")
  {
    echo "export DOCKER_USERNAME='$DOCKER_USERNAME'"
    echo "export DOCKER_PASSWORD='$DOCKER_PASSWORD'"
  } >> "$INPUT_FILE"
  
  # Collect inputs for LDAP
  echo "=== LDAP Configuration ==="
  LDAP_PASSWORD=$(promptSecret "Please enter LDAP password for admin: ")
  KERBEROS_PASSWORD=$(promptSecret "Please enter Kerberos password: ")
  KERBEROS_KDC_PASSWORD=$(promptSecret "Please enter Kerberos kdc password: ")
  KERBEROS_ADM_PASSWORD=$(promptSecret "Please enter Kerberos adm password: ")

  {
    echo "export LDAP_PASSWORD='$LDAP_PASSWORD'"
    echo "export KERBEROS_PASSWORD='$KERBEROS_PASSWORD'"
    echo "export KERBEROS_KDC_PASSWORD='$KERBEROS_KDC_PASSWORD'"
    echo "export KERBEROS_ADM_PASSWORD='$KERBEROS_ADM_PASSWORD'"
  } >> "$INPUT_FILE"

  # Collect inputs for Keycloak
  echo "=== Keycloak Configuration ==="
  KEYCLOAK_ADMIN_USERNAME=$(promptUserInput "Please enter keycloak admin username (admin): " "admin")
  KEYCLOAK_ADMIN_PASSWORD=$(promptSecret "Please enter keycloak admin password: ")
  echo "export KEYCLOAK_ADMIN_USERNAME='$KEYCLOAK_ADMIN_USERNAME'" >> "$INPUT_FILE"
  echo "export KEYCLOAK_ADMIN_PASSWORD='$KEYCLOAK_ADMIN_PASSWORD'" >> "$INPUT_FILE"

  # Collect inputs for PostgreSQL
  POSTGRESQL_USERNAME=$(promptUserInput "Please enter postgresql username (postgres): " "postgres")
  POSTGRESQL_PASSWORD=$(promptSecret "Please enter postgresql password: ")
  echo "export POSTGRESQL_USERNAME='$POSTGRESQL_USERNAME'" >> "$INPUT_FILE"
  echo "export POSTGRESQL_PASSWORD='$POSTGRESQL_PASSWORD'" >> "$INPUT_FILE"

  # Collect inputs for OIDC
  OIDC_CLIENT_ID=$(promptUserInput "Please enter OIDC client id (${OIDC_CLIENT_ID}): " "${OIDC_CLIENT_ID}")
  REALM=$(promptUserInput "Please enter realm name (${REALM}): " "${REALM}")
  echo "export OIDC_CLIENT_ID='$OIDC_CLIENT_ID'" >> "$INPUT_FILE"
  echo "export REALM='$REALM'" >> "$INPUT_FILE"
  
  SAMPLE_USER_PASSWORD=$(promptSecret "Please enter sample user password (for user: skmaji1): ")
  echo "export SAMPLE_USER_PASSWORD='$SAMPLE_USER_PASSWORD'" >> "$INPUT_FILE"

  # Collect OAuth2 client secret
  ACTIVE_PROFILE=$(promptUserInput "Enter Active Profile (keycloak): " "keycloak")
  OIDC_ISSUE_URL=$(promptUserInput "Enter OIDC Issue URL (https://keycloak.gokcloud.com/realms/$REALM): " "https://keycloak.gokcloud.com/realms/$REALM")
  OIDC_USERNAME_CLAIM=$(promptUserInput "Enter OIDC Username Claim (${OIDC_USERNAME_CLAIM}): " "${OIDC_USERNAME_CLAIM}")
  OIDC_GROUPS_CLAIM=$(promptUserInput "Enter OIDC Groups Claim (${OIDC_GROUPS_CLAIM}): " "${OIDC_GROUPS_CLAIM}")
  AUTH0_DOMAIN=$(promptUserInput "Enter Auth0 Domain (${AUTH0_DOMAIN}): " "${AUTH0_DOMAIN}")
  APP_HOST=$(promptUserInput "Enter App Host (${APP_HOST}): " "${APP_HOST}")
  JWKS_URL=$(promptUserInput "Enter JWKS URL (${JWKS_URL}): " "${JWKS_URL}")
  {
    echo "export ACTIVE_PROFILE='$ACTIVE_PROFILE'"
    echo "export OIDC_ISSUE_URL='$OIDC_ISSUE_URL'"
    echo "export OIDC_USERNAME_CLAIM='$OIDC_USERNAME_CLAIM'"
    echo "export OIDC_GROUPS_CLAIM='$OIDC_GROUPS_CLAIM'"
    echo "export AUTH0_DOMAIN='$AUTH0_DOMAIN'"
    echo "export APP_HOST='$APP_HOST'"
    echo "JWKS_URL='$JWKS_URL'"
  } >> "$INPUT_FILE"
  
  # Collect Vault inputs if needed
  # Add other inputs as needed...
  
  echo "All inputs collected and saved to $INPUT_FILE"
}

function installBaseServices(){
  STATE_FILE="/root/base_services_install_state"
  INPUT_FILE="/root/base_services_inputs"
  
  # Check current state
  if [ -f "$STATE_FILE" ]; then
    STATE=$(cat "$STATE_FILE")
    case "$STATE" in
      "post_reboot")
        echo "Continuing base services installation after reboot..."
        postRebootBaseServices
        return
        ;;
      "completed")
        echo "Base services installation already completed"
        return
        ;;
    esac
  fi
  
  # Pre-collect all user inputs
  echo "Collecting required inputs for base services installation..."
  collectUserInputs
  
  # Pre-reboot services
  echo "Installing pre-reboot base services..."
  echo "starting_install" > "$STATE_FILE"
  
  gok install ingress
  gok install cert-manager
  
  # Check if reboot is needed
  if [[ "$CERTMANAGER_CHALANGE_TYPE" == "selfsigned" ]]; then
    echo "post_reboot" > "$STATE_FILE"
    
    # Create a separate script file instead of inline bash
    cat > /root/post-reboot-script.sh <<'SCRIPT_EOF'
#!/bin/bash
# Wait for Kubernetes API to be available before proceeding
export MOUNT_PATH=/root
source $MOUNT_PATH/kubernetes/install_k8s/util
source $MOUNT_PATH/kubernetes/install_k8s/gok
export KUBECONFIG=/root/.kube/config

for i in {1..30}; do
  if kubectl get nodes &>/dev/null; then
    echo "Kubernetes is up and running."
    break
  else
    kubectl get nodes
    echo "Waiting for Kubernetes API to be available... ($i/30)"
    sleep 10
  fi
done

if ! kubectl get nodes &>/dev/null; then
  echo "Kubernetes API is not available after waiting. Exiting."
  exit 1
fi

installBaseServices
SCRIPT_EOF
    
    chmod +x /root/post-reboot-script.sh
    
    # Create systemd service for post-reboot continuation
    cat > /etc/systemd/system/base-services-post-reboot-1.service <<EOF
[Unit]
Description=Base Services Post Reboot Setup
After=multi-user.target

[Service]
Type=simple
ExecStart=/root/post-reboot-script.sh
Restart=no
StandardOutput=journal
StandardError=journal

[Install]
WantedBy=multi-user.target
EOF

    systemctl enable base-services-post-reboot-1.service
    
    echoWarning "Self-signed certificate added to root ca, rebooting system for it to take effect"
    sudo reboot
  else
    postRebootBaseServices
  fi
}

postRebootBaseServices(){
  echo "Installing post-reboot base services..."
  
  # Load saved inputs
  INPUT_FILE="/root/base_services_inputs"
  if [ -f "$INPUT_FILE" ]; then
    source "$INPUT_FILE"
    echo "Loaded saved user inputs"
  else
    echoFailed "Input file not found! Cannot continue without user inputs."
    return 1
  fi
  
  # Clean up systemd service and script
  if [ -f "/etc/systemd/system/base-services-post-reboot-1.service" ]; then
    systemctl disable base-services-post-reboot-1.service
    rm -f /etc/systemd/system/base-services-post-reboot-1.service
    rm -f /root/post-reboot-script.sh
    systemctl daemon-reload
  fi
  
  # Install services using saved inputs
  gok install kyverno
  gok install registry
  gok install base
  gok install ldap
  gok install keycloak
  gok install oauth2
  gok install rabibtmq
  gok install vault
  
  # Clean up input file
  rm -f "$INPUT_FILE"
  
  echo "completed" > "/root/base_services_install_state"
  echoSuccess "Base services installation completed."
}

function resetBaseServices(){
  echo "Resetting base services..."
  echo "This will reset the following components: vault, rabbitmq, oauth2, keycloak, ldap, registry, kyverno, cert-manager, ingress."
  sleep 5
  gok reset vault
  echoSuccess "Vault reset completed."
  gok reset rabbitmq
  echoSuccess "RabbitMQ reset completed."
  gok reset oauth2
  echoSuccess "OAuth2 reset completed."
  gok reset keycloak
  echoSuccess "Keycloak reset completed."
  gok reset ldap
  echoSuccess "LDAP reset completed."
  gok reset registry
  echoSuccess "Registry reset completed."
  gok reset kyverno
  echoSuccess "Kyverno reset completed."
  gok reset cert-manager
  echoSuccess "Cert-Manager reset completed."
  gok reset ingress
  echoSuccess "Ingress reset completed."
}


function installCmd() {
  updateSys
  installDeps
  COMPONENT=$1
  if [ "$COMPONENT" == "docker" ]; then
    dockrInst
  elif [ "$COMPONENT" == "cert-manager" ]; then
    certmanagerInst
    setupCertiIssuers
  elif [ "$COMPONENT" == "monitoring" ]; then
    installPrometheusGrafanaWithCertMgr
  elif [ "$COMPONENT" == "ldap" ]; then
    installLdap
  elif [ "$COMPONENT" == "dashboard" ]; then
    installDashboardwithCertManager
  elif [ "$COMPONENT" == "jupyter" ]; then
    jupyterHubInst
  elif [ "$COMPONENT" == "devworkspace" ]; then
    createDevWorkspace
  elif [ "$COMPONENT" == "che" ]; then
    eclipseCheInst
  elif [ "$COMPONENT" == "ttyd" ]; then
    ttydInst
  elif [ "$COMPONENT" == "cloudshell" ]; then
    cloudshellInst
  elif [ "$COMPONENT" == "console" ]; then
    consoleInst
  elif [ "$COMPONENT" == "argocd" ]; then
    argocdInst
  elif [ "$COMPONENT" == "gok-agent" ]; then
    gokAgentInstall
  elif [ "$COMPONENT" == "gok-controller" ]; then
    gokControllerInstall
  elif [ "$COMPONENT" == "controller" ]; then
    gok install gok-agent
    gok install gok-controller
  elif [ "$COMPONENT" == "chart" ]; then
    chartInst
  elif [ "$COMPONENT" == "rabbitmq" ]; then
    rabbitmqInst
  elif [ "$COMPONENT" == "vault" ]; then
    vaultInstall
  elif [ "$COMPONENT" == "keycloak" ]; then
    installKeycloakWithCertMgr
  elif [ "$COMPONENT" == "ingress" ]; then
    ingressInst
  elif [ "$COMPONENT" == "registry" ]; then
    installRegistryWithCertMgr
  elif [ "$COMPONENT" == "fluentd" ]; then
    fluentdInst
  elif [ "$COMPONENT" == "opensearch" ]; then
    opensearchInst
    opensearchDashInst
  elif [ "$COMPONENT" == "jenkins" ]; then
    jenkinsInst
  elif [ "$COMPONENT" == "spinnaker" ]; then
    spinnakerInst
  elif [ "$COMPONENT" == "oauth2" ]; then
    oauth2ProxyInst
  elif [ "$COMPONENT" == "istio" ]; then
    istioInst
  elif [ "$COMPONENT" == "kyverno" ]; then
    kyvernoInst
  elif [ "$COMPONENT" == "base" ]; then
    baseInst
  elif [ "$COMPONENT" == "kubernetes-worker" ]; then
    dockrInst
    k8sInst "kubernetes-worker"
  elif [ "$COMPONENT" == "base-services" ]; then
    installBaseServices
  elif [ "$COMPONENT" == "kubernetes" ]; then
    dockrInst
    haInst
    k8sInst "kubernetes"
    helmInst
    taintNode
    calicoInst
    waitForServiceAvailable kube-system
    dnsUtils
    customDns
    kcurl
    oauthAdmin
    echo "source $MOUNT_PATH/kubernetes/install_k8s/util" >> /etc/bash.bashrc
    echo "source $MOUNT_PATH/kubernetes/install_k8s/gok" >> /etc/bash.bashrc
    ln -s $MOUNT_PATH/kubernetes/install_k8s/gok /bin/gok
    cat <<EOF > /etc/rc.local
#!/bin/bash
/bin/gok start kubernetes
exit 0
EOF
    chmod +x /etc/rc.local
    systemctl enable rc-local
    source /etc/bash.bashrc
    figlet "Please wait for 1 minute"
  fi
}

function startCmd() {
  COMPONENT=$1
  if [ "$COMPONENT" == "kubernetes" ]; then
    disableSwap
    startHa
    startKubelet
  elif [ "$COMPONENT" == "proxy" ]; then
    startHa
  elif [ "$COMPONET" == "kubelet" ]; then
    startKubelet
  fi
}

function resetCmd() {
  COMPONENT=$1
  if [ "$COMPONENT" == "kubernetes" ]; then
    kubeadm reset <<EOF
y
EOF
  elif [ "$COMPONENT" == "monitoring" ]; then
    prometheusGrafanaResetv2

    emptyLocalFsStorage "Monitoring" "prometheus-pv" "prometheus-storage" "/data/volumes/pv1"
    emptyLocalFsStorage "Monitoring" "alertmanager-pv" "alertmanager-storage" "/data/volumes/pv2"
  elif [ "$COMPONENT" == "dashboard" ]; then
    dashboardReset
  elif [ "$COMPONENT" == "keycloak" ]; then
    keycloakReset
  elif [ "$COMPONENT" == "vault" ]; then
    vaultReset
  elif [ "$COMPONENT" == "ingress" ]; then
    ingressUnInst
  elif [ "$COMPONENT" == "chart" ]; then
    resetChart
  elif [ "$COMPONENT" == "ldap" ]; then
    ldapReset
  elif [ "$COMPONENT" == "gok-agent" ]; then
    gokAgentReset
  elif [ "$COMPONENT" == "gok-controller" ]; then
    gokControllerReset
  elif [ "$COMPONENT" == "controller" ]; then
    gok reset gok-agent
    gok reset gok-controller
  elif [ "$COMPONENT" == "argocd" ]; then
    argocdReset
  elif [ "$COMPONENT" == "devworkspace" ]; then
    deleteDevWorkspace
  elif [ "$COMPONENT" == "registry" ]; then
    resetDockerRegistry
  elif [ "$COMPONENT" == "fluentd" ]; then
    fluentdReset
  elif [ "$COMPONENT" == "jupyter" ]; then
    jupyterHubReset
  elif [ "$COMPONENT" == "rabbitmq" ]; then
    rabbitmqReset
  elif [ "$COMPONENT" == "che" ]; then
    resetEclipseChe
  elif [ "$COMPONENT" == "ttyd" ]; then
    ttydReset
  elif [ "$COMPONENT" == "cloudshell" ]; then
    cloudshellReset
  elif [ "$COMPONENT" == "console" ]; then
    consoleReset
  elif [ "$COMPONENT" == "opensearch" ]; then
    opensearchDashReset
    opensearchReset
  elif [ "$COMPONENT" == "jenkins" ]; then
    jenkinsReset
  elif [ "$COMPONENT" == "spinnaker" ]; then
    spinnakerReset
  elif [ "$COMPONENT" == "oauth2" ]; then
    oauth2ProxyReset
  elif [ "$COMPONENT" == "cert-manager" ]; then
    certManagerReset
  elif [ "$COMPONENT" == "istio" ]; then
    istioReset
  elif [ "$COMPONENT" == "kyverno" ]; then
    kyvernoReset
  elif [ "$COMPONENT" == "base-services" ]; then
    resetBaseServices
  elif [ "$COMPONENT" == "kubernetes-worker" ]; then
    kubeadm reset <<EOF
y
EOF
  fi
}

function deployCmd() {
  COMPONENT=$2
  if [ "$COMPONENT" == "app1" ]; then
    createApp1
  fi
}

function patchCmd() {
  RESOURCE=$1
  NAME=$2
  NS=$3
  OPTIONS=$4
  SUBDOMAIN=$5
  if [ "$RESOURCE" == "ingress" ]; then
    if [ "$OPTIONS" == "letsencrypt" ]; then
      patchLetsEncrypt "$NAME" "$NS" "$SUBDOMAIN"
    elif [ "$OPTIONS" == "ldap" ]; then
      patchLdapSecure "$NAME" "$NS"
    elif [ "$OPTIONS" == "localtls" ]; then
      patchLocalTls "$NAME" "$NS"
    fi
  fi
}

function createCmd() {
  RESOURCE=$1
  NAME=$2
  ADDITIONAL=$3
  if [ "$RESOURCE" == "secret" ]; then
    if [ "$NAME" == "apphost" ]; then
      hostSecret
    fi
  elif [ "$RESOURCE" == "certificate" ]; then
    certificateRequestForNs "$NAME" "$ADDITIONAL"
  elif [ "$RESOURCE" == "kubeconfig" ]; then
    createKubeConfig "$NAME"
  fi
}

# Call the appropriate function based on the command
if [ "$CMD" == "bash" ]; then
  bashCmd
elif [ "$CMD" == "help" ]; then
  helpCmd
elif [ "$CMD" == "desc" ]; then
  descCmd
elif [ "$CMD" == "logs" ]; then
  logsCmd
elif [ "$CMD" == "status" ]; then
  statusCmd
elif [ "$CMD" == "taint-node" ]; then
  taintNodeCmd
elif [ "$CMD" == "install" ]; then
  installCmd "$2"
elif [ "$CMD" == "start" ]; then
  startCmd "$2"
elif [ "$CMD" == "reset" ]; then
  resetCmd "$2"
elif [ "$CMD" == "deploy" ]; then
  deployCmd "$2"
elif [ "$CMD" == "patch" ]; then
  patchCmd "$2" "$3" "$4" "$5" "$6"
elif [ "$CMD" == "create" ]; then
  createCmd "$2" "$3" "$4"
fi