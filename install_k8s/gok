#!/bin/bash

# Set default paths if not provided
: ${MOUNT_PATH:=/home/sumit/Documents/repository}
: ${WORKING_DIR:=$MOUNT_PATH/kubernetes/install_k8s}

# Source config files with error handling
if [ -f "$WORKING_DIR/config" ]; then
    source "$WORKING_DIR/config"
else
    echo "Warning: Config file not found at $WORKING_DIR/config" >&2
fi

release=$2
CMD=$1

getOAuth0Config(){
 IFS='' read -r -d '' OAUTH <<"EOF"
export OIDC_ISSUE_URL=https://skmaji.auth0.com/
export OIDC_CLIENT_ID=C3UHISO3z60iF1JLG8L7VPUSWOASrJfO
export OIDC_USERNAME_CLAIM=sub
export OIDC_GROUPS_CLAIM=http://localhost:8080/claims/groups
export AUTH0_DOMAIN=skmaji.auth0.com
export APP_HOST=kube.gokcloud.com
export JWKS_URL=$OIDC_ISSUE_URL/.well-known/jwks.json
EOF
echo "$OAUTH"
}

getKeycloakConfig(){
  # Source the centralized OAuth/OIDC configuration from keycloak/config
  source "${MOUNT_PATH}/kubernetes/install_k8s/keycloak/config"

  # Export the OAuth/OIDC configuration values
  IFS='' read -r -d '' OAUTH <<"EOF"
export OIDC_ISSUE_URL=$OIDC_ISSUE_URL
export OIDC_CLIENT_ID=$OIDC_CLIENT_ID
export OIDC_USERNAME_CLAIM=$OIDC_USERNAME_CLAIM
export OIDC_GROUPS_CLAIM=$OIDC_GROUPS_CLAIM
export REALM=$REALM
export AUTH0_DOMAIN=$AUTH0_DOMAIN
export APP_HOST=$APP_HOST
export JWKS_URL=$JWKS_URL
EOF
echo "$OAUTH"
}

# Create root_config if MOUNT_PATH is writable
if [ -w "${MOUNT_PATH}" ]; then
    cat <<EOF  > ${MOUNT_PATH}/root_config
export LETS_ENCRYPT_PROD_URL=https://acme-v02.api.letsencrypt.org/directory
export LETS_ENCRYPT_STAGING_URL=https://acme-staging-v02.api.letsencrypt.org/directory
#dns, http, selfsigned
export CERTMANAGER_CHALANGE_TYPE=selfsigned
#staging, prod
export LETS_ENCRYPT_ENV=staging
export REGISTRY=registry
export KEYCLOAK=keycloak
export SPINNAKER=spinnaker
export VAULT=vault
export JUPYTERHUB=jupyterhub
export ARGOCD=argocd
export DEFAULT_SUBDOMAIN=kube
export GROUP_NAME=$GOK_ROOT_DOMAIN
#ldap, oidc
export AUTHENTICATION_METHOD=oidc
export IDENTITY_PROVIDER=${IDENTITY_PROVIDER}
`
case ${IDENTITY_PROVIDER} in
  "oauth0")
    echo "$(getOAuth0Config)"
    ;;
  "keycloak")
    echo "$(getKeycloakConfig)"
    ;;
  *)
    echo "Unsupported identity provider: ${IDENTITY_PROVIDER}"
    ;;
esac
`
EOF
fi

# Source root_config if it exists
if [ -f "${MOUNT_PATH}/root_config" ]; then
    source ${MOUNT_PATH}/root_config
fi

rootDomain(){
  echo "$GOK_ROOT_DOMAIN"
}

sedRootDomain(){
  rootDomain | sed 's/\./-/g'
}

registrySubdomain(){
  echo "$REGISTRY"
}

defaultSubdomain(){
  echo "$DEFAULT_SUBDOMAIN"
}

keycloakSubdomain(){
  echo "$KEYCLOAK"
}

argocdSubdomain(){
  echo "$ARGOCD"
}

jupyterHubSubdomain(){
  echo "$JUPYTERHUB"
}

fullDefaultUrl(){
  echo "${DEFAULT_SUBDOMAIN}.${GOK_ROOT_DOMAIN}"
}

fullRegistryUrl(){
  echo "${REGISTRY}.${GOK_ROOT_DOMAIN}"
}

fullKeycloakUrl(){
  echo "${KEYCLOAK}.${GOK_ROOT_DOMAIN}"
}

fullVaultUrl(){
  echo "${VAULT}.${GOK_ROOT_DOMAIN}"
}

fullSpinnakerUrl(){
  echo "${SPINNAKER}.${GOK_ROOT_DOMAIN}"
}

# =============================================================================
# ðŸš€ GOK ENHANCED LOGGING SYSTEM
# =============================================================================

# Color constants
readonly COLOR_RESET='\033[0m'
readonly COLOR_BOLD='\033[1m'
readonly COLOR_DIM='\033[2m'
readonly COLOR_RED='\033[31m'
readonly COLOR_GREEN='\033[32m'
readonly COLOR_YELLOW='\033[33m'
readonly COLOR_BLUE='\033[34m'
readonly COLOR_MAGENTA='\033[35m'
readonly COLOR_CYAN='\033[36m'
readonly COLOR_WHITE='\033[37m'
readonly COLOR_BRIGHT_GREEN='\033[92m'
readonly COLOR_BRIGHT_YELLOW='\033[93m'
readonly COLOR_BRIGHT_BLUE='\033[94m'
readonly COLOR_BRIGHT_MAGENTA='\033[95m'
readonly COLOR_BRIGHT_CYAN='\033[96m'

# Emoji constants
readonly EMOJI_SUCCESS="âœ…"
readonly EMOJI_ERROR="âŒ"
readonly EMOJI_WARNING="âš ï¸"
readonly EMOJI_INFO="â„¹ï¸"
readonly EMOJI_ROCKET="ðŸš€"
readonly EMOJI_GEAR="âš™ï¸"
readonly EMOJI_CLOCK="â±ï¸"
readonly EMOJI_CHECKMARK="âœ“"
readonly EMOJI_CROSS="âœ—"
readonly EMOJI_ARROW="âž¤"
readonly EMOJI_STAR="â­"
readonly EMOJI_PACKAGE="ðŸ“¦"
readonly EMOJI_NETWORK="ðŸŒ"
readonly EMOJI_SHIELD="ðŸ›¡ï¸"
readonly EMOJI_TOOLS="ðŸ”§"
readonly EMOJI_CHART="ðŸ“Š"
readonly EMOJI_DEBUG="ðŸ”"
readonly EMOJI_KEY="ðŸ”‘"
readonly EMOJI_LINK="ðŸ”—"
readonly EMOJI_BOOK="ðŸ“š"
readonly EMOJI_LIGHTBULB="ðŸ’¡"
readonly EMOJI_FIRE="ðŸ”¥"

# Get current timestamp
get_timestamp() {
    date '+%Y-%m-%d %H:%M:%S'
}

# Get elapsed time since start
get_elapsed_time() {
    local start_time=${1:-$GOK_START_TIME}
    if [[ -n "$start_time" ]]; then
        local current_time=$(date +%s)
        local elapsed=$((current_time - start_time))
        local minutes=$((elapsed / 60))
        local seconds=$((elapsed % 60))
        if [[ $minutes -gt 0 ]]; then
            echo "${minutes}m ${seconds}s"
        else
            echo "${seconds}s"
        fi
    else
        echo "0s"
    fi
}

# Initialize timing
GOK_START_TIME=$(date +%s)

# Enhanced logging functions
log_header() {
    local title="$1"
    local subtitle="${2:-}"
    echo
    echo -e "${COLOR_BRIGHT_CYAN}${COLOR_BOLD}=================================================================${COLOR_RESET}"
    echo -e "${COLOR_BRIGHT_CYAN}${COLOR_BOLD}  ${EMOJI_ROCKET} GOK Platform - $title${COLOR_RESET}"
    if [[ -n "$subtitle" ]]; then
        echo -e "${COLOR_CYAN}  $subtitle${COLOR_RESET}"
    fi
    echo -e "${COLOR_BRIGHT_CYAN}${COLOR_BOLD}=================================================================${COLOR_RESET}"
    echo
}

log_section() {
    local title="$1"
    local emoji="${2:-$EMOJI_GEAR}"
    echo
    echo -e "${COLOR_BRIGHT_BLUE}${COLOR_BOLD}--- $emoji $title ---${COLOR_RESET}"
}

log_success() {
    local message="$1"
    local timestamp=$(get_timestamp)
    echo -e "${COLOR_BRIGHT_GREEN}${COLOR_BOLD}[$timestamp] ${EMOJI_SUCCESS} $message${COLOR_RESET}"
}

log_error() {
    local message="$1"
    local timestamp=$(get_timestamp)
    echo -e "${COLOR_RED}${COLOR_BOLD}[$timestamp] ${EMOJI_ERROR} $message${COLOR_RESET}" >&2
}

log_warning() {
    local message="$1"
    local timestamp=$(get_timestamp)
    echo -e "${COLOR_BRIGHT_YELLOW}${COLOR_BOLD}[$timestamp] ${EMOJI_WARNING} $message${COLOR_RESET}"
}

log_info() {
    local message="$1"
    local timestamp=$(get_timestamp)
    echo -e "${COLOR_BRIGHT_CYAN}[$timestamp] ${EMOJI_INFO} $message${COLOR_RESET}"
}

log_step() {
    local step="$1"
    local message="$2"
    local timestamp=$(get_timestamp)
    echo -e "${COLOR_BLUE}[$timestamp] ${COLOR_BOLD}Step $step:${COLOR_RESET} ${COLOR_CYAN}$message${COLOR_RESET}"
}

log_substep() {
    local message="$1"
    echo -e "  ${COLOR_DIM}${EMOJI_ARROW} $message${COLOR_RESET}"
}

log_debug() {
    local message="$1"
    local timestamp=$(get_timestamp)
    # Only show debug/system logs in verbose mode or when there's an error
    if is_verbose_mode || [[ "${2:-}" == "error" ]]; then
        echo -e "${COLOR_DIM}[$timestamp] ${EMOJI_DEBUG} $message${COLOR_RESET}"
    fi
}

log_progress() {
    local current="$1"
    local total="$2"
    local message="${3:-Processing}"
    local percentage=$((current * 100 / total))
    local filled=$((percentage / 2))
    local empty=$((50 - filled))
    
    local bar=""
    for ((i=0; i<filled; i++)); do bar+="â–ˆ"; done
    for ((i=0; i<empty; i++)); do bar+="â–‘"; done
    
    echo -ne "\r${COLOR_BRIGHT_BLUE}$message [${COLOR_BRIGHT_GREEN}$bar${COLOR_BRIGHT_BLUE}] $percentage% ($current/$total)${COLOR_RESET}"
    if [[ $current -eq $total ]]; then
        echo
    fi
}

log_component_start() {
    local component="$1"
    local description="${2:-Installing component}"
    # Sanitize component name for variable names (replace hyphens with underscores)
    local var_name=$(echo "${component^^}" | tr '-' '_')
    echo
    echo -e "${COLOR_BRIGHT_MAGENTA}${COLOR_BOLD}â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”${COLOR_RESET}"
    echo -e "${COLOR_BRIGHT_MAGENTA}${COLOR_BOLD}â”‚ ${EMOJI_PACKAGE} Starting: $component${COLOR_RESET}"
    echo -e "${COLOR_MAGENTA}â”‚ $description${COLOR_RESET}"
    echo -e "${COLOR_BRIGHT_MAGENTA}${COLOR_BOLD}â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜${COLOR_RESET}"
    export "GOK_${var_name}_START_TIME=$(date +%s)"
}

log_component_success() {
    local component="$1"
    local message="${2:-Installation completed successfully}"
    # Sanitize component name for variable names (replace hyphens with underscores)
    local var_name=$(echo "${component^^}" | tr '-' '_')
    local start_var="GOK_${var_name}_START_TIME"
    local elapsed=$(get_elapsed_time "${!start_var}")
    echo
    echo -e "${COLOR_BRIGHT_GREEN}${COLOR_BOLD}â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”${COLOR_RESET}"
    echo -e "${COLOR_BRIGHT_GREEN}${COLOR_BOLD}â”‚ ${EMOJI_SUCCESS} Success: $component${COLOR_RESET}"
    echo -e "${COLOR_GREEN}â”‚ $message${COLOR_RESET}"
    echo -e "${COLOR_GREEN}â”‚ ${EMOJI_CLOCK} Installation time: $elapsed${COLOR_RESET}"
    echo -e "${COLOR_BRIGHT_GREEN}${COLOR_BOLD}â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜${COLOR_RESET}"
}

# ============================================================================
# LOG SUPPRESSION AND SUMMARY FUNCTIONS
# ============================================================================

# Execute a command with suppressed output and return status
execute_with_suppression() {
    local temp_file=$(mktemp)
    local error_file=$(mktemp)
    
    # Execute command with both stdout and stderr captured
    if "$@" >"$temp_file" 2>"$error_file"; then
        rm -f "$temp_file" "$error_file"
        return 0
    else
        local exit_code=$?
        echo
        echo -e "${COLOR_RED}${COLOR_BOLD}â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”${COLOR_RESET}" >&2
        echo -e "${COLOR_RED}${COLOR_BOLD}â”‚ ${EMOJI_ERROR} COMMAND EXECUTION FAILED - DEBUGGING INFORMATION ${COLOR_RESET}${COLOR_RED}${COLOR_BOLD}â”‚${COLOR_RESET}" >&2
        echo -e "${COLOR_RED}${COLOR_BOLD}â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜${COLOR_RESET}" >&2
        echo -e "${COLOR_YELLOW}${COLOR_BOLD}${EMOJI_GEAR} Failed Command:${COLOR_RESET}" >&2
        echo -e "${COLOR_WHITE}  $*${COLOR_RESET}" >&2
        echo -e "${COLOR_YELLOW}${COLOR_BOLD}${EMOJI_CROSS} Exit Code: ${COLOR_RED}$exit_code${COLOR_RESET}" >&2
        
        # Show error output if available
        if [[ -s "$error_file" ]]; then
            echo -e "${COLOR_RED}${COLOR_BOLD}${EMOJI_ERROR} Error Output:${COLOR_RESET}" >&2
            echo -e "${COLOR_RED}â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€${COLOR_RESET}" >&2
            cat "$error_file" >&2
            echo -e "${COLOR_RED}â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€${COLOR_RESET}" >&2
        fi
        
        # Show standard output if available (may contain useful debugging info)
        if [[ -s "$temp_file" ]]; then
            echo -e "${COLOR_YELLOW}${COLOR_BOLD}${EMOJI_INFO} Standard Output:${COLOR_RESET}" >&2
            echo -e "${COLOR_YELLOW}â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€${COLOR_RESET}" >&2
            cat "$temp_file" >&2
            echo -e "${COLOR_YELLOW}â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€${COLOR_RESET}" >&2
        fi
        
        echo -e "${COLOR_CYAN}${COLOR_BOLD}${EMOJI_LIGHTBULB} Debugging Tips:${COLOR_RESET}" >&2
        echo -e "  ${COLOR_CYAN}â€¢ Check system logs: journalctl -u kubelet${COLOR_RESET}" >&2
        echo -e "  ${COLOR_CYAN}â€¢ Verify resources: kubectl get all -A${COLOR_RESET}" >&2
        echo -e "  ${COLOR_CYAN}â€¢ Check events: kubectl get events --sort-by='.lastTimestamp'${COLOR_RESET}" >&2
        echo
        
        rm -f "$temp_file" "$error_file"
        return $exit_code
    fi
}

# Execute helm install with log suppression and summary
helm_install_with_summary() {
    local component="$1"
    local namespace="${2:-default}"
    shift 2
    
    log_info "Installing $component via Helm..."
    local temp_file=$(mktemp)
    local error_file=$(mktemp)
    local start_time=$(date +%s)
    
    if helm install "$@" >"$temp_file" 2>"$error_file"; then
        local end_time=$(date +%s)
        local duration=$((end_time - start_time))
        
        # Extract useful information from output
        local release_info=$(grep -E "NAME:|NAMESPACE:|STATUS:|REVISION:" "$temp_file" 2>/dev/null || echo "")
        
        log_success "Helm installation completed for $component"
        echo -e "${COLOR_CYAN}${COLOR_BOLD}ðŸ“‹ Installation Summary:${COLOR_RESET}"
        echo -e "  ${COLOR_GREEN}â€¢ Component: ${COLOR_BOLD}$component${COLOR_RESET}"
        echo -e "  ${COLOR_GREEN}â€¢ Namespace: ${COLOR_BOLD}$namespace${COLOR_RESET}"
        echo -e "  ${COLOR_GREEN}â€¢ Duration: ${COLOR_BOLD}${duration}s${COLOR_RESET}"
        if [[ -n "$release_info" ]]; then
            echo -e "  ${COLOR_GREEN}â€¢ Release Info:${COLOR_RESET}"
            echo "$release_info" | sed 's/^/    /'
        fi
        
        rm -f "$temp_file" "$error_file"
        return 0
    else
        local exit_code=$?
        echo
        echo -e "${COLOR_RED}${COLOR_BOLD}â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”${COLOR_RESET}" >&2
        echo -e "${COLOR_RED}${COLOR_BOLD}â”‚ ${EMOJI_ERROR} HELM INSTALLATION FAILED - DEBUGGING INFORMATION${COLOR_RESET}${COLOR_RED}${COLOR_BOLD} â”‚${COLOR_RESET}" >&2  
        echo -e "${COLOR_RED}${COLOR_BOLD}â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜${COLOR_RESET}" >&2
        echo -e "${COLOR_YELLOW}${COLOR_BOLD}${EMOJI_PACKAGE} Component: ${COLOR_WHITE}$component${COLOR_RESET}" >&2
        echo -e "${COLOR_YELLOW}${COLOR_BOLD}${EMOJI_NETWORK} Namespace: ${COLOR_WHITE}$namespace${COLOR_RESET}" >&2
        echo -e "${COLOR_YELLOW}${COLOR_BOLD}${EMOJI_CROSS} Exit Code: ${COLOR_RED}$exit_code${COLOR_RESET}" >&2
        
        # Show Helm-specific error details
        if [[ -s "$error_file" ]]; then
            echo -e "${COLOR_RED}${COLOR_BOLD}${EMOJI_ERROR} Helm Error Details:${COLOR_RESET}" >&2
            echo -e "${COLOR_RED}â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•${COLOR_RESET}" >&2
            cat "$error_file" >&2
            echo -e "${COLOR_RED}â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•${COLOR_RESET}" >&2
        fi
        
        # Show full Helm output for debugging
        if [[ -s "$temp_file" ]]; then
            echo -e "${COLOR_YELLOW}${COLOR_BOLD}${EMOJI_INFO} Full Helm Output:${COLOR_RESET}" >&2
            echo -e "${COLOR_YELLOW}â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•${COLOR_RESET}" >&2
            cat "$temp_file" >&2
            echo -e "${COLOR_YELLOW}â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•${COLOR_RESET}" >&2
        fi
        
        # Show helpful debugging commands
        echo -e "${COLOR_CYAN}${COLOR_BOLD}${EMOJI_TOOLS} Debugging Commands:${COLOR_RESET}" >&2
        echo -e "  ${COLOR_CYAN}â€¢ helm list -A${COLOR_RESET} ${COLOR_DIM}(check all releases)${COLOR_RESET}" >&2
        echo -e "  ${COLOR_CYAN}â€¢ helm status $component -n $namespace${COLOR_RESET} ${COLOR_DIM}(check release status)${COLOR_RESET}" >&2
        echo -e "  ${COLOR_CYAN}â€¢ kubectl get pods -n $namespace${COLOR_RESET} ${COLOR_DIM}(check pods)${COLOR_RESET}" >&2
        echo -e "  ${COLOR_CYAN}â€¢ kubectl describe pods -n $namespace${COLOR_RESET} ${COLOR_DIM}(pod details)${COLOR_RESET}" >&2
        echo -e "  ${COLOR_CYAN}â€¢ kubectl logs -n $namespace -l app=$component${COLOR_RESET} ${COLOR_DIM}(application logs)${COLOR_RESET}" >&2
        echo
        
        rm -f "$temp_file" "$error_file"
        return $exit_code
    fi
}

# Execute helm uninstall with log suppression and summary
helm_uninstall_with_summary() {
    local component="$1"
    local namespace="${2:-default}"
    shift 2
    
    log_info "Uninstalling $component via Helm..."
    local temp_file=$(mktemp)
    local error_file=$(mktemp)
    
    if helm uninstall "$@" >"$temp_file" 2>"$error_file"; then
        log_success "Helm uninstallation completed for $component"
        echo -e "${COLOR_CYAN}${COLOR_BOLD}ðŸ“‹ Uninstallation Summary:${COLOR_RESET}"
        echo -e "  ${COLOR_GREEN}â€¢ Component: ${COLOR_BOLD}$component${COLOR_RESET}"
        echo -e "  ${COLOR_GREEN}â€¢ Namespace: ${COLOR_BOLD}$namespace${COLOR_RESET}"
        echo -e "  ${COLOR_GREEN}â€¢ Status: ${COLOR_BOLD}Successfully removed${COLOR_RESET}"
        
        rm -f "$temp_file" "$error_file"
        return 0
    else
        local exit_code=$?
        echo
        echo -e "${COLOR_RED}${COLOR_BOLD}â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”${COLOR_RESET}" >&2
        echo -e "${COLOR_RED}${COLOR_BOLD}â”‚ ${EMOJI_ERROR} HELM UNINSTALLATION FAILED - DEBUGGING INFORMATION ${COLOR_RESET}${COLOR_RED}${COLOR_BOLD}â”‚${COLOR_RESET}" >&2  
        echo -e "${COLOR_RED}${COLOR_BOLD}â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜${COLOR_RESET}" >&2
        echo -e "${COLOR_YELLOW}${COLOR_BOLD}${EMOJI_PACKAGE} Component: ${COLOR_WHITE}$component${COLOR_RESET}" >&2
        echo -e "${COLOR_YELLOW}${COLOR_BOLD}${EMOJI_NETWORK} Namespace: ${COLOR_WHITE}$namespace${COLOR_RESET}" >&2
        echo -e "${COLOR_YELLOW}${COLOR_BOLD}${EMOJI_CROSS} Exit Code: ${COLOR_RED}$exit_code${COLOR_RESET}" >&2
        
        # Show Helm uninstall error details
        if [[ -s "$error_file" ]]; then
            echo -e "${COLOR_RED}${COLOR_BOLD}${EMOJI_ERROR} Helm Error Details:${COLOR_RESET}" >&2
            echo -e "${COLOR_RED}â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•${COLOR_RESET}" >&2
            cat "$error_file" >&2
            echo -e "${COLOR_RED}â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•${COLOR_RESET}" >&2
        fi
        
        # Show Helm output if available
        if [[ -s "$temp_file" ]]; then
            echo -e "${COLOR_YELLOW}${COLOR_BOLD}${EMOJI_INFO} Helm Output:${COLOR_RESET}" >&2
            echo -e "${COLOR_YELLOW}â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•${COLOR_RESET}" >&2
            cat "$temp_file" >&2
            echo -e "${COLOR_YELLOW}â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•${COLOR_RESET}" >&2
        fi
        
        # Show helpful debugging commands for uninstallation issues
        echo -e "${COLOR_CYAN}${COLOR_BOLD}${EMOJI_TOOLS} Debugging Commands:${COLOR_RESET}" >&2
        echo -e "  ${COLOR_CYAN}â€¢ helm list -A${COLOR_RESET} ${COLOR_DIM}(check all releases)${COLOR_RESET}" >&2
        echo -e "  ${COLOR_CYAN}â€¢ helm status $component -n $namespace${COLOR_RESET} ${COLOR_DIM}(check release status)${COLOR_RESET}" >&2
        echo -e "  ${COLOR_CYAN}â€¢ kubectl get all -n $namespace${COLOR_RESET} ${COLOR_DIM}(check remaining resources)${COLOR_RESET}" >&2
        echo -e "  ${COLOR_CYAN}â€¢ kubectl delete namespace $namespace --force --grace-period=0${COLOR_RESET} ${COLOR_DIM}(force namespace deletion)${COLOR_RESET}" >&2
        echo -e "  ${COLOR_CYAN}â€¢ helm uninstall $component -n $namespace --no-hooks${COLOR_RESET} ${COLOR_DIM}(skip hooks)${COLOR_RESET}" >&2
        echo
        
        rm -f "$temp_file" "$error_file"
        return $exit_code
    fi
}

# Execute kubectl operations with log suppression and summary
kubectl_with_summary() {
    local operation="$1"
    local resource_type="${2:-resource}"
    shift 2
    
    local temp_file=$(mktemp)
    local error_file=$(mktemp)
    
    if kubectl "$operation" "$resource_type" "$@" >"$temp_file" 2>"$error_file"; then
        case "$operation" in
            "apply"|"create")
                log_success "Successfully applied $resource_type"
                ;;
            "delete")
                log_success "Successfully deleted $resource_type"
                ;;
            *)
                log_success "Successfully executed kubectl $operation"
                ;;
        esac
        rm -f "$temp_file" "$error_file"
        return 0
    else
        local exit_code=$?
        echo
        echo -e "${COLOR_RED}${COLOR_BOLD}â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”${COLOR_RESET}" >&2
        echo -e "${COLOR_RED}${COLOR_BOLD}â”‚ ${EMOJI_ERROR} KUBECTL OPERATION FAILED - DEBUGGING INFORMATION${COLOR_RESET}${COLOR_RED}${COLOR_BOLD} â”‚${COLOR_RESET}" >&2
        echo -e "${COLOR_RED}${COLOR_BOLD}â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜${COLOR_RESET}" >&2
        echo -e "${COLOR_YELLOW}${COLOR_BOLD}${EMOJI_GEAR} Operation: ${COLOR_WHITE}kubectl $operation${COLOR_RESET}" >&2
        echo -e "${COLOR_YELLOW}${COLOR_BOLD}${EMOJI_PACKAGE} Resource: ${COLOR_WHITE}$resource_type${COLOR_RESET}" >&2
        echo -e "${COLOR_YELLOW}${COLOR_BOLD}${EMOJI_CROSS} Exit Code: ${COLOR_RED}$exit_code${COLOR_RESET}" >&2
        
        # Show kubectl error details
        if [[ -s "$error_file" ]]; then
            echo -e "${COLOR_RED}${COLOR_BOLD}${EMOJI_ERROR} kubectl Error Details:${COLOR_RESET}" >&2
            echo -e "${COLOR_RED}â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•${COLOR_RESET}" >&2
            cat "$error_file" >&2
            echo -e "${COLOR_RED}â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•${COLOR_RESET}" >&2
        fi
        
        # Show kubectl output if available
        if [[ -s "$temp_file" ]]; then
            echo -e "${COLOR_YELLOW}${COLOR_BOLD}${EMOJI_INFO} kubectl Output:${COLOR_RESET}" >&2
            echo -e "${COLOR_YELLOW}â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•${COLOR_RESET}" >&2
            cat "$temp_file" >&2
            echo -e "${COLOR_YELLOW}â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•${COLOR_RESET}" >&2
        fi
        
        # Show helpful debugging commands based on operation type
        echo -e "${COLOR_CYAN}${COLOR_BOLD}${EMOJI_TOOLS} Debugging Commands:${COLOR_RESET}" >&2
        case "$operation" in
            "apply"|"create")
                echo -e "  ${COLOR_CYAN}â€¢ kubectl get events --sort-by='.lastTimestamp'${COLOR_RESET} ${COLOR_DIM}(recent events)${COLOR_RESET}" >&2
                echo -e "  ${COLOR_CYAN}â€¢ kubectl describe $resource_type${COLOR_RESET} ${COLOR_DIM}(resource details)${COLOR_RESET}" >&2
                ;;
            "delete")
                echo -e "  ${COLOR_CYAN}â€¢ kubectl get $resource_type${COLOR_RESET} ${COLOR_DIM}(check if still exists)${COLOR_RESET}" >&2
                echo -e "  ${COLOR_CYAN}â€¢ kubectl patch $resource_type <name> -p '{\"metadata\":{\"finalizers\":null}}'${COLOR_RESET} ${COLOR_DIM}(force delete)${COLOR_RESET}" >&2
                ;;
            *)
                echo -e "  ${COLOR_CYAN}â€¢ kubectl get all -A${COLOR_RESET} ${COLOR_DIM}(check all resources)${COLOR_RESET}" >&2
                echo -e "  ${COLOR_CYAN}â€¢ kubectl cluster-info${COLOR_RESET} ${COLOR_DIM}(cluster status)${COLOR_RESET}" >&2
                ;;
        esac
        echo -e "  ${COLOR_CYAN}â€¢ kubectl version${COLOR_RESET} ${COLOR_DIM}(check kubectl/cluster versions)${COLOR_RESET}" >&2
        echo
        
        rm -f "$temp_file" "$error_file"
        return $exit_code
    fi
}

# Display installation summary
show_installation_summary() {
    local component="$1"
    local namespace="${2:-default}"
    local additional_info="${3:-}"
    
    echo
    echo -e "${COLOR_BRIGHT_CYAN}${COLOR_BOLD}â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”${COLOR_RESET}"
    echo -e "${COLOR_BRIGHT_CYAN}${COLOR_BOLD}â”‚ ${EMOJI_PACKAGE} $component Installation Complete${COLOR_RESET}"
    echo -e "${COLOR_CYAN}â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤${COLOR_RESET}"
    echo -e "${COLOR_CYAN}â”‚ ${EMOJI_CHECKMARK} Component: ${COLOR_BOLD}$component${COLOR_RESET}"
    echo -e "${COLOR_CYAN}â”‚ ${EMOJI_NETWORK} Namespace: ${COLOR_BOLD}$namespace${COLOR_RESET}"
    echo -e "${COLOR_CYAN}â”‚ ${EMOJI_CLOCK} Completed: ${COLOR_BOLD}$(date '+%Y-%m-%d %H:%M:%S')${COLOR_RESET}"
    if [[ -n "$additional_info" ]]; then
        echo -e "${COLOR_CYAN}â”‚ ${EMOJI_INFO} Info: ${COLOR_BOLD}$additional_info${COLOR_RESET}"
    fi
    echo -e "${COLOR_BRIGHT_CYAN}${COLOR_BOLD}â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜${COLOR_RESET}"
    echo
}

log_component_error() {
    local component="$1"
    local message="${2:-Installation failed}"
    # Sanitize component name for variable names (replace hyphens with underscores)
    local var_name=$(echo "${component^^}" | tr '-' '_')
    local start_var="GOK_${var_name}_START_TIME"
    local elapsed=$(get_elapsed_time "${!start_var}")
    echo
    echo -e "${COLOR_RED}${COLOR_BOLD}â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”${COLOR_RESET}"
    echo -e "${COLOR_RED}${COLOR_BOLD}â”‚ ${EMOJI_ERROR} Failed: $component${COLOR_RESET}"
    echo -e "${COLOR_RED}â”‚ $message${COLOR_RESET}"
    echo -e "${COLOR_RED}â”‚ ${EMOJI_CLOCK} Time elapsed: $elapsed${COLOR_RESET}"
    echo -e "${COLOR_RED}${COLOR_BOLD}â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜${COLOR_RESET}"
}

log_next_steps() {
    local title="$1"
    shift
    local steps=("$@")
    
    echo
    echo -e "${COLOR_BRIGHT_YELLOW}${COLOR_BOLD}â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”${COLOR_RESET}"
    echo -e "${COLOR_BRIGHT_YELLOW}${COLOR_BOLD}â”‚ ${EMOJI_LIGHTBULB} Next Steps: $title${COLOR_RESET}"
    echo -e "${COLOR_BRIGHT_YELLOW}${COLOR_BOLD}â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜${COLOR_RESET}"
    
    local step_num=1
    for step in "${steps[@]}"; do
        echo -e "${COLOR_YELLOW}${step_num}. $step${COLOR_RESET}"
        ((step_num++))
    done
    echo
}

log_urls() {
    local title="$1"
    shift
    local urls=("$@")
    
    echo -e "${COLOR_BRIGHT_CYAN}${COLOR_BOLD}${EMOJI_LINK} $title:${COLOR_RESET}"
    for url in "${urls[@]}"; do
        echo -e "  ${COLOR_CYAN}â€¢ $url${COLOR_RESET}"
    done
    echo
}

log_credentials() {
    local service="$1"
    local username="$2"
    local password_info="$3"
    
    echo -e "${COLOR_BRIGHT_GREEN}${COLOR_BOLD}${EMOJI_KEY} $service Credentials:${COLOR_RESET}"
    echo -e "  ${COLOR_GREEN}Username: ${COLOR_BOLD}$username${COLOR_RESET}"
    echo -e "  ${COLOR_GREEN}Password: $password_info${COLOR_RESET}"
    echo
}

log_troubleshooting() {
    local component="$1"
    shift
    local tips=("$@")
    
    echo -e "${COLOR_BRIGHT_YELLOW}${COLOR_BOLD}${EMOJI_TOOLS} Troubleshooting $component:${COLOR_RESET}"
    for tip in "${tips[@]}"; do
        echo -e "  ${COLOR_YELLOW}â€¢ $tip${COLOR_RESET}"
    done
    echo
}

show_spinner() {
    local pid=$1
    local message="${2:-Processing}"
    local delay=0.1
    local spinstr='|/-\'
    
    while [ "$(ps a | awk '{print $1}' | grep $pid)" ]; do
        local temp=${spinstr#?}
        printf "\r${COLOR_BRIGHT_BLUE}%s [%c]${COLOR_RESET}" "$message" "$spinstr"
        local spinstr=$temp${spinstr%"$temp"}
        sleep $delay
    done
    printf "\r%*s\r" ${#message} ""
}

# Legacy compatibility functions
echoSuccess() {
    log_success "$1"
}

echoFailed() {
    log_error "$1"
}

echoWarning() {
    log_warning "$1"
}

# =============================================================================
# ðŸŒ REMOTE EXECUTION SYSTEM
# =============================================================================
# Enhanced remote execution with shell redirection support
# Fixed: Shell redirections (>, >>, |, &&, ||) now work properly with sudo
# The system automatically wraps complex commands in "bash -c" for proper privilege escalation

# Remote execution configuration
declare -A REMOTE_HOSTS
declare -A REMOTE_USERS
declare -A REMOTE_KEYS
declare -A REMOTE_SUDOS

# Default remote host configuration
DEFAULT_REMOTE_CONFIG_FILE="$HOME/.gok_remote_config"

# Load default remote configuration
load_default_remote_config() {
    if [ -f "$DEFAULT_REMOTE_CONFIG_FILE" ]; then
        source "$DEFAULT_REMOTE_CONFIG_FILE" 2>/dev/null
        if [ -n "$DEFAULT_REMOTE_HOST" ] && [ -n "$DEFAULT_REMOTE_USER" ]; then
            return 0
        fi
    fi
    return 1
}

# Save default remote configuration  
save_default_remote_config() {
    local host="$1"
    local user="$2"
    local key_file="${3:-$HOME/.ssh/id_rsa}"
    local sudo_mode="${4:-auto}"
    
    cat > "$DEFAULT_REMOTE_CONFIG_FILE" << EOF
# GOK Default Remote Configuration
DEFAULT_REMOTE_HOST="$host"
DEFAULT_REMOTE_USER="$user" 
DEFAULT_REMOTE_KEY="$key_file"
DEFAULT_REMOTE_SUDO="$sudo_mode"
EOF
    
    log_success "Default remote configuration saved: $user@$host (sudo: $sudo_mode)"
}

# Check if command needs sudo privileges
needs_sudo() {
    local command="$1"
    
    # Commands that typically need root privileges
    local sudo_commands=(
        "systemctl" "service" "journalctl" "mount" "umount"
        "apt" "apt-get" "yum" "dnf" "zypper" "pacman"
        "docker" "kubectl" "kubeadm" "crictl"
        "iptables" "ip" "ifconfig" "netstat" "ss"
        "modprobe" "lsmod" "insmod" "rmmod"
        "fdisk" "parted" "mkfs" "fsck" "lsblk"
        "useradd" "userdel" "usermod" "groupadd" "passwd"
        "chown" "chmod" "chgrp" "setfacl" "getfacl"
        "crontab" "at" "batch"
        "nano" "vim" "vi" "emacs" # when editing system files
        "cat" "echo" "tee" # when writing to system locations
        "cp" "mv" "rm" "mkdir" "rmdir" # when operating on system directories
        "find" "locate" "updatedb"
        "dmesg" "lscpu" "lsusb" "lspci" "hwinfo"
        "./gok" "gok" # GOK commands often need sudo
    )
    
    # Extract the first word (command) from the command string
    local first_command=$(echo "$command" | awk '{print $1}' | sed 's|.*/||') # Remove path if present
    
    # Check if command is in sudo_commands list
    for sudo_cmd in "${sudo_commands[@]}"; do
        if [[ "$first_command" == "$sudo_cmd" ]]; then
            return 0  # Needs sudo
        fi
    done
    
    # Check if command contains paths that typically need sudo
    if [[ "$command" == *"/etc/"* ]] || [[ "$command" == *"/var/"* ]] || [[ "$command" == *"/usr/"* ]] || [[ "$command" == *"/opt/"* ]] || [[ "$command" == *"/sys/"* ]] || [[ "$command" == *"/proc/"* ]]; then
        return 0  # Needs sudo
    fi
    
    # Check if command already starts with sudo
    if [[ "$command" == sudo* ]]; then
        return 1  # Already has sudo
    fi
    
    return 1  # Doesn't need sudo
}

# Simple remote exec using default configuration with automatic sudo
simple_remote_exec() {
    local commands="$*"
    
    # Load default configuration
    if ! load_default_remote_config; then
        log_error "No default remote configuration found!"
        echo ""
        echo "Please set up your default remote host first:"
        echo "  gok remote setup <host> <user> [key_file]"
        echo ""
        echo "Example:"
        echo "  gok remote setup 10.0.0.244 sumit"
        return 1
    fi
    
    # Use stored sudo preference, fall back to environment variable, then default to auto
    local use_sudo="${DEFAULT_REMOTE_SUDO:-${GOK_REMOTE_SUDO:-auto}}"
    
    # Determine if sudo should be used and handle shell redirections
    local final_commands="$commands"
    
    # Automatically set MOUNT_PATH=/root for gok commands
    if [[ "$commands" == *"./gok"* ]]; then
        final_commands="export MOUNT_PATH=/root && $commands"
    fi
    
    if [[ "$use_sudo" == "always" ]] || [[ "$use_sudo" == "auto" && $(needs_sudo "$final_commands") ]]; then
        if [[ "$final_commands" != sudo* ]]; then
            # Check if command contains shell redirections or pipes
            if [[ "$final_commands" == *">"* ]] || [[ "$final_commands" == *">>"* ]] || [[ "$final_commands" == *"|"* ]] || [[ "$final_commands" == *"&&"* ]] || [[ "$final_commands" == *"||"* ]]; then
                # Wrap complex commands in bash -c to ensure proper sudo context for redirections
                final_commands="sudo bash -c \"$final_commands\""
            else
                final_commands="sudo $final_commands"
            fi
        fi
    fi
    
    log_info "Executing on default remote ($DEFAULT_REMOTE_USER@$DEFAULT_REMOTE_HOST): $final_commands"
    
    # Execute command using SSH
    ssh -i "$DEFAULT_REMOTE_KEY" -o StrictHostKeyChecking=no -o ConnectTimeout=10 \
        "$DEFAULT_REMOTE_USER@$DEFAULT_REMOTE_HOST" "$final_commands"
}

# Setup SSH keys for passwordless authentication
setup_ssh_keys() {
    local key_file="${1:-$HOME/.ssh/id_rsa}"
    
    log_info "Setting up SSH keys for remote access..."
    
    # Generate SSH key if it doesn't exist
    if [ ! -f "$key_file" ]; then
        log_info "Generating SSH key pair..."
        ssh-keygen -t rsa -b 4096 -f "$key_file" -N ""
        if [ $? -eq 0 ]; then
            log_success "SSH key pair generated: $key_file"
        else
            log_error "Failed to generate SSH key pair"
            return 1
        fi
    else
        log_info "SSH key already exists: $key_file"
    fi
    
    # Set proper permissions
    chmod 600 "$key_file"
    chmod 644 "${key_file}.pub"
    
    log_success "SSH keys are ready for use"
}

# Copy SSH key to remote host
copy_ssh_key() {
    local host="$1"
    local user="${2:-root}"
    local key_file="${3:-$HOME/.ssh/id_rsa}"
    
    log_info "Copying SSH key to $user@$host..."
    
    # Ensure SSH key exists
    if [ ! -f "$key_file" ]; then
        log_warning "SSH key not found, generating it first..."
        setup_ssh_keys "$key_file"
    fi
    
    # Copy the key
    ssh-copy-id -i "${key_file}.pub" "$user@$host"
    if [ $? -eq 0 ]; then
        log_success "SSH key successfully copied to $user@$host"
        
        # Test the connection
        log_info "Testing SSH connection..."
        ssh -i "$key_file" -o StrictHostKeyChecking=no -o ConnectTimeout=10 \
            "$user@$host" "echo 'SSH connection successful'; hostname; date"
        if [ $? -eq 0 ]; then
            log_success "SSH connection test successful"
        else
            log_warning "SSH connection test failed, but key was copied"
        fi
    else
        log_error "Failed to copy SSH key to $user@$host"
        return 1
    fi
}

# Configure passwordless sudo for remote user
setup_passwordless_sudo() {
    local host="$1"
    local user="${2:-root}"
    local key_file="${3:-$HOME/.ssh/id_rsa}"
    local user_password="$4"
    
    log_info "Setting up passwordless sudo for $user@$host..."
    
    # Skip if user is already root
    if [ "$user" = "root" ]; then
        log_info "User is root, passwordless sudo not needed"
        return 0
    fi
    
    # Test if passwordless sudo is already working
    ssh -i "$key_file" -o StrictHostKeyChecking=no -o ConnectTimeout=10 \
        "$user@$host" "sudo -n whoami" >/dev/null 2>&1
    
    if [ $? -eq 0 ]; then
        log_info "Passwordless sudo already configured for $user"
        return 0
    fi
    
    # If no password provided, prompt for it
    if [ -z "$user_password" ]; then
        echo -n "Enter password for $user on $host: "
        read -s user_password
        echo
    fi
    
    # Configure passwordless sudo
    log_info "Configuring passwordless sudo..."
    ssh -i "$key_file" -o StrictHostKeyChecking=no -o ConnectTimeout=10 \
        "$user@$host" "echo '$user_password' | sudo -S bash -c 'echo \"$user ALL=(ALL) NOPASSWD: ALL\" > /etc/sudoers.d/$user && chmod 440 /etc/sudoers.d/$user'" 2>/dev/null
    
    if [ $? -eq 0 ]; then
        # Test passwordless sudo
        ssh -i "$key_file" -o StrictHostKeyChecking=no -o ConnectTimeout=10 \
            "$user@$host" "sudo whoami" >/dev/null 2>&1
        
        if [ $? -eq 0 ]; then
            log_success "Passwordless sudo configured successfully for $user"
            return 0
        else
            log_error "Passwordless sudo configuration failed - testing failed"
            return 1
        fi
    else
        log_error "Failed to configure passwordless sudo (wrong password?)"
        return 1
    fi
}

# Configure remote host
configure_remote_host() {
    local alias="$1"
    local host="$2"
    local user="${3:-root}"
    local key_file="${4:-$HOME/.ssh/id_rsa}"
    local sudo_mode="${5:-auto}"
    local auto_setup="${6:-true}"
    
    log_info "Configuring remote host: $alias ($user@$host, sudo: $sudo_mode)"
    
    # Automatically setup SSH keys if requested
    if [ "$auto_setup" = "true" ]; then
        setup_ssh_keys "$key_file"
        copy_ssh_key "$host" "$user" "$key_file"
        if [ $? -ne 0 ]; then
            log_error "Failed to setup SSH access to $user@$host"
            return 1
        fi
    fi
    
    REMOTE_HOSTS["$alias"]="$host"
    REMOTE_USERS["$alias"]="$user"
    REMOTE_KEYS["$alias"]="$key_file"
    REMOTE_SUDOS["$alias"]="$sudo_mode"
    
    log_success "Configured remote host: $alias ($user@$host, sudo: $sudo_mode)"
}

# Execute command on remote host with automatic sudo
remote_exec() {
    local alias="$1"
    shift
    local commands="$*"
    
    if [[ -z "${REMOTE_HOSTS[$alias]}" ]]; then
        log_error "Remote host '$alias' not configured"
        log_info "Available hosts: ${!REMOTE_HOSTS[*]}"
        return 1
    fi
    
    local host="${REMOTE_HOSTS[$alias]}"
    local user="${REMOTE_USERS[$alias]}"
    local key_file="${REMOTE_KEYS[$alias]}"
    local use_sudo="${REMOTE_SUDOS[$alias]:-${GOK_REMOTE_SUDO:-auto}}"
    
    # Determine if sudo should be used and handle shell redirections
    local final_commands="$commands"
    
    # Automatically set MOUNT_PATH=/root for gok commands
    if [[ "$commands" == *"./gok"* ]]; then
        final_commands="export MOUNT_PATH=/root && $commands"
    fi
    
    if [[ "$use_sudo" == "always" ]] || [[ "$use_sudo" == "auto" && $(needs_sudo "$final_commands") ]]; then
        if [[ "$final_commands" != sudo* ]]; then
            # Check if command contains shell redirections or pipes
            if [[ "$final_commands" == *">"* ]] || [[ "$final_commands" == *">>"* ]] || [[ "$final_commands" == *"|"* ]] || [[ "$final_commands" == *"&&"* ]] || [[ "$final_commands" == *"||"* ]]; then
                # Wrap complex commands in bash -c to ensure proper sudo context for redirections
                final_commands="sudo bash -c \"$final_commands\""
            else
                final_commands="sudo $final_commands"
            fi
        fi
    fi
    
    log_info "Executing on $alias ($user@$host): $final_commands"
    
    ssh -i "$key_file" -o StrictHostKeyChecking=no -o ConnectTimeout=10 \
        "$user@$host" "$final_commands"
}

# Execute command on multiple remote hosts
remote_exec_all() {
    local commands="$*"
    local failed_hosts=()
    
    log_info "Executing on all configured hosts: $commands"
    
    for alias in "${!REMOTE_HOSTS[@]}"; do
        echo
        log_section "Executing on $alias" "${EMOJI_NETWORK}"
        
        if remote_exec "$alias" "$commands"; then
            log_success "Command succeeded on $alias"
        else
            log_error "Command failed on $alias"
            failed_hosts+=("$alias")
        fi
    done
    
    if [[ ${#failed_hosts[@]} -gt 0 ]]; then
        echo
        log_warning "Commands failed on: ${failed_hosts[*]}"
        return 1
    else
        log_success "Commands executed successfully on all hosts"
        return 0
    fi
}

# Copy file to remote host
remote_copy() {
    local alias="$1"
    local local_file="$2"
    local remote_path="$3"
    
    if [[ -z "${REMOTE_HOSTS[$alias]}" ]]; then
        log_error "Remote host '$alias' not configured"
        return 1
    fi
    
    local host="${REMOTE_HOSTS[$alias]}"
    local user="${REMOTE_USERS[$alias]}"
    local key_file="${REMOTE_KEYS[$alias]}"
    
    log_info "Copying $local_file to $alias:$remote_path"
    
    scp -i "$key_file" -o StrictHostKeyChecking=no \
        "$local_file" "$user@$host:$remote_path"
}

# Setup default remote hosts from config
setup_default_remote_hosts() {
    # Configure master node
    if [[ -n "$MASTER_HOST_IP" ]]; then
        configure_remote_host "master" "$MASTER_HOST_IP" "root" "$HOME/.ssh/id_rsa" "auto" "true"
    fi
    
    # Configure additional nodes if defined
    if [[ -n "$NODE_HOST_IPS" ]]; then
        IFS=',' read -ra NODE_IPS <<< "$NODE_HOST_IPS"
        local node_num=1
        for ip in "${NODE_IPS[@]}"; do
            configure_remote_host "node$node_num" "$ip" "root" "$HOME/.ssh/id_rsa" "auto" "true"
            ((node_num++))
        done
    fi
}

# Show configured remote hosts
show_remote_hosts() {
    echo
    log_section "Configured Remote Hosts" "${EMOJI_NETWORK}"
    
    if [[ ${#REMOTE_HOSTS[@]} -eq 0 ]]; then
        log_warning "No remote hosts configured"
        echo
        echo -e "${COLOR_CYAN}To configure remote hosts:${COLOR_RESET}"
        echo -e "  ${COLOR_YELLOW}gok remote add master 192.168.1.100 ubuntu${COLOR_RESET}"
        echo -e "  ${COLOR_YELLOW}gok remote add node1 192.168.1.101 ubuntu${COLOR_RESET}"
        return 0
    fi
    
    for alias in "${!REMOTE_HOSTS[@]}"; do
        local host="${REMOTE_HOSTS[$alias]}"
        local user="${REMOTE_USERS[$alias]}"
        local key_file="${REMOTE_KEYS[$alias]}"
        
        echo -e "  ${COLOR_GREEN}${alias}${COLOR_RESET}: ${COLOR_CYAN}${user}@${host}${COLOR_RESET}"
        echo -e "    ${COLOR_DIM}Key: ${key_file}${COLOR_RESET}"
        
        # Test connectivity
        if ssh -i "$key_file" -o StrictHostKeyChecking=no -o ConnectTimeout=5 \
               "$user@$host" "echo 'Connection test'" >/dev/null 2>&1; then
            echo -e "    ${COLOR_GREEN}${EMOJI_SUCCESS} Connected${COLOR_RESET}"
        else
            echo -e "    ${COLOR_RED}${EMOJI_ERROR} Connection failed${COLOR_RESET}"
        fi
        echo
    done
}

# Auto-setup SSH and remote host configuration
auto_setup_remote_host() {
    local host="$1"
    local user="${2:-root}"
    local alias="${3:-${host//\./_}}"  # Replace dots with underscores for alias
    local key_file="${4:-$HOME/.ssh/id_rsa}"
    
    log_info "Auto-setting up remote access to $user@$host..."
    
    # Step 1: Setup SSH keys if needed
    if [ ! -f "$key_file" ]; then
        log_info "SSH keys not found, generating them..."
        setup_ssh_keys "$key_file"
        if [ $? -ne 0 ]; then
            log_error "Failed to generate SSH keys"
            return 1
        fi
    fi
    
    # Step 2: Copy SSH key if not already copied  
    log_info "Ensuring SSH key is copied to $user@$host..."
    ssh -i "$key_file" -o StrictHostKeyChecking=no -o ConnectTimeout=5 \
        "$user@$host" "echo 'test'" >/dev/null 2>&1
    
    if [ $? -ne 0 ]; then
        log_info "SSH key access not working, copying key..."
        copy_ssh_key "$host" "$user" "$key_file"
        if [ $? -ne 0 ]; then
            log_error "Failed to setup SSH access to $user@$host"
            return 1
        fi
    else
        log_info "SSH key access already working"
    fi
    
    # Step 3: Configure remote host if not already configured
    if [[ -z "${REMOTE_HOSTS[$alias]}" ]]; then
        log_info "Configuring remote host alias: $alias"
        configure_remote_host "$alias" "$host" "$user" "$key_file" "auto" "false"  # Don't auto-setup again
    else
        log_info "Remote host '$alias' already configured"
    fi
    
    log_success "Auto-setup complete for $alias ($user@$host)"
    return 0
}

# Smart remote exec that auto-configures hosts
smart_remote_exec() {
    local target="$1"
    shift
    local commands="$*"
    
    # If target looks like host:user format, auto-configure it
    if [[ "$target" == *":"* ]]; then
        local host="${target%:*}"
        local user="${target#*:}"
        local alias="${host//\./_}_${user}"
        
        log_info "Auto-configuring new remote host: $user@$host as '$alias'"
        auto_setup_remote_host "$host" "$user" "$alias"
        if [ $? -eq 0 ]; then
            target="$alias"
        else
            log_error "Failed to auto-configure $user@$host"
            return 1
        fi
    fi
    
    # Check if target is configured, if not try to auto-configure as root@target
    if [[ -z "${REMOTE_HOSTS[$target]}" ]] && [[ "$target" != "all" ]]; then
        # If target looks like an IP or hostname, try to auto-configure it
        if [[ "$target" =~ ^[0-9]{1,3}\.[0-9]{1,3}\.[0-9]{1,3}\.[0-9]{1,3}$ ]] || [[ "$target" =~ ^[a-zA-Z0-9.-]+$ ]]; then
            log_info "Auto-configuring new remote host: root@$target"
            auto_setup_remote_host "$target" "root" "$target"
            if [ $? -ne 0 ]; then
                log_error "Failed to auto-configure root@$target"
                return 1
            fi
        fi
    fi
    
    # Now execute the command using the standard remote_exec
    if [[ "$target" == "all" ]]; then
        remote_exec_all "$commands"
    else
        remote_exec "$target" "$commands"
    fi
}

# Remote command shortcuts for common operations
remote_status() {
    local alias="${1:-all}"
    
    local commands="
        echo '=== System Info ==='
        hostname && date
        echo '=== Kubernetes Status ==='
        kubectl get nodes 2>/dev/null || echo 'kubectl not available'
        echo '=== Docker Status ==='
        systemctl is-active docker 2>/dev/null || echo 'Docker not running'
        echo '=== Load Average ==='
        uptime
        echo '=== Disk Usage ==='
        df -h / | tail -1
    "
    
    if [[ "$alias" == "all" ]]; then
        remote_exec_all "$commands"
    else
        remote_exec "$alias" "$commands"
    fi
}

# Install GOK on remote hosts
remote_install_gok() {
    local alias="${1:-all}"
    
    local commands="
        # Download GOK script
        curl -fsSL https://raw.githubusercontent.com/sumitmaji/kubernetes/main/install_k8s/gok -o /tmp/gok
        chmod +x /tmp/gok
        sudo mv /tmp/gok /usr/local/bin/gok
        
        # Verify installation
        gok --version || echo 'GOK installation completed'
    "
    
    if [[ "$alias" == "all" ]]; then
        remote_exec_all "$commands"
    else
        remote_exec "$alias" "$commands"
    fi
}

# =============================================================================
# ðŸ“Š COMPONENT STATUS TRACKING SYSTEM
# =============================================================================

# Component status tracking
declare -A GOK_COMPONENT_STATUS
declare -A GOK_COMPONENT_START_TIME
declare -A GOK_COMPONENT_END_TIME
declare -A GOK_COMPONENT_DESCRIPTION

# Status constants
readonly STATUS_IDLE="idle"
readonly STATUS_STARTING="starting"
readonly STATUS_IN_PROGRESS="in_progress"
readonly STATUS_SUCCESS="success"
readonly STATUS_FAILED="failed"
readonly STATUS_SKIPPED="skipped"

# Initialize component tracking
init_component_tracking() {
    local component="$1"
    local description="$2"
    
    GOK_COMPONENT_STATUS["$component"]="$STATUS_IDLE"
    GOK_COMPONENT_DESCRIPTION["$component"]="$description"
}

# Start component installation
start_component() {
    local component="$1"
    local description="${2:-Installing $component}"
    
    GOK_COMPONENT_STATUS["$component"]="$STATUS_STARTING"
    GOK_COMPONENT_START_TIME["$component"]=$(date +%s)
    
    log_component_start "$component" "$description"
    GOK_COMPONENT_STATUS["$component"]="$STATUS_IN_PROGRESS"
}

# Complete component installation successfully
complete_component() {
    local component="$1"
    local message="${2:-Installation completed successfully}"
    
    GOK_COMPONENT_STATUS["$component"]="$STATUS_SUCCESS"
    GOK_COMPONENT_END_TIME["$component"]=$(date +%s)
    
    log_component_success "$component" "$message"
}

# Mark component installation as failed
fail_component() {
    local component="$1"
    local message="${2:-Installation failed}"
    local error_details="${3:-}"
    
    GOK_COMPONENT_STATUS["$component"]="$STATUS_FAILED"
    GOK_COMPONENT_END_TIME["$component"]=$(date +%s)
    
    log_component_error "$component" "$message"
    
    if [[ -n "$error_details" ]]; then
        echo -e "${COLOR_RED}Error details: $error_details${COLOR_RESET}"
    fi
    
    show_component_troubleshooting "$component"
}

# Skip component installation
skip_component() {
    local component="$1"
    local reason="${2:-Component already installed or not required}"
    
    GOK_COMPONENT_STATUS["$component"]="$STATUS_SKIPPED"
    GOK_COMPONENT_END_TIME["$component"]=$(date +%s)
    
    log_warning "Skipped $component: $reason"
}

# Get component status
get_component_status() {
    local component="$1"
    echo "${GOK_COMPONENT_STATUS[$component]:-idle}"
}

# Check if component is installed/successful
is_component_successful() {
    local component="$1"
    [[ "${GOK_COMPONENT_STATUS[$component]}" == "$STATUS_SUCCESS" ]]
}

# Get component installation time
get_component_time() {
    local component="$1"
    local start_time="${GOK_COMPONENT_START_TIME[$component]}"
    local end_time="${GOK_COMPONENT_END_TIME[$component]}"
    
    if [[ -n "$start_time" && -n "$end_time" ]]; then
        local elapsed=$((end_time - start_time))
        local minutes=$((elapsed / 60))
        local seconds=$((elapsed % 60))
        if [[ $minutes -gt 0 ]]; then
            echo "${minutes}m ${seconds}s"
        else
            echo "${seconds}s"
        fi
    else
        echo "Unknown"
    fi
}

# Show installation summary
show_installation_summary() {
    local total_components=0
    local successful_components=0
    local failed_components=0
    local skipped_components=0
    
    echo
    log_header "Installation Summary" "$(get_timestamp)"
    
    echo -e "${COLOR_BRIGHT_CYAN}${COLOR_BOLD}Component Status Overview:${COLOR_RESET}"
    echo
    
    for component in "${!GOK_COMPONENT_STATUS[@]}"; do
        local status="${GOK_COMPONENT_STATUS[$component]}"
        local description="${GOK_COMPONENT_DESCRIPTION[$component]}"
        local time=$(get_component_time "$component")
        
        ((total_components++))
        
        case "$status" in
            "$STATUS_SUCCESS")
                ((successful_components++))
                echo -e "  ${EMOJI_SUCCESS} ${COLOR_GREEN}${COLOR_BOLD}$component${COLOR_RESET} ${COLOR_GREEN}($time)${COLOR_RESET}"
                echo -e "    ${COLOR_DIM}$description${COLOR_RESET}"
                ;;
            "$STATUS_FAILED")
                ((failed_components++))
                echo -e "  ${EMOJI_ERROR} ${COLOR_RED}${COLOR_BOLD}$component${COLOR_RESET} ${COLOR_RED}($time)${COLOR_RESET}"
                echo -e "    ${COLOR_DIM}$description${COLOR_RESET}"
                ;;
            "$STATUS_SKIPPED")
                ((skipped_components++))
                echo -e "  ${EMOJI_WARNING} ${COLOR_YELLOW}${COLOR_BOLD}$component${COLOR_RESET} ${COLOR_YELLOW}(skipped)${COLOR_RESET}"
                echo -e "    ${COLOR_DIM}$description${COLOR_RESET}"
                ;;
            *)
                echo -e "  ${EMOJI_INFO} ${COLOR_BLUE}${COLOR_BOLD}$component${COLOR_RESET} ${COLOR_BLUE}($status)${COLOR_RESET}"
                echo -e "    ${COLOR_DIM}$description${COLOR_RESET}"
                ;;
        esac
        echo
    done
    
    # Summary statistics
    echo -e "${COLOR_BRIGHT_CYAN}${COLOR_BOLD}Summary Statistics:${COLOR_RESET}"
    echo -e "  ${COLOR_GREEN}${EMOJI_SUCCESS} Successful: $successful_components${COLOR_RESET}"
    echo -e "  ${COLOR_RED}${EMOJI_ERROR} Failed: $failed_components${COLOR_RESET}"
    echo -e "  ${COLOR_YELLOW}${EMOJI_WARNING} Skipped: $skipped_components${COLOR_RESET}"
    echo -e "  ${COLOR_BLUE}${EMOJI_PACKAGE} Total: $total_components${COLOR_RESET}"
    
    local total_time=$(get_elapsed_time "$GOK_START_TIME")
    echo -e "  ${COLOR_CYAN}${EMOJI_CLOCK} Total time: $total_time${COLOR_RESET}"
    echo
    
    # Overall result
    if [[ $failed_components -eq 0 ]]; then
        log_success "All components installed successfully!"
    else
        log_error "$failed_components component(s) failed to install"
        return 1
    fi
}

# Component-specific troubleshooting
show_component_troubleshooting() {
    local component="$1"
    
    case "$component" in
        "kubernetes")
            log_troubleshooting "Kubernetes" \
                "Check if all nodes are ready: kubectl get nodes" \
                "Verify cluster status: kubectl cluster-info" \
                "Check system pods: kubectl get pods -n kube-system" \
                "Review kubelet logs: journalctl -u kubelet -f"
            ;;
        "cert-manager")
            log_troubleshooting "Cert-Manager" \
                "Check cert-manager pods: kubectl get pods -n cert-manager" \
                "Verify webhook: kubectl get validatingwebhookconfiguration" \
                "Test certificate creation: kubectl describe certificate" \
                "Check DNS resolution for ACME challenge"
            ;;
        "vault")
            log_troubleshooting "Vault" \
                "Check vault status: kubectl get pods -n vault" \
                "Verify vault seal status: kubectl exec -n vault vault-0 -- vault status" \
                "Check vault logs: kubectl logs -n vault vault-0" \
                "Ensure proper RBAC permissions are set"
            ;;
        "monitoring")
            log_troubleshooting "Monitoring" \
                "Check Prometheus: kubectl get pods -n monitoring" \
                "Verify Grafana access: kubectl get svc -n monitoring" \
                "Check metrics endpoints: kubectl get servicemonitor" \
                "Review storage availability for persistent volumes"
            ;;
        *)
            log_troubleshooting "$component" \
                "Check pod status: kubectl get pods -A | grep $component" \
                "Review logs: kubectl logs -l app=$component" \
                "Verify service endpoints: kubectl get svc | grep $component" \
                "Check ingress configuration: kubectl get ingress"
            ;;
    esac
}

# =============================================================================
# ðŸŽ¯ NEXT-STEPS GUIDANCE SYSTEM
# =============================================================================

# Show comprehensive next steps after component installation
show_component_next_steps() {
    local component="$1"
    
    case "$component" in
        "kubernetes")
            log_next_steps "Kubernetes Cluster" \
                "Verify cluster: kubectl get nodes" \
                "Check system pods: kubectl get pods -n kube-system" \
                "Install networking: gok install cert-manager" \
                "Install ingress: gok install ingress" \
                "Set up monitoring: gok install monitoring"
            
            log_urls "Cluster Information" \
                "Dashboard: https://$(fullDefaultUrl)/dashboard" \
                "API Server: https://$(kubectl config view --minify -o jsonpath='{.clusters[0].cluster.server}')"
            
            log_info "Next recommended components: cert-manager â†’ ingress â†’ monitoring"
            ;;
            
        "cert-manager")
            log_next_steps "Certificate Manager" \
                "Verify installation: kubectl get pods -n cert-manager" \
                "Test webhook: gok checkCMWebhook (after installing curl pod)" \
                "Install ingress: gok install ingress" \
                "Create certificates: gok create certificate production"
            
            log_info "Cert-manager is now managing TLS certificates automatically"
            log_warning "Let's Encrypt rate limits apply - use staging for testing"
            ;;
            
        "ingress")
            log_next_steps "NGINX Ingress Controller" \
                "Check ingress: kubectl get pods -n ingress-nginx" \
                "Verify external IP: kubectl get svc -n ingress-nginx" \
                "Test with sample app: gok deploy app1" \
                "Install dashboard: gok install dashboard"
            
            log_urls "Ingress Access" \
                "Default host: https://$(fullDefaultUrl)" \
                "Dashboard: https://$(fullDefaultUrl)/dashboard"
            
            log_info "All applications will now be accessible through ingress"
            ;;
            
        "monitoring")
            log_next_steps "Prometheus & Grafana Monitoring" \
                "Check monitoring pods: kubectl get pods -n monitoring" \
                "Access Grafana dashboard" \
                "Import custom dashboards" \
                "Configure alerting rules"
            
            log_urls "Monitoring Dashboards" \
                "Grafana: https://grafana.$(rootDomain)" \
                "Prometheus: https://prometheus.$(rootDomain)" \
                "AlertManager: https://alertmanager.$(rootDomain)"
            
            log_credentials "Grafana" "admin" \
                "Run: kubectl get secret -n monitoring grafana-admin-password -o jsonpath='{.data.password}' | base64 -d"
            
            log_info "Pre-configured dashboards: Kubernetes cluster, node metrics, pod metrics"
            ;;
            
        "vault")
            log_next_steps "HashiCorp Vault" \
                "Check vault status: kubectl get pods -n vault" \
                "Initialize vault: kubectl exec -n vault vault-0 -- vault operator init" \
                "Unseal vault with generated keys" \
                "Configure authentication methods" \
                "Set up secret engines and policies"
            
            log_urls "Vault Access" \
                "Vault UI: https://$(fullVaultUrl)" \
                "Vault API: https://$(fullVaultUrl)/v1/"
            
            log_warning "IMPORTANT: Save vault unseal keys and root token securely!"
            log_info "Vault is running in HA mode with auto-unseal configured"
            ;;
            
        "keycloak")
            log_next_steps "Keycloak Identity Management" \
                "Access Keycloak admin console" \
                "Create realms and clients" \
                "Configure user federation (LDAP if needed)" \
                "Set up OAuth2 integration: gok install oauth2"
            
            log_urls "Keycloak Access" \
                "Admin Console: https://$(fullKeycloakUrl)/admin" \
                "Account Console: https://$(fullKeycloakUrl)/realms/master/account"
            
            log_credentials "Keycloak Admin" "admin" \
                "Run: kubectl get secret -n keycloak keycloak-admin-password -o jsonpath='{.data.password}' | base64 -d"
            
            log_info "Default realm 'master' created. Create application-specific realms as needed."
            ;;
            
        "argocd")
            log_next_steps "ArgoCD GitOps" \
                "Access ArgoCD UI" \
                "Connect Git repositories" \
                "Create applications and sync policies" \
                "Set up RBAC and SSO integration"
            
            log_urls "ArgoCD Access" \
                "ArgoCD UI: https://argocd.$(rootDomain)" \
                "ArgoCD CLI: argocd login argocd.$(rootDomain)"
            
            log_credentials "ArgoCD Admin" "admin" \
                "Run: kubectl get secret -n argocd argocd-initial-admin-secret -o jsonpath='{.data.password}' | base64 -d"
            
            log_info "ArgoCD is configured for GitOps deployment workflows"
            ;;
            
        "jupyter")
            log_next_steps "JupyterHub Development Environment" \
                "Access JupyterHub" \
                "Configure user authentication" \
                "Install additional Python packages" \
                "Create shared notebooks and datasets"
            
            log_urls "JupyterHub Access" \
                "JupyterHub: https://jupyter.$(rootDomain)" \
                "Admin Panel: https://jupyter.$(rootDomain)/hub/admin"
            
            log_info "Default spawner configured with data science libraries pre-installed"
            ;;
            
        "registry")
            log_next_steps "Container Registry" \
                "Configure Docker daemon to use registry" \
                "Push first image: docker push $(fullRegistryUrl)/my-app:latest" \
                "Set up image scanning and policies" \
                "Configure registry webhooks"
            
            log_urls "Registry Access" \
                "Registry UI: https://$(fullRegistryUrl)" \
                "Registry API: https://$(fullRegistryUrl)/v2/"
            
            log_info "Registry is configured with TLS and basic authentication"
            ;;
            
        "gok-controller"|"controller")
            log_next_steps "GOK Platform Services" \
                "Verify controller: kubectl get pods -n gok-controller" \
                "Check agent: kubectl get pods -n gok-agent" \
                "Test command execution through web interface" \
                "Configure authentication and authorization"
            
            log_urls "GOK Platform Access" \
                "GOK Controller: https://gok-controller.$(rootDomain)" \
                "GOK Agent: Internal service for command execution"
            
            log_info "GOK distributed command execution platform is ready"
            ;;
            
        "istio")
            log_next_steps "Istio Service Mesh" \
                "Enable automatic sidecar injection for namespaces" \
                "Deploy sample applications with Istio" \
                "Configure traffic management and security policies" \
                "Set up observability with Kiali and Jaeger"
            
            log_info "Istio service mesh is installed and ready for microservices"
            ;;
            
        *)
            log_next_steps "$component Installation Complete" \
                "Check component status: kubectl get pods -A | grep $component" \
                "Review component logs: kubectl logs -l app=$component" \
                "Verify service endpoints: kubectl get svc | grep $component" \
                "Check ingress configuration: kubectl get ingress"
            
            log_info "Component $component has been installed successfully"
            ;;
    esac
}

# Show platform overview after major installations
show_platform_overview() {
    echo
    log_header "GOK Platform Overview" "Current Installation Status"
    
    echo -e "${COLOR_BRIGHT_CYAN}${COLOR_BOLD}ðŸ—ï¸  PLATFORM COMPONENTS STATUS${COLOR_RESET}"
    echo
    
    # Core Infrastructure
    echo -e "${COLOR_YELLOW}${COLOR_BOLD}Core Infrastructure:${COLOR_RESET}"
    check_and_display_component "kubernetes" "Kubernetes Cluster"
    check_and_display_component "cert-manager" "Certificate Manager"
    check_and_display_component "ingress" "NGINX Ingress"
    check_and_display_component "monitoring" "Prometheus & Grafana"
    echo
    
    # Security & Identity
    echo -e "${COLOR_YELLOW}${COLOR_BOLD}Security & Identity:${COLOR_RESET}"
    check_and_display_component "vault" "HashiCorp Vault"
    check_and_display_component "keycloak" "Keycloak"
    check_and_display_component "oauth2" "OAuth2 Proxy"
    echo
    
    # Development & DevOps
    echo -e "${COLOR_YELLOW}${COLOR_BOLD}Development & DevOps:${COLOR_RESET}"
    check_and_display_component "argocd" "ArgoCD"
    check_and_display_component "jenkins" "Jenkins"
    check_and_display_component "jupyter" "JupyterHub"
    check_and_display_component "registry" "Container Registry"
    echo
    
    # GOK Platform
    echo -e "${COLOR_YELLOW}${COLOR_BOLD}GOK Platform:${COLOR_RESET}"
    check_and_display_component "gok-controller" "GOK Controller"
    check_and_display_component "gok-agent" "GOK Agent"
    echo
    
    # Provide recommendations
    suggest_next_installations
}

# Check if component is installed and display status
check_and_display_component() {
    local component="$1"
    local display_name="$2"
    
    # Simple check - you can enhance this with actual status checking
    if kubectl get deployment "$component" >/dev/null 2>&1 || kubectl get statefulset "$component" >/dev/null 2>&1; then
        echo -e "  ${EMOJI_SUCCESS} ${COLOR_GREEN}$display_name${COLOR_RESET}"
    else
        echo -e "  ${EMOJI_CROSS} ${COLOR_DIM}$display_name${COLOR_RESET}"
    fi
}

# Suggest next installations based on current state
suggest_next_installations() {
    echo -e "${COLOR_BRIGHT_YELLOW}${COLOR_BOLD}ðŸ’¡ SUGGESTED NEXT STEPS${COLOR_RESET}"
    echo
    
    # Check if basic infrastructure is ready
    if ! kubectl get deployment cert-manager >/dev/null 2>&1; then
        echo -e "${COLOR_CYAN}â€¢ Install certificate management: ${COLOR_BOLD}gok install cert-manager${COLOR_RESET}"
    fi
    
    if ! kubectl get deployment ingress-nginx-controller >/dev/null 2>&1; then
        echo -e "${COLOR_CYAN}â€¢ Install ingress controller: ${COLOR_BOLD}gok install ingress${COLOR_RESET}"
    fi
    
    if ! kubectl get deployment -n monitoring prometheus-operator >/dev/null 2>&1; then
        echo -e "${COLOR_CYAN}â€¢ Install monitoring stack: ${COLOR_BOLD}gok install monitoring${COLOR_RESET}"
    fi
    
    if ! kubectl get statefulset -n vault vault >/dev/null 2>&1; then
        echo -e "${COLOR_CYAN}â€¢ Install secrets management: ${COLOR_BOLD}gok install vault${COLOR_RESET}"
    fi
    
    if ! kubectl get deployment -n keycloak keycloak >/dev/null 2>&1; then
        echo -e "${COLOR_CYAN}â€¢ Install identity management: ${COLOR_BOLD}gok install keycloak${COLOR_RESET}"
    fi
    
    echo -e "${COLOR_CYAN}â€¢ Generate your first microservice: ${COLOR_BOLD}gok generate python-api my-service${COLOR_RESET}"
    echo -e "${COLOR_CYAN}â€¢ Check complete platform status: ${COLOR_BOLD}gok status${COLOR_RESET}"
    echo
}

# Suggest and optionally install the next recommended module after successful installation
suggest_and_install_next_module() {
    local current_component="$1"
    local auto_install="${2:-false}"  # Optional parameter for auto-installation
    
    # Define the logical installation dependency chain
    declare -A NEXT_MODULE_MAP
    NEXT_MODULE_MAP["docker"]="kubernetes"
    NEXT_MODULE_MAP["kubernetes"]="ingress"
    NEXT_MODULE_MAP["ingress"]="cert-manager"
    NEXT_MODULE_MAP["cert-manager"]="kyverno"
    NEXT_MODULE_MAP["kyverno"]="registry"
    NEXT_MODULE_MAP["registry"]="base"
    NEXT_MODULE_MAP["base"]="ldap"
    NEXT_MODULE_MAP["ldap"]="keycloak"
    NEXT_MODULE_MAP["keycloak"]="oauth2"
    NEXT_MODULE_MAP["oauth2"]="rabbitmq"
    NEXT_MODULE_MAP["rabbitmq"]="vault"
    NEXT_MODULE_MAP["vault"]="monitoring"
    NEXT_MODULE_MAP["helm"]="kubernetes"
    
    # Define component descriptions
    declare -A MODULE_DESCRIPTIONS
    MODULE_DESCRIPTIONS["docker"]="Container runtime engine"
    MODULE_DESCRIPTIONS["kubernetes"]="Container orchestration platform"
    MODULE_DESCRIPTIONS["ingress"]="HTTP/HTTPS traffic routing and load balancing"
    MODULE_DESCRIPTIONS["cert-manager"]="Automated TLS certificate management"
    MODULE_DESCRIPTIONS["kyverno"]="Policy engine for Kubernetes security and governance"
    MODULE_DESCRIPTIONS["registry"]="Container image registry for storing and managing images"
    MODULE_DESCRIPTIONS["base"]="Core platform services and base infrastructure"
    MODULE_DESCRIPTIONS["ldap"]="LDAP directory service for user and group management"
    MODULE_DESCRIPTIONS["oauth2"]="Authentication proxy and SSO"
    MODULE_DESCRIPTIONS["rabbitmq"]="Message broker for asynchronous communication"
    MODULE_DESCRIPTIONS["vault"]="Secrets management and secure storage"
    MODULE_DESCRIPTIONS["monitoring"]="Prometheus & Grafana observability stack"
    MODULE_DESCRIPTIONS["keycloak"]="Identity and access management"
    MODULE_DESCRIPTIONS["argocd"]="GitOps continuous delivery"
    MODULE_DESCRIPTIONS["jenkins"]="CI/CD automation and pipeline management"
    MODULE_DESCRIPTIONS["jupyter"]="Interactive data science and development"
    MODULE_DESCRIPTIONS["dashboard"]="Kubernetes web-based management interface"
    
    # Get the next recommended module
    local next_module="${NEXT_MODULE_MAP[$current_component]}"
    
    if [[ -z "$next_module" ]]; then
        # No specific next module defined, show general suggestions
        log_info "ðŸŽ‰ $current_component installation completed successfully!"
        echo ""
        suggest_next_installations
        return 0
    fi
    
    # Check if the next module is already installed
    local is_installed=false
    case "$next_module" in
        "kubernetes")
            kubectl get nodes >/dev/null 2>&1 && is_installed=true
            ;;
        "ingress")
            kubectl get deployment ingress-nginx-controller -n ingress-nginx >/dev/null 2>&1 && is_installed=true
            ;;
        "cert-manager")
            kubectl get deployment cert-manager -n cert-manager >/dev/null 2>&1 && is_installed=true
            ;;
        "kyverno")
            kubectl get deployment kyverno -n kyverno >/dev/null 2>&1 && is_installed=true
            ;;
        "registry")
            kubectl get deployment registry -n registry >/dev/null 2>&1 && is_installed=true
            ;;
        "base")
            kubectl get configmap base-config -n kube-system >/dev/null 2>&1 && is_installed=true
            ;;
        "ldap")
            kubectl get deployment ldap -n ldap >/dev/null 2>&1 && is_installed=true
            ;;
        "oauth2")
            kubectl get deployment oauth2-proxy -n oauth2 >/dev/null 2>&1 && is_installed=true
            ;;
        "rabbitmq")
            kubectl get statefulset rabbitmq -n rabbitmq >/dev/null 2>&1 && is_installed=true
            ;;
        "vault")
            kubectl get statefulset -n vault vault >/dev/null 2>&1 && is_installed=true
            ;;
        "monitoring")
            kubectl get deployment -n monitoring prometheus-operator >/dev/null 2>&1 && is_installed=true
            ;;
        "keycloak")
            kubectl get deployment -n keycloak keycloak >/dev/null 2>&1 && is_installed=true
            ;;
        "argocd")
            kubectl get deployment -n argocd argocd-server >/dev/null 2>&1 && is_installed=true
            ;;
        "docker")
            command -v docker >/dev/null 2>&1 && docker info >/dev/null 2>&1 && is_installed=true
            ;;
    esac
    
    echo ""
    log_header "Next Recommended Installation" "Suggested Module: $next_module"
    
    if [[ "$is_installed" == "true" ]]; then
        log_success "âœ“ $next_module is already installed"
        echo -e "  ${COLOR_DIM}${MODULE_DESCRIPTIONS[$next_module]}${COLOR_RESET}"
        echo ""
        # Recursively suggest the next module after this one
        suggest_and_install_next_module "$next_module" "$auto_install"
        return 0
    fi
    
    # Show the recommendation
    echo -e "${COLOR_BRIGHT_GREEN}${COLOR_BOLD}ðŸŽ¯ Recommended Next Step:${COLOR_RESET}"
    echo ""
    echo -e "  ${COLOR_CYAN}Module:${COLOR_RESET} ${COLOR_BOLD}$next_module${COLOR_RESET}"
    echo -e "  ${COLOR_CYAN}Purpose:${COLOR_RESET} ${MODULE_DESCRIPTIONS[$next_module]}"
    echo -e "  ${COLOR_CYAN}Command:${COLOR_RESET} ${COLOR_BOLD}gok install $next_module${COLOR_RESET}"
    echo ""
    
    # Show why this is recommended after the current component
    case "$current_component -> $next_module" in
        "kubernetes -> ingress")
            echo -e "${COLOR_YELLOW}ðŸ“‹ Why install ingress next?${COLOR_RESET}"
            echo -e "  â€¢ Enables external HTTP/HTTPS access to cluster services"
            echo -e "  â€¢ Provides load balancing and traffic routing"
            echo -e "  â€¢ Essential for accessing web-based applications and dashboards"
            echo -e "  â€¢ Foundation for exposing services to external users"
            ;;
        "ingress -> cert-manager")
            echo -e "${COLOR_YELLOW}ðŸ“‹ Why install cert-manager next?${COLOR_RESET}"
            echo -e "  â€¢ Provides automated TLS certificate management"
            echo -e "  â€¢ Secures ingress traffic with HTTPS encryption"
            echo -e "  â€¢ Integrates with Let's Encrypt for free certificates"
            echo -e "  â€¢ Required for production-grade secure communications"
            ;;
        "cert-manager -> kyverno")
            echo -e "${COLOR_YELLOW}ðŸ“‹ Why install kyverno next?${COLOR_RESET}"
            echo -e "  â€¢ Policy engine for Kubernetes security and governance"
            echo -e "  â€¢ Enforces security policies and best practices"
            echo -e "  â€¢ Validates and mutates Kubernetes resources"
            echo -e "  â€¢ Essential for compliance and security hardening"
            ;;
        "kyverno -> registry")
            echo -e "${COLOR_YELLOW}ðŸ“‹ Why install registry next?${COLOR_RESET}"
            echo -e "  â€¢ Private container image registry for your applications"
            echo -e "  â€¢ Secure storage and management of container images"
            echo -e "  â€¢ Required for deploying custom applications"
            echo -e "  â€¢ Integrates with CI/CD pipelines for image distribution"
            ;;
        "registry -> base")
            echo -e "${COLOR_YELLOW}ðŸ“‹ Why install base next?${COLOR_RESET}"
            echo -e "  â€¢ Core platform services and base infrastructure"
            echo -e "  â€¢ Provides shared services and configurations"
            echo -e "  â€¢ Foundation for other platform components"
            echo -e "  â€¢ Sets up common utilities and base services"
            ;;
        "base -> ldap")
            echo -e "${COLOR_YELLOW}ðŸ“‹ Why install ldap next?${COLOR_RESET}"
            echo -e "  â€¢ LDAP directory service for centralized user management"
            echo -e "  â€¢ Provides user and group authentication"
            echo -e "  â€¢ Foundation for enterprise identity integration"
            echo -e "  â€¢ Required for OAuth2 and SSO functionality"
            ;;
        "ldap -> oauth2")
            echo -e "${COLOR_YELLOW}ðŸ“‹ Why install oauth2 next?${COLOR_RESET}"
            echo -e "  â€¢ Authentication proxy for securing applications"
            echo -e "  â€¢ Integrates with LDAP for user authentication"
            echo -e "  â€¢ Provides single sign-on (SSO) across services"
            echo -e "  â€¢ Protects cluster resources and dashboards"
            ;;
        "oauth2 -> rabbitmq")
            echo -e "${COLOR_YELLOW}ðŸ“‹ Why install rabbitmq next?${COLOR_RESET}"
            echo -e "  â€¢ Message broker for asynchronous communication"
            echo -e "  â€¢ Enables event-driven architecture patterns"
            echo -e "  â€¢ Provides reliable message queuing and routing"
            echo -e "  â€¢ Essential for microservices communication"
            ;;
        "rabbitmq -> vault")
            echo -e "${COLOR_YELLOW}ðŸ“‹ Why install vault next?${COLOR_RESET}"
            echo -e "  â€¢ Secure secrets management for applications"
            echo -e "  â€¢ Centralized storage for API keys, passwords, certificates"
            echo -e "  â€¢ Integrates with RabbitMQ for secure credential storage"
            echo -e "  â€¢ Required for production-grade security"
            ;;
        "vault -> monitoring")
            echo -e "${COLOR_YELLOW}ðŸ“‹ Why install monitoring next?${COLOR_RESET}"
            echo -e "  â€¢ Comprehensive cluster observability with Prometheus & Grafana"
            echo -e "  â€¢ Essential for production cluster health monitoring"
            echo -e "  â€¢ Monitors all previously installed components"
            echo -e "  â€¢ Provides alerting and performance insights"
            ;;
    esac
    echo ""
    
    # Offer to install the next module
    if [[ "$auto_install" == "true" ]]; then
        log_info "Auto-installing $next_module..."
        gok install "$next_module"
        return $?
    else
        echo -e "${COLOR_BRIGHT_YELLOW}${COLOR_BOLD}ðŸš€ Would you like to install $next_module now?${COLOR_RESET}"
        echo -e "${COLOR_DIM}This will start the installation process for the next recommended component.${COLOR_RESET}"
        echo ""
        
        read -p "Install $next_module now? (y/N): " -r install_choice
        if [[ "$install_choice" =~ ^[Yy]$ ]]; then
            echo ""
            log_info "Starting installation of $next_module..."
            echo ""
            gok install "$next_module"
            return $?
        else
            log_info "Skipping $next_module installation"
            echo -e "  ${COLOR_DIM}You can install it later with: ${COLOR_BOLD}gok install $next_module${COLOR_RESET}"
            echo ""
            
            # Show alternative next steps
            echo -e "${COLOR_CYAN}Alternative next steps:${COLOR_RESET}"
            echo -e "  â€¢ View platform status: ${COLOR_BOLD}gok status${COLOR_RESET}"
            echo -e "  â€¢ Check component summary: ${COLOR_BOLD}gok ${current_component}Summary${COLOR_RESET}"
            echo -e "  â€¢ Install different component: ${COLOR_BOLD}gok install help${COLOR_RESET}"
            echo ""
            return 0
        fi
    fi
}

# =============================================================================
# ï¿½ REPOSITORY FIX UTILITIES
# =============================================================================

# Fix Helm repository 404 errors and other package repository issues
fix_helm_repository_errors() {
    log_header "Repository Fix Utility" "Resolving Helm & Package Repository Issues"
    
    echo -e "${COLOR_BRIGHT_CYAN}${COLOR_BOLD}ðŸ”§ FIXING HELM REPOSITORY ERRORS${COLOR_RESET}"
    echo
    
    local fix_success=true
    
    # Step 1: Clean up old Helm repositories
    log_step "1" "Removing old/problematic Helm repositories"
    if sudo find /etc/apt/sources.list.d -name "*helm*" -type f -delete 2>/dev/null; then
        log_success "Old Helm repository files removed"
    else
        log_info "No old Helm repository files found"
    fi
    
    # Remove deprecated apt-key entries
    if sudo apt-key list 2>/dev/null | grep -q "Helm"; then
        log_info "Removing deprecated Helm apt-key entries"
        sudo apt-key del $(sudo apt-key list 2>/dev/null | grep -A1 "Helm" | grep pub | cut -d'/' -f2 | cut -d' ' -f1) >/dev/null 2>&1 || true
        log_success "Deprecated key entries cleaned"
    fi
    
    # Step 2: Update package lists after cleanup
    log_step "2" "Updating package lists after cleanup"
    if sudo apt-get update >/dev/null 2>&1; then
        log_success "Package lists updated successfully"
    else
        log_warning "Package update had some warnings (continuing...)"
    fi
    
    # Step 3: Verify Helm installation method
    log_step "3" "Checking current Helm installation"
    if command -v helm >/dev/null 2>&1; then
        local helm_path=$(which helm)
        local helm_version=$(helm version --short --client 2>/dev/null | grep -oE 'v[0-9]+\.[0-9]+\.[0-9]+' || echo "unknown")
        
        if [[ "$helm_path" == *"/snap/"* ]]; then
            log_success "Helm is properly installed via snap (version: $helm_version)"
            echo -e "  ${COLOR_GREEN}âœ“ Path: $helm_path${COLOR_RESET}"
            echo -e "  ${COLOR_GREEN}âœ“ Installation method: Snap package manager${COLOR_RESET}"
            echo -e "  ${COLOR_GREEN}âœ“ No APT repository conflicts${COLOR_RESET}"
        else
            log_info "Helm installed via: $helm_path (version: $helm_version)"
        fi
    else
        log_warning "Helm not found - would you like to install it?"
        echo -e "  ${COLOR_YELLOW}Run: ${COLOR_BOLD}sudo ./gok install helm${COLOR_RESET}"
    fi
    
    # Step 4: Setup modern Helm repository (if using APT method)
    log_step "4" "Setting up modern Helm repository (if needed)"
    if [[ "$helm_path" != *"/snap/"* ]] && command -v helm >/dev/null 2>&1; then
        log_info "Setting up modern Helm APT repository..."
        if install_helm_via_apt_fix; then
            log_success "Modern Helm repository configured successfully"
        else
            log_warning "Modern APT setup failed - recommend using snap: sudo snap install helm --classic"
            fix_success=false
        fi
    else
        log_info "Using snap installation - no APT repository needed"
    fi
    
    # Step 5: Test repository access
    log_step "5" "Testing package repository access"
    if sudo apt-get update >/dev/null 2>&1; then
        log_success "All package repositories accessible"
        
        # Check for any remaining 404 errors
        local error_check=$(sudo apt-get update 2>&1 | grep -i "404\|not found\|failed\|err:" || true)
        if [[ -z "$error_check" ]]; then
            log_success "No 404 or repository errors detected"
        else
            log_warning "Some repository warnings detected:"
            echo "$error_check" | head -3
        fi
    else
        log_error "Package repository access issues detected"
        fix_success=false
    fi
    
    # Summary and recommendations
    echo
    log_header "Fix Summary" "Repository Status"
    
    if [[ "$fix_success" == "true" ]]; then
        echo -e "${COLOR_BRIGHT_GREEN}${COLOR_BOLD}âœ… REPOSITORY ISSUES RESOLVED${COLOR_RESET}"
        echo
        echo -e "${COLOR_GREEN}âœ“${COLOR_RESET} Old Helm repositories cleaned up"
        echo -e "${COLOR_GREEN}âœ“${COLOR_RESET} Package lists updated successfully"
        echo -e "${COLOR_GREEN}âœ“${COLOR_RESET} No 404 errors detected"
        echo -e "${COLOR_GREEN}âœ“${COLOR_RESET} Helm installation verified"
    else
        echo -e "${COLOR_BRIGHT_YELLOW}${COLOR_BOLD}âš ï¸  PARTIAL RESOLUTION${COLOR_RESET}"
        echo
        echo -e "${COLOR_YELLOW}!${COLOR_RESET} Some issues may require manual intervention"
    fi
    
    echo
    echo -e "${COLOR_BRIGHT_CYAN}${COLOR_BOLD}ðŸ“‹ PREVENTION TIPS:${COLOR_RESET}"
    echo -e "${COLOR_CYAN}â€¢ Use snap for Helm: ${COLOR_BOLD}sudo snap install helm --classic${COLOR_RESET}"
    echo -e "${COLOR_CYAN}â€¢ Regular updates: ${COLOR_BOLD}sudo snap refresh helm${COLOR_RESET}"
    echo -e "${COLOR_CYAN}â€¢ Avoid mixing installation methods (APT + Snap + Script)${COLOR_RESET}"
    echo -e "${COLOR_CYAN}â€¢ Run this fix utility: ${COLOR_BOLD}./gok fix helm-repository${COLOR_RESET}"
    echo
    
    echo -e "${COLOR_BRIGHT_CYAN}${COLOR_BOLD}ðŸ› ï¸  TROUBLESHOOTING:${COLOR_RESET}"
    echo -e "${COLOR_CYAN}â€¢ If issues persist: ${COLOR_BOLD}sudo apt-get clean && sudo apt-get update${COLOR_RESET}"
    echo -e "${COLOR_CYAN}â€¢ Check logs: ${COLOR_BOLD}sudo apt-get update -o Debug::Acquire::http=true${COLOR_RESET}"
    echo -e "${COLOR_CYAN}â€¢ Fresh Helm install: ${COLOR_BOLD}sudo snap remove helm && sudo snap install helm --classic${COLOR_RESET}"
    echo
    
    return $([[ "$fix_success" == "true" ]] && echo 0 || echo 1)
}

# Helper function for modern Helm APT setup
install_helm_via_apt_fix() {
    # Clean up any old repositories first
    sudo rm -f /etc/apt/sources.list.d/helm*.list 2>/dev/null || true
    
    # Add official Helm APT repository with proper key management
    if curl -fsSL https://baltocdn.com/helm/signing.asc | gpg --dearmor | sudo tee /usr/share/keyrings/helm.gpg > /dev/null 2>&1; then
        echo "deb [arch=$(dpkg --print-architecture) signed-by=/usr/share/keyrings/helm.gpg] https://baltocdn.com/helm/stable/debian/ all main" | sudo tee /etc/apt/sources.list.d/helm-stable-debian.list >/dev/null
        
        if sudo apt-get update >/dev/null 2>&1; then
            return 0
        fi
    fi
    return 1
}

# =============================================================================
# ï¿½ðŸ” INSTALLATION VALIDATION SYSTEM
# =============================================================================

# Validate component installation with comprehensive health checks
validate_component_installation() {
    local component="$1"
    local timeout="${2:-300}" # Default 5 minutes timeout
    
    log_info "Validating $component installation..."
    
    case "$component" in
        "kubernetes")
            validate_kubernetes_cluster "$timeout"
            ;;
        "docker")
            validate_docker_installation "$timeout"
            ;;
        "helm")
            validate_helm_installation "$timeout"
            ;;
        "cert-manager")
            validate_cert_manager "$timeout"
            ;;
        "ingress")
            validate_ingress_controller "$timeout"
            ;;
        "monitoring")
            validate_monitoring_stack "$timeout"
            ;;
        "vault")
            validate_vault_installation "$timeout"
            ;;
        "keycloak")
            validate_keycloak_installation "$timeout"
            ;;
        "argocd")
            validate_argocd_installation "$timeout"
            ;;
        "jupyter")
            validate_jupyter_installation "$timeout"
            ;;
        "registry")
            validate_registry_installation "$timeout"
            ;;
        "base")
            validate_base_installation "$timeout"
            ;;
        "gok-controller"|"controller")
            validate_gok_controller_installation "$timeout"
            ;;
        *)
            validate_generic_component "$component" "$timeout"
            ;;
    esac
}

# Generic component validation with enhanced checks
validate_generic_component() {
    local component="$1"
    local timeout="$2"
    local validation_passed=true
    
    log_step "1" "Checking $component deployments"
    local deployments=$(kubectl get deployment -A --no-headers 2>/dev/null | grep "$component" | head -3)
    if [[ -n "$deployments" ]]; then
        echo "$deployments" | while read -r deploy_line; do
            local namespace=$(echo "$deploy_line" | awk '{print $1}')
            local deployment=$(echo "$deploy_line" | awk '{print $2}')
            if check_deployment_readiness "$deployment" "$namespace"; then
                log_success "Deployment $deployment is ready in $namespace"
            else
                log_warning "Deployment $deployment has issues in $namespace"
            fi
        done
    fi
    
    log_step "2" "Checking $component pods"
    if ! wait_for_pods_ready "$component" "$timeout" "$component"; then
        log_error "Pods not ready for $component"
        validation_passed=false
    else
        log_success "All $component pods are ready"
    fi
    
    log_step "3" "Checking $component services"
    local services=$(kubectl get svc -A --no-headers 2>/dev/null | grep "$component" | head -5)
    if [[ -n "$services" ]]; then
        log_success "$component services are available"
        echo "$services" | while read -r svc_line; do
            local namespace=$(echo "$svc_line" | awk '{print $1}')
            local service=$(echo "$svc_line" | awk '{print $2}')
            if check_service_connectivity "$service" "$namespace"; then
                echo -e "${COLOR_GREEN}    âœ“ Service $service in $namespace is accessible${COLOR_RESET}"
            else
                echo -e "${COLOR_YELLOW}    âš  Service $service in $namespace has connectivity issues${COLOR_RESET}"
            fi
        done
    else
        log_warning "No services found for $component"
    fi
    
    return $([[ "$validation_passed" == "true" ]] && echo 0 || echo 1)
}

# Docker installation validation
validate_docker_installation() {
    local timeout="$1"
    local validation_passed=true
    
    log_step "1" "Checking Docker daemon status"
    if systemctl is-active --quiet docker; then
        log_success "Docker daemon is running"
    else
        log_error "Docker daemon is not running"
        validation_passed=false
    fi
    
    log_step "2" "Checking Docker version"
    if docker --version >/dev/null 2>&1; then
        local version=$(docker --version | grep -oE '[0-9]+\.[0-9]+\.[0-9]+')
        log_success "Docker version: $version"
    else
        log_error "Cannot get Docker version"
        validation_passed=false
    fi
    
    log_step "3" "Testing Docker functionality"
    if docker run --rm hello-world >/dev/null 2>&1; then
        log_success "Docker can run containers successfully"
    else
        log_warning "Docker container test failed (may require proxy configuration)"
    fi
    
    log_step "4" "Checking Docker group membership"
    if groups $USER | grep -q docker; then
        log_success "User is in docker group"
    else
        log_warning "User not in docker group - may need 'sudo' for docker commands"
    fi
    
    return $([[ "$validation_passed" == "true" ]] && echo 0 || echo 1)
}

# Helm installation validation
validate_helm_installation() {
    local timeout="$1"
    local validation_passed=true
    
    log_step "1" "Checking Helm binary"
    if command -v helm >/dev/null 2>&1; then
        local version=$(helm version --short --client 2>/dev/null | grep -oE 'v[0-9]+\.[0-9]+\.[0-9]+' || echo "unknown")
        log_success "Helm is installed (version: $version)"
    else
        log_error "Helm binary not found in PATH"
        validation_passed=false
        return 1
    fi
    
    log_step "2" "Testing Helm functionality"
    if helm version --short >/dev/null 2>&1; then
        log_success "Helm version command works"
    else
        log_error "Helm version command failed"
        validation_passed=false
    fi
    
    log_step "3" "Checking Helm repositories"
    local repo_count=$(helm repo list 2>/dev/null | tail -n +2 | wc -l)
    if [[ $repo_count -gt 0 ]]; then
        log_success "Helm has $repo_count repositories configured"
    else
        log_info "No Helm repositories configured yet (this is normal for new installations)"
    fi
    
    log_step "4" "Testing repository access"
    if helm repo add bitnami https://charts.bitnami.com/bitnami >/dev/null 2>&1 && \
       helm repo update >/dev/null 2>&1; then
        log_success "Helm can access remote repositories"
        # Clean up test repository
        helm repo remove bitnami >/dev/null 2>&1 || true
    else
        log_warning "Cannot access remote repositories (may require proxy configuration)"
    fi
    
    return $([[ "$validation_passed" == "true" ]] && echo 0 || echo 1)
}

# Kubernetes cluster validation
validate_kubernetes_cluster() {
    local timeout="$1"
    local validation_passed=true
    
    log_step "1" "Checking cluster API server connectivity"
    if kubectl cluster-info >/dev/null 2>&1; then
        log_success "Kubernetes API server is accessible"
    else
        log_error "Cannot connect to Kubernetes API server"
        validation_passed=false
    fi
    
    log_step "2" "Checking node status"
    local ready_nodes=$(kubectl get nodes --no-headers | grep -c " Ready ")
    local total_nodes=$(kubectl get nodes --no-headers | wc -l)
    
    if [[ $ready_nodes -eq $total_nodes && $ready_nodes -gt 0 ]]; then
        log_success "All $total_nodes nodes are ready"
    else
        log_error "Only $ready_nodes out of $total_nodes nodes are ready"
        validation_passed=false
    fi
    
    log_step "3" "Checking system components"
    if wait_for_pods_ready "kube-system" "$timeout"; then
        log_success "All system components are ready"
    else
        log_error "Some system components are not ready"
        validation_passed=false
    fi
    
    return $([[ "$validation_passed" == "true" ]] && echo 0 || echo 1)
}

# Cert-manager validation
validate_cert_manager() {
    local timeout="$1"
    local validation_passed=true
    
    log_step "1" "Checking cert-manager deployment"
    if check_deployment_readiness "cert-manager" "cert-manager"; then
        log_success "Cert-manager deployment is ready"
    else
        log_error "Cert-manager deployment has issues"
        validation_passed=false
    fi
    
    log_step "2" "Checking cert-manager pods"
    if ! wait_for_pods_ready "cert-manager" "$timeout" "cert-manager"; then
        log_error "Cert-manager pods not ready"
        validation_passed=false
    else
        log_success "Cert-manager pods are ready"
    fi
    
    log_step "2" "Checking cert-manager webhook"
    if kubectl get validatingwebhookconfiguration cert-manager-webhook >/dev/null 2>&1; then
        log_success "Cert-manager webhook is configured"
    else
        log_error "Cert-manager webhook not found"
        validation_passed=false
    fi
    
    log_step "3" "Testing cert-manager functionality"
    # Create a test certificate using the correct ClusterIssuer name
    cat <<EOF | kubectl apply -f - >/dev/null 2>&1
apiVersion: cert-manager.io/v1
kind: Certificate
metadata:
  name: test-certificate
  namespace: default
spec:
  secretName: test-certificate-secret
  issuerRef:
    name: selfsigned-cluster-issuer
    kind: ClusterIssuer
  dnsNames:
  - test.example.com
EOF
    
    # Wait for certificate to be processed (increased timeout for stability)
    sleep 15
    
    # Check certificate status with debug information
    local cert_status=$(kubectl get certificate test-certificate -o jsonpath='{.status.conditions[0].type}' 2>/dev/null)
    
    if [[ "$cert_status" == "Ready" ]]; then
        log_success "Cert-manager is functioning correctly"
        kubectl delete certificate test-certificate >/dev/null 2>&1
        kubectl delete secret test-certificate-secret >/dev/null 2>&1
    else
        log_warning "Cert-manager test certificate creation needs verification"
        log_info "Certificate status: '$cert_status' (expected: 'Ready')"
        # Clean up failed certificate
        kubectl delete certificate test-certificate >/dev/null 2>&1
        kubectl delete secret test-certificate-secret >/dev/null 2>&1
    fi
    
    return $([[ "$validation_passed" == "true" ]] && echo 0 || echo 1)
}

# Ingress controller validation with enhanced checks
validate_ingress_controller() {
    local timeout="$1"
    local validation_passed=true
    
    log_step "1" "Checking ingress controller deployment"
    if check_deployment_readiness "ingress-nginx-controller" "ingress-nginx"; then
        log_success "Ingress controller deployment is ready"
    else
        log_error "Ingress controller deployment has issues"
        validation_passed=false
    fi
    
    log_step "2" "Checking ingress controller pods"
    if ! wait_for_pods_ready "ingress-nginx" "$timeout" "ingress-nginx"; then
        log_error "Ingress controller pods not ready"
        validation_passed=false
    else
        log_success "Ingress controller pods are ready"
    fi
    
    log_step "3" "Checking ingress controller service"
    if check_service_connectivity "ingress-nginx-controller" "ingress-nginx"; then
        log_success "Ingress controller service is accessible"
        
        # Check for external IP or NodePort
        local service_type=$(kubectl get svc -n ingress-nginx ingress-nginx-controller -o jsonpath='{.spec.type}' 2>/dev/null)
        if [[ "$service_type" == "LoadBalancer" ]]; then
            local external_ip=$(kubectl get svc -n ingress-nginx ingress-nginx-controller -o jsonpath='{.status.loadBalancer.ingress[0].ip}' 2>/dev/null)
            if [[ -n "$external_ip" && "$external_ip" != "null" ]]; then
                log_success "LoadBalancer has external IP: $external_ip"
            else
                log_warning "LoadBalancer service exists but no external IP assigned yet"
            fi
        elif [[ "$service_type" == "NodePort" ]]; then
            local node_port=$(kubectl get svc -n ingress-nginx ingress-nginx-controller -o jsonpath='{.spec.ports[?(@.name=="http")].nodePort}' 2>/dev/null)
            log_info "NodePort service configured on port: $node_port"
        fi
    else
        log_error "Ingress controller service not accessible"
        validation_passed=false
    fi
    
    log_step "4" "Checking ingress class"
    if kubectl get ingressclass nginx >/dev/null 2>&1; then
        log_success "Nginx ingress class is available"
    else
        log_warning "Nginx ingress class not found"
    fi
    
    return $([[ "$validation_passed" == "true" ]] && echo 0 || echo 1)
}

# Monitoring stack validation
validate_monitoring_stack() {
    local timeout="$1"
    local validation_passed=true
    
    log_step "1" "Checking monitoring namespace pods"
    if ! wait_for_pods_ready "monitoring" "$timeout"; then
        log_error "Monitoring pods not ready"
        validation_passed=false
    else
        log_success "Monitoring pods are ready"
    fi
    
    log_step "2" "Checking Prometheus"
    if kubectl get statefulset -n monitoring prometheus-prometheus >/dev/null 2>&1; then
        log_success "Prometheus is deployed"
    else
        log_error "Prometheus not found"
        validation_passed=false
    fi
    
    log_step "3" "Checking Grafana"
    if kubectl get deployment -n monitoring grafana >/dev/null 2>&1; then
        log_success "Grafana is deployed"
    else
        log_error "Grafana not found"
        validation_passed=false
    fi
    
    return $([[ "$validation_passed" == "true" ]] && echo 0 || echo 1)
}

# Vault installation validation
validate_vault_installation() {
    local timeout="$1"
    local validation_passed=true
    
    log_step "1" "Checking Vault pods"
    if ! wait_for_pods_ready "vault" "$timeout"; then
        log_error "Vault pods not ready"
        validation_passed=false
    else
        log_success "Vault pods are ready"
    fi
    
    log_step "2" "Checking Vault status"
    local vault_status=$(kubectl exec -n vault vault-0 -- vault status -format=json 2>/dev/null | jq -r '.sealed // "unknown"')
    case "$vault_status" in
        "false")
            log_success "Vault is unsealed and ready"
            ;;
        "true")
            log_warning "Vault is sealed - requires manual unsealing"
            ;;
        *)
            log_warning "Could not determine Vault status"
            ;;
    esac
    
    return $([[ "$validation_passed" == "true" ]] && echo 0 || echo 1)
}

# GOK Controller validation
validate_gok_controller_installation() {
    local timeout="$1"
    local validation_passed=true
    
    log_step "1" "Checking GOK Controller pods"
    if ! wait_for_pods_ready "gok-controller" "$timeout"; then
        log_error "GOK Controller pods not ready"
        validation_passed=false
    else
        log_success "GOK Controller pods are ready"
    fi
    
    log_step "2" "Checking GOK Agent pods"
    if ! wait_for_pods_ready "gok-agent" "$timeout"; then
        log_error "GOK Agent pods not ready"
        validation_passed=false
    else
        log_success "GOK Agent pods are ready"
    fi
    
    return $([[ "$validation_passed" == "true" ]] && echo 0 || echo 1)
}

# =============================================================================
# ðŸ” ENHANCED DEPLOYMENT VERIFICATION SYSTEM
# =============================================================================

# Check for Docker image pull issues in pods
check_image_pull_issues() {
    local namespace="$1"
    local component="$2"
    
    log_info "ðŸ³ Checking for Docker image pull issues in $component..."
    
    # Get pods with image pull errors
    local image_pull_pods=$(kubectl get pods -n "$namespace" -o jsonpath='{range .items[*]}{.metadata.name}{" "}{.status.containerStatuses[*].state.waiting.reason}{"\n"}{end}' 2>/dev/null | grep -E "(ImagePullBackOff|ErrImagePull)" | cut -d' ' -f1)
    
    if [[ -n "$image_pull_pods" ]]; then
        log_error "Found pods with image pull issues:"
        echo "$image_pull_pods" | while read -r pod; do
            if [[ -n "$pod" ]]; then
                echo -e "${COLOR_RED}  âŒ Pod: ${COLOR_BOLD}$pod${COLOR_RESET}"
                
                # Get detailed image pull error information
                local image_error=$(kubectl describe pod "$pod" -n "$namespace" 2>/dev/null | grep -A 3 -B 1 "Failed to pull image\|Error response from daemon")
                if [[ -n "$image_error" ]]; then
                    echo -e "${COLOR_YELLOW}     Error details:${COLOR_RESET}"
                    echo "$image_error" | sed 's/^/       /'
                fi
                
                # Get the image name causing issues
                local failed_image=$(kubectl get pod "$pod" -n "$namespace" -o jsonpath='{.spec.containers[*].image}' 2>/dev/null)
                if [[ -n "$failed_image" ]]; then
                    echo -e "${COLOR_CYAN}     Image: ${COLOR_BOLD}$failed_image${COLOR_RESET}"
                fi
            fi
        done
        
        # Provide troubleshooting suggestions
        echo
        echo -e "${COLOR_BRIGHT_CYAN}${COLOR_BOLD}ðŸ› ï¸  TROUBLESHOOTING SUGGESTIONS:${COLOR_RESET}"
        echo -e "${COLOR_CYAN}â€¢ Check Docker registry connectivity: ${COLOR_BOLD}docker pull <image>${COLOR_RESET}"
        echo -e "${COLOR_CYAN}â€¢ Verify image exists in registry: ${COLOR_BOLD}curl -s <registry-url>/v2/<image>/tags/list${COLOR_RESET}"
        echo -e "${COLOR_CYAN}â€¢ Check image pull secrets: ${COLOR_BOLD}kubectl get secrets -n $namespace${COLOR_RESET}"
        echo -e "${COLOR_CYAN}â€¢ Verify network/proxy settings for image registry access${COLOR_RESET}"
        echo
        return 1
    else
        log_success "No Docker image pull issues found"
        return 0
    fi
}

# Check for resource constraint issues
check_resource_constraints() {
    local namespace="$1"
    local component="$2"
    
    log_info "ðŸ“Š Checking for resource constraint issues in $component..."
    
    # Check for pods with resource-related issues
    local resource_issues=$(kubectl get pods -n "$namespace" -o jsonpath='{range .items[*]}{.metadata.name}{" "}{.status.containerStatuses[*].state.waiting.reason}{" "}{.status.phase}{"\n"}{end}' 2>/dev/null | grep -E "(OutOfMemory|OutOfCpu|Evicted|OutOfDisk)")
    
    if [[ -n "$resource_issues" ]]; then
        log_warning "Found pods with resource constraint issues:"
        echo "$resource_issues" | while read -r line; do
            if [[ -n "$line" ]]; then
                local pod=$(echo "$line" | cut -d' ' -f1)
                echo -e "${COLOR_YELLOW}  âš ï¸  Pod: ${COLOR_BOLD}$pod${COLOR_RESET}"
                
                # Get resource usage and limits
                kubectl top pod "$pod" -n "$namespace" 2>/dev/null | tail -n +2 | while read -r usage_line; do
                    echo -e "${COLOR_CYAN}     Resource usage: $usage_line${COLOR_RESET}"
                done
            fi
        done
        
        # Show node resource availability
        echo -e "${COLOR_BRIGHT_CYAN}${COLOR_BOLD}ðŸ“ˆ NODE RESOURCE STATUS:${COLOR_RESET}"
        kubectl top nodes 2>/dev/null | head -10
        echo
        return 1
    else
        log_success "No resource constraint issues found"
        return 0
    fi
}

# Check deployment readiness with detailed diagnostics
check_deployment_readiness() {
    local deployment="$1"
    local namespace="$2"
    
    log_info "ðŸš€ Analyzing deployment readiness: $deployment"
    
    # Get deployment status
    local deployment_info=$(kubectl get deployment "$deployment" -n "$namespace" -o jsonpath='{.status.readyReplicas}/{.status.replicas} {.status.conditions[?(@.type=="Available")].status} {.status.conditions[?(@.type=="Progressing")].status}' 2>/dev/null)
    
    if [[ -z "$deployment_info" ]]; then
        log_error "Deployment $deployment not found in namespace $namespace"
        return 1
    fi
    
    local ready_replicas=$(echo "$deployment_info" | cut -d' ' -f1 | cut -d'/' -f1)
    local total_replicas=$(echo "$deployment_info" | cut -d' ' -f1 | cut -d'/' -f2)
    local available_status=$(echo "$deployment_info" | cut -d' ' -f2)
    local progressing_status=$(echo "$deployment_info" | cut -d' ' -f3)
    
    echo -e "${COLOR_CYAN}  ðŸ“Š Ready: ${COLOR_BOLD}${ready_replicas:-0}/${total_replicas:-0}${COLOR_RESET}"
    echo -e "${COLOR_CYAN}  ðŸ“ˆ Available: ${COLOR_BOLD}$available_status${COLOR_RESET}"
    echo -e "${COLOR_CYAN}  ðŸ”„ Progressing: ${COLOR_BOLD}$progressing_status${COLOR_RESET}"
    
    # Check if deployment is healthy
    if [[ "$ready_replicas" == "$total_replicas" ]] && [[ "$available_status" == "True" ]]; then
        log_success "Deployment $deployment is ready and healthy"
        return 0
    else
        log_warning "Deployment $deployment is not fully ready"
        
        # Get replica set issues
        echo -e "${COLOR_BRIGHT_CYAN}${COLOR_BOLD}ðŸ” REPLICA SET ANALYSIS:${COLOR_RESET}"
        kubectl describe deployment "$deployment" -n "$namespace" 2>/dev/null | grep -A 10 "Conditions:\|Events:" | tail -20
        
        return 1
    fi
}

# Check StatefulSet readiness (similar to deployment readiness but for StatefulSets)
check_statefulset_readiness() {
    local statefulset="$1"
    local namespace="$2"
    
    log_info "ðŸš€ Analyzing StatefulSet readiness: $statefulset"
    
    # Get StatefulSet status
    local sts_info=$(kubectl get statefulset "$statefulset" -n "$namespace" -o jsonpath='{.status.readyReplicas}/{.status.replicas} {.status.currentReplicas} {.status.updatedReplicas}' 2>/dev/null)
    
    if [[ -z "$sts_info" ]]; then
        log_error "StatefulSet $statefulset not found in namespace $namespace"
        return 1
    fi
    
    local ready_replicas=$(echo "$sts_info" | cut -d' ' -f1 | cut -d'/' -f1)
    local total_replicas=$(echo "$sts_info" | cut -d' ' -f1 | cut -d'/' -f2)
    local current_replicas=$(echo "$sts_info" | cut -d' ' -f2)
    local updated_replicas=$(echo "$sts_info" | cut -d' ' -f3)
    
    echo -e "${COLOR_CYAN}  ðŸ“Š Ready: ${COLOR_BOLD}${ready_replicas:-0}/${total_replicas:-0}${COLOR_RESET}"
    echo -e "${COLOR_CYAN}  ðŸ“ˆ Current: ${COLOR_BOLD}${current_replicas:-0}${COLOR_RESET}"
    echo -e "${COLOR_CYAN}  ðŸ”„ Updated: ${COLOR_BOLD}${updated_replicas:-0}${COLOR_RESET}"
    
    # Check if StatefulSet is healthy
    if [[ "$ready_replicas" == "$total_replicas" ]] && [[ "$ready_replicas" -gt 0 ]]; then
        log_success "StatefulSet $statefulset is ready and healthy"
        return 0
    else
        log_warning "StatefulSet $statefulset is not fully ready"
        
        # Get StatefulSet details
        echo -e "${COLOR_BRIGHT_CYAN}${COLOR_BOLD}ðŸ” STATEFULSET ANALYSIS:${COLOR_RESET}"
        kubectl describe statefulset "$statefulset" -n "$namespace" 2>/dev/null | grep -A 10 "Conditions:\|Events:" | tail -20
        
        return 1
    fi
}

# Check service endpoints and connectivity
check_service_connectivity() {
    local service="$1"
    local namespace="$2"
    
    log_info "ðŸŒ Checking service connectivity: $service"
    
    # Check if service exists
    if ! kubectl get service "$service" -n "$namespace" >/dev/null 2>&1; then
        log_error "Service $service not found in namespace $namespace"
        return 1
    fi
    
    # Get service endpoints
    local endpoints=$(kubectl get endpoints "$service" -n "$namespace" -o jsonpath='{.subsets[*].addresses[*].ip}' 2>/dev/null)
    
    if [[ -n "$endpoints" ]]; then
        local endpoint_count=$(echo "$endpoints" | wc -w)
        log_success "Service $service has $endpoint_count ready endpoint(s)"
        echo -e "${COLOR_CYAN}  ðŸŽ¯ Endpoints: ${COLOR_BOLD}$endpoints${COLOR_RESET}"
        
        # Get service ports
        local ports=$(kubectl get service "$service" -n "$namespace" -o jsonpath='{.spec.ports[*].port}' 2>/dev/null)
        echo -e "${COLOR_CYAN}  ðŸ”Œ Ports: ${COLOR_BOLD}$ports${COLOR_RESET}"
        
        return 0
    else
        log_warning "Service $service has no ready endpoints"
        
        # Check for pods that should be backing this service
        local selector=$(kubectl get service "$service" -n "$namespace" -o jsonpath='{.spec.selector}' 2>/dev/null)
        if [[ -n "$selector" ]]; then
            echo -e "${COLOR_YELLOW}  ðŸ·ï¸  Service selector: $selector${COLOR_RESET}"
            
            # Try to find matching pods
            local matching_pods=$(kubectl get pods -n "$namespace" -l "$(echo "$selector" | sed 's/map\[//;s/\]//;s/ /,/g')" --no-headers 2>/dev/null | wc -l)
            echo -e "${COLOR_CYAN}  ðŸ“¦ Matching pods: ${COLOR_BOLD}$matching_pods${COLOR_RESET}"
        fi
        
        return 1
    fi
}

# Check ingress configuration and certificates
check_ingress_status() {
    local ingress="$1"
    local namespace="$2"
    
    log_info "ðŸŒ Checking ingress status: $ingress"
    
    # Check if ingress exists
    if ! kubectl get ingress "$ingress" -n "$namespace" >/dev/null 2>&1; then
        log_error "Ingress $ingress not found in namespace $namespace"
        return 1
    fi
    
    # Get ingress hosts and addresses
    local hosts=$(kubectl get ingress "$ingress" -n "$namespace" -o jsonpath='{.spec.rules[*].host}' 2>/dev/null)
    local addresses=$(kubectl get ingress "$ingress" -n "$namespace" -o jsonpath='{.status.loadBalancer.ingress[*].ip}' 2>/dev/null)
    
    echo -e "${COLOR_CYAN}  ðŸ  Hosts: ${COLOR_BOLD}$hosts${COLOR_RESET}"
    echo -e "${COLOR_CYAN}  ðŸ“ Addresses: ${COLOR_BOLD}${addresses:-<pending>}${COLOR_RESET}"
    
    # Check TLS certificates
    local tls_secrets=$(kubectl get ingress "$ingress" -n "$namespace" -o jsonpath='{.spec.tls[*].secretName}' 2>/dev/null)
    if [[ -n "$tls_secrets" ]]; then
        echo -e "${COLOR_CYAN}  ðŸ” TLS Secrets: ${COLOR_BOLD}$tls_secrets${COLOR_RESET}"
        
        # Check certificate validity
        for secret in $tls_secrets; do
            if kubectl get secret "$secret" -n "$namespace" >/dev/null 2>&1; then
                log_success "TLS certificate $secret exists"
            else
                log_warning "TLS certificate $secret not found or not ready"
            fi
        done
    fi
    
    if [[ -n "$addresses" ]]; then
        log_success "Ingress $ingress is configured and has addresses"
        return 0
    else
        log_warning "Ingress $ingress is configured but pending addresses"
        return 1
    fi
}

# Check StatefulSet with persistent volume issues
check_statefulset_status() {
    local statefulset="$1"
    local namespace="$2"
    
    log_info "ðŸ’¾ Checking StatefulSet status: $statefulset"
    
    # Check if StatefulSet exists
    if ! kubectl get statefulset "$statefulset" -n "$namespace" >/dev/null 2>&1; then
        log_error "StatefulSet $statefulset not found in namespace $namespace"
        return 1
    fi
    
    # Get StatefulSet status
    local ss_info=$(kubectl get statefulset "$statefulset" -n "$namespace" -o jsonpath='{.status.readyReplicas}/{.status.replicas} {.status.currentReplicas}' 2>/dev/null)
    local ready_replicas=$(echo "$ss_info" | cut -d' ' -f1 | cut -d'/' -f1)
    local total_replicas=$(echo "$ss_info" | cut -d' ' -f1 | cut -d'/' -f2)
    
    echo -e "${COLOR_CYAN}  ðŸ“Š Ready: ${COLOR_BOLD}${ready_replicas:-0}/${total_replicas:-0}${COLOR_RESET}"
    
    # Check persistent volume claims
    local pvcs=$(kubectl get pvc -n "$namespace" | grep "$statefulset" 2>/dev/null)
    if [[ -n "$pvcs" ]]; then
        echo -e "${COLOR_CYAN}  ðŸ’½ Persistent Volume Claims:${COLOR_RESET}"
        echo "$pvcs" | while read -r pvc_line; do
            echo -e "${COLOR_DIM}    $pvc_line${COLOR_RESET}"
        done
        
        # Check for pending PVCs
        local pending_pvcs=$(echo "$pvcs" | grep "Pending" | wc -l)
        if [[ $pending_pvcs -gt 0 ]]; then
            log_warning "$pending_pvcs PVC(s) are in Pending state"
            echo -e "${COLOR_YELLOW}  ðŸ’¡ Check storage class and available storage${COLOR_RESET}"
        fi
    fi
    
    if [[ "$ready_replicas" == "$total_replicas" ]]; then
        log_success "StatefulSet $statefulset is ready"
        return 0
    else
        log_warning "StatefulSet $statefulset is not fully ready"
        return 1
    fi
}

# Enhanced utility function to wait for pods to be ready with detailed diagnostics
wait_for_pods_ready() {
    local namespace="$1"
    local timeout="${2:-300}"
    local component="${3:-$namespace}"
    local start_time=$(date +%s)
    local last_status_check=0
    local detailed_check_performed=false
    
    echo -e "${COLOR_CYAN}â³ Waiting for $component pods to become ready (timeout: ${timeout}s)...${COLOR_RESET}"
    
    while true; do
        local current_time=$(date +%s)
        local elapsed=$((current_time - start_time))
        
        if [[ $elapsed -gt $timeout ]]; then
            log_error "Timeout waiting for $component pods to be ready after ${timeout}s"
            
            # Perform final diagnostic check on timeout
            echo -e "${COLOR_BRIGHT_RED}${COLOR_BOLD}ðŸ” TIMEOUT DIAGNOSTIC REPORT:${COLOR_RESET}"
            perform_pod_diagnostics "$namespace" "$component"
            return 1
        fi
        
        # Get pod status information
        local pod_status=$(kubectl get pods -n "$namespace" --no-headers 2>/dev/null)
        local not_ready_pods=$(echo "$pod_status" | grep -v -E "1/1.*Running|.*Completed" | wc -l)
        local total_pods=$(echo "$pod_status" | wc -l)
        local ready_pods=$((total_pods - not_ready_pods))
        
        # Show progress every 30 seconds or on status change
        if [[ $((elapsed - last_status_check)) -ge 30 ]] || [[ $not_ready_pods -ne ${prev_not_ready:-$not_ready_pods} ]]; then
            if [[ $total_pods -gt 0 ]]; then
                echo -e "${COLOR_YELLOW}  ðŸ“Š Progress: ${COLOR_BOLD}${ready_pods}/${total_pods}${COLOR_RESET} pods ready (${elapsed}s elapsed)"
                
                # Show not ready pods briefly
                if [[ $not_ready_pods -gt 0 ]]; then
                    echo "$pod_status" | grep -v -E "1/1.*Running|.*Completed" | head -3 | while read -r pod_line; do
                        local pod_name=$(echo "$pod_line" | awk '{print $1}')
                        local pod_status_detail=$(echo "$pod_line" | awk '{print $3}')
                        echo -e "${COLOR_DIM}    â€¢ $pod_name: $pod_status_detail${COLOR_RESET}"
                    done
                    
                    if [[ $not_ready_pods -gt 3 ]]; then
                        echo -e "${COLOR_DIM}    ... and $((not_ready_pods - 3)) more${COLOR_RESET}"
                    fi
                fi
            else
                echo -e "${COLOR_YELLOW}  â³ No pods found yet in namespace $namespace${COLOR_RESET}"
            fi
            last_status_check=$elapsed
        fi
        
        # If all pods are ready, success!
        if [[ $not_ready_pods -eq 0 ]] && [[ $total_pods -gt 0 ]]; then
            log_success "All $component pods are ready ($total_pods/$total_pods)"
            return 0
        fi
        
        # Perform detailed diagnostics after 60 seconds if pods are still not ready
        if [[ $elapsed -gt 60 ]] && [[ "$detailed_check_performed" == "false" ]]; then
            echo -e "${COLOR_BRIGHT_YELLOW}${COLOR_BOLD}ðŸ” PERFORMING DETAILED DIAGNOSTICS:${COLOR_RESET}"
            perform_pod_diagnostics "$namespace" "$component"
            detailed_check_performed=true
        fi
        
        prev_not_ready=$not_ready_pods
        sleep 10
    done
}

# Perform comprehensive pod diagnostics
perform_pod_diagnostics() {
    local namespace="$1"
    local component="$2"
    
    echo -e "${COLOR_BRIGHT_CYAN}${COLOR_BOLD}ðŸ”¬ DIAGNOSTIC ANALYSIS FOR $component:${COLOR_RESET}"
    
    # 1. Check for image pull issues
    if check_image_pull_issues "$namespace" "$component"; then
        echo -e "${COLOR_GREEN}  âœ“ No image pull issues${COLOR_RESET}"
    fi
    
    # 2. Check for resource constraints
    if check_resource_constraints "$namespace" "$component"; then
        echo -e "${COLOR_GREEN}  âœ“ No resource constraint issues${COLOR_RESET}"
    fi
    
    # 3. Show current pod status with details
    echo -e "${COLOR_BRIGHT_CYAN}${COLOR_BOLD}ðŸ“‹ CURRENT POD STATUS:${COLOR_RESET}"
    local pod_details=$(kubectl get pods -n "$namespace" -o wide 2>/dev/null)
    if [[ -n "$pod_details" ]]; then
        echo "$pod_details" | head -10
        
        # Show events for problematic pods
        echo -e "${COLOR_BRIGHT_CYAN}${COLOR_BOLD}ðŸ“° RECENT EVENTS:${COLOR_RESET}"
        kubectl get events -n "$namespace" --sort-by='.lastTimestamp' 2>/dev/null | tail -5 | while read -r event_line; do
            echo -e "${COLOR_DIM}  $event_line${COLOR_RESET}"
        done
    else
        echo -e "${COLOR_RED}  No pods found in namespace $namespace${COLOR_RESET}"
    fi
    
    # 4. Check for failed pods and get their logs
    local failed_pods=$(kubectl get pods -n "$namespace" --no-headers 2>/dev/null | grep -E "(Error|CrashLoopBackOff|ImagePullBackOff|ErrImagePull)" | head -2)
    if [[ -n "$failed_pods" ]]; then
        echo -e "${COLOR_BRIGHT_CYAN}${COLOR_BOLD}ðŸ“ FAILED POD LOGS (last 10 lines):${COLOR_RESET}"
        echo "$failed_pods" | while read -r pod_line; do
            local pod_name=$(echo "$pod_line" | awk '{print $1}')
            echo -e "${COLOR_YELLOW}  Pod: $pod_name${COLOR_RESET}"
            kubectl logs "$pod_name" -n "$namespace" --tail=10 2>/dev/null | sed 's/^/    /' || echo "    (No logs available)"
            echo
        done
    fi
    
    # 5. Provide component-specific troubleshooting
    provide_component_troubleshooting "$component" "$namespace"
}

# Provide component-specific troubleshooting suggestions
provide_component_troubleshooting() {
    local component="$1"
    local namespace="$2"
    
    echo -e "${COLOR_BRIGHT_CYAN}${COLOR_BOLD}ðŸ’¡ TROUBLESHOOTING SUGGESTIONS FOR $component:${COLOR_RESET}"
    
    case "$component" in
        "cert-manager")
            echo -e "${COLOR_CYAN}â€¢ Check ClusterIssuer: ${COLOR_BOLD}kubectl get clusterissuer${COLOR_RESET}"
            echo -e "${COLOR_CYAN}â€¢ Verify webhook: ${COLOR_BOLD}kubectl get validatingwebhookconfiguration cert-manager-webhook${COLOR_RESET}"
            echo -e "${COLOR_CYAN}â€¢ Check certificates: ${COLOR_BOLD}kubectl get certificates -A${COLOR_RESET}"
            ;;
        "ingress-nginx")
            echo -e "${COLOR_CYAN}â€¢ Check LoadBalancer service: ${COLOR_BOLD}kubectl get svc -n ingress-nginx${COLOR_RESET}"
            echo -e "${COLOR_CYAN}â€¢ Verify ingress class: ${COLOR_BOLD}kubectl get ingressclass${COLOR_RESET}"
            echo -e "${COLOR_CYAN}â€¢ Test ingress: ${COLOR_BOLD}kubectl get ingress -A${COLOR_RESET}"
            ;;
        "keycloak")
            echo -e "${COLOR_CYAN}â€¢ Check database connectivity and persistent volumes${COLOR_RESET}"
            echo -e "${COLOR_CYAN}â€¢ Verify secrets: ${COLOR_BOLD}kubectl get secrets -n keycloak${COLOR_RESET}"
            echo -e "${COLOR_CYAN}â€¢ Check service: ${COLOR_BOLD}kubectl get svc -n keycloak${COLOR_RESET}"
            ;;
        "ldap")
            echo -e "${COLOR_CYAN}â€¢ Check LDAP configuration and secrets${COLOR_RESET}"
            echo -e "${COLOR_CYAN}â€¢ Verify persistent volumes: ${COLOR_BOLD}kubectl get pvc -n ldap${COLOR_RESET}"
            echo -e "${COLOR_CYAN}â€¢ Test connectivity: ${COLOR_BOLD}kubectl exec -n ldap <pod> -- ldapsearch${COLOR_RESET}"
            ;;
        "oauth2")
            echo -e "${COLOR_CYAN}â€¢ Check OAuth2 configuration and upstream services${COLOR_RESET}"
            echo -e "${COLOR_CYAN}â€¢ Verify Keycloak connectivity from OAuth2 proxy${COLOR_RESET}"
            echo -e "${COLOR_CYAN}â€¢ Check ingress and certificates${COLOR_RESET}"
            ;;
        "monitoring")
            echo -e "${COLOR_CYAN}â€¢ Check storage class for Prometheus: ${COLOR_BOLD}kubectl get storageclass${COLOR_RESET}"
            echo -e "${COLOR_CYAN}â€¢ Verify service monitors: ${COLOR_BOLD}kubectl get servicemonitor -A${COLOR_RESET}"
            echo -e "${COLOR_CYAN}â€¢ Check Grafana access: ${COLOR_BOLD}kubectl get svc -n monitoring${COLOR_RESET}"
            ;;
        *)
            echo -e "${COLOR_CYAN}â€¢ Check deployment: ${COLOR_BOLD}kubectl describe deployment -n $namespace${COLOR_RESET}"
            echo -e "${COLOR_CYAN}â€¢ Verify services: ${COLOR_BOLD}kubectl get svc -n $namespace${COLOR_RESET}"
            echo -e "${COLOR_CYAN}â€¢ Check configmaps/secrets: ${COLOR_BOLD}kubectl get cm,secrets -n $namespace${COLOR_RESET}"
            ;;
    esac
    
    echo -e "${COLOR_CYAN}â€¢ General troubleshooting: ${COLOR_BOLD}kubectl describe pods -n $namespace${COLOR_RESET}"
    echo -e "${COLOR_CYAN}â€¢ Check events: ${COLOR_BOLD}kubectl get events -n $namespace --sort-by='.lastTimestamp'${COLOR_RESET}"
    echo
}

# Keycloak validation with comprehensive checks
validate_keycloak_installation() {
    local timeout="$1"
    local validation_passed=true
    
    log_step "1" "Checking Keycloak StatefulSet status"
    if check_statefulset_readiness "keycloak" "keycloak"; then
        log_success "Keycloak StatefulSet is ready"
    else
        log_error "Keycloak StatefulSet has issues"
        validation_passed=false
    fi
    
    log_step "2" "Checking Keycloak pods"
    if ! wait_for_pods_ready "keycloak" "$timeout" "keycloak"; then
        log_error "Keycloak pods not ready"
        validation_passed=false
    else
        log_success "Keycloak pods are ready"
    fi
    
    log_step "3" "Checking Keycloak service connectivity"
  if check_service_connectivity "keycloak-http" "keycloak"; then
        log_success "Keycloak service is accessible"
    else
        log_warning "Keycloak service connectivity issues detected"
    fi
    
    log_step "4" "Checking Keycloak ingress configuration"
    if kubectl get ingress keycloak -n keycloak >/dev/null 2>&1; then
        if check_ingress_status "keycloak" "keycloak"; then
            log_success "Keycloak ingress is configured and ready"
        else
            log_warning "Keycloak ingress has configuration issues"
        fi
    else
        log_info "Keycloak ingress not configured (using NodePort/LoadBalancer)"
    fi
    
    return $([[ "$validation_passed" == "true" ]] && echo 0 || echo 1)
}

# ArgoCD validation with comprehensive checks
validate_argocd_installation() {
    local timeout="$1"
    local validation_passed=true
    
    log_step "1" "Checking ArgoCD server deployment"
    if check_deployment_readiness "argocd-server" "argocd"; then
        log_success "ArgoCD server deployment is ready"
    else
        log_error "ArgoCD server deployment has issues"
        validation_passed=false
    fi
    
    log_step "2" "Checking ArgoCD pods"
    if ! wait_for_pods_ready "argocd" "$timeout" "argocd"; then
        log_error "ArgoCD pods not ready"
        validation_passed=false
    else
        log_success "ArgoCD pods are ready"
    fi
    
    log_step "3" "Checking ArgoCD services"
    if check_service_connectivity "argocd-server" "argocd"; then
        log_success "ArgoCD server service is accessible"
    else
        log_warning "ArgoCD server service connectivity issues detected"
    fi
    
    return $([[ "$validation_passed" == "true" ]] && echo 0 || echo 1)
}

# JupyterHub validation with comprehensive checks
validate_jupyter_installation() {
    local timeout="$1"
    local validation_passed=true
    
    log_step "1" "Checking JupyterHub hub deployment"
    if check_deployment_readiness "hub" "jupyterhub"; then
        log_success "JupyterHub hub deployment is ready"
    else
        log_error "JupyterHub hub deployment has issues"
        validation_passed=false
    fi
    
    log_step "2" "Checking JupyterHub pods"
    if ! wait_for_pods_ready "jupyterhub" "$timeout" "jupyterhub"; then
        log_error "JupyterHub pods not ready"
        validation_passed=false
    else
        log_success "JupyterHub pods are ready"
    fi
    
    log_step "3" "Checking JupyterHub proxy service"
    if check_service_connectivity "proxy-public" "jupyterhub"; then
        log_success "JupyterHub proxy service is accessible"
    else
        log_warning "JupyterHub proxy service connectivity issues detected"
    fi
    
    return $([[ "$validation_passed" == "true" ]] && echo 0 || echo 1)
}

# Registry validation with comprehensive checks
validate_registry_installation() {
    local timeout="$1"
    local validation_passed=true
    
    log_step "1" "Checking Registry deployment"
    if check_deployment_readiness "registry" "registry"; then
        log_success "Registry deployment is ready"
    else
        log_error "Registry deployment has issues"
        validation_passed=false
    fi
    
    log_step "2" "Checking Registry pods"
    if ! wait_for_pods_ready "registry" "$timeout" "registry"; then
        log_error "Registry pods not ready"
        validation_passed=false
    else
        log_success "Registry pods are ready"
    fi
    
    log_step "3" "Checking Registry service"
    if check_service_connectivity "registry" "registry"; then
        log_success "Registry service is accessible"
    else
        log_warning "Registry service connectivity issues detected"
    fi
    
    log_step "4" "Checking Registry ingress"
    if kubectl get ingress registry -n registry >/dev/null 2>&1; then
        if check_ingress_status "registry" "registry"; then
            log_success "Registry ingress is configured and ready"
        else
            log_warning "Registry ingress has configuration issues"
        fi
    else
        log_info "Registry ingress not configured (using NodePort/LoadBalancer)"
    fi
    
    return $([[ "$validation_passed" == "true" ]] && echo 0 || echo 1)
}

# Base platform services validation
validate_base_installation() {
    local timeout="$1"
    local validation_passed=true
    
    log_step "1" "Checking base installation marker"
    if kubectl get configmap base-config -n kube-system >/dev/null 2>&1; then
        log_success "Base installation marker found"
    else
        log_warning "Base installation marker not found (but installation may have succeeded)"
        # Don't fail validation for missing marker as it's not critical
    fi
    
    log_step "2" "Checking base platform components"
    local base_dir="$MOUNT_PATH/kubernetes/install_k8s/base"
    if [[ -d "$base_dir" ]]; then
        log_success "Base platform directory exists"
    else
        log_error "Base platform directory not found"
        validation_passed=false
    fi
    
    return $([[ "$validation_passed" == "true" ]] && echo 0 || echo 1)
}

# =============================================================================
# ðŸŽ¯ INTERACTIVE INSTALLATION MODE
# =============================================================================

# Interactive installation wizard
interactive_installation() {
    log_header "GOK Interactive Installation Wizard" "Guided Platform Setup"
    
    echo -e "${COLOR_BRIGHT_CYAN}Welcome to the GOK Platform Interactive Installation!${COLOR_RESET}"
    echo -e "${COLOR_CYAN}This wizard will guide you through setting up your Kubernetes platform.${COLOR_RESET}"
    echo
    
    # Check prerequisites
    if ! check_prerequisites; then
        log_error "Prerequisites check failed. Please resolve the issues and try again."
        return 1
    fi
    
    # Show installation options
    show_installation_categories
    
    # Get user selection
    local selection
    while true; do
        echo
        echo -e "${COLOR_BRIGHT_YELLOW}Select installation type:${COLOR_RESET}"
        echo -e "${COLOR_CYAN}1) ${COLOR_BOLD}Quick Start${COLOR_RESET} - Essential components for a basic cluster"
        echo -e "${COLOR_CYAN}2) ${COLOR_BOLD}Full Platform${COLOR_RESET} - Complete GOK platform with all features"
        echo -e "${COLOR_CYAN}3) ${COLOR_BOLD}Custom Selection${COLOR_RESET} - Choose specific components"
        echo -e "${COLOR_CYAN}4) ${COLOR_BOLD}Development Setup${COLOR_RESET} - Optimized for development workflows"
        echo -e "${COLOR_CYAN}5) ${COLOR_BOLD}Production Setup${COLOR_RESET} - Production-ready configuration"
        echo -e "${COLOR_CYAN}6) ${COLOR_BOLD}Exit${COLOR_RESET} - Exit the installer"
        echo
        
        read -p "$(echo -e "${COLOR_BRIGHT_WHITE}Enter your choice (1-6): ${COLOR_RESET}")" selection
        
        case "$selection" in
            1) install_quick_start; break ;;
            2) install_full_platform; break ;;
            3) install_custom_selection; break ;;
            4) install_development_setup; break ;;
            5) install_production_setup; break ;;
            6) log_info "Installation cancelled by user"; return 0 ;;
            *) log_warning "Invalid selection. Please choose 1-6." ;;
        esac
    done
}

# Check system prerequisites
check_prerequisites() {
    log_section "Prerequisites Check" "${EMOJI_GEAR}"
    local prereq_passed=true
    
    # Check if running as root or with sudo
    if [[ $EUID -ne 0 ]]; then
        log_error "This script must be run as root or with sudo privileges"
        prereq_passed=false
    else
        log_success "Running with appropriate privileges"
    fi
    
    # Check system resources
    local mem_gb=$(free -g | awk '/^Mem:/{print $2}')
    local cpu_cores=$(nproc)
    
    if [[ $mem_gb -lt 4 ]]; then
        log_warning "System has ${mem_gb}GB RAM. Minimum 4GB recommended for basic installation."
    else
        log_success "Memory: ${mem_gb}GB (sufficient)"
    fi
    
    if [[ $cpu_cores -lt 2 ]]; then
        log_warning "System has ${cpu_cores} CPU cores. Minimum 2 cores recommended."
    else
        log_success "CPU: ${cpu_cores} cores (sufficient)"
    fi
    
    # Check disk space
    local disk_gb=$(df / | awk 'NR==2{printf "%.0f", $4/1024/1024}')
    if [[ $disk_gb -lt 20 ]]; then
        log_warning "Available disk space: ${disk_gb}GB. Minimum 20GB recommended."
    else
        log_success "Disk space: ${disk_gb}GB available (sufficient)"
    fi
    
    # Check network connectivity
    if ping -c 1 google.com >/dev/null 2>&1; then
        log_success "Internet connectivity: Available"
    else
        log_error "Internet connectivity: Not available (required for installation)"
        prereq_passed=false
    fi
    
    # Check if Docker is installed
    if command -v docker >/dev/null 2>&1; then
        log_success "Docker: Installed"
    else
        log_info "Docker: Not installed (will be installed automatically)"
    fi
    
    # Check if kubectl is installed
    if command -v kubectl >/dev/null 2>&1; then
        log_success "kubectl: Installed"
    else
        log_info "kubectl: Not installed (will be installed with Kubernetes)"
    fi
    
    return $([[ "$prereq_passed" == "true" ]] && echo 0 || echo 1)
}

# Show available installation categories
show_installation_categories() {
    echo
    log_section "Available Components" "${EMOJI_PACKAGE}"
    
    echo -e "${COLOR_BRIGHT_BLUE}${COLOR_BOLD}ðŸ“¦ Core Infrastructure (Essential):${COLOR_RESET}"
    echo -e "  ${COLOR_CYAN}â€¢ docker            - Container runtime${COLOR_RESET}"
    echo -e "  ${COLOR_CYAN}â€¢ kubernetes        - Container orchestration platform${COLOR_RESET}"
    echo -e "  ${COLOR_CYAN}â€¢ cert-manager      - Automated TLS certificate management${COLOR_RESET}"
    echo -e "  ${COLOR_CYAN}â€¢ ingress           - NGINX ingress controller${COLOR_RESET}"
    echo
    
    echo -e "${COLOR_BRIGHT_GREEN}${COLOR_BOLD}ðŸ“Š Monitoring & Observability:${COLOR_RESET}"
    echo -e "  ${COLOR_CYAN}â€¢ monitoring        - Prometheus + Grafana stack${COLOR_RESET}"
    echo -e "  ${COLOR_CYAN}â€¢ fluentd           - Log collection and forwarding${COLOR_RESET}"
    echo -e "  ${COLOR_CYAN}â€¢ opensearch        - Search and analytics engine${COLOR_RESET}"
    echo
    
    echo -e "${COLOR_BRIGHT_YELLOW}${COLOR_BOLD}ðŸ” Security & Identity:${COLOR_RESET}"
    echo -e "  ${COLOR_CYAN}â€¢ vault             - HashiCorp Vault secrets management${COLOR_RESET}"
    echo -e "  ${COLOR_CYAN}â€¢ keycloak          - Identity and access management${COLOR_RESET}"
    echo -e "  ${COLOR_CYAN}â€¢ oauth2            - OAuth2 proxy for authentication${COLOR_RESET}"
    echo -e "  ${COLOR_CYAN}â€¢ ldap              - LDAP directory service${COLOR_RESET}"
    echo
    
    echo -e "${COLOR_BRIGHT_MAGENTA}${COLOR_BOLD}ðŸš€ Development & DevOps:${COLOR_RESET}"
    echo -e "  ${COLOR_CYAN}â€¢ argocd            - GitOps continuous delivery${COLOR_RESET}"
    echo -e "  ${COLOR_CYAN}â€¢ jenkins           - CI/CD automation server${COLOR_RESET}"
    echo -e "  ${COLOR_CYAN}â€¢ jupyter           - JupyterHub for data science${COLOR_RESET}"
    echo -e "  ${COLOR_CYAN}â€¢ registry          - Container image registry${COLOR_RESET}"
    echo -e "  ${COLOR_CYAN}â€¢ spinnaker         - Multi-cloud deployment platform${COLOR_RESET}"
    echo
    
    echo -e "${COLOR_BRIGHT_RED}${COLOR_BOLD}ðŸŒ Service Mesh & Messaging:${COLOR_RESET}"
    echo -e "  ${COLOR_CYAN}â€¢ istio             - Service mesh for microservices${COLOR_RESET}"
    echo -e "  ${COLOR_CYAN}â€¢ rabbitmq          - Message broker${COLOR_RESET}"
    echo
    
    echo -e "${COLOR_BRIGHT_CYAN}${COLOR_BOLD}ðŸ—ï¸ GOK Platform:${COLOR_RESET}"
    echo -e "  ${COLOR_CYAN}â€¢ gok-controller    - GOK distributed system controller${COLOR_RESET}"
    echo -e "  ${COLOR_CYAN}â€¢ gok-agent         - GOK distributed system agent${COLOR_RESET}"
    echo -e "  ${COLOR_CYAN}â€¢ controller        - Install both gok-controller and gok-agent${COLOR_RESET}"
}

# Quick start installation
install_quick_start() {
    log_header "Quick Start Installation" "Essential components for a basic cluster"
    
    local components=("docker" "kubernetes" "cert-manager" "ingress" "monitoring")
    
    echo -e "${COLOR_BRIGHT_CYAN}Quick Start includes:${COLOR_RESET}"
    for component in "${components[@]}"; do
        echo -e "  ${COLOR_GREEN}â€¢ $component${COLOR_RESET}"
    done
    echo
    
    if confirm_installation "Quick Start"; then
        install_component_list "${components[@]}"
        show_platform_overview
    fi
}

# Full platform installation
install_full_platform() {
    log_header "Full Platform Installation" "Complete GOK platform with all features"
    
    local components=(
        "docker" "kubernetes" "cert-manager" "ingress" "monitoring"
        "vault" "keycloak" "oauth2" "argocd" "jenkins" "jupyter"
        "registry" "gok-controller" "rabbitmq"
    )
    
    echo -e "${COLOR_BRIGHT_CYAN}Full Platform includes:${COLOR_RESET}"
    for component in "${components[@]}"; do
        echo -e "  ${COLOR_GREEN}â€¢ $component${COLOR_RESET}"
    done
    echo
    
    local estimated_time="45-60 minutes"
    echo -e "${COLOR_YELLOW}Estimated installation time: ${COLOR_BOLD}$estimated_time${COLOR_RESET}"
    echo
    
    if confirm_installation "Full Platform"; then
        install_component_list "${components[@]}"
        show_platform_overview
    fi
}

# Development setup installation
install_development_setup() {
    log_header "Development Setup" "Optimized for development workflows"
    
    local components=(
        "docker" "kubernetes" "cert-manager" "ingress"
        "monitoring" "argocd" "jupyter" "registry" "gok-controller"
    )
    
    echo -e "${COLOR_BRIGHT_CYAN}Development Setup includes:${COLOR_RESET}"
    for component in "${components[@]}"; do
        echo -e "  ${COLOR_GREEN}â€¢ $component${COLOR_RESET}"
    done
    echo
    
    if confirm_installation "Development Setup"; then
        install_component_list "${components[@]}"
        show_development_next_steps
    fi
}

# Production setup installation
install_production_setup() {
    log_header "Production Setup" "Production-ready configuration with security"
    
    local components=(
        "docker" "kubernetes" "cert-manager" "ingress" "monitoring"
        "vault" "keycloak" "oauth2" "argocd" "fluentd" "opensearch"
        "kyverno" "gok-controller" "istio"
    )
    
    echo -e "${COLOR_BRIGHT_CYAN}Production Setup includes:${COLOR_RESET}"
    for component in "${components[@]}"; do
        echo -e "  ${COLOR_GREEN}â€¢ $component${COLOR_RESET}"
    done
    echo
    
    local estimated_time="60-90 minutes"
    echo -e "${COLOR_YELLOW}Estimated installation time: ${COLOR_BOLD}$estimated_time${COLOR_RESET}"
    echo
    
    if confirm_installation "Production Setup"; then
        install_component_list "${components[@]}"
        show_production_next_steps
    fi
}

# Custom component selection
install_custom_selection() {
    log_header "Custom Component Selection" "Choose specific components to install"
    
    local all_components=(
        "docker:Container runtime"
        "kubernetes:Container orchestration"
        "cert-manager:Certificate management"
        "ingress:NGINX ingress controller"
        "monitoring:Prometheus + Grafana"
        "vault:HashiCorp Vault"
        "keycloak:Identity management"
        "oauth2:OAuth2 proxy"
        "ldap:LDAP directory"
        "argocd:GitOps deployment"
        "jenkins:CI/CD server"
        "jupyter:JupyterHub"
        "registry:Container registry"
        "spinnaker:Multi-cloud deployment"
        "istio:Service mesh"
        "rabbitmq:Message broker"
        "fluentd:Log collection"
        "opensearch:Search engine"
        "kyverno:Policy engine"
        "gok-controller:GOK platform controller"
        "gok-agent:GOK platform agent"
    )
    
    local selected_components=()
    
    echo -e "${COLOR_BRIGHT_CYAN}Select components to install (y/N for each):${COLOR_RESET}"
    echo
    
    for component_desc in "${all_components[@]}"; do
        local component="${component_desc%%:*}"
        local description="${component_desc##*:}"
        
        echo -e "${COLOR_CYAN}Install ${COLOR_BOLD}$component${COLOR_RESET} ${COLOR_DIM}($description)${COLOR_RESET}?"
        read -p "$(echo -e "${COLOR_WHITE}[y/N]: ${COLOR_RESET}")" choice
        
        case "$choice" in
            [Yy]|[Yy][Ee][Ss])
                selected_components+=("$component")
                echo -e "  ${COLOR_GREEN}âœ“ Added $component${COLOR_RESET}"
                ;;
            *)
                echo -e "  ${COLOR_DIM}- Skipped $component${COLOR_RESET}"
                ;;
        esac
        echo
    done
    
    if [[ ${#selected_components[@]} -eq 0 ]]; then
        log_warning "No components selected. Exiting."
        return 0
    fi
    
    echo -e "${COLOR_BRIGHT_CYAN}Selected components:${COLOR_RESET}"
    for component in "${selected_components[@]}"; do
        echo -e "  ${COLOR_GREEN}â€¢ $component${COLOR_RESET}"
    done
    echo
    
    if confirm_installation "Custom Selection"; then
        # Check dependencies
        check_component_dependencies "${selected_components[@]}"
        install_component_list "${selected_components[@]}"
    fi
}

# Check component dependencies and suggest additions
check_component_dependencies() {
    local components=("$@")
    local missing_deps=()
    
    # Basic dependency checking
    local has_kubernetes=false
    local has_cert_manager=false
    local has_ingress=false
    
    for component in "${components[@]}"; do
        case "$component" in
            "kubernetes") has_kubernetes=true ;;
            "cert-manager") has_cert_manager=true ;;
            "ingress") has_ingress=true ;;
        esac
    done
    
    # Check if Kubernetes is selected for components that need it
    for component in "${components[@]}"; do
        case "$component" in
            "cert-manager"|"ingress"|"monitoring"|"vault"|"keycloak"|"argocd"|"jenkins"|"jupyter"|"registry"|"gok-controller"|"gok-agent")
                if [[ "$has_kubernetes" != "true" ]]; then
                    missing_deps+=("kubernetes")
                    break
                fi
                ;;
        esac
    done
    
    # Check if cert-manager is needed
    for component in "${components[@]}"; do
        case "$component" in
            "ingress"|"monitoring"|"vault"|"keycloak"|"argocd")
                if [[ "$has_cert_manager" != "true" ]]; then
                    missing_deps+=("cert-manager")
                    break
                fi
                ;;
        esac
    done
    
    if [[ ${#missing_deps[@]} -gt 0 ]]; then
        echo
        log_warning "Missing required dependencies detected!"
        echo -e "${COLOR_YELLOW}The following components are recommended:${COLOR_RESET}"
        for dep in "${missing_deps[@]}"; do
            echo -e "  ${COLOR_YELLOW}â€¢ $dep${COLOR_RESET}"
        done
        echo
        
        read -p "$(echo -e "${COLOR_BRIGHT_WHITE}Add missing dependencies? [Y/n]: ${COLOR_RESET}")" add_deps
        case "$add_deps" in
            [Nn]|[Nn][Oo])
                log_info "Proceeding without dependencies (may cause installation issues)"
                ;;
            *)
                components+=("${missing_deps[@]}")
                log_success "Added dependencies to installation list"
                ;;
        esac
    fi
}

# Confirm installation with user
confirm_installation() {
    local installation_type="$1"
    
    echo -e "${COLOR_BRIGHT_WHITE}Proceed with ${COLOR_BOLD}$installation_type${COLOR_RESET} ${COLOR_BRIGHT_WHITE}installation?${COLOR_RESET}"
    read -p "$(echo -e "${COLOR_BRIGHT_WHITE}[Y/n]: ${COLOR_RESET}")" confirm
    
    case "$confirm" in
        [Nn]|[Nn][Oo])
            log_info "Installation cancelled by user"
            return 1
            ;;
        *)
            return 0
            ;;
    esac
}

# Install list of components with progress tracking
install_component_list() {
    local components=("$@")
    local total_components=${#components[@]}
    local current_component=0
    
    log_header "Installation Progress" "Installing $total_components components"
    
    for component in "${components[@]}"; do
        ((current_component++))
        
        log_progress "$current_component" "$total_components" "Installing $component"
        
        start_component "$component" "Installing $component ($current_component/$total_components)"
        
        # Call the actual installation function
        if installCmd "$component" >/dev/null 2>&1; then
            # Validate installation
            if validate_component_installation "$component" 180; then
                complete_component "$component" "Successfully installed and validated"
                show_component_next_steps "$component"
            else
                complete_component "$component" "Installed but validation warnings detected"
            fi
        else
            fail_component "$component" "Installation failed"
        fi
        
        echo
    done
    
    show_installation_summary
}

# Show development-specific next steps
show_development_next_steps() {
    log_next_steps "Development Environment Ready" \
        "Generate your first microservice: gok generate python-api my-service" \
        "Access JupyterHub for data science: https://jupyter.$(rootDomain)" \
        "Set up Git repositories in ArgoCD" \
        "Use container registry for your images: $(fullRegistryUrl)" \
        "Monitor your applications via Grafana dashboards"
    
    log_info "Your development environment is ready for microservice development!"
}

# Show production-specific next steps
show_production_next_steps() {
    log_next_steps "Production Environment Ready" \
        "Configure backup and disaster recovery procedures" \
        "Set up monitoring alerts and notification channels" \
        "Configure authentication and RBAC policies" \
        "Review security policies and compliance settings" \
        "Set up automated certificate renewal monitoring" \
        "Configure log retention and archival policies"
    
    log_warning "Remember to secure your Vault unseal keys and root tokens!"
    log_info "Your production environment is ready for enterprise workloads!"
}

replaceEnvVariable(){
  wget -O- $1 | envsubst
}

promptUserInput(){
 MSG=$1
 DEFAULT=$2
id=$(python3 -c "
import sys
sys.stderr.write('${MSG}')
id=input()
print(id)
")
output=${id:-$DEFAULT}
echo $output
}

promptSecret(){
  MSG=$1
  secret=$(python3 -c "
import getpass
secret = getpass.getpass('${MSG}')
print(secret)
")
  echo $secret
}

dataFromSecret(){
  NAME=$1
  NS=$2
  KEY=$3
  kubectl get secret $NAME -n $NS -o jsonpath="{['data']['$KEY']}" | base64 --decode
}

createApp1() {
  log_info "Creating test app1 deployment..."
  cat <<EOF | kubectl apply -f - >/dev/null 2>&1
apiVersion: apps/v1
kind: Deployment
metadata:
  name: app1
  namespace: default
spec:
  replicas: 1
  selector:
    matchLabels:
      app: app1
  template:
    metadata:
      labels:
        app: app1
    spec:
      containers:
      - name: app1
        image: dockersamples/static-site
        env:
        - name: AUTHOR
          value: app1
        ports:
        - containerPort: 80
---
apiVersion: v1
kind: Service
metadata:
  name: appsvc1
  namespace: default
spec:
  ports:
  - port: 80
    protocol: TCP
    targetPort: 80
  selector:
    app: app1
---
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  annotations:
    nginx.ingress.kubernetes.io/rewrite-target: /
    kubernetes.io/ingress.class: "nginx"
  name: app-ingress
  namespace: default
spec:
  rules:
  - host: $(fullDefaultUrl)
    http:
      paths:
      - backend:
          service:
            name: appsvc1
            port:
              number: 80
        path: /app1
        pathType: Prefix
EOF
  if [[ $? -eq 0 ]]; then
    log_success "Test app1 deployment created successfully"
  else
    log_error "Failed to create test app1 deployment"
    return 1
  fi
}

#This deploys a pod that has curl installed
kcurl(){
  log_info "Creating curl test pod..."
  cat <<EOF | kubectl apply -f - >/dev/null 2>&1
apiVersion: v1
kind: Pod
metadata:
  name: curl
  namespace: default
  labels:
    app: curl
spec:
  containers:
  - name: main
    image: curlimages/curl
    command: ["sleep", "9999999"]
EOF
  echo "Commands"
  echo "checkCurl https://kubernetes"
}

checkCurl(){
  kubectl exec -i -t curl -n default -- curl -kv "$@"
}

checkCMWebhook(){
  kubectl exec -i -t curl -n default -- curl -kv \
      --cacert <(kubectl -n cert-manager get secret cert-manager-webhook-ca -ojsonpath='{.data.ca\.crt}' | base64 -d) \
      https://cert-manager-webhook.cert-manager.svc:443/validate 2>&1 -d@- <<'EOF' | sed '/^* /d; /bytes data]$/d; s/> //; s/< //'
{"kind":"AdmissionReview","apiVersion":"admission.k8s.io/v1","request":{"requestKind":{"group":"cert-manager.io","version":"v1","kind":"Certificate"},"requestResource":{"group":"cert-manager.io","version":"v1","resource":"certificates"},"name":"foo","namespace":"default","operation":"CREATE","object":{"apiVersion":"cert-manager.io/v1","kind":"Certificate","spec":{"dnsNames":["foo"],"issuerRef":{"group":"cert-manager.io","kind":"Issuer","name":"letsencrypt"},"secretName":"foo","usages":["digital signature"]}}}}
EOF
}

#This gives token to join a new node to kubernetes cluster
join(){
  kubeadm token create --print-join-command
}

dnsUtils(){
  log_info "Creating DNS utilities test pod..."
  cat <<EOF | kubectl apply -f - >/dev/null 2>&1
apiVersion: v1
kind: Pod
metadata:
  name: dnsutils
  namespace: default
spec:
  containers:
  - name: dnsutils
    image: k8s.gcr.io/e2e-test-images/jessie-dnsutils:1.3
    command:
      - sleep
      - "3600"
    imagePullPolicy: IfNotPresent
  restartPolicy: Always
EOF
  if [[ $? -eq 0 ]]; then
    log_success "DNS utilities test pod created successfully"
    echo "Commands:"
    echo "  checkDns kubernetes.default.svc.cloud.uat"
  else
    log_error "Failed to create DNS utilities test pod"
    return 1
  fi
}

istioInst(){
  helm repo add istio https://istio-release.storage.googleapis.com/charts
  helm repo update
  helm install istio-base istio/base -n istio-system \
    --create-namespace \
    --set defaultRevision=default

  helm install istiod istio/istiod -n istio-system --wait
  helm ls -n istio-system

}

enableIstio(){
  NAMESPACE=$1
  kubectl label namespace $NAMESPACE istio-injection=enabled
}

istioReset(){
  log_info "Removing Istio service mesh components"
  
  if helm list -n istio-system 2>/dev/null | grep -q istiod; then
    if helm_uninstall_with_summary "istiod" "istio-system" -n istio-system istiod; then
      log_success "Istio control plane (istiod) removed"
    fi
  else
    log_info "Istio control plane (istiod) not found"
  fi
  
  if helm list -n istio-system 2>/dev/null | grep -q istio-base; then
    if helm_uninstall_with_summary "istio-base" "istio-system" -n istio-system istio-base; then
      log_success "Istio base components removed"
    fi
  else
    log_info "Istio base components not found"
  fi
  
  if kubectl get namespace istio-system >/dev/null 2>&1; then
    if kubectl_with_summary delete "namespace" istio-system; then
      log_success "Istio namespace removed"
    fi
  else
    log_info "Istio namespace not found"
  fi
}

checkDns(){
  kubectl exec -i -t dnsutils -n default -- nslookup "$@"
}

getpod() {
  pod=$(kubectl get po -l app.kubernetes.io/name="$release" 2>/dev/null | awk "/${release}/" | awk '{print $1}' | head -n 1)
  echo "$pod"
}

# =============================================================================
# ðŸ“¦ SYSTEM UPDATE & DEPENDENCY MANAGEMENT
# Enhanced with progress bars, optional verbose logging, and smart caching
# =============================================================================

# Global flags for verbose output and caching (can be set via environment or command line)
: ${GOK_VERBOSE:=false}
: ${GOK_SHOW_PROGRESS:=true}
: ${GOK_UPDATE_CACHE_HOURS:=6}  # Cache system updates for 6 hours by default
: ${GOK_CACHE_DIR:=/tmp/gok-cache}

# Function to check if verbose mode is enabled (checks all args)
is_verbose_mode() {
  if [[ "$GOK_VERBOSE" == "true" ]]; then return 0; fi
  for arg in "$@"; do
    [[ "$arg" == "--verbose" || "$arg" == "-v" ]] && return 0
  done
  return 1
}

# Function to check if system update cache is still valid
is_update_cache_valid() {
    local cache_file="$GOK_CACHE_DIR/last_update"
    local cache_hours="${GOK_UPDATE_CACHE_HOURS:-6}"
    
    # Create cache directory if it doesn't exist
    mkdir -p "$GOK_CACHE_DIR"
    
    # Check if cache file exists
    if [[ ! -f "$cache_file" ]]; then
        return 1  # No cache, need update
    fi
    
    # Get cache timestamp
    local cache_time=$(cat "$cache_file" 2>/dev/null || echo "0")
    local current_time=$(date +%s)
    local cache_age_hours=$(( (current_time - cache_time) / 3600 ))
    
    if [[ $cache_age_hours -lt $cache_hours ]]; then
        log_info "System update cache is valid (updated ${cache_age_hours}h ago, cache expires in $((cache_hours - cache_age_hours))h)"
        return 0  # Cache is valid
    else
        log_info "System update cache expired (${cache_age_hours}h old, cache limit: ${cache_hours}h)"
        return 1  # Cache expired, need update
    fi
}

# Function to mark system update cache as fresh
mark_update_cache() {
    local cache_file="$GOK_CACHE_DIR/last_update"
    mkdir -p "$GOK_CACHE_DIR"
    date +%s > "$cache_file"
    log_info "System update cache marked fresh"
}

# Function to check if dependency installation cache is still valid
is_deps_cache_valid() {
    local cache_file="$GOK_CACHE_DIR/last_deps_install"
    local cache_hours="${GOK_DEPS_CACHE_HOURS:-${GOK_UPDATE_CACHE_HOURS:-6}}"
    
    # Create cache directory if it doesn't exist
    mkdir -p "$GOK_CACHE_DIR"
    
    # Check if cache file exists
    if [[ ! -f "$cache_file" ]]; then
        return 1  # No cache, need to install deps
    fi
    
    # Get cache timestamp
    local cache_time=$(cat "$cache_file" 2>/dev/null || echo "0")
    local current_time=$(date +%s)
    local cache_age_hours=$(( (current_time - cache_time) / 3600 ))
    
    if [[ $cache_age_hours -lt $cache_hours ]]; then
        log_info "Dependencies cache is valid (installed ${cache_age_hours}h ago, cache expires in $((cache_hours - cache_age_hours))h)"
        return 0  # Cache is valid
    else
        log_info "Dependencies cache expired (${cache_age_hours}h old, cache limit: ${cache_hours}h)"
        return 1  # Cache expired, need to install deps
    fi
}

# Function to mark dependency installation cache as fresh
mark_deps_cache() {
    local cache_file="$GOK_CACHE_DIR/last_deps_install"
    mkdir -p "$GOK_CACHE_DIR"
    date +%s > "$cache_file"
    log_info "Dependencies cache marked fresh"
}

# Enhanced system update with progress bar and smart caching
updateSys() {
    local verbose_flag="${1:-}"
    local force_update=false
    local skip_update=false
    local start_time=$(date +%s)
    
    # Parse additional flags
    for arg in "$@"; do
        case "$arg" in
            --force-update)
                force_update=true
                shift
                ;;
            --skip-update)
                skip_update=true
                shift
                ;;
        esac
    done
    
    # Skip update if explicitly requested
    if [[ "$skip_update" == "true" ]]; then
        log_info "System update skipped (--skip-update flag)"
        return 0
    fi
    
    # Check cache unless force update is requested
    if [[ "$force_update" == "false" ]] && is_update_cache_valid; then
        log_success "System update skipped (cache is fresh)"
        return 0
    fi
    
    log_step "System Update" "Updating package repositories"
    
    if is_verbose_mode "$verbose_flag"; then
        log_info "Running in verbose mode - showing detailed output"
        if ! apt-get update; then
            log_error "System update failed"
            return 1
        fi
    else
        # Run with progress bar
        log_substep "Downloading package information"
        
        # Create a temporary file for capturing output
        local temp_log=$(mktemp)
        local pid
        
        # Start the update process in background
        {
            apt-get update > "$temp_log" 2>&1
            echo $? > "${temp_log}.exit"
        } &
        pid=$!
        
        # Show progress while update is running
        local progress=0
        local spinner_chars="|/-\\"
        local spinner_idx=0
        
        while kill -0 $pid 2>/dev/null; do
            local char=${spinner_chars:spinner_idx:1}
            printf "\r${COLOR_BLUE}  Updating repositories [%c] %d%%${COLOR_RESET}" "$char" "$progress"
            
            spinner_idx=$(( (spinner_idx + 1) % 4 ))
            progress=$(( (progress + 2) % 101 ))
            sleep 0.1
        done
        
        # Wait for process to complete and get exit code
        wait $pid
        local exit_code
        if [[ -f "${temp_log}.exit" ]]; then
            exit_code=$(cat "${temp_log}.exit")
        else
            exit_code=1
        fi
        
        printf "\r${COLOR_GREEN}  Repository update completed [âœ“] 100%%${COLOR_RESET}\n"
        
        # Check for errors
        if [[ $exit_code -ne 0 ]]; then
            log_error "System update failed - showing detailed output:"
            cat "$temp_log"
            rm -f "$temp_log" "${temp_log}.exit"
            return 1
        fi
        
        # Check for important warnings in output
        if grep -qi "error\|fail\|warning" "$temp_log"; then
            log_warning "Update completed with warnings - use --verbose flag for details"
            log_info "Run: GOK_VERBOSE=true gok install <component> for detailed output"
        fi
        
        # Clean up
        rm -f "$temp_log" "${temp_log}.exit"
    fi
    
    local end_time=$(date +%s)
    local duration=$((end_time - start_time))
    
    # Mark cache as fresh after successful update
    mark_update_cache
    
    log_success "System update completed in ${duration}s"
    
    return 0
}

setupDockerRegistry(){
  if [[ -n $TRACE ]]; then
    set -x
  fi

  EXPORTDIR=$MOUNT_PATH

  if [ "$(hostname)" == 'master.cloud.com' ]; then
      mkdir -p "$EXPORTDIR"/certs
      mkdir -p /mnt/registry
      rm -rf /mnt/registry/config.yml
      wget https://github.com/sumitmaji/kubernetes/raw/master/install_k8s/registry/config.yml -P /mnt/registry/
      pushd "$EXPORTDIR" || exit
      USERNAME=master.cloud.com
      FILENAME=registry
      CERTIFICATE_KEY_NAME=$USERNAME
      rm /etc/docker/certs.d/master.cloud.com:5000/${CERTIFICATE_KEY_NAME}.crt
      rm /root/certs/${CERTIFICATE_KEY_NAME}.crt
      rm /root/certs/${CERTIFICATE_KEY_NAME}.key
      if [ -f /etc/docker/certs.d/master.cloud.com:5000/${CERTIFICATE_KEY_NAME}.crt ]
      then
        echo "The file is present, not creating it!!!!!"
      else
        createCertificate -i "${MASTER_HOST_IP}" -h master.cloud.com -t server -f registry
      fi

      docker stop registry
      docker rm registry

      docker run -d \
        --restart=always \
        --name registry \
        -v $EXPORTDIR/certs:/root/certs \
        -e REGISTRY_HTTP_ADDR=0.0.0.0:5000 \
        -e REGISTRY_HTTP_TLS_CERTIFICATE=/root/certs/${CERTIFICATE_KEY_NAME}.crt \
        -e REGISTRY_HTTP_TLS_KEY=/root/certs/${CERTIFICATE_KEY_NAME}.key \
        -v /mnt/registry:/var/lib/registry \
        -v /mnt/registry/config.yml:/etc/docker/registry/config.yml \
        -p 5000:5000 \
        registry:latest

      mkdir -p /etc/docker/certs.d/master.cloud.com:5000

      cp "$EXPORTDIR"/certs/${CERTIFICATE_KEY_NAME}.crt /etc/docker/certs.d/master.cloud.com:5000/${CERTIFICATE_KEY_NAME}.crt

      popd || exit
  else
      mkdir -p /etc/docker/certs.d/master.cloud.com:5000
      cp "$EXPORTDIR"/certs/${USERNAME}.crt /etc/docker/certs.d/master.cloud.com:5000/${USERNAME}.crt
  fi
}

# Enhanced dependency installation with progress tracking
installDeps() {
    local verbose_flag="${1:-}"
    local force_deps=false
    local skip_deps=false
    local start_time=$(date +%s)
    
    # Parse additional flags
    for arg in "$@"; do
        case "$arg" in
            --force-deps)
                force_deps=true
                shift
                ;;
            --skip-deps)
                skip_deps=true
                shift
                ;;
        esac
    done
    
    # Skip dependencies if explicitly requested
    if [[ "$skip_deps" == "true" ]]; then
        log_info "Dependencies installation skipped (--skip-deps flag)"
        return 0
    fi
    
    # Check cache unless force install is requested
    if [[ "$force_deps" == "false" ]] && is_deps_cache_valid; then
        log_success "Dependencies installation skipped (cache is fresh)"
        return 0
    fi
    
    log_step "Dependencies" "Installing essential system dependencies"
    
    # Define packages to install
    local packages=(
        "net-tools:Network utilities and tools"
        "jq:JSON processor for command line"
        "python3:Python 3 interpreter"
        "python3-pip:Python package installer"
        "curl:Command line tool for transferring data"
        "wget:Network downloader"
        "gnupg:GNU Privacy Guard"
        "software-properties-common:Manage software repositories"
        "apt-transport-https:HTTPS transport for APT"
        "ca-certificates:Common CA certificates"
    )
    
    local total_packages=${#packages[@]}
    local current_package=0
    local failed_packages=()
    
    for package_info in "${packages[@]}"; do
        IFS=':' read -r package_name package_desc <<< "$package_info"
        current_package=$((current_package + 1))
        
        log_substep "Installing $package_name ($current_package/$total_packages)"
        
        if is_verbose_mode "$verbose_flag"; then
            log_info "$package_desc"
            if ! apt-get install -y "$package_name"; then
                log_error "Failed to install $package_name"
                failed_packages+=("$package_name")
            fi
        else
            # Install with progress indication
            local temp_log=$(mktemp)
            local pid
            
            # Start installation in background
            {
                apt-get install -y "$package_name" > "$temp_log" 2>&1
                echo $? > "${temp_log}.exit"
            } &
            pid=$!
            
            # Show progress
            local dots=""
            while kill -0 $pid 2>/dev/null; do
                printf "\r${COLOR_CYAN}    Installing $package_name${dots}${COLOR_RESET}"
                dots="${dots}."
                if [[ ${#dots} -gt 3 ]]; then dots=""; fi
                sleep 0.3
            done
            
            wait $pid
            local exit_code
            if [[ -f "${temp_log}.exit" ]]; then
                exit_code=$(cat "${temp_log}.exit")
            else
                exit_code=1
            fi
            
            if [[ $exit_code -eq 0 ]]; then
                printf "\r${COLOR_GREEN}    âœ“ $package_name installed successfully${COLOR_RESET}\n"
            else
                printf "\r${COLOR_RED}    âœ— $package_name installation failed${COLOR_RESET}\n"
                failed_packages+=("$package_name")
                
                # Show error details for failed packages
                log_error "Failed to install $package_name - error details:"
                tail -10 "$temp_log" | while read line; do
                    log_error "  $line"
                done
            fi
            
            rm -f "$temp_log" "${temp_log}.exit"
        fi
        
        # Update progress
        log_progress "$current_package" "$total_packages" "Installing dependencies"
    done
    
    echo # New line after progress
    
    # Summary
    local successful_packages=$((total_packages - ${#failed_packages[@]}))
    local end_time=$(date +%s)
    local duration=$((end_time - start_time))
    
    if [[ ${#failed_packages[@]} -eq 0 ]]; then
        # Mark cache as fresh after successful installation
        mark_deps_cache
        log_success "All $total_packages dependencies installed successfully in ${duration}s"
    else
        log_warning "$successful_packages/$total_packages dependencies installed successfully"
        log_error "Failed packages: ${failed_packages[*]}"
        
        # Provide troubleshooting suggestions
        log_troubleshooting "Dependency Installation" \
            "Run with verbose mode: GOK_VERBOSE=true gok install <component>" \
            "Update package cache: apt-get update" \
            "Check available disk space: df -h" \
            "Check network connectivity: ping archive.ubuntu.com" \
            "Try installing manually: apt-get install ${failed_packages[*]}"
        
        if [[ ${#failed_packages[@]} -gt 3 ]]; then
            log_error "Too many failed packages - aborting installation"
            return 1
        else
            # Mark partial cache as fresh if most packages succeeded
            if [[ $successful_packages -gt $((total_packages / 2)) ]]; then
                mark_deps_cache
            fi
            log_warning "Continuing with partial dependency installation"
        fi
    fi
    
    # Verify critical dependencies
    verify_critical_dependencies
    
    return 0
}

# Verify that critical dependencies are available
verify_critical_dependencies() {
    log_substep "Verifying critical dependencies"
    
    local critical_commands=(
        "curl:HTTP client"
        "wget:Download utility" 
        "jq:JSON processor"
        "python3:Python interpreter"
    )
    
    local missing_critical=()
    
    for cmd_info in "${critical_commands[@]}"; do
        IFS=':' read -r cmd_name cmd_desc <<< "$cmd_info"
        
        if ! command -v "$cmd_name" >/dev/null 2>&1; then
            missing_critical+=("$cmd_name")
            log_error "Critical dependency missing: $cmd_name ($cmd_desc)"
        else
            log_substep "âœ“ $cmd_name available"
        fi
    done
    
    if [[ ${#missing_critical[@]} -gt 0 ]]; then
        log_error "Critical dependencies missing: ${missing_critical[*]}"
        log_error "Installation cannot continue without these dependencies"
        return 1
    fi
    
    log_success "All critical dependencies verified"
    return 0
}

# Comprehensive HA dependency validation for Kubernetes (validation only)
validate_ha_dependency_for_kubernetes() {
    local verbose_flag="${1:-}"
    
    log_step "HA Validation" "Validating HA proxy installation"
    
    # Get system info for verbose mode  
    local mem_gb=$(free -g | awk '/^Mem:/{print $2}')
    local cpu_cores=$(nproc)
    local network_interfaces=$(ip link show | grep -c "state UP" || echo 0)
    
    if is_verbose_mode "$verbose_flag"; then
        log_info "HA dependency analysis:"
        echo -e "${COLOR_DIM}  â€¢ API_SERVERS: ${API_SERVERS:-'not set'}${COLOR_RESET}"
        echo -e "${COLOR_DIM}  â€¢ HA_PROXY_PORT: ${HA_PROXY_PORT:-'not set'}${COLOR_RESET}"
        echo -e "${COLOR_DIM}  â€¢ System resources: ${mem_gb}GB RAM, ${cpu_cores} cores${COLOR_RESET}"
        echo -e "${COLOR_DIM}  â€¢ Network interfaces: ${network_interfaces}${COLOR_RESET}"
    fi
    
    # Validate API_SERVERS format if set
    if [[ -n "$API_SERVERS" ]] && [[ "$API_SERVERS" != *":"* ]]; then
        log_error "API_SERVERS is malformed: '$API_SERVERS'"
        log_error "Expected format: 'IP:hostname' or 'IP1:host1,IP2:host2'"
        return 1
    fi
    
    # Check if HA proxy is installed and running
    if ! validate_ha_proxy_installation "$verbose_flag"; then
        log_error "HA proxy validation failed"
        return 1
    fi
    
    log_success "HA proxy validation passed"
    return 0
}

# Validate HA proxy installation comprehensively
validate_ha_proxy_installation() {
    local verbose_flag="${1:-}"
    local validation_passed=true
    
    log_substep "Checking HA proxy container status"
    
    # Check if Docker is available for HA proxy
    if ! command -v docker >/dev/null 2>&1; then
        log_error "Docker not found - required for HA proxy container"
        return 1
    fi
    
    # Check if HA proxy container exists and is running
    local ha_container=$(docker ps --filter "name=master-proxy" --format "{{.Names}}" 2>/dev/null || true)
    if [[ -z "$ha_container" ]]; then
        # Check if container exists but is stopped
        local ha_container_stopped=$(docker ps -a --filter "name=master-proxy" --format "{{.Names}}" 2>/dev/null || true)
        if [[ -n "$ha_container_stopped" ]]; then
            log_error "HA proxy container 'master-proxy' exists but is not running"
            if is_verbose_mode "$verbose_flag"; then
                log_info "Container status:"
                docker ps -a --filter "name=master-proxy" --format "table {{.Names}}\t{{.Status}}\t{{.Ports}}" 2>/dev/null || true
            fi
            validation_passed=false
        else
            log_error "HA proxy container 'master-proxy' not found"
            validation_passed=false
        fi
    else
        log_success "HA proxy container 'master-proxy' is running"
        
        if is_verbose_mode "$verbose_flag"; then
            log_info "HA proxy container details:"
            docker ps --filter "name=master-proxy" --format "table {{.Names}}\t{{.Status}}\t{{.Ports}}" 2>/dev/null || true
        fi
    fi
    
    # Check if HA proxy port is accessible
    local ha_port="${HA_PROXY_PORT:-6643}"
    local ha_host="${HA_PROXY_HOSTNAME:-localhost}"
    
    log_substep "Testing HA proxy connectivity on ${ha_host}:${ha_port}"
    
    # Test port connectivity
    if command -v nc >/dev/null 2>&1; then
        if nc -z "$ha_host" "$ha_port" 2>/dev/null; then
            log_success "HA proxy port ${ha_port} is accessible"
        else
            log_error "HA proxy port ${ha_port} is not accessible on ${ha_host}"
            validation_passed=false
        fi
    elif command -v telnet >/dev/null 2>&1; then
        if timeout 5 bash -c "</dev/tcp/${ha_host}/${ha_port}" 2>/dev/null; then
            log_success "HA proxy port ${ha_port} is accessible"
        else
            log_error "HA proxy port ${ha_port} is not accessible on ${ha_host}"
            validation_passed=false
        fi
    else
        log_warning "Cannot test port connectivity (nc/telnet not available)"
    fi
    
    # Check HA proxy configuration file
    if [[ -f "/opt/haproxy.cfg" ]]; then
        log_success "HA proxy configuration file exists"
        
        if is_verbose_mode "$verbose_flag"; then
            log_info "HA proxy configuration summary:"
            grep -E "(bind|server)" /opt/haproxy.cfg 2>/dev/null | head -5 | while read line; do
                echo -e "${COLOR_DIM}  $line${COLOR_RESET}"
            done
        fi
    else
        log_warning "HA proxy configuration file not found at /opt/haproxy.cfg"
    fi
    
    # Show detailed diagnostics if validation failed
    if [[ "$validation_passed" == "false" ]]; then
        log_error "HA proxy installation validation failed"
        
        echo -e "${COLOR_BRIGHT_YELLOW}${COLOR_BOLD}ðŸ”§ HA PROXY DIAGNOSTICS:${COLOR_RESET}"
        echo -e "${COLOR_YELLOW}â€¢ Container status:${COLOR_RESET}"
        docker ps -a --filter "name=master-proxy" 2>/dev/null || echo "  No master-proxy container found"
        
        echo -e "${COLOR_YELLOW}â€¢ Docker containers:${COLOR_RESET}"
        docker ps --format "table {{.Names}}\t{{.Status}}\t{{.Ports}}" | head -5 2>/dev/null || echo "  Cannot list containers"
        
        echo -e "${COLOR_YELLOW}â€¢ Network connectivity:${COLOR_RESET}"
        echo "  Testing ${ha_host}:${ha_port}..."
        
        echo -e "${COLOR_YELLOW}â€¢ Port bindings:${COLOR_RESET}"
        netstat -tlnp 2>/dev/null | grep ":${ha_port}" || echo "  Port ${ha_port} not bound"
        
        return 1
    fi
    
    return 0
}

# Remove Kubernetes packages during reset
remove_kubernetes_packages() {
    local verbose_flag="${1:-}"
    
    log_substep "Removing Kubernetes packages (kubeadm, kubectl, kubelet)"
    
    # List of Kubernetes packages to remove
    local k8s_packages=("kubeadm" "kubectl" "kubelet" "kubernetes-cni" "cri-tools")
    
    # Check if any packages are actually installed
    local installed_packages=()
    for package in "${k8s_packages[@]}"; do
        if dpkg -l | grep -q "^ii.*$package" 2>/dev/null; then
            installed_packages+=("$package")
        fi
    done
    
    # If no packages are installed, exit gracefully
    if [[ ${#installed_packages[@]} -eq 0 ]]; then
        log_info "No Kubernetes packages found to remove."
        return 0
    fi
    
    if is_verbose_mode "$verbose_flag"; then
        log_info "Verbose mode: Showing detailed package removal output"
        
        # Check which packages are installed first
        log_info "Checking installed Kubernetes packages..."
        for package in "${k8s_packages[@]}"; do
            if dpkg -l | grep -q "^ii.*$package" 2>/dev/null; then
                echo -e "${COLOR_DIM}  â€¢ $package: installed${COLOR_RESET}"
            else
                echo -e "${COLOR_DIM}  â€¢ $package: not installed${COLOR_RESET}"
            fi
        done
        
        # Remove only installed packages with verbose output
        log_info "Removing installed Kubernetes packages..."
        if [[ ${#installed_packages[@]} -gt 0 ]]; then
            sudo apt-get remove --purge -y "${installed_packages[@]}" 2>&1 | while read line; do
                echo -e "${COLOR_DIM}  $line${COLOR_RESET}"
            done
        else
            echo -e "${COLOR_DIM}  No packages to remove${COLOR_RESET}"
        fi
        
        # Clean up package cache
        log_info "Cleaning up package cache..."
        sudo apt-get autoremove -y 2>&1 | while read line; do
            echo -e "${COLOR_DIM}  $line${COLOR_RESET}"
        done
        
        sudo apt-get autoclean 2>&1 | while read line; do
            echo -e "${COLOR_DIM}  $line${COLOR_RESET}"
        done
        
    else
        # Silent removal with progress indication - only remove installed packages
        {
            if [[ ${#installed_packages[@]} -gt 0 ]]; then
                sudo apt-get remove --purge -y "${installed_packages[@]}" >/dev/null 2>&1
            fi
            sudo apt-get autoremove -y >/dev/null 2>&1
            sudo apt-get autoclean >/dev/null 2>&1
        } &
        
        local pid=$!
        local dots=""
        while kill -0 $pid 2>/dev/null; do
            printf "\r    Removing Kubernetes packages${dots}"
            dots="${dots}."
            if [[ ${#dots} -gt 3 ]]; then dots=""; fi
            sleep 0.5
        done
        
        wait $pid
        local exit_code=$?
        
        if [[ $exit_code -eq 0 ]]; then
            printf "\r    âœ“ Kubernetes packages removed successfully\n"
        else
            printf "\r    âš  Package removal completed with warnings\n"
            log_warning "Some packages may not have been removed completely"
        fi
    fi
    
    # Remove Kubernetes repository configuration
    log_substep "Removing Kubernetes repository configuration"
    
    if is_verbose_mode "$verbose_flag"; then
        log_info "Removing repository files..."
        if [[ -f /etc/apt/sources.list.d/kubernetes.list ]]; then
            echo -e "${COLOR_DIM}  â€¢ Removing /etc/apt/sources.list.d/kubernetes.list${COLOR_RESET}"
            sudo rm -f /etc/apt/sources.list.d/kubernetes.list
        fi
        
        if [[ -f /etc/apt/keyrings/kubernetes-apt-keyring.gpg ]]; then
            echo -e "${COLOR_DIM}  â€¢ Removing /etc/apt/keyrings/kubernetes-apt-keyring.gpg${COLOR_RESET}"
            sudo rm -f /etc/apt/keyrings/kubernetes-apt-keyring.gpg
        fi
        
        log_info "Updating package lists..."
        sudo apt-get update 2>&1 | while read line; do
            echo -e "${COLOR_DIM}  $line${COLOR_RESET}"
        done
    else
        sudo rm -f /etc/apt/sources.list.d/kubernetes.list
        sudo rm -f /etc/apt/keyrings/kubernetes-apt-keyring.gpg
        sudo apt-get update >/dev/null 2>&1
        log_success "Repository configuration removed"
    fi
    
    log_success "Kubernetes packages and repository removed successfully"
}

ingressUnInst() {
  log_info "Checking ingress-nginx installation status"
  
  local controller_count=$(kubectl get po -n ingress-nginx -l app.kubernetes.io/component=controller -o json 2>/dev/null | jq '.items | length' 2>/dev/null || echo "0")
  
  if [[ "$controller_count" == "1" ]]; then
    log_info "Active ingress-nginx controller found - proceeding with removal"
    
    if helm list -n ingress-nginx 2>/dev/null | grep -q ingress-nginx; then
      if helm_uninstall_with_summary "ingress-nginx" "ingress-nginx" -n ingress-nginx ingress-nginx; then
        log_success "Ingress-nginx Helm release removed"
      fi
    else
      log_info "No ingress-nginx Helm release found"
    fi
    
    if kubectl get namespace ingress-nginx >/dev/null 2>&1; then
      if kubectl_with_summary delete "namespace" ingress-nginx; then
        log_success "Ingress-nginx namespace removed"
      fi
    else
      log_info "Ingress-nginx namespace not found"
    fi
  else
    log_info "No active ingress-nginx controller found - nothing to remove"
  fi
}

patchNginxConfig() {
  echo "Patching nginx-configuration ConfigMap to enable debug logging..."

  # Patch the ConfigMap
  kubectl patch configmap ingress-nginx-controller -n ingress-nginx --type merge --patch "$(
    cat <<EOF
data:
  enable-vts-status: "true"
  log-format-upstream: '{"time": "\$time_iso8601", "remote_addr": "\$remote_addr", "x-forwarded-for": "\$http_x_forwarded_for", "request_id": "\$request_id", "remote_user": "\$remote_user", "bytes_sent": "\$bytes_sent", "request_time": "\$request_time", "status": "\$status", "vhost": "\$host", "request_proto": "\$server_protocol", "path": "\$uri", "request_query": "\$args", "request_length": "\$request_length", "duration": "\$request_time", "method": "\$request_method", "http_referrer": "\$http_referer", "http_user_agent": "\$http_user_agent", "upstream_addr": "\$upstream_addr", "upstream_status": "\$upstream_status", "upstream_response_length": "\$upstream_response_length", "upstream_response_time": "\$upstream_response_time", "upstream_cache_status": "\$upstream_cache_status", "authorization": "\$http_authorization", "request_body": "\$request_body"}'
EOF
  )"

  # Restart the ingress-nginx-controller deployment
  echo "Restarting ingress-nginx-controller deployment..."
  kubectl rollout restart deployment ingress-nginx-controller -n ingress-nginx

  echo "Nginx configuration patched and ingress-nginx-controller restarted successfully!"
}

ingressInst() {
  log_info "Installing NGINX Ingress Controller..."
  
  # Add helm repository quietly
  log_step "Adding NGINX ingress Helm repository"
  helm repo add ingress-nginx https://kubernetes.github.io/ingress-nginx >/dev/null 2>&1
  helm repo update >/dev/null 2>&1
  
  # Install ingress with clean output
  log_step "Installing NGINX Ingress Controller v4.12.1"
  
  # Capture helm install output to temporary file
  local temp_log="/tmp/helm_ingress_install.log"
  local install_success=false
  
  if helm install \
    ingress-nginx ingress-nginx/ingress-nginx --version 4.12.1 \
    --namespace ingress-nginx \
    --create-namespace \
    --set controller.service.nodePorts.http=80 \
    --set controller.service.nodePorts.https=443 \
    --set controller.service.type=NodePort \
    --set defaultBackend.enabled=true \
    > "$temp_log" 2>&1; then
    install_success=true
  fi
  
  # Extract and show summary information
  if [[ "$install_success" == "true" ]]; then
    log_success "NGINX Ingress Controller installed successfully"
    
    # Show key configuration summary
    echo ""
    log_header "ðŸ“‹ Installation Summary" "NGINX Ingress Controller"
    echo -e "  ${COLOR_GREEN}âœ“${COLOR_RESET} ${COLOR_BOLD}Version:${COLOR_RESET} 4.12.1"
    echo -e "  ${COLOR_GREEN}âœ“${COLOR_RESET} ${COLOR_BOLD}Namespace:${COLOR_RESET} ingress-nginx"
    echo -e "  ${COLOR_GREEN}âœ“${COLOR_RESET} ${COLOR_BOLD}Service Type:${COLOR_RESET} NodePort"
    echo -e "  ${COLOR_GREEN}âœ“${COLOR_RESET} ${COLOR_BOLD}HTTP Port:${COLOR_RESET} 80"
    echo -e "  ${COLOR_GREEN}âœ“${COLOR_RESET} ${COLOR_BOLD}HTTPS Port:${COLOR_RESET} 443"
    echo -e "  ${COLOR_GREEN}âœ“${COLOR_RESET} ${COLOR_BOLD}Default Backend:${COLOR_RESET} Enabled"
    echo ""
    
    # Extract useful information from helm output
    if grep -q "STATUS: deployed" "$temp_log"; then
      echo -e "${COLOR_GREEN}ðŸ“¦ Deployment Status:${COLOR_RESET} Successfully deployed"
    fi
    
    # Show next steps
    echo -e "${COLOR_CYAN}ðŸš€ Next Steps:${COLOR_RESET}"
    echo -e "   â€¢ Ingress controller will be available shortly"
    echo -e "   â€¢ Check status: ${COLOR_BOLD}kubectl get pods -n ingress-nginx${COLOR_RESET}"
    echo -e "   â€¢ Create ingress resources to route traffic"
    echo ""
  else
    log_error "NGINX Ingress Controller installation failed"
    echo ""
    echo -e "${COLOR_RED}ðŸ“‹ Error Details:${COLOR_RESET}"
    # Show only relevant error information
    if grep -i "error\|failed\|timeout" "$temp_log" >/dev/null; then
      grep -i "error\|failed\|timeout" "$temp_log" | head -5 | sed 's/^/   â€¢ /'
    else
      echo "   â€¢ Check helm status and cluster resources"
    fi
    echo ""
    rm -f "$temp_log"
    return 1
  fi
  
  # Clean up temporary file
  rm -f "$temp_log"
  
  # Wait for service to be available
  log_step "Waiting for ingress controller to be ready"
  waitForServiceAvailable ingress-nginx
  
  # Suggest next module installation
  suggest_and_install_next_module "ingress"
}

# Show comprehensive NGINX Ingress Controller installation summary
ingressSummary() {
  local verbose_mode="${1:-false}"
  
  echo -e "${COLOR_BRIGHT_BLUE}${COLOR_BOLD}"
  echo "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—"
  echo "â•‘                    NGINX INGRESS CONTROLLER SUMMARY                         â•‘"
  echo "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
  echo -e "${COLOR_RESET}"
  echo ""
  
  # Check if we can connect to cluster
  local cluster_accessible=false
  if kubectl cluster-info >/dev/null 2>&1; then
    cluster_accessible=true
  fi
  
  # 1. Ingress Controller Installation Status
  log_step "1" "Installation Status"
  
  if [[ "$cluster_accessible" == "true" ]]; then
    # Check if ingress-nginx namespace exists
    if kubectl get namespace ingress-nginx >/dev/null 2>&1; then
      echo -e "  ${COLOR_GREEN}âœ“ Namespace: ingress-nginx exists${COLOR_RESET}"
      
      # Check Helm installation
      local helm_status=$(helm list -n ingress-nginx --output json 2>/dev/null | jq -r '.[] | select(.name=="ingress-nginx") | .status' 2>/dev/null || echo "not-found")
      if [[ "$helm_status" == "deployed" ]]; then
        local chart_version=$(helm list -n ingress-nginx --output json 2>/dev/null | jq -r '.[] | select(.name=="ingress-nginx") | .chart' 2>/dev/null || echo "unknown")
        echo -e "  ${COLOR_GREEN}âœ“ Helm Chart: $chart_version (deployed)${COLOR_RESET}"
      elif [[ "$helm_status" == "failed" ]]; then
        echo -e "  ${COLOR_RED}âœ— Helm Chart: deployment failed${COLOR_RESET}"
      else
        echo -e "  ${COLOR_RED}âœ— Helm Chart: not installed via Helm${COLOR_RESET}"
      fi
    else
      echo -e "  ${COLOR_RED}âœ— Namespace: ingress-nginx not found${COLOR_RESET}"
    fi
  else
    echo -e "  ${COLOR_RED}âœ— Installation status: cluster not accessible${COLOR_RESET}"
    echo -e "    ${COLOR_DIM}Run: gok k8sInst to install Kubernetes first${COLOR_RESET}"
  fi
  
  echo ""
  
  # 2. Controller Pods Status
  if [[ "$cluster_accessible" == "true" ]]; then
    log_step "2" "Controller Pods"
    
    local ingress_pods=$(kubectl get pods -n ingress-nginx --no-headers 2>/dev/null)
    if [[ -n "$ingress_pods" ]]; then
      echo "$ingress_pods" | while read -r line; do
        local pod_name=$(echo "$line" | awk '{print $1}')
        local ready_status=$(echo "$line" | awk '{print $2}')
        local pod_status=$(echo "$line" | awk '{print $3}')
        local restarts=$(echo "$line" | awk '{print $4}')
        local age=$(echo "$line" | awk '{print $5}')
        
        if [[ "$pod_status" == "Running" ]]; then
          echo -e "    ${COLOR_GREEN}âœ“ ${pod_name}: ${pod_status} (${ready_status}) - ${age}${COLOR_RESET}"
        elif [[ "$pod_status" == "Pending" ]]; then
          echo -e "    ${COLOR_YELLOW}âš  ${pod_name}: ${pod_status} (${ready_status}) - ${age}${COLOR_RESET}"
        else
          echo -e "    ${COLOR_RED}âœ— ${pod_name}: ${pod_status} (${ready_status}) - ${age}${COLOR_RESET}"
        fi
      done
    else
      echo -e "    ${COLOR_RED}âœ— No ingress controller pods found${COLOR_RESET}"
    fi
    
    echo ""
  fi
  
  # 3. Service Configuration
  log_step "3" "Service Configuration"
  
  if [[ "$cluster_accessible" == "true" ]]; then
    # Check ingress controller service
    if kubectl get service ingress-nginx-controller -n ingress-nginx >/dev/null 2>&1; then
      local service_type=$(kubectl get service ingress-nginx-controller -n ingress-nginx -o jsonpath='{.spec.type}' 2>/dev/null)
      local http_port=$(kubectl get service ingress-nginx-controller -n ingress-nginx -o jsonpath='{.spec.ports[?(@.name=="http")].nodePort}' 2>/dev/null)
      local https_port=$(kubectl get service ingress-nginx-controller -n ingress-nginx -o jsonpath='{.spec.ports[?(@.name=="https")].nodePort}' 2>/dev/null)
      
      echo -e "  ${COLOR_GREEN}âœ“ Service: ingress-nginx-controller${COLOR_RESET}"
      echo -e "    ${COLOR_DIM}â€¢ Type: ${service_type}${COLOR_RESET}"
      if [[ -n "$http_port" ]]; then
        echo -e "    ${COLOR_DIM}â€¢ HTTP NodePort: ${http_port}${COLOR_RESET}"
      fi
      if [[ -n "$https_port" ]]; then
        echo -e "    ${COLOR_DIM}â€¢ HTTPS NodePort: ${https_port}${COLOR_RESET}"
      fi
      
      # Check service endpoints
      local endpoints=$(kubectl get endpoints ingress-nginx-controller -n ingress-nginx -o jsonpath='{.subsets[*].addresses[*].ip}' 2>/dev/null)
      if [[ -n "$endpoints" ]]; then
        echo -e "    ${COLOR_DIM}â€¢ Endpoints: ${endpoints}${COLOR_RESET}"
      fi
    else
      echo -e "  ${COLOR_RED}âœ— Service: ingress-nginx-controller not found${COLOR_RESET}"
    fi
    
    # Check default backend
    if kubectl get service ingress-nginx-defaultbackend -n ingress-nginx >/dev/null 2>&1; then
      echo -e "  ${COLOR_GREEN}âœ“ Default Backend: available${COLOR_RESET}"
    else
      echo -e "  ${COLOR_YELLOW}âš  Default Backend: not configured${COLOR_RESET}"
    fi
  else
    echo -e "  ${COLOR_RED}âœ— Service status: cluster not accessible${COLOR_RESET}"
  fi
  
  echo ""
  
  # 4. Ingress Classes
  log_step "4" "Ingress Classes"
  
  if [[ "$cluster_accessible" == "true" ]]; then
    local ingress_classes=$(kubectl get ingressclass --no-headers 2>/dev/null)
    if [[ -n "$ingress_classes" ]]; then
      echo "$ingress_classes" | while read -r line; do
        local class_name=$(echo "$line" | awk '{print $1}')
        local controller=$(echo "$line" | awk '{print $2}')
        local is_default=$(echo "$line" | awk '{print $3}')
        
        if [[ "$is_default" == "<default>" ]]; then
          echo -e "    ${COLOR_GREEN}âœ“ ${class_name}: ${controller} (default)${COLOR_RESET}"
        else
          echo -e "    ${COLOR_CYAN}â€¢ ${class_name}: ${controller}${COLOR_RESET}"
        fi
      done
    else
      echo -e "    ${COLOR_RED}âœ— No ingress classes found${COLOR_RESET}"
    fi
  else
    echo -e "    ${COLOR_RED}âœ— Ingress classes: cluster not accessible${COLOR_RESET}"
  fi
  
  echo ""
  
  # 5. Active Ingress Resources
  log_step "5" "Active Ingress Resources"
  
  if [[ "$cluster_accessible" == "true" ]]; then
    local ingress_resources=$(kubectl get ingress --all-namespaces --no-headers 2>/dev/null)
    if [[ -n "$ingress_resources" ]]; then
      echo "$ingress_resources" | while read -r line; do
        local namespace=$(echo "$line" | awk '{print $1}')
        local ingress_name=$(echo "$line" | awk '{print $2}')
        local class_name=$(echo "$line" | awk '{print $3}')
        local hosts=$(echo "$line" | awk '{print $4}')
        local address=$(echo "$line" | awk '{print $5}')
        local age=$(echo "$line" | awk '{print $6}')
        
        if [[ -n "$address" && "$address" != "<none>" ]]; then
          echo -e "    ${COLOR_GREEN}âœ“ ${namespace}/${ingress_name}: ${hosts} â†’ ${address} (${age})${COLOR_RESET}"
        else
          echo -e "    ${COLOR_YELLOW}âš  ${namespace}/${ingress_name}: ${hosts} (pending address) (${age})${COLOR_RESET}"
        fi
        if [[ -n "$class_name" && "$class_name" != "<none>" ]]; then
          echo -e "      ${COLOR_DIM}Class: ${class_name}${COLOR_RESET}"
        fi
      done
    else
      echo -e "    ${COLOR_CYAN}â€¢ No ingress resources found${COLOR_RESET}"
      echo -e "      ${COLOR_DIM}Create ingress resources to route external traffic${COLOR_RESET}"
    fi
  else
    echo -e "    ${COLOR_RED}âœ— Ingress resources: cluster not accessible${COLOR_RESET}"
  fi
  
  echo ""
  
  # 6. Configuration Summary
  log_step "6" "Configuration Summary"
  
  if [[ "$cluster_accessible" == "true" ]]; then
    # Get current configuration from the deployment
    local controller_image=$(kubectl get deployment ingress-nginx-controller -n ingress-nginx -o jsonpath='{.spec.template.spec.containers[0].image}' 2>/dev/null || echo "unknown")
    
    echo -e "  ${COLOR_CYAN}Controller Configuration:${COLOR_RESET}"
    echo -e "    ${COLOR_DIM}â€¢ Image: ${controller_image}${COLOR_RESET}"
    echo -e "    ${COLOR_DIM}â€¢ Service Type: NodePort${COLOR_RESET}"
    echo -e "    ${COLOR_DIM}â€¢ HTTP Port: 80 â†’ NodePort${COLOR_RESET}"
    echo -e "    ${COLOR_DIM}â€¢ HTTPS Port: 443 â†’ NodePort${COLOR_RESET}"
    echo -e "    ${COLOR_DIM}â€¢ Default Backend: Enabled${COLOR_RESET}"
    
    # Check for custom configurations
    local config_map=$(kubectl get configmap ingress-nginx-controller -n ingress-nginx -o jsonpath='{.data}' 2>/dev/null)
    if [[ -n "$config_map" && "$config_map" != "{}" ]]; then
      echo -e "    ${COLOR_DIM}â€¢ Custom ConfigMap: configured${COLOR_RESET}"
    else
      echo -e "    ${COLOR_DIM}â€¢ Custom ConfigMap: default settings${COLOR_RESET}"
    fi
  else
    echo -e "  ${COLOR_RED}âœ— Configuration: cluster not accessible${COLOR_RESET}"
  fi
  
  echo ""
  
  # 7. Next Steps
  log_step "7" "Available Commands"
  
  echo -e "  ${COLOR_CYAN}Ingress Management:${COLOR_RESET}"
  echo -e "    ${COLOR_DIM}gok ingressInst                 # Install/reinstall NGINX Ingress Controller${COLOR_RESET}"
  echo -e "    ${COLOR_DIM}gok ingressSummary              # Show ingress summary (this command)${COLOR_RESET}"
  
  echo -e "  ${COLOR_CYAN}Testing & Verification:${COLOR_RESET}"
  echo -e "    ${COLOR_DIM}kubectl get pods -n ingress-nginx              # Check controller pods${COLOR_RESET}"
  echo -e "    ${COLOR_DIM}kubectl get svc -n ingress-nginx               # Check services${COLOR_RESET}"
  echo -e "    ${COLOR_DIM}kubectl get ingress --all-namespaces           # List all ingress resources${COLOR_RESET}"
  
  echo -e "  ${COLOR_CYAN}Configuration:${COLOR_RESET}"
  echo -e "    ${COLOR_DIM}kubectl get ingressclass                       # List ingress classes${COLOR_RESET}"
  echo -e "    ${COLOR_DIM}kubectl describe ingress <name> -n <namespace> # Inspect ingress resource${COLOR_RESET}"
  
  if [[ "$cluster_accessible" == "true" ]]; then
    local node_ip=$(kubectl get nodes -o jsonpath='{.items[0].status.addresses[?(@.type=="InternalIP")].address}' 2>/dev/null)
    if [[ -n "$node_ip" ]]; then
      echo -e "  ${COLOR_CYAN}Access Information:${COLOR_RESET}"
      echo -e "    ${COLOR_DIM}â€¢ Node IP: ${node_ip}${COLOR_RESET}"
      echo -e "    ${COLOR_DIM}â€¢ Access your services via: http://<domain>:NodePort${COLOR_RESET}"
      echo -e "    ${COLOR_DIM}â€¢ Configure your DNS or /etc/hosts to point domains to ${node_IP}${COLOR_RESET}"
    fi
  fi
  
  echo ""
  echo -e "${COLOR_BRIGHT_GREEN}ðŸ“‹ Summary Complete${COLOR_RESET}"
  echo ""
}

# Comprehensive cert-manager Summary function
certManagerSummary() {
  local verbose_mode="${1:-false}"
  
  echo -e "${COLOR_BRIGHT_BLUE}${COLOR_BOLD}"
  echo "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—"
  echo "â•‘                      CERT-MANAGER SUMMARY                                   â•‘"
  echo "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
  echo -e "${COLOR_RESET}"
  echo ""
  
  local cluster_accessible=false
  if kubectl cluster-info >/dev/null 2>&1; then
    cluster_accessible=true
  fi
  
  # 1. Installation Status
  log_step "1" "Installation Status"
  if [[ "$cluster_accessible" == "true" ]]; then
    if kubectl get namespace cert-manager >/dev/null 2>&1; then
      echo -e "  ${COLOR_GREEN}âœ“ Namespace: cert-manager exists${COLOR_RESET}"
      local helm_status=$(helm list -n cert-manager --output json 2>/dev/null | jq -r '.[] | select(.name=="cert-manager") | .status' 2>/dev/null || echo "not-found")
      if [[ "$helm_status" == "deployed" ]]; then
        local chart_version=$(helm list -n cert-manager --output json 2>/dev/null | jq -r '.[] | select(.name=="cert-manager") | .chart' 2>/dev/null || echo "unknown")
        echo -e "  ${COLOR_GREEN}âœ“ Helm Chart: $chart_version (deployed)${COLOR_RESET}"
      else
        echo -e "  ${COLOR_RED}âœ— Helm Chart: not deployed${COLOR_RESET}"
      fi
    else
      echo -e "  ${COLOR_RED}âœ— Namespace: cert-manager not found${COLOR_RESET}"
    fi
  else
    echo -e "  ${COLOR_RED}âœ— Installation status: cluster not accessible${COLOR_RESET}"
  fi
  echo ""
  
  # 2. Pods Status
  if [[ "$cluster_accessible" == "true" ]]; then
    log_step "2" "cert-manager Pods"
    local pods=$(kubectl get pods -n cert-manager --no-headers 2>/dev/null)
    if [[ -n "$pods" ]]; then
      echo "$pods" | while read -r line; do
        local pod_name=$(echo "$line" | awk '{print $1}')
        local pod_status=$(echo "$line" | awk '{print $3}')
        local ready_status=$(echo "$line" | awk '{print $2}')
        if [[ "$pod_status" == "Running" ]]; then
          echo -e "    ${COLOR_GREEN}âœ“ ${pod_name}: ${pod_status} (${ready_status})${COLOR_RESET}"
        else
          echo -e "    ${COLOR_RED}âœ— ${pod_name}: ${pod_status} (${ready_status})${COLOR_RESET}"
        fi
      done
    else
      echo -e "    ${COLOR_RED}âœ— No cert-manager pods found${COLOR_RESET}"
    fi
    echo ""
  fi

  # 3. Issuers and Certificates
  if [[ "$cluster_accessible" == "true" ]]; then
    log_step "3" "Certificate Issuers"
    local issuers=$(kubectl get clusterissuers --no-headers 2>/dev/null)
    if [[ -n "$issuers" ]]; then
      echo "$issuers" | while read -r line; do
        local issuer_name=$(echo "$line" | awk '{print $1}')
        local ready=$(echo "$line" | awk '{print $2}')
        if [[ "$ready" == "True" ]]; then
          echo -e "    ${COLOR_GREEN}âœ“ ClusterIssuer: ${issuer_name} (Ready)${COLOR_RESET}"
        else
          echo -e "    ${COLOR_YELLOW}âš  ClusterIssuer: ${issuer_name} (Not Ready)${COLOR_RESET}"
        fi
      done
    else
      echo -e "    ${COLOR_CYAN}â€¢ No ClusterIssuers configured${COLOR_RESET}"
    fi
    
    local certificates=$(kubectl get certificates --all-namespaces --no-headers 2>/dev/null)
    if [[ -n "$certificates" ]]; then
      echo -e "  ${COLOR_CYAN}Active Certificates:${COLOR_RESET}"
      echo "$certificates" | head -5 | while read -r line; do
        local namespace=$(echo "$line" | awk '{print $1}')
        local cert_name=$(echo "$line" | awk '{print $2}')
        local ready=$(echo "$line" | awk '{print $3}')
        if [[ "$ready" == "True" ]]; then
          echo -e "    ${COLOR_GREEN}âœ“ ${namespace}/${cert_name}${COLOR_RESET}"
        else
          echo -e "    ${COLOR_YELLOW}âš  ${namespace}/${cert_name}${COLOR_RESET}"
        fi
      done
    fi
    echo ""
  fi
  
  # 4. Available Commands
  log_step "4" "Available Commands"
  echo -e "  ${COLOR_CYAN}cert-manager Management:${COLOR_RESET}"
  echo -e "    ${COLOR_DIM}gok install cert-manager        # Install cert-manager${COLOR_RESET}"
  echo -e "    ${COLOR_DIM}gok certManagerSummary          # Show this summary${COLOR_RESET}"
  echo -e "    ${COLOR_DIM}gok certManagerReset            # Reset cert-manager${COLOR_RESET}"
  echo ""
  echo -e "${COLOR_BRIGHT_GREEN}ðŸ“‹ Summary Complete${COLOR_RESET}"
  echo ""
}

# Comprehensive Kyverno Summary function
kyvernoSummary() {
  local verbose_mode="${1:-false}"
  
  echo -e "${COLOR_BRIGHT_BLUE}${COLOR_BOLD}"
  echo "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—"
  echo "â•‘                        KYVERNO POLICY ENGINE SUMMARY                        â•‘"
  echo "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
  echo -e "${COLOR_RESET}"
  echo ""
  
  local cluster_accessible=false
  if kubectl cluster-info >/dev/null 2>&1; then
    cluster_accessible=true
  fi
  
  # 1. Installation Status
  log_step "1" "Installation Status"
  if [[ "$cluster_accessible" == "true" ]]; then
    if kubectl get namespace kyverno >/dev/null 2>&1; then
      echo -e "  ${COLOR_GREEN}âœ“ Namespace: kyverno exists${COLOR_RESET}"
      local helm_status=$(helm list -n kyverno --output json 2>/dev/null | jq -r '.[] | select(.name=="kyverno") | .status' 2>/dev/null || echo "not-found")
      if [[ "$helm_status" == "deployed" ]]; then
        local chart_version=$(helm list -n kyverno --output json 2>/dev/null | jq -r '.[] | select(.name=="kyverno") | .chart' 2>/dev/null || echo "unknown")
        echo -e "  ${COLOR_GREEN}âœ“ Helm Chart: $chart_version (deployed)${COLOR_RESET}"
      else
        echo -e "  ${COLOR_RED}âœ— Helm Chart: not deployed${COLOR_RESET}"
      fi
    else
      echo -e "  ${COLOR_RED}âœ— Namespace: kyverno not found${COLOR_RESET}"
    fi
  else
    echo -e "  ${COLOR_RED}âœ— Installation status: cluster not accessible${COLOR_RESET}"
  fi
  echo ""
  
  # 2. Kyverno Pods
  if [[ "$cluster_accessible" == "true" ]]; then
    log_step "2" "Kyverno Pods"
    local pods=$(kubectl get pods -n kyverno --no-headers 2>/dev/null)
    if [[ -n "$pods" ]]; then
      echo "$pods" | while read -r line; do
        local pod_name=$(echo "$line" | awk '{print $1}')
        local pod_status=$(echo "$line" | awk '{print $3}')
        local ready_status=$(echo "$line" | awk '{print $2}')
        if [[ "$pod_status" == "Running" ]]; then
          echo -e "    ${COLOR_GREEN}âœ“ ${pod_name}: ${pod_status} (${ready_status})${COLOR_RESET}"
        else
          echo -e "    ${COLOR_RED}âœ— ${pod_name}: ${pod_status} (${ready_status})${COLOR_RESET}"
        fi
      done
    else
      echo -e "    ${COLOR_RED}âœ— No Kyverno pods found${COLOR_RESET}"
    fi
    echo ""
  fi

  # 3. Policies
  if [[ "$cluster_accessible" == "true" ]]; then
    log_step "3" "Policy Status"
    local policies=$(kubectl get clusterpolicies --no-headers 2>/dev/null)
    if [[ -n "$policies" ]]; then
      echo -e "  ${COLOR_CYAN}Cluster Policies:${COLOR_RESET}"
      echo "$policies" | head -5 | while read -r line; do
        local policy_name=$(echo "$line" | awk '{print $1}')
        echo -e "    ${COLOR_GREEN}â€¢ ${policy_name}${COLOR_RESET}"
      done
    else
      echo -e "    ${COLOR_CYAN}â€¢ No cluster policies configured${COLOR_RESET}"
    fi
    
    local namespaced_policies=$(kubectl get policies --all-namespaces --no-headers 2>/dev/null)
    if [[ -n "$namespaced_policies" ]]; then
      echo -e "  ${COLOR_CYAN}Namespaced Policies:${COLOR_RESET}"
      echo "$namespaced_policies" | head -3 | while read -r line; do
        local namespace=$(echo "$line" | awk '{print $1}')
        local policy_name=$(echo "$line" | awk '{print $2}')
        echo -e "    ${COLOR_GREEN}â€¢ ${namespace}/${policy_name}${COLOR_RESET}"
      done
    fi
    echo ""
  fi
  
  # 4. Available Commands
  log_step "4" "Available Commands"
  echo -e "  ${COLOR_CYAN}Kyverno Management:${COLOR_RESET}"
  echo -e "    ${COLOR_DIM}gok install kyverno             # Install Kyverno${COLOR_RESET}"
  echo -e "    ${COLOR_DIM}gok kyvernoSummary              # Show this summary${COLOR_RESET}"
  echo -e "    ${COLOR_DIM}gok kyvernoReset                # Reset Kyverno${COLOR_RESET}"
  echo ""
  echo -e "${COLOR_BRIGHT_GREEN}ðŸ“‹ Summary Complete${COLOR_RESET}"
  echo ""
}

# Registry Summary function
registrySummary() {
  local verbose_mode="${1:-false}"
  
  echo -e "${COLOR_BRIGHT_BLUE}${COLOR_BOLD}"
  echo "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—"
  echo "â•‘                        CONTAINER REGISTRY SUMMARY                           â•‘"
  echo "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
  echo -e "${COLOR_RESET}"
  echo ""
  
  local cluster_accessible=false
  if kubectl cluster-info >/dev/null 2>&1; then
    cluster_accessible=true
  fi
  
  # 1. Installation Status
  log_step "1" "Installation Status"
  if [[ "$cluster_accessible" == "true" ]]; then
    if kubectl get namespace registry >/dev/null 2>&1; then
      echo -e "  ${COLOR_GREEN}âœ“ Namespace: registry exists${COLOR_RESET}"
      local registry_deployment=$(kubectl get deployment registry -n registry --no-headers 2>/dev/null)
      if [[ -n "$registry_deployment" ]]; then
        echo -e "  ${COLOR_GREEN}âœ“ Registry deployment found${COLOR_RESET}"
      else
        echo -e "  ${COLOR_RED}âœ— Registry deployment not found${COLOR_RESET}"
      fi
    else
      echo -e "  ${COLOR_RED}âœ— Namespace: registry not found${COLOR_RESET}"
    fi
  else
    echo -e "  ${COLOR_RED}âœ— Installation status: cluster not accessible${COLOR_RESET}"
  fi
  echo ""
  
  # 2. Registry Pods
  if [[ "$cluster_accessible" == "true" ]]; then
    log_step "2" "Registry Pods"
    local pods=$(kubectl get pods -n registry --no-headers 2>/dev/null)
    if [[ -n "$pods" ]]; then
      echo "$pods" | while read -r line; do
        local pod_name=$(echo "$line" | awk '{print $1}')
        local pod_status=$(echo "$line" | awk '{print $3}')
        local ready_status=$(echo "$line" | awk '{print $2}')
        if [[ "$pod_status" == "Running" ]]; then
          echo -e "    ${COLOR_GREEN}âœ“ ${pod_name}: ${pod_status} (${ready_status})${COLOR_RESET}"
        else
          echo -e "    ${COLOR_RED}âœ— ${pod_name}: ${pod_status} (${ready_status})${COLOR_RESET}"
        fi
      done
    else
      echo -e "    ${COLOR_RED}âœ— No registry pods found${COLOR_RESET}"
    fi
    echo ""
  fi

  # 3. Service and Access
  if [[ "$cluster_accessible" == "true" ]]; then
    log_step "3" "Service Configuration"
    local registry_service=$(kubectl get service registry -n registry --no-headers 2>/dev/null)
    if [[ -n "$registry_service" ]]; then
      local service_type=$(kubectl get service registry -n registry -o jsonpath='{.spec.type}' 2>/dev/null)
      local port=$(kubectl get service registry -n registry -o jsonpath='{.spec.ports[0].port}' 2>/dev/null)
      echo -e "  ${COLOR_GREEN}âœ“ Service: registry (Type: ${service_type}, Port: ${port})${COLOR_RESET}"
      
      if [[ "$service_type" == "NodePort" ]]; then
        local nodeport=$(kubectl get service registry -n registry -o jsonpath='{.spec.ports[0].nodePort}' 2>/dev/null)
        echo -e "    ${COLOR_DIM}â€¢ NodePort: ${nodeport}${COLOR_RESET}"
      fi
    else
      echo -e "  ${COLOR_RED}âœ— Registry service not found${COLOR_RESET}"
    fi
    echo ""
  fi
  
  # 4. Available Commands
  log_step "4" "Available Commands"
  echo -e "  ${COLOR_CYAN}Registry Management:${COLOR_RESET}"
  echo -e "    ${COLOR_DIM}gok install registry            # Install container registry${COLOR_RESET}"
  echo -e "    ${COLOR_DIM}gok registrySummary             # Show this summary${COLOR_RESET}"
  echo -e "    ${COLOR_DIM}gok reset registry              # Reset registry${COLOR_RESET}"
  echo ""
  echo -e "${COLOR_BRIGHT_GREEN}ðŸ“‹ Summary Complete${COLOR_RESET}"
  echo ""
}

# LDAP Summary function
ldapSummary() {
  local verbose_mode="${1:-false}"
  
  echo -e "${COLOR_BRIGHT_BLUE}${COLOR_BOLD}"
  echo "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—"
  echo "â•‘                           LDAP DIRECTORY SUMMARY                            â•‘"
  echo "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
  echo -e "${COLOR_RESET}"
  echo ""
  
  local cluster_accessible=false
  if kubectl cluster-info >/dev/null 2>&1; then
    cluster_accessible=true
  fi
  
  # 1. Installation Status
  log_step "1" "Installation Status"
  if [[ "$cluster_accessible" == "true" ]]; then
    if kubectl get namespace ldap >/dev/null 2>&1; then
      echo -e "  ${COLOR_GREEN}âœ“ Namespace: ldap exists${COLOR_RESET}"
      local ldap_deployment=$(kubectl get deployment ldap -n ldap --no-headers 2>/dev/null)
      if [[ -n "$ldap_deployment" ]]; then
        echo -e "  ${COLOR_GREEN}âœ“ LDAP deployment found${COLOR_RESET}"
      else
        echo -e "  ${COLOR_RED}âœ— LDAP deployment not found${COLOR_RESET}"
      fi
    else
      echo -e "  ${COLOR_RED}âœ— Namespace: ldap not found${COLOR_RESET}"
    fi
  else
    echo -e "  ${COLOR_RED}âœ— Installation status: cluster not accessible${COLOR_RESET}"
  fi
  echo ""
  
  # 2. LDAP Pods
  if [[ "$cluster_accessible" == "true" ]]; then
    log_step "2" "LDAP Pods"
    local pods=$(kubectl get pods -n ldap --no-headers 2>/dev/null)
    if [[ -n "$pods" ]]; then
      echo "$pods" | while read -r line; do
        local pod_name=$(echo "$line" | awk '{print $1}')
        local pod_status=$(echo "$line" | awk '{print $3}')
        local ready_status=$(echo "$line" | awk '{print $2}')
        if [[ "$pod_status" == "Running" ]]; then
          echo -e "    ${COLOR_GREEN}âœ“ ${pod_name}: ${pod_status} (${ready_status})${COLOR_RESET}"
        else
          echo -e "    ${COLOR_RED}âœ— ${pod_name}: ${pod_status} (${ready_status})${COLOR_RESET}"
        fi
      done
    else
      echo -e "    ${COLOR_RED}âœ— No LDAP pods found${COLOR_RESET}"
    fi
    echo ""
  fi

  # 3. Service Configuration
  if [[ "$cluster_accessible" == "true" ]]; then
    log_step "3" "Service Configuration"
    local ldap_service=$(kubectl get service ldap -n ldap --no-headers 2>/dev/null)
    if [[ -n "$ldap_service" ]]; then
      local service_type=$(kubectl get service ldap -n ldap -o jsonpath='{.spec.type}' 2>/dev/null)
      echo -e "  ${COLOR_GREEN}âœ“ Service: ldap (Type: ${service_type})${COLOR_RESET}"
      echo -e "    ${COLOR_DIM}â€¢ LDAP Port: 389${COLOR_RESET}"
      echo -e "    ${COLOR_DIM}â€¢ LDAPS Port: 636${COLOR_RESET}"
      
      # LDAP Access Information
      echo ""
      echo -e "  ${COLOR_CYAN}ðŸ”— LDAP Access:${COLOR_RESET}"
      echo -e "    ${COLOR_DIM}â€¢ Server: ldap.default.svc.cloud.uat${COLOR_RESET}"
      echo -e "    ${COLOR_DIM}â€¢ Base DN: dc=default,dc=svc,dc=cloud,dc=uat${COLOR_RESET}"
      
      # Check for LDAP ingress and get web interface URL dynamically
      local ldap_ingress=$(kubectl get ingress -n ldap --no-headers 2>/dev/null | head -1)
      if [[ -n "$ldap_ingress" ]]; then
        local ingress_name=$(echo "$ldap_ingress" | awk '{print $1}')
        local ldap_host=$(kubectl get ingress "$ingress_name" -n ldap -o jsonpath='{.spec.rules[0].host}' 2>/dev/null)
        if [[ -n "$ldap_host" ]]; then
          echo -e "    ${COLOR_DIM}â€¢ Web Interface: https://${ldap_host}/phpldapadmin/${COLOR_RESET}"
        else
          echo -e "    ${COLOR_DIM}â€¢ Web Interface: https://master.cloud.com/phpldapadmin/${COLOR_RESET}"
        fi
      else
        echo -e "    ${COLOR_DIM}â€¢ Web Interface: https://master.cloud.com/phpldapadmin/${COLOR_RESET}"
      fi
      
      echo ""
      echo -e "  ${COLOR_CYAN}ðŸ”‘ LDAP Credentials:${COLOR_RESET}"
      echo -e "    ${COLOR_DIM}â€¢ Admin User: cn=admin,dc=default,dc=svc,dc=cloud,dc=uat${COLOR_RESET}"
      echo -e "    ${COLOR_DIM}â€¢ Admin Password: sumit/admin${COLOR_RESET}"
      echo -e "    ${COLOR_DIM}â€¢ Web UI Username: sumit${COLOR_RESET}"
      echo -e "    ${COLOR_DIM}â€¢ Web UI Password: sumit${COLOR_RESET}"
      echo ""
      echo -e "  ${COLOR_CYAN}ðŸ‘¤ Sample User:${COLOR_RESET}"
      echo -e "    ${COLOR_DIM}â€¢ User: cn=smaji,ou=users,dc=default,dc=svc,dc=cloud,dc=uat${COLOR_RESET}"
    else
      echo -e "  ${COLOR_RED}âœ— LDAP service not found${COLOR_RESET}"
    fi
    echo ""
  fi
  
  # 4. Available Commands
  log_step "4" "Available Commands"
  echo -e "  ${COLOR_CYAN}LDAP Management:${COLOR_RESET}"
  echo -e "    ${COLOR_DIM}gok install ldap                # Install LDAP directory${COLOR_RESET}"
  echo -e "    ${COLOR_DIM}gok ldapSummary                 # Show this summary${COLOR_RESET}"
  echo -e "    ${COLOR_DIM}gok reset ldap                  # Reset LDAP${COLOR_RESET}"
  echo ""
  echo -e "${COLOR_BRIGHT_GREEN}ðŸ“‹ Summary Complete${COLOR_RESET}"
  echo ""
}

# Function to create permanent admin account in Keycloak
create_permanent_keycloak_admin() {
  local max_attempts=30
  local attempt=1
  local keycloak_url=""
  local admin_user=""
  local admin_password=""

  # Get Keycloak URL from ingress
  keycloak_url=$(kubectl get ingress keycloak -n keycloak -o jsonpath='{.spec.rules[0].host}' 2>/dev/null)
  if [[ -z "$keycloak_url" ]]; then
    log_warning "Could not determine Keycloak URL from ingress"
    return 1
  fi

  # Get admin credentials from secrets
  admin_user=$(kubectl get secret keycloak-secrets -n keycloak -o jsonpath='{.data.admin-user}' 2>/dev/null | base64 --decode)
  admin_password=$(kubectl get secret keycloak-secrets -n keycloak -o jsonpath='{.data.admin-password}' 2>/dev/null | base64 --decode)

  if [[ -z "$admin_user" || -z "$admin_password" ]]; then
    log_warning "Could not retrieve admin credentials from secrets"
    return 1
  fi

  # Wait for Keycloak to be fully ready (not just pods, but admin API)
  log_substep "Waiting for Keycloak admin API to be ready..."
  while [[ $attempt -le $max_attempts ]]; do
    if curl -s -k "https://${keycloak_url}/realms/master/.well-known/openid-connect-configuration" >/dev/null 2>&1; then
      log_substep "Keycloak admin API is ready"
      break
    fi

    if [[ $attempt -eq $max_attempts ]]; then
      log_warning "Keycloak admin API not ready after ${max_attempts} attempts"
      return 1
    fi

    sleep 5
    attempt=$((attempt + 1))
  done

  # Get admin token
  log_substep "Authenticating with Keycloak..."
  local token_response=$(curl -s -k -X POST "https://${keycloak_url}/realms/master/protocol/openid-connect/token" \
    -H "Content-Type: application/x-www-form-urlencoded" \
    -d "grant_type=password&client_id=admin-cli&username=${admin_user}&password=${admin_password}")

  local access_token=$(echo "$token_response" | grep -o '"access_token":"[^"]*"' | cut -d'"' -f4 2>/dev/null)

  if [[ -z "$access_token" ]]; then
    log_warning "Failed to authenticate with Keycloak admin API"
    return 1
  fi

  # Check if permanent admin user already exists
  log_substep "Checking for existing permanent admin user..."
  local existing_users=$(curl -s -k -X GET "https://${keycloak_url}/admin/realms/master/users" \
    -H "Authorization: Bearer ${access_token}" \
    -H "Content-Type: application/json")

  if echo "$existing_users" | grep -q '"username":"admin"'; then
    log_substep "Permanent admin user already exists"
    return 0
  fi

  # Create permanent admin user
  log_substep "Creating permanent admin user..."
  local create_response=$(curl -s -k -X POST "https://${keycloak_url}/admin/realms/master/users" \
    -H "Authorization: Bearer ${access_token}" \
    -H "Content-Type: application/json" \
    -d '{
      "username": "keycloak-admin",
      "enabled": true,
      "emailVerified": true,
      "firstName": "Administrator",
      "lastName": "User",
      "credentials": [{
        "type": "password",
        "value": "'${admin_password}'",
        "temporary": false
      }]
    }')

  if [[ $? -ne 0 ]]; then
    log_warning "Failed to create permanent admin user"
    return 1
  fi

  # Get the user ID of the newly created user
  sleep 2  # Wait a moment for user creation to complete
  local user_id=$(curl -s -k -X GET "https://${keycloak_url}/admin/realms/master/users?username=admin" \
    -H "Authorization: Bearer ${access_token}" \
    -H "Content-Type: application/json" | grep -o '"id":"[^"]*"' | head -1 | cut -d'"' -f4)

  if [[ -z "$user_id" ]]; then
    log_warning "Could not retrieve user ID for permanent admin user"
    return 1
  fi

  # Assign realm-admin role
  log_substep "Assigning admin role to permanent user..."
  local role_response=$(curl -s -k -X POST "https://${keycloak_url}/admin/realms/master/users/${user_id}/role-mappings/realm" \
    -H "Authorization: Bearer ${access_token}" \
    -H "Content-Type: application/json" \
    -d '[{"id": "realm-admin", "name": "realm-admin"}]')

  if [[ $? -ne 0 ]]; then
    log_warning "Failed to assign admin role to permanent user"
    return 1
  fi

  log_substep "Permanent admin account created and configured successfully"
  return 0
}

# Keycloak Summary function
keycloakSummary() {
  local verbose_mode="${1:-false}"
  
  echo -e "${COLOR_BRIGHT_BLUE}${COLOR_BOLD}"
  echo "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—"
  echo "â•‘                      KEYCLOAK IDENTITY MANAGEMENT SUMMARY                   â•‘"
  echo "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
  echo -e "${COLOR_RESET}"
  echo ""
  
  local cluster_accessible=false
  if kubectl cluster-info >/dev/null 2>&1; then
    cluster_accessible=true
  fi
  
  # 1. Installation Status
  log_step "1" "Installation Status"
  if [[ "$cluster_accessible" == "true" ]]; then
    if kubectl get namespace keycloak >/dev/null 2>&1; then
      echo -e "  ${COLOR_GREEN}âœ“ Namespace: keycloak exists${COLOR_RESET}"
      local keycloak_statefulset=$(kubectl get statefulset keycloak -n keycloak --no-headers 2>/dev/null)
      if [[ -n "$keycloak_statefulset" ]]; then
        echo -e "  ${COLOR_GREEN}âœ“ Keycloak StatefulSet found${COLOR_RESET}"
      else
        echo -e "  ${COLOR_RED}âœ— Keycloak StatefulSet not found${COLOR_RESET}"
      fi
    else
      echo -e "  ${COLOR_RED}âœ— Namespace: keycloak not found${COLOR_RESET}"
    fi
  else
    echo -e "  ${COLOR_RED}âœ— Installation status: cluster not accessible${COLOR_RESET}"
  fi
  echo ""
  
  # 2. Keycloak Pods
  if [[ "$cluster_accessible" == "true" ]]; then
    log_step "2" "Keycloak Pods"
    local pods=$(kubectl get pods -n keycloak --no-headers 2>/dev/null | grep keycloak)
    if [[ -n "$pods" ]]; then
      echo "$pods" | while read -r line; do
        local pod_name=$(echo "$line" | awk '{print $1}')
        local pod_status=$(echo "$line" | awk '{print $3}')
        local ready_status=$(echo "$line" | awk '{print $2}')
        if [[ "$pod_status" == "Running" ]]; then
          echo -e "    ${COLOR_GREEN}âœ“ ${pod_name}: ${pod_status} (${ready_status})${COLOR_RESET}"
        else
          echo -e "    ${COLOR_RED}âœ— ${pod_name}: ${pod_status} (${ready_status})${COLOR_RESET}"
        fi
      done
    else
      echo -e "    ${COLOR_RED}âœ— No Keycloak pods found${COLOR_RESET}"
    fi
    echo ""
  fi

  # 3. PostgreSQL Database
  if [[ "$cluster_accessible" == "true" ]]; then
    log_step "3" "PostgreSQL Database"
    local postgres_pods=$(kubectl get pods -n keycloak --no-headers 2>/dev/null | grep postgres)
    if [[ -n "$postgres_pods" ]]; then
      echo -e "  ${COLOR_GREEN}âœ“ PostgreSQL pods found${COLOR_RESET}"
      echo "$postgres_pods" | while read -r line; do
        local pod_name=$(echo "$line" | awk '{print $1}')
        local pod_status=$(echo "$line" | awk '{print $3}')
        local ready_status=$(echo "$line" | awk '{print $2}')
        if [[ "$pod_status" == "Running" ]]; then
          echo -e "    ${COLOR_GREEN}âœ“ ${pod_name}: ${pod_status} (${ready_status})${COLOR_RESET}"
        else
          echo -e "    ${COLOR_RED}âœ— ${pod_name}: ${pod_status} (${ready_status})${COLOR_RESET}"
        fi
      done
      
      # Check PostgreSQL service
      local postgres_service=$(kubectl get service keycloak-postgresql -n keycloak --no-headers 2>/dev/null)
      if [[ -n "$postgres_service" ]]; then
        echo -e "  ${COLOR_GREEN}âœ“ PostgreSQL service: keycloak-postgresql${COLOR_RESET}"
      fi
      
      # Check PostgreSQL PVC
      local postgres_pvc=$(kubectl get pvc postgres-pvc -n keycloak --no-headers 2>/dev/null)
      if [[ -n "$postgres_pvc" ]]; then
        echo -e "  ${COLOR_GREEN}âœ“ PostgreSQL persistent storage configured${COLOR_RESET}"
      fi
    else
      echo -e "  ${COLOR_RED}âœ— PostgreSQL pods not found${COLOR_RESET}"
    fi
    echo ""
  fi

  # 4. Service and Access
  if [[ "$cluster_accessible" == "true" ]]; then
    log_step "4" "Service and Access"
    local keycloak_service=$(kubectl get service keycloak-http -n keycloak --no-headers 2>/dev/null)
    if [[ -n "$keycloak_service" ]]; then
      local service_type=$(kubectl get service keycloak-http -n keycloak -o jsonpath='{.spec.type}' 2>/dev/null)
      local port=$(kubectl get service keycloak-http -n keycloak -o jsonpath='{.spec.ports[0].port}' 2>/dev/null)
      echo -e "  ${COLOR_GREEN}âœ“ Service: keycloak (Type: ${service_type}, Port: ${port})${COLOR_RESET}"
      
      # Check for ingress
      local ingress=$(kubectl get ingress keycloak -n keycloak --no-headers 2>/dev/null)
      if [[ -n "$ingress" ]]; then
        local hosts=$(kubectl get ingress keycloak -n keycloak -o jsonpath='{.spec.rules[*].host}' 2>/dev/null)
        echo -e "  ${COLOR_GREEN}âœ“ Ingress: ${hosts}${COLOR_RESET}"
        
        # Keycloak Access URLs - dynamically get host from ingress
        local keycloak_host=$(kubectl get ingress keycloak -n keycloak -o jsonpath='{.spec.rules[0].host}' 2>/dev/null)
        if [[ -n "$keycloak_host" ]]; then
          echo ""
          echo -e "  ${COLOR_CYAN}ðŸ”— Keycloak Access:${COLOR_RESET}"
          echo -e "    ${COLOR_DIM}â€¢ Admin Console: https://${keycloak_host}/admin${COLOR_RESET}"
          echo -e "    ${COLOR_DIM}â€¢ Account Console: https://${keycloak_host}/realms/master/account${COLOR_RESET}"
          echo ""
          echo -e "  ${COLOR_CYAN}ðŸ”‘ Keycloak Admin Credentials:${COLOR_RESET}"
          echo -e "    ${COLOR_DIM}Username: admin${COLOR_RESET}"
          echo -e "    ${COLOR_DIM}Password: Run: kubectl get secret -n keycloak keycloak-admin-password -o jsonpath='{.data.password}' | base64 -d${COLOR_RESET}"
          echo ""
          echo -e "  ${COLOR_GREEN}âœ… Permanent Admin Account:${COLOR_RESET}"
          echo -e "    ${COLOR_DIM}â€¢ Permanent admin account created automatically during installation${COLOR_RESET}"
          echo -e "    ${COLOR_DIM}â€¢ No manual setup required - login directly with admin credentials${COLOR_RESET}"
          echo -e "    ${COLOR_DIM}â€¢ Account has full realm-admin privileges${COLOR_RESET}"
        fi
      fi
    else
      echo -e "  ${COLOR_RED}âœ— Keycloak service not found${COLOR_RESET}"
    fi
    echo ""
  fi
  
  # 5. Available Commands
  log_step "5" "Available Commands"
  echo -e "  ${COLOR_CYAN}Keycloak Management:${COLOR_RESET}"
  echo -e "    ${COLOR_DIM}gok install keycloak            # Install Keycloak${COLOR_RESET}"
  echo -e "    ${COLOR_DIM}gok keycloakSummary             # Show this summary${COLOR_RESET}"
  echo -e "    ${COLOR_DIM}gok reset keycloak              # Reset Keycloak${COLOR_RESET}"
  echo ""
  echo -e "${COLOR_BRIGHT_GREEN}ðŸ“‹ Summary Complete${COLOR_RESET}"
  echo ""
}

# OAuth2 Proxy Summary function
oauth2ProxySummary() {
  local verbose_mode="${1:-false}"
  
  echo -e "${COLOR_BRIGHT_BLUE}${COLOR_BOLD}"
  echo "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—"
  echo "â•‘                        OAUTH2 PROXY SUMMARY                                 â•‘"
  echo "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
  echo -e "${COLOR_RESET}"
  echo ""
  
  local cluster_accessible=false
  if kubectl cluster-info >/dev/null 2>&1; then
    cluster_accessible=true
  fi
  
  # 1. Installation Status
  log_step "1" "Installation Status"
  if [[ "$cluster_accessible" == "true" ]]; then
    if kubectl get namespace oauth2 >/dev/null 2>&1; then
      echo -e "  ${COLOR_GREEN}âœ“ Namespace: oauth2 exists${COLOR_RESET}"
      local oauth2_deployment=$(kubectl get deployment oauth2proxy -n oauth2 --no-headers 2>/dev/null)
      if [[ -n "$oauth2_deployment" ]]; then
        echo -e "  ${COLOR_GREEN}âœ“ OAuth2 Proxy deployment found${COLOR_RESET}"
      else
        echo -e "  ${COLOR_RED}âœ— OAuth2 Proxy deployment not found${COLOR_RESET}"
      fi
    else
      echo -e "  ${COLOR_RED}âœ— Namespace: oauth2 not found${COLOR_RESET}"
    fi
  else
    echo -e "  ${COLOR_RED}âœ— Installation status: cluster not accessible${COLOR_RESET}"
  fi
  echo ""
  
  # 2. OAuth2 Proxy Pods
  if [[ "$cluster_accessible" == "true" ]]; then
    log_step "2" "OAuth2 Proxy Pods"
    local pods=$(kubectl get pods -n oauth2 --no-headers 2>/dev/null)
    if [[ -n "$pods" ]]; then
      echo "$pods" | while read -r line; do
        local pod_name=$(echo "$line" | awk '{print $1}')
        local pod_status=$(echo "$line" | awk '{print $3}')
        local ready_status=$(echo "$line" | awk '{print $2}')
        if [[ "$pod_status" == "Running" ]]; then
          echo -e "    ${COLOR_GREEN}âœ“ ${pod_name}: ${pod_status} (${ready_status})${COLOR_RESET}"
        else
          echo -e "    ${COLOR_RED}âœ— ${pod_name}: ${pod_status} (${ready_status})${COLOR_RESET}"
        fi
      done
    else
      echo -e "    ${COLOR_RED}âœ— No OAuth2 Proxy pods found${COLOR_RESET}"
    fi
    echo ""
  fi

  # 3. Service Configuration
  if [[ "$cluster_accessible" == "true" ]]; then
    log_step "3" "Service Configuration"
    local oauth2_service=$(kubectl get service oauth2proxy -n oauth2 --no-headers 2>/dev/null)
    if [[ -n "$oauth2_service" ]]; then
      local service_type=$(kubectl get service oauth2proxy -n oauth2 -o jsonpath='{.spec.type}' 2>/dev/null)
      local port=$(kubectl get service oauth2proxy -n oauth2 -o jsonpath='{.spec.ports[0].port}' 2>/dev/null)
      echo -e "  ${COLOR_GREEN}âœ“ Service: oauth2proxy (Type: ${service_type}, Port: ${port})${COLOR_RESET}"
    else
      echo -e "  ${COLOR_RED}âœ— OAuth2 Proxy service not found${COLOR_RESET}"
    fi
    echo ""
  fi
  
  # 4. Available Commands
  log_step "4" "Available Commands"
  echo -e "  ${COLOR_CYAN}OAuth2 Proxy Management:${COLOR_RESET}"
  echo -e "    ${COLOR_DIM}gok install oauth2              # Install OAuth2 Proxy${COLOR_RESET}"
  echo -e "    ${COLOR_DIM}gok oauth2ProxySummary          # Show this summary${COLOR_RESET}"
  echo -e "    ${COLOR_DIM}gok oauth2ProxyReset            # Reset OAuth2 Proxy${COLOR_RESET}"
  echo ""
  echo -e "${COLOR_BRIGHT_GREEN}ðŸ“‹ Summary Complete${COLOR_RESET}"
  echo ""
}

# RabbitMQ Summary function
rabbitmqSummary() {
  local verbose_mode="${1:-false}"
  
  echo -e "${COLOR_BRIGHT_BLUE}${COLOR_BOLD}"
  echo "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—"
  echo "â•‘                         RABBITMQ MESSAGE BROKER SUMMARY                     â•‘"
  echo "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
  echo -e "${COLOR_RESET}"
  echo ""
  
  local cluster_accessible=false
  if kubectl cluster-info >/dev/null 2>&1; then
    cluster_accessible=true
  fi
  
  # 1. Installation Status
  log_step "1" "Installation Status"
  if [[ "$cluster_accessible" == "true" ]]; then
    if kubectl get namespace rabbitmq >/dev/null 2>&1; then
      echo -e "  ${COLOR_GREEN}âœ“ Namespace: rabbitmq exists${COLOR_RESET}"
      local rabbitmq_statefulset=$(kubectl get statefulset rabbitmq -n rabbitmq --no-headers 2>/dev/null)
      if [[ -n "$rabbitmq_statefulset" ]]; then
        echo -e "  ${COLOR_GREEN}âœ“ RabbitMQ StatefulSet found${COLOR_RESET}"
      else
        echo -e "  ${COLOR_RED}âœ— RabbitMQ StatefulSet not found${COLOR_RESET}"
      fi
    else
      echo -e "  ${COLOR_RED}âœ— Namespace: rabbitmq not found${COLOR_RESET}"
    fi
  else
    echo -e "  ${COLOR_RED}âœ— Installation status: cluster not accessible${COLOR_RESET}"
  fi
  echo ""
  
  # 2. RabbitMQ Pods
  if [[ "$cluster_accessible" == "true" ]]; then
    log_step "2" "RabbitMQ Pods"
    local pods=$(kubectl get pods -n rabbitmq --no-headers 2>/dev/null)
    if [[ -n "$pods" ]]; then
      echo "$pods" | while read -r line; do
        local pod_name=$(echo "$line" | awk '{print $1}')
        local pod_status=$(echo "$line" | awk '{print $3}')
        local ready_status=$(echo "$line" | awk '{print $2}')
        if [[ "$pod_status" == "Running" ]]; then
          echo -e "    ${COLOR_GREEN}âœ“ ${pod_name}: ${pod_status} (${ready_status})${COLOR_RESET}"
        else
          echo -e "    ${COLOR_RED}âœ— ${pod_name}: ${pod_status} (${ready_status})${COLOR_RESET}"
        fi
      done
    else
      echo -e "    ${COLOR_RED}âœ— No RabbitMQ pods found${COLOR_RESET}"
    fi
    echo ""
  fi

  # 3. Service Configuration
  if [[ "$cluster_accessible" == "true" ]]; then
    log_step "3" "Service Configuration"
    local rabbitmq_service=$(kubectl get service rabbitmq -n rabbitmq --no-headers 2>/dev/null)
    if [[ -n "$rabbitmq_service" ]]; then
      local service_type=$(kubectl get service rabbitmq -n rabbitmq -o jsonpath='{.spec.type}' 2>/dev/null)
      echo -e "  ${COLOR_GREEN}âœ“ Service: rabbitmq (Type: ${service_type})${COLOR_RESET}"
      echo -e "    ${COLOR_DIM}â€¢ AMQP Port: 5672${COLOR_RESET}"
      echo -e "    ${COLOR_DIM}â€¢ Management Port: 15672${COLOR_RESET}"
    else
      echo -e "  ${COLOR_RED}âœ— RabbitMQ service not found${COLOR_RESET}"
    fi
    echo ""
  fi
  
  # 4. Available Commands
  log_step "4" "Available Commands"
  echo -e "  ${COLOR_CYAN}RabbitMQ Management:${COLOR_RESET}"
  echo -e "    ${COLOR_DIM}gok install rabbitmq            # Install RabbitMQ${COLOR_RESET}"
  echo -e "    ${COLOR_DIM}gok rabbitmqSummary             # Show this summary${COLOR_RESET}"
  echo -e "    ${COLOR_DIM}gok reset rabbitmq              # Reset RabbitMQ${COLOR_RESET}"
  echo ""
  echo -e "${COLOR_BRIGHT_GREEN}ðŸ“‹ Summary Complete${COLOR_RESET}"
  echo ""
}

# Vault Summary function
vaultSummary() {
  local verbose_mode="${1:-false}"
  
  echo -e "${COLOR_BRIGHT_BLUE}${COLOR_BOLD}"
  echo "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—"
  echo "â•‘                         HASHICORP VAULT SUMMARY                             â•‘"
  echo "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
  echo -e "${COLOR_RESET}"
  echo ""
  
  local cluster_accessible=false
  if kubectl cluster-info >/dev/null 2>&1; then
    cluster_accessible=true
  fi
  
  # 1. Installation Status
  log_step "1" "Installation Status"
  if [[ "$cluster_accessible" == "true" ]]; then
    if kubectl get namespace vault >/dev/null 2>&1; then
      echo -e "  ${COLOR_GREEN}âœ“ Namespace: vault exists${COLOR_RESET}"
      local vault_statefulset=$(kubectl get statefulset vault -n vault --no-headers 2>/dev/null)
      if [[ -n "$vault_statefulset" ]]; then
        echo -e "  ${COLOR_GREEN}âœ“ Vault StatefulSet found${COLOR_RESET}"
      else
        echo -e "  ${COLOR_RED}âœ— Vault StatefulSet not found${COLOR_RESET}"
      fi
    else
      echo -e "  ${COLOR_RED}âœ— Namespace: vault not found${COLOR_RESET}"
    fi
  else
    echo -e "  ${COLOR_RED}âœ— Installation status: cluster not accessible${COLOR_RESET}"
  fi
  echo ""
  
  # 2. Vault Pods
  if [[ "$cluster_accessible" == "true" ]]; then
    log_step "2" "Vault Pods"
    local pods=$(kubectl get pods -n vault --no-headers 2>/dev/null)
    if [[ -n "$pods" ]]; then
      echo "$pods" | while read -r line; do
        local pod_name=$(echo "$line" | awk '{print $1}')
        local pod_status=$(echo "$line" | awk '{print $3}')
        local ready_status=$(echo "$line" | awk '{print $2}')
        if [[ "$pod_status" == "Running" ]]; then
          echo -e "    ${COLOR_GREEN}âœ“ ${pod_name}: ${pod_status} (${ready_status})${COLOR_RESET}"
        else
          echo -e "    ${COLOR_RED}âœ— ${pod_name}: ${pod_status} (${ready_status})${COLOR_RESET}"
        fi
      done
    else
      echo -e "    ${COLOR_RED}âœ— No Vault pods found${COLOR_RESET}"
    fi
    echo ""
  fi

  # 3. Vault Status
  if [[ "$cluster_accessible" == "true" ]]; then
    log_step "3" "Vault Configuration"
    local vault_service=$(kubectl get service vault -n vault --no-headers 2>/dev/null)
    if [[ -n "$vault_service" ]]; then
      local service_type=$(kubectl get service vault -n vault -o jsonpath='{.spec.type}' 2>/dev/null)
      echo -e "  ${COLOR_GREEN}âœ“ Service: vault (Type: ${service_type})${COLOR_RESET}"
      echo -e "    ${COLOR_DIM}â€¢ API Port: 8200${COLOR_RESET}"
      echo -e "    ${COLOR_DIM}â€¢ Cluster Port: 8201${COLOR_RESET}"
    else
      echo -e "  ${COLOR_RED}âœ— Vault service not found${COLOR_RESET}"
    fi
    echo ""
  fi
  
  # 4. Available Commands
  log_step "4" "Available Commands"
  echo -e "  ${COLOR_CYAN}Vault Management:${COLOR_RESET}"
  echo -e "    ${COLOR_DIM}gok install vault               # Install HashiCorp Vault${COLOR_RESET}"
  echo -e "    ${COLOR_DIM}gok vaultSummary                # Show this summary${COLOR_RESET}"
  echo -e "    ${COLOR_DIM}gok vaultReset                  # Reset Vault${COLOR_RESET}"
  echo ""
  echo -e "${COLOR_BRIGHT_GREEN}ðŸ“‹ Summary Complete${COLOR_RESET}"
  echo ""
}

# Comprehensive NGINX Ingress Controller reset function
ingressReset() {
  log_info "Resetting NGINX Ingress Controller..."
  
  # Check if cluster is accessible
  if ! kubectl cluster-info >/dev/null 2>&1; then
    log_warning "Cluster not accessible - cannot perform ingress reset"
    log_info "Start your Kubernetes cluster first: gok start kubernetes"
    return 1
  fi
  
  log_step "1" "Checking ingress controller installation"
  
  # Check if ingress-nginx namespace exists
  if ! kubectl get namespace ingress-nginx >/dev/null 2>&1; then
    log_info "NGINX Ingress Controller namespace not found - nothing to reset"
    return 0
  fi
  
  log_success "Found ingress-nginx namespace"
  
  # Show what will be reset
  log_step "2" "Components to be reset"
  
  echo -e "  ${COLOR_YELLOW}ðŸ“¦ Helm Release:${COLOR_RESET}"
  local helm_status=$(helm list -n ingress-nginx --output json 2>/dev/null | jq -r '.[] | select(.name=="ingress-nginx") | .status' 2>/dev/null || echo "not-found")
  if [[ "$helm_status" != "not-found" ]]; then
    echo -e "    â€¢ ingress-nginx (status: $helm_status)"
  else
    echo -e "    â€¢ No Helm release found"
  fi
  
  echo -e "  ${COLOR_YELLOW}ðŸ—‚ï¸ Namespace:${COLOR_RESET}"
  echo -e "    â€¢ ingress-nginx (all resources will be deleted)"
  
  echo -e "  ${COLOR_YELLOW}ðŸ”— Ingress Resources:${COLOR_RESET}"
  local ingress_count=$(kubectl get ingress --all-namespaces --no-headers 2>/dev/null | wc -l || echo "0")
  if [[ "$ingress_count" -gt 0 ]]; then
    echo -e "    â€¢ $ingress_count ingress resource(s) will be orphaned"
    echo -e "    â€¢ ${COLOR_DIM}Note: Existing ingress resources will remain but lose their controller${COLOR_RESET}"
  else
    echo -e "    â€¢ No active ingress resources found"
  fi
  
  echo -e "  ${COLOR_YELLOW}ðŸ“‹ Configuration:${COLOR_RESET}"
  if kubectl get ingressclass nginx >/dev/null 2>&1; then
    echo -e "    â€¢ IngressClass 'nginx' will be removed"
  fi
  if kubectl get ingressclass ingress-nginx >/dev/null 2>&1; then
    echo -e "    â€¢ IngressClass 'ingress-nginx' will be removed"
  fi
  
  echo ""
  
  # Confirmation prompt
  log_warning "This will permanently remove the NGINX Ingress Controller"
  echo -e "${COLOR_RED}âš ï¸  Impact:${COLOR_RESET}"
  echo -e "  â€¢ All external traffic routing will stop"
  echo -e "  â€¢ Existing ingress resources will become inactive"
  echo -e "  â€¢ SSL/TLS termination will be unavailable"
  echo -e "  â€¢ LoadBalancer and NodePort services will still work"
  echo ""
  
  read -p "Are you sure you want to reset NGINX Ingress Controller? (y/N): " -r confirm
  if [[ ! "$confirm" =~ ^[Yy]$ ]]; then
    log_info "Reset cancelled by user"
    return 0
  fi
  
  echo ""
  log_step "3" "Removing ingress controller components"
  
  # Step 1: Uninstall Helm release
  log_substep "Uninstalling Helm release"
  if helm list -n ingress-nginx --output json 2>/dev/null | jq -e '.[] | select(.name=="ingress-nginx")' >/dev/null; then
    if helm uninstall ingress-nginx -n ingress-nginx >/dev/null 2>&1; then
      log_success "Helm release uninstalled successfully"
    else
      log_warning "Failed to uninstall Helm release (may not exist)"
    fi
  else
    log_info "No Helm release found to uninstall"
  fi
  
  # Step 2: Clean up any remaining resources
  log_substep "Cleaning up remaining resources"
  
  # Delete any remaining pods forcefully
  local remaining_pods=$(kubectl get pods -n ingress-nginx --no-headers 2>/dev/null | wc -l || echo "0")
  if [[ "$remaining_pods" -gt 0 ]]; then
    log_info "Removing $remaining_pods remaining pod(s)"
    kubectl delete pods --all -n ingress-nginx --force --grace-period=0 >/dev/null 2>&1
  fi
  
  # Delete any remaining services
  local remaining_services=$(kubectl get services -n ingress-nginx --no-headers 2>/dev/null | wc -l || echo "0")
  if [[ "$remaining_services" -gt 0 ]]; then
    log_info "Removing $remaining_services remaining service(s)"
    kubectl delete services --all -n ingress-nginx >/dev/null 2>&1
  fi
  
  # Delete any remaining deployments
  local remaining_deployments=$(kubectl get deployments -n ingress-nginx --no-headers 2>/dev/null | wc -l || echo "0")
  if [[ "$remaining_deployments" -gt 0 ]]; then
    log_info "Removing $remaining_deployments remaining deployment(s)"
    kubectl delete deployments --all -n ingress-nginx >/dev/null 2>&1
  fi
  
  # Delete any remaining configmaps
  local remaining_configmaps=$(kubectl get configmaps -n ingress-nginx --no-headers 2>/dev/null | wc -l || echo "0")
  if [[ "$remaining_configmaps" -gt 0 ]]; then
    log_info "Removing $remaining_configmaps remaining configmap(s)"
    kubectl delete configmaps --all -n ingress-nginx >/dev/null 2>&1
  fi
  
  # Delete any remaining secrets
  local remaining_secrets=$(kubectl get secrets -n ingress-nginx --no-headers 2>/dev/null | grep -v default-token | wc -l || echo "0")
  if [[ "$remaining_secrets" -gt 0 ]]; then
    log_info "Removing $remaining_secrets remaining secret(s)"
    kubectl delete secrets --all -n ingress-nginx >/dev/null 2>&1
  fi
  
  log_success "Remaining resources cleaned up"
  
  # Step 3: Delete IngressClasses
  log_substep "Removing IngressClasses"
  
  local ingress_classes=("nginx" "ingress-nginx")
  for class_name in "${ingress_classes[@]}"; do
    if kubectl get ingressclass "$class_name" >/dev/null 2>&1; then
      if kubectl delete ingressclass "$class_name" >/dev/null 2>&1; then
        log_info "IngressClass '$class_name' deleted"
      else
        log_warning "Failed to delete IngressClass '$class_name'"
      fi
    fi
  done
  
  # Step 4: Delete namespace
  log_substep "Removing namespace"
  if kubectl delete namespace ingress-nginx >/dev/null 2>&1; then
    log_success "Namespace 'ingress-nginx' deleted successfully"
  else
    log_warning "Failed to delete namespace (may not exist)"
  fi
  
  # Step 5: Wait for namespace to be fully deleted
  log_substep "Waiting for namespace cleanup"
  local timeout=30
  local elapsed=0
  while kubectl get namespace ingress-nginx >/dev/null 2>&1 && [[ $elapsed -lt $timeout ]]; do
    sleep 2
    elapsed=$((elapsed + 2))
    if [[ $((elapsed % 10)) -eq 0 ]]; then
      log_info "Still waiting for namespace cleanup... (${elapsed}s/${timeout}s)"
    fi
  done
  
  if kubectl get namespace ingress-nginx >/dev/null 2>&1; then
    log_warning "Namespace still exists after ${timeout}s - may need manual cleanup"
  else
    log_success "Namespace cleanup completed"
  fi
  
  echo ""
  log_step "4" "Reset Summary"
  
  # Check what's left
  local orphaned_ingress=$(kubectl get ingress --all-namespaces --no-headers 2>/dev/null | wc -l || echo "0")
  
  echo -e "  ${COLOR_GREEN}âœ“ NGINX Ingress Controller uninstalled${COLOR_RESET}"
  echo -e "  ${COLOR_GREEN}âœ“ Helm release removed${COLOR_RESET}"
  echo -e "  ${COLOR_GREEN}âœ“ Namespace deleted${COLOR_RESET}"
  echo -e "  ${COLOR_GREEN}âœ“ IngressClasses removed${COLOR_RESET}"
  
  if [[ "$orphaned_ingress" -gt 0 ]]; then
    echo -e "  ${COLOR_YELLOW}âš  $orphaned_ingress orphaned ingress resource(s) remain${COLOR_RESET}"
    echo -e "    ${COLOR_DIM}These resources are inactive without a controller${COLOR_RESET}"
  else
    echo -e "  ${COLOR_GREEN}âœ“ No orphaned ingress resources${COLOR_RESET}"
  fi
  
  echo ""
  log_success "NGINX Ingress Controller reset completed successfully!"
  
  # Show next steps
  echo ""
  log_step "5" "Next Steps"
  
  echo -e "  ${COLOR_CYAN}ðŸ“¦ Reinstall Ingress Controller:${COLOR_RESET}"
  echo -e "    ${COLOR_DIM}gok install ingress                     # Install NGINX Ingress Controller${COLOR_RESET}"
  
  echo -e "  ${COLOR_CYAN}ðŸ” Check Remaining Resources:${COLOR_RESET}"
  echo -e "    ${COLOR_DIM}kubectl get ingress --all-namespaces    # List orphaned ingress resources${COLOR_RESET}"
  echo -e "    ${COLOR_DIM}kubectl get ingressclass                # Verify IngressClasses removed${COLOR_RESET}"
  
  if [[ "$orphaned_ingress" -gt 0 ]]; then
    echo -e "  ${COLOR_CYAN}ðŸ—‘ï¸ Clean Up Orphaned Resources:${COLOR_RESET}"
    echo -e "    ${COLOR_DIM}kubectl delete ingress <name> -n <namespace>  # Remove individual ingress${COLOR_RESET}"
    echo -e "    ${COLOR_DIM}kubectl get ingress --all-namespaces          # List all ingress resources${COLOR_RESET}"
  fi
  
  echo -e "  ${COLOR_CYAN}ðŸ”„ Alternative Solutions:${COLOR_RESET}"
  echo -e "    ${COLOR_DIM}gok install istio                       # Install Istio service mesh${COLOR_RESET}"
  echo -e "    ${COLOR_DIM}# Use LoadBalancer services directly     # Direct service exposure${COLOR_RESET}"
  
  echo ""
  echo -e "${COLOR_BRIGHT_GREEN}ðŸŽ‰ Ingress reset complete!${COLOR_RESET} Your cluster is ready for a fresh ingress installation."
  echo ""
}

resetChart(){
  log_component_start "chartmuseum-reset" "Removing ChartMuseum installation"
  
  log_step "1" "Uninstalling ChartMuseum Helm release"
  if helm_uninstall_with_summary "chartmuseum" "chartmuseum" chartmuseum --namespace chartmuseum; then
    log_success "ChartMuseum Helm release removed"
  fi
  
  log_step "2" "Cleaning up storage"
  emptyLocalFsStorage "ChartMuseum" "chart-pv" "chart-storage" "/data/volumes/chart-storage"
  
  log_step "3" "Removing namespace"
  if kubectl_with_summary delete "namespace" chartmuseum; then
    show_installation_summary "chartmuseum" "chartmuseum" "Helm chart repository removed"
    log_component_success "chartmuseum-reset" "ChartMuseum has been reset successfully"
  else
    log_error "Failed to remove ChartMuseum namespace"
    return 1
  fi
}

baseInst(){
  local start_time=$(date +%s)
  
  log_component_start "base-install" "Installing base platform services and infrastructure"
  
  log_step "1" "Updating system packages with smart caching"
  if ! updateSys; then
    log_error "Failed to update system packages"
    return 1
  fi
  
  log_step "2" "Installing dependencies with smart caching"
  if ! installDeps; then
    log_error "Failed to install base dependencies"
    return 1
  fi
  
  log_step "3" "Preparing base installation directory"
  local base_dir="$MOUNT_PATH/kubernetes/install_k8s/base"
  if [[ ! -d "$base_dir" ]]; then
    log_error "Base installation directory not found: $base_dir"
    return 1
  fi
  
  if execute_with_suppression pushd "$base_dir"; then
    log_success "Base installation directory prepared"
  else
    log_error "Failed to access base installation directory"
    return 1
  fi
  
  log_step "4" "Setting executable permissions for base scripts"
  if execute_with_suppression find . -type f -name "*.sh" -exec chmod +x {} \;; then
    log_success "Executable permissions set for base scripts"
  else
    log_error "Failed to set executable permissions"
    popd || true
    return 1
  fi
  
  log_step "5" "Building and installing base platform components"
  
  # Enhanced build with detailed progress tracking and caching integration
  build_base_platform_with_progress
  
  log_step "6" "Validating base platform installation"
  if validate_base_installation; then
    log_success "Base platform installation validation completed"
  else
    log_warning "Base validation had issues but installation may still work"
  fi

  log_step "7" "Completing base installation cleanup"
  if execute_with_suppression popd; then
    log_success "Base installation directory cleanup completed"
  else
    log_warning "Directory cleanup had issues but installation completed"
  fi
  
  # Create a marker to indicate base is installed with additional metadata
  if kubectl get configmap base-config -n kube-system >/dev/null 2>&1; then
    log_info "Updating existing base installation marker"
    execute_with_suppression kubectl patch configmap base-config -n kube-system --patch "{\"data\":{\"updated\":\"$(date)\",\"version\":\"enhanced\"}}"
  else
    if execute_with_suppression kubectl create configmap base-config \
      --from-literal=installed="$(date)" \
      --from-literal=version="enhanced" \
      --from-literal=caching-enabled="true" \
      -n kube-system; then
      log_success "Base installation marker created with metadata"
    else
      log_warning "Failed to create base installation marker (but installation completed successfully)"
    fi
  fi

  local end_time=$(date +%s)
  local duration=$((end_time - start_time))
  
  show_installation_summary "base" "base" "Base platform services and infrastructure"
  log_component_success "base-install" "Base platform services installed successfully"
  log_success "Base platform installation completed in ${duration}s"
  
  # Show base-specific next steps and recommendations
  show_base_next_steps
}

# Enhanced base platform build with detailed progress tracking
build_base_platform_with_progress() {
  local start_time=$(date +%s)
  
  # Source configuration to get image details
  if [[ ! -f "configuration" ]]; then
    log_error "Configuration file not found in base directory"
    return 1
  fi
  
  source configuration
  source "$MOUNT_PATH/kubernetes/install_k8s/util" 2>/dev/null || true
  
  # Get registry information
  local registry_url=$(fullRegistryUrl 2>/dev/null || echo "localhost:5000")
  local image_name="${IMAGE_NAME:-gok-base}"
  local repo_name="${REPO_NAME:-gok-base}"
  local full_image_url="${registry_url}/${repo_name}"
  
  log_substep "Registry: ${COLOR_CYAN}${registry_url}${COLOR_RESET}"
  log_substep "Image: ${COLOR_CYAN}${image_name}${COLOR_RESET}"
  log_substep "Target: ${COLOR_CYAN}${full_image_url}${COLOR_RESET}"
  
  # Step 1: Docker Build with Enhanced Progress
  log_info "ðŸ³ Building base platform Docker image: ${COLOR_BOLD}${image_name}${COLOR_RESET}"
  
  local temp_build_log=$(mktemp)
  local temp_build_error=$(mktemp)
  
  # Start Docker build in background with enhanced arguments
  docker build \
    --build-arg REGISTRY="$registry_url" \
    --build-arg BUILD_DATE="$(date -u +'%Y-%m-%dT%H:%M:%SZ')" \
    --build-arg VERSION="enhanced" \
    -t "$image_name" . >"$temp_build_log" 2>"$temp_build_error" &
  local build_pid=$!
  
  # Show build progress with base-specific stages
  local build_progress=0
  local build_steps=10
  local spinner_chars="|/-\\"
  local spinner_idx=0
  local build_stage="Initializing"
  
  while kill -0 $build_pid 2>/dev/null; do
    local char=${spinner_chars:spinner_idx:1}
    build_progress=$(( (build_progress + 1) % (build_steps * 8) ))
    local progress_percent=$(( build_progress * 100 / (build_steps * 8) ))
    
    # Update stage based on progress
    case $((build_progress / 8)) in
      0) build_stage="Preparing Ubuntu base image" ;;
      1) build_stage="Installing system utilities" ;;
      2) build_stage="Setting up SSH server" ;;
      3) build_stage="Installing network tools" ;;
      4) build_stage="Adding Python runtime" ;;
      5) build_stage="Installing curl and wget" ;;
      6) build_stage="Copying platform scripts" ;;
      7) build_stage="Setting up permissions" ;;
      8) build_stage="Configuring volumes" ;;
      9) build_stage="Finalizing base image" ;;
    esac
    
    printf "\r${COLOR_BLUE}  Building base image [%c] ${COLOR_CYAN}%d%%${COLOR_RESET} - ${COLOR_DIM}%s${COLOR_RESET}" "$char" "$progress_percent" "$build_stage"
    
    spinner_idx=$(( (spinner_idx + 1) % 4 ))
    sleep 0.4
  done
  
  wait $build_pid
  local build_exit_code=$?
  
  if [[ $build_exit_code -eq 0 ]]; then
    printf "\r${COLOR_GREEN}  âœ“ Base platform Docker build completed [100%%]${COLOR_RESET}\n"
    log_success "Base platform Docker image built successfully: ${image_name}"
    
    # Show warnings if present but don't fail
    if is_verbose_mode && [[ -s "$temp_build_log" ]]; then
      if grep -q "warning" "$temp_build_log"; then
        log_info "Build warnings (non-critical):"
        grep -i "warning" "$temp_build_log" | head -5 | while read line; do
          log_warning "  $line"
        done
      fi
    fi
  else
    printf "\r${COLOR_RED}  âœ— Base platform Docker build failed${COLOR_RESET}\n"
    log_error "Base platform Docker build failed - error details:"
    if [[ -s "$temp_build_error" ]]; then
      cat "$temp_build_error" >&2
    fi
    rm -f "$temp_build_log" "$temp_build_error"
    return 1
  fi
  
  # Step 2: Docker Tag
  log_info "ðŸ·ï¸  Tagging image for registry: ${COLOR_BOLD}${full_image_url}${COLOR_RESET}"
  if docker tag "$image_name" "$full_image_url" >/dev/null 2>&1; then
    log_success "Image tagged successfully"
  else
    log_error "Failed to tag Docker image"
    rm -f "$temp_build_log" "$temp_build_error"
    return 1
  fi
  
  # Step 3: Docker Push with Enhanced Progress
  log_info "ðŸ“¤ Pushing base platform image to registry: ${COLOR_BOLD}${registry_url}${COLOR_RESET}"
  log_substep "Target repository: ${COLOR_CYAN}${repo_name}${COLOR_RESET}"
  
  # Start Docker push in background
  docker push "$full_image_url" >"$temp_build_log" 2>"$temp_build_error" &
  local push_pid=$!
  
  # Show enhanced push progress with base-specific stages
  local push_progress=0
  local push_steps=8
  local push_stage="Preparing"
  
  while kill -0 $push_pid 2>/dev/null; do
    local char=${spinner_chars:spinner_idx:1}
    push_progress=$(( (push_progress + 1) % (push_steps * 12) ))
    local progress_percent=$(( push_progress * 100 / (push_steps * 12) ))
    
    # Update stage based on progress
    case $((push_progress / 12)) in
      0) push_stage="Preparing base layers" ;;
      1) push_stage="Uploading Ubuntu base" ;;
      2) push_stage="Uploading system tools" ;;
      3) push_stage="Uploading Python runtime" ;;
      4) push_stage="Uploading platform scripts" ;;
      5) push_stage="Uploading configurations" ;;
      6) push_stage="Uploading final layers" ;;
      7) push_stage="Finalizing push" ;;
    esac
    
    printf "\r${COLOR_MAGENTA}  Pushing base to registry [%c] ${COLOR_CYAN}%d%%${COLOR_RESET} - ${COLOR_DIM}%s${COLOR_RESET}" "$char" "$progress_percent" "$push_stage"
    
    spinner_idx=$(( (spinner_idx + 1) % 4 ))
    sleep 0.4
  done
  
  wait $push_pid
  local push_exit_code=$?
  
  if [[ $push_exit_code -eq 0 ]]; then
    printf "\r${COLOR_GREEN}  âœ“ Base platform push completed [100%%] - Image available at ${COLOR_BOLD}${full_image_url}${COLOR_RESET}\n"
    log_success "Base platform image pushed successfully to registry"
  else
    printf "\r${COLOR_RED}  âœ— Base platform push failed${COLOR_RESET}\n"
    log_error "Base platform Docker push failed - error details:"
    if [[ -s "$temp_build_error" ]]; then
      if is_verbose_mode; then
        cat "$temp_build_error" >&2
      else
        tail -10 "$temp_build_error" >&2
        log_info "Use --verbose flag to see full push logs"
      fi
    fi
    rm -f "$temp_build_log" "$temp_build_error"
    return 1
  fi
  
  # Clean up temporary files
  rm -f "$temp_build_log" "$temp_build_error"
  
  # Build completion summary
  local end_time=$(date +%s)
  local duration=$((end_time - start_time))
  log_success "Base platform build completed in ${duration}s"
  
  # Show Dockerfile summary
  show_dockerfile_summary
  
  return 0
}

# Show comprehensive Dockerfile summary after build completion
show_dockerfile_summary() {
  log_info "ðŸ“‹ Base Platform Dockerfile Summary"
  echo
  
  echo -e "${COLOR_BRIGHT_BLUE}${COLOR_BOLD}ðŸ³ Base Platform Container Details${COLOR_RESET}"
  echo -e "${COLOR_CYAN}â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”${COLOR_RESET}"
  
  echo -e "${COLOR_YELLOW}${COLOR_BOLD}Base Image:${COLOR_RESET}"
  echo -e "  ðŸ“¦ ${COLOR_GREEN}ubuntu:trusty${COLOR_RESET} - Ubuntu 14.04 LTS (Trusty Tahr)"
  echo -e "     ${COLOR_DIM}â€¢ Stable, long-term support base system${COLOR_RESET}"
  echo -e "     ${COLOR_DIM}â€¢ Optimized for containerized environments${COLOR_RESET}"
  echo
  
  echo -e "${COLOR_YELLOW}${COLOR_BOLD}Installed System Tools:${COLOR_RESET}"
  echo -e "  ðŸ”§ ${COLOR_GREEN}openssh-server & openssh-client${COLOR_RESET} - SSH connectivity and remote access"
  echo -e "  ðŸŒ ${COLOR_GREEN}net-tools${COLOR_RESET} - Network utilities (ifconfig, netstat, route)"
  echo -e "  ðŸ“¡ ${COLOR_GREEN}iputils-ping${COLOR_RESET} - Network connectivity testing (ping command)"
  echo -e "  ðŸ“¥ ${COLOR_GREEN}curl${COLOR_RESET} - HTTP/HTTPS data transfer and API communication"
  echo -e "  ðŸ“¥ ${COLOR_GREEN}wget${COLOR_RESET} - File downloading and web content retrieval"
  echo -e "  ðŸ ${COLOR_GREEN}python${COLOR_RESET} - Python runtime for scripting and automation"
  echo
  
  echo -e "${COLOR_YELLOW}${COLOR_BOLD}Container Structure:${COLOR_RESET}"
  echo -e "  ðŸ“ ${COLOR_CYAN}/container/scripts/${COLOR_RESET} - Platform initialization and setup scripts"
  echo -e "  ðŸ“ ${COLOR_CYAN}/usr/local/repository${COLOR_RESET} - Volume mount for persistent data"
  echo -e "     ${COLOR_DIM}â€¢ Houses GOK platform configuration and state${COLOR_RESET}"
  echo -e "     ${COLOR_DIM}â€¢ Shared across container lifecycle${COLOR_RESET}"
  echo
  
  echo -e "${COLOR_YELLOW}${COLOR_BOLD}Container Features:${COLOR_RESET}"
  echo -e "  âœ… ${COLOR_GREEN}Minimal footprint${COLOR_RESET} - Optimized package selection with cleanup"
  echo -e "  âœ… ${COLOR_GREEN}Network-ready${COLOR_RESET} - Full networking stack for service communication"
  echo -e "  âœ… ${COLOR_GREEN}SSH-enabled${COLOR_RESET} - Remote access and management capabilities"
  echo -e "  âœ… ${COLOR_GREEN}Script automation${COLOR_RESET} - Custom setup and initialization scripts"
  echo -e "  âœ… ${COLOR_GREEN}Persistent storage${COLOR_RESET} - Volume support for data persistence"
  echo
  
  echo -e "${COLOR_YELLOW}${COLOR_BOLD}Use Cases:${COLOR_RESET}"
  echo -e "  ðŸŽ¯ ${COLOR_MAGENTA}Base platform services${COLOR_RESET} - Foundation for GOK platform components"
  echo -e "  ðŸŽ¯ ${COLOR_MAGENTA}Development environment${COLOR_RESET} - Containerized development workspace"
  echo -e "  ðŸŽ¯ ${COLOR_MAGENTA}CI/CD pipeline support${COLOR_RESET} - Build and deployment automation"
  echo -e "  ðŸŽ¯ ${COLOR_MAGENTA}Microservice foundation${COLOR_RESET} - Starting point for custom services"
  echo
  
  echo -e "${COLOR_BRIGHT_GREEN}${COLOR_BOLD}ðŸ’¡ Container Benefits:${COLOR_RESET}"
  echo -e "${COLOR_GREEN}â€¢ Consistent runtime environment across all deployments${COLOR_RESET}"
  echo -e "${COLOR_GREEN}â€¢ Pre-configured with essential system utilities${COLOR_RESET}"
  echo -e "${COLOR_GREEN}â€¢ Minimal attack surface with optimized package selection${COLOR_RESET}"
  echo -e "${COLOR_GREEN}â€¢ Ready for integration with Kubernetes and container orchestration${COLOR_RESET}"
  echo -e "${COLOR_GREEN}â€¢ Supports both development and production workloads${COLOR_RESET}"
  echo
}

# Validate base platform installation
validate_base_installation() {
  log_info "Validating base platform installation..."
  
  # Check if base installation marker exists
  if kubectl get configmap base-config -n kube-system >/dev/null 2>&1; then
    log_success "Base platform installation marker found"
    
    # Check marker metadata
    local version=$(kubectl get configmap base-config -n kube-system -o jsonpath='{.data.version}' 2>/dev/null || echo "unknown")
    local caching=$(kubectl get configmap base-config -n kube-system -o jsonpath='{.data.caching-enabled}' 2>/dev/null || echo "false")
    
    if [[ "$version" == "enhanced" ]]; then
      log_success "Base platform enhanced version confirmed"
    else
      log_info "Base platform version: ${version}"
    fi
    
    if [[ "$caching" == "true" ]]; then
      log_success "Smart caching enabled and operational"
    else
      log_info "Smart caching status: ${caching}"
    fi
  else
    log_warning "Base platform installation marker not found"
    return 1
  fi
  
  # Check if Docker registry is accessible
  local registry_url=$(fullRegistryUrl 2>/dev/null || echo "localhost:5000")
  if docker images | grep -q "gok-base"; then
    log_success "Base platform Docker image found locally"
  else
    log_warning "Base platform Docker image not found locally"
  fi
  
  # Check if caching directory exists and is working
  if [[ -d "$GOK_CACHE_DIR" ]]; then
    log_success "GOK cache directory operational"
    
    # Check cache effectiveness
    if [[ -f "$GOK_CACHE_DIR/update_cache" ]]; then
      local cache_age=$(stat -c %Y "$GOK_CACHE_DIR/update_cache" 2>/dev/null || echo 0)
      local current_time=$(date +%s)
      local hours_old=$(( (current_time - cache_age) / 3600 ))
      
      if [[ $hours_old -lt ${GOK_UPDATE_CACHE_HOURS:-6} ]]; then
        log_success "System update cache is fresh (${hours_old}h old)"
      else
        log_info "System update cache is aging (${hours_old}h old)"
      fi
    fi
  else
    log_warning "GOK cache directory not found"
  fi
  
  return 0
}

# Show base platform next steps and recommendations
show_base_next_steps() {
  echo
  echo -e "${COLOR_BRIGHT_YELLOW}${COLOR_BOLD}ðŸš€ Base Platform Post-Installation Steps${COLOR_RESET}"
  echo
  
  echo -e "${COLOR_MAGENTA}${COLOR_BOLD}Immediate Next Steps:${COLOR_RESET}"
  echo -e "${COLOR_CYAN}  1. Verify Docker registry connectivity and image availability${COLOR_RESET}"
  echo -e "${COLOR_CYAN}  2. Test smart caching system with other component installations${COLOR_RESET}"
  echo -e "${COLOR_CYAN}  3. Configure additional registry repositories if needed${COLOR_RESET}"
  echo -e "${COLOR_CYAN}  4. Set up persistent volume storage for platform data${COLOR_RESET}"
  echo
  
  echo -e "${COLOR_BRIGHT_MAGENTA}${COLOR_BOLD}ðŸŽ¯ Recommended Next Installation: Registry${COLOR_RESET}"
  echo -e "${COLOR_CYAN}Base platform provides the foundation - now add a container registry for image management!${COLOR_RESET}"
  echo
  echo -e "${COLOR_YELLOW}${COLOR_BOLD}Why install Registry next?${COLOR_RESET}"
  echo -e "${COLOR_GREEN}â€¢ ðŸ—‚ï¸  Central container image storage and distribution${COLOR_RESET}"
  echo -e "${COLOR_GREEN}â€¢ ðŸ”’ Secure, private registry for your platform images${COLOR_RESET}"
  echo -e "${COLOR_GREEN}â€¢ ðŸš€ Faster deployments with local image caching${COLOR_RESET}"
  echo -e "${COLOR_GREEN}â€¢ ðŸ” Image vulnerability scanning and management${COLOR_RESET}"
  echo -e "${COLOR_GREEN}â€¢ ðŸ“ˆ Scalable image distribution across your cluster${COLOR_RESET}"
  echo
  
  echo -e "${COLOR_BRIGHT_GREEN}${COLOR_BOLD}Alternative Next Steps:${COLOR_RESET}"
  echo -e "${COLOR_YELLOW}Core Infrastructure:${COLOR_RESET}"
  echo -e "${COLOR_CYAN}  â€¢ ${COLOR_BOLD}gok install registry${COLOR_RESET}        - Container registry for image management"
  echo -e "${COLOR_CYAN}  â€¢ ${COLOR_BOLD}gok install cert-manager${COLOR_RESET}   - SSL certificate management"
  echo -e "${COLOR_CYAN}  â€¢ ${COLOR_BOLD}gok install ingress${COLOR_RESET}        - Load balancing and routing"
  echo
  echo -e "${COLOR_YELLOW}Development Tools:${COLOR_RESET}"
  echo -e "${COLOR_CYAN}  â€¢ ${COLOR_BOLD}gok install jenkins${COLOR_RESET}        - CI/CD pipeline automation"
  echo -e "${COLOR_CYAN}  â€¢ ${COLOR_BOLD}gok install argocd${COLOR_RESET}         - GitOps continuous deployment"
  echo -e "${COLOR_CYAN}  â€¢ ${COLOR_BOLD}gok install console${COLOR_RESET}        - Web-based cluster management"
  echo
  echo -e "${COLOR_YELLOW}Authentication:${COLOR_RESET}"
  echo -e "${COLOR_CYAN}  â€¢ ${COLOR_BOLD}gok install ldap${COLOR_RESET}           - Directory services and user management"
  echo -e "${COLOR_CYAN}  â€¢ ${COLOR_BOLD}gok install keycloak${COLOR_RESET}       - Modern identity and access management"
  echo -e "${COLOR_CYAN}  â€¢ ${COLOR_BOLD}gok install oauth2${COLOR_RESET}         - OAuth2 proxy for authentication"
  echo
  echo -e "${COLOR_BRIGHT_GREEN}${COLOR_BOLD}Install Registry now?${COLOR_RESET}"
  echo -e "${COLOR_CYAN}  Command: ${COLOR_BOLD}gok install registry${COLOR_RESET}"
  echo
  
  # Suggest and install Registry as the next logical step
  suggest_and_install_next_module "base"
}


chartInst() {
  log_component_start "chartmuseum" "Installing ChartMuseum Helm repository"
  
  log_step "1" "Creating namespace and storage"
  kubectl_with_summary create "namespace" chartmuseum || log_info "Namespace chartmuseum already exists"
  createLocalStorageClassAndPV "chart-storage" "chart-pv" "/data/volumes/chart-storage"

  log_step "2" "Adding ChartMuseum Helm repository"
  execute_with_suppression helm repo add chartmuseum https://chartmuseum.github.io/charts
  execute_with_suppression helm repo update
  
  log_step "3" "Installing ChartMuseum via Helm"
  if helm_install_with_summary "chartmuseum" "chartmuseum" \
    chartmuseum chartmuseum/chartmuseum \
    --namespace chartmuseum \
    --set persistence.enabled=true \
    --set persistence.size=10Gi \
    --set persistence.storageClass="chart-storage" \
    --set env.open.STORAGE="local" \
    --set env.open.STORAGE_LOCAL_ROOTDIR="/charts" \
    --values "$MOUNT_PATH"/kubernetes/install_k8s/chart-registry/values.yaml; then
    
    log_step "4" "Configuring ingress"
    gok patch ingress chartmuseum chartmuseum letsencrypt chart

    log_step "5" "Waiting for ChartMuseum to be ready"
    log_info "Waiting for ChartMuseum to be ready (timeout: 120s)..."
    if kubectl --namespace chartmuseum wait --for=condition=Ready pods --all --timeout=120s >/dev/null 2>&1; then
      log_success "ChartMuseum is now up and running!"
      log_info "Access it at: https://chartmuseum.gokcloud.com"
      
      log_step "6" "Installing Helm push plugin"
      if execute_with_suppression helm plugin install https://github.com/chartmuseum/helm-push; then
        log_success "Helm push plugin installed"
      fi

      log_step "7" "Adding repository to Helm"
      if execute_with_suppression helm repo add gok https://chart.gokcloud.com --username sumit --password abcdef; then
        execute_with_suppression helm repo update
        log_success "ChartMuseum repository added to Helm"
      fi
      
      show_installation_summary "chartmuseum" "chartmuseum" "Helm chart repository with persistent storage"
      log_component_success "chartmuseum" "ChartMuseum installed successfully"
      
      # Call next module suggestion
      suggest_and_install_next_module "chartmuseum"
    else
      log_error "ChartMuseum setup timed out. Please check the logs."
      return 1
    fi
  else
    log_error "ChartMuseum installation failed"
    return 1
  fi
}

dockrInst() {
  log_section "Docker Container Runtime Installation" "$EMOJI_PACKAGE"
  
  # Pre-installation validation
  log_step "1" "Validating system requirements for Docker"
  
  # Check if running as root or with sudo
  if [[ $EUID -ne 0 ]]; then
    log_error "Docker installation requires root privileges"
    return 1
  fi
  
  # Check system compatibility
  local os_info=$(lsb_release -d 2>/dev/null | cut -f2 || echo "Unknown")
  log_info "Operating System: $os_info"
  
  # Check if Docker is already installed
  if command -v docker >/dev/null 2>&1; then
    local docker_version=$(docker --version 2>/dev/null | cut -d' ' -f3 | cut -d',' -f1)
    log_warning "Docker is already installed (version: $docker_version)"
    
    # Validate existing installation
    if validate_docker_installation; then
      log_success "Existing Docker installation is working correctly"
      show_docker_next_steps
      return 0
    else
      log_warning "Existing Docker installation has issues, proceeding with reinstallation"
    fi
  fi
  
  # Step 2: Install prerequisites
  log_step "2" "Installing Docker prerequisites and dependencies"
  log_substep "Installing required packages"
  
  if ! apt-get update; then
    log_error "Failed to update package list"
    return 1
  fi
  
  if ! apt-get install -y \
    apt-transport-https \
    ca-certificates \
    curl \
    software-properties-common \
    gnupg \
    lsb-release; then
    log_error "Failed to install prerequisite packages"
    return 1
  fi
  
  log_success "Prerequisites installed successfully"
  
  # Step 3: Add Docker repository
  log_step "3" "Adding Docker official repository"
  
  log_substep "Creating keyrings directory"
  if ! sudo install -m 0755 -d /etc/apt/keyrings; then
    log_error "Failed to create keyrings directory"
    return 1
  fi
  
  log_substep "Adding Docker GPG key"
  if ! sudo curl -fsSL https://download.docker.com/linux/ubuntu/gpg -o /etc/apt/keyrings/docker.asc; then
    log_error "Failed to download Docker GPG key"
    return 1
  fi
  
  if ! sudo chmod a+r /etc/apt/keyrings/docker.asc; then
    log_error "Failed to set permissions on Docker GPG key"
    return 1
  fi
  
  log_substep "Adding Docker repository to sources"
  if ! echo \
    "deb [arch=$(dpkg --print-architecture) signed-by=/etc/apt/keyrings/docker.asc] https://download.docker.com/linux/ubuntu \
    $(. /etc/os-release && echo "$VERSION_CODENAME") stable" | \
    sudo tee /etc/apt/sources.list.d/docker.list > /dev/null; then
    log_error "Failed to add Docker repository"
    return 1
  fi
  
  log_substep "Updating package list with Docker repository"
  if ! sudo apt-get update; then
    log_error "Failed to update package list after adding Docker repository"
    return 1
  fi
  
  log_success "Docker repository added successfully"
  
  # Step 4: Install Docker Engine
  log_step "4" "Installing Docker Engine and components"
  
  local docker_packages="docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin"
  log_substep "Installing: $docker_packages"
  
  if ! apt-get install -y $docker_packages; then
    log_error "Failed to install Docker packages"
    return 1
  fi
  
  log_success "Docker Engine installed successfully"
  
  # Step 5: Configure Docker daemon
  log_step "5" "Configuring Docker daemon for Kubernetes compatibility"
  
  log_substep "Creating systemd service directory"
  if ! sudo mkdir -p /etc/systemd/system/docker.service.d; then
    log_error "Failed to create Docker systemd directory"
    return 1
  fi
  
  log_substep "Creating Docker daemon configuration"
  if ! sudo tee /etc/docker/daemon.json <<EOF
{
  "exec-opts": ["native.cgroupdriver=systemd"],
  "log-driver": "json-file",
  "log-opts": {
    "max-size": "100m",
    "max-file": "3"
  },
  "storage-driver": "overlay2",
  "storage-opts": [
    "overlay2.override_kernel_check=true"
  ],
  "live-restore": true,
  "default-address-pools": [
    {
      "base": "172.17.0.0/12",
      "size": 24
    }
  ]
}
EOF
  then
    log_error "Failed to create Docker daemon configuration"
    return 1
  fi
  
  log_success "Docker daemon configured for Kubernetes"
  
  # Step 6: Configure and start services
  log_step "6" "Starting and enabling Docker services"
  
  log_substep "Reloading systemd daemon"
  if ! sudo systemctl daemon-reload; then
    log_error "Failed to reload systemd daemon"
    return 1
  fi
  
  log_substep "Starting Docker service"
  if ! sudo systemctl start docker; then
    log_error "Failed to start Docker service"
    return 1
  fi
  
  log_substep "Enabling Docker service for auto-start"
  if ! sudo systemctl enable docker; then
    log_error "Failed to enable Docker service"
    return 1
  fi
  
  # Configure containerd
  log_substep "Configuring containerd for Kubernetes"
  
  if ! sudo systemctl enable containerd; then
    log_error "Failed to enable containerd service"
    return 1
  fi
  
  if ! sudo systemctl start containerd; then
    log_error "Failed to start containerd service"
    return 1
  fi
  
  # Remove default containerd config to use defaults
  if [[ -f /etc/containerd/config.toml ]]; then
    log_substep "Removing default containerd configuration"
    sudo rm /etc/containerd/config.toml
    sudo systemctl restart containerd
  fi
  
  log_success "Docker and containerd services started successfully"
  
  # Step 7: Post-installation validation
  log_step "7" "Validating Docker installation"
  
  if validate_docker_installation; then
    log_success "Docker installation validation passed"
  else
    log_error "Docker installation validation failed"
    return 1
  fi
  
  # Step 8: Set up user permissions (if not root)
  if [[ -n "$SUDO_USER" ]]; then
    log_step "8" "Configuring user permissions for Docker"
    log_substep "Adding user $SUDO_USER to docker group"
    
    if ! sudo usermod -aG docker "$SUDO_USER"; then
      log_warning "Failed to add user to docker group - manual setup may be required"
    else
      log_success "User $SUDO_USER added to docker group"
      log_info "User $SUDO_USER will need to log out and back in for group membership to take effect"
    fi
  fi
  
  # Show Docker information
  local docker_version=$(docker --version | cut -d' ' -f3 | cut -d',' -f1)
  local containerd_version=$(containerd --version | cut -d' ' -f3)
  
  log_success "Docker installation completed successfully!"
  log_info "Docker version: $docker_version"
  log_info "Containerd version: $containerd_version"
  
  # Show next steps
  show_docker_next_steps
  
  return 0
}

# Docker installation validation function
validate_docker_installation() {
  log_substep "Checking Docker daemon status"
  if ! sudo systemctl is-active --quiet docker; then
    log_error "Docker service is not running"
    return 1
  fi
  
  log_substep "Checking containerd status"
  if ! sudo systemctl is-active --quiet containerd; then
    log_error "Containerd service is not running"
    return 1
  fi
  
  log_substep "Testing Docker functionality"
  if ! docker info >/dev/null 2>&1; then
    log_error "Docker daemon is not responding properly"
    return 1
  fi
  
  log_substep "Testing container creation"
  if ! timeout 30 docker run --rm hello-world >/dev/null 2>&1; then
    log_warning "Docker hello-world test failed - may need internet connectivity"
  else
    log_substep "Container test passed"
  fi
  
  log_substep "Checking Docker configuration"
  local cgroup_driver=$(docker info 2>/dev/null | grep "Cgroup Driver" | cut -d: -f2 | tr -d ' ')
  if [[ "$cgroup_driver" != "systemd" ]]; then
    log_warning "Docker cgroup driver is not set to systemd (current: $cgroup_driver)"
  else
    log_substep "Cgroup driver correctly set to systemd"
  fi
  
  return 0
}

# Show Docker next steps
show_docker_next_steps() {
  log_next_steps "Docker Installation Complete" \
    "Test Docker functionality: docker run hello-world" \
    "Check Docker service status: systemctl status docker" \
    "View Docker system information: docker info" \
    "Verify container runtime: docker version" \
    "Install Kubernetes cluster: gok install kubernetes"
  
  log_urls "Docker Resources & Documentation" \
    "Docker Documentation: https://docs.docker.com/" \
    "Docker Hub Registry: https://hub.docker.com/" \
    "Kubernetes Container Runtime Guide: https://kubernetes.io/docs/setup/production-environment/container-runtimes/" \
    "Docker Best Practices: https://docs.docker.com/develop/best-practices/"
  
  log_credentials "Docker Management" "Current User" \
    "Docker group membership: Required for non-root access" \
    "Restart required: Log out and back in to apply group changes" \
    "Test access: docker ps (should work without sudo)"
  
  # Enhanced HA proxy detection and recommendation
  check_and_suggest_ha_setup
  
  log_info "Docker container runtime is now ready for Kubernetes installation"
  
  # Show Docker system status
  show_docker_system_status
}

# Enhanced HA setup detection and suggestions
check_and_suggest_ha_setup() {
  local suggest_ha=false
  local ha_reason=""
  
  # Check for multiple API servers configuration
  if [[ -n "$API_SERVERS" ]] && [[ "$API_SERVERS" == *","* ]]; then
    suggest_ha=true
    local server_count=$(echo "$API_SERVERS" | tr ',' '\n' | wc -l)
    ha_reason="Multiple API servers detected ($server_count servers) in API_SERVERS configuration"
  fi
  
  # Check for multiple network interfaces (potential multi-node setup)
  local interface_count=$(ip route | grep -E "^[0-9]" | wc -l)
  if [[ $interface_count -gt 2 ]]; then
    suggest_ha=true
    ha_reason="${ha_reason:+$ha_reason; }Multiple network interfaces detected (potential multi-node setup)"
  fi
  
  # Check available RAM (high RAM might indicate server-class machine)
  local total_ram_gb=$(free -g | awk '/^Mem:/{print $2}')
  if [[ $total_ram_gb -gt 8 ]]; then
    suggest_ha=true
    ha_reason="${ha_reason:+$ha_reason; }High RAM detected (${total_ram_gb}GB) - suitable for HA setup"
  fi
  
  if [[ "$suggest_ha" == "true" ]]; then
    echo
    log_header "High Availability Recommendation" "Multi-Master Kubernetes Setup Detected"
    
    log_warning "HA setup recommended: $ha_reason"
    
    log_next_steps "High Availability Container Setup" \
      "Set up API servers: export API_SERVERS='192.168.1.10:master1,192.168.1.11:master2,192.168.1.12:master3'" \
      "Configure HA proxy port: export HA_PROXY_PORT=8443 (optional, defaults to 8443)" \
      "Install HA proxy container: Call haInst() function or use manual setup" \
      "Verify HA proxy: docker ps | grep master-proxy" \
      "Proceed with Kubernetes: gok install kubernetes (will use HA endpoint)"
    
    log_urls "HA Container Management" \
      "HA Proxy Configuration: /opt/haproxy.cfg" \
      "Container Logs: docker logs master-proxy" \
      "HA Endpoint: https://localhost:\${HA_PROXY_PORT:-8443}"
    
    log_info "HA proxy container will provide load balancing and failover for Kubernetes API servers"
    
    # Show how to call haInst function
    echo
    echo -e "${COLOR_BRIGHT_YELLOW}${COLOR_BOLD}ðŸš€ Ready to set up HA proxy container?${COLOR_RESET}"
    echo -e "${COLOR_CYAN}Run the following commands:${COLOR_RESET}"
    echo
    echo -e "${COLOR_GREEN}${COLOR_BOLD}# Configure your API servers${COLOR_RESET}"
    echo -e "${COLOR_CYAN}export API_SERVERS='192.168.1.10:master1,192.168.1.11:master2,192.168.1.12:master3'${COLOR_RESET}"
    echo
    echo -e "${COLOR_GREEN}${COLOR_BOLD}# Set up HA proxy container${COLOR_RESET}"
    echo -e "${COLOR_CYAN}haInst  # Call the HA installation function directly${COLOR_RESET}"
    echo
    echo -e "${COLOR_GREEN}${COLOR_BOLD}# Or configure manually and install Kubernetes${COLOR_RESET}"
    echo -e "${COLOR_CYAN}gok install kubernetes  # Will detect HA setup automatically${COLOR_RESET}"
    echo
  else
    log_info "Single-node setup detected - HA proxy not required for basic installation"
    
    if [[ -z "$API_SERVERS" ]]; then
      log_info "To enable HA setup later, configure: export API_SERVERS='ip1:node1,ip2:node2'"
    fi
  fi
}

# Show Docker system status
show_docker_system_status() {
  echo
  log_section "Docker System Status" "$EMOJI_CHECKMARK"
  
  # Docker daemon status
  if systemctl is-active --quiet docker; then
    log_success "Docker daemon: Running"
  else
    log_error "Docker daemon: Not running"
  fi
  
  # Containerd status
  if systemctl is-active --quiet containerd; then
    log_success "Containerd: Running"
  else
    log_error "Containerd: Not running"
  fi
  
  # Docker version info
  local docker_version=$(docker --version 2>/dev/null | cut -d' ' -f3 | cut -d',' -f1)
  local containerd_version=$(containerd --version 2>/dev/null | cut -d' ' -f3)
  
  log_info "Docker version: ${docker_version:-Unknown}"
  log_info "Containerd version: ${containerd_version:-Unknown}"
  
  # Docker configuration check
  local cgroup_driver=$(docker info 2>/dev/null | grep "Cgroup Driver" | cut -d: -f2 | tr -d ' ')
  if [[ "$cgroup_driver" == "systemd" ]]; then
    log_success "Cgroup driver: systemd (Kubernetes compatible)"
  else
    log_warning "Cgroup driver: $cgroup_driver (may need systemd for Kubernetes)"
  fi
  
  # Storage driver check
  local storage_driver=$(docker info 2>/dev/null | grep "Storage Driver" | cut -d: -f2 | tr -d ' ')
  log_info "Storage driver: ${storage_driver:-Unknown}"
  
  # Container count
  local running_containers=$(docker ps -q | wc -l)
  local total_containers=$(docker ps -aq | wc -l)
  log_info "Containers: $running_containers running, $total_containers total"
}

customDns() {
  echo "Going to add custom dns server"
  #Adding custom dns server
  cat <<EOF | kubectl apply -f -
apiVersion: v1
data:
  Corefile: |
    .:53 {
        errors
        health {
           lameduck 5s
        }
        ready
        kubernetes cloud.uat in-addr.arpa ip6.arpa {
           pods insecure
           fallthrough in-addr.arpa ip6.arpa
           ttl 30
        }
        prometheus :9153
        forward . /etc/resolv.conf {
           max_concurrent 1000
        }
        cache 30
        loop
        reload
        loadbalance
    }
    cloud.com:53 {
        errors
        cache 30
        forward . ${MASTER_HOST_IP}
    }
    gokcloud.com:53 {
        errors
        cache 30
        forward . ${MASTER_HOST_IP}
    }
kind: ConfigMap
metadata:
  name: coredns
  namespace: kube-system
EOF

  kubectl delete pod --namespace kube-system -l k8s-app=kube-dns
}

taintNode() {
  log_info "Configuring master node for pod scheduling (removing NoSchedule taint)"

  # Improved IP detection using multiple methods
  local node_ip=""
  
  # Try multiple methods to get the node IP
  if command -v ip >/dev/null 2>&1; then
    # Modern approach using ip command
    node_ip=$(ip route get 8.8.8.8 2>/dev/null | awk '{print $7}' | head -1)
  fi
  
  # Fallback to ifconfig if ip command failed
  if [[ -z "$node_ip" ]] && command -v ifconfig >/dev/null 2>&1; then
    # Try common interface names
    for interface in eth1 enp1s0.100 eth0 enp0s3 enp0s8; do
      node_ip=$(ifconfig "$interface" 2>/dev/null | awk '/inet / {print $2}' | sed 's/addr://')
      [[ -n "$node_ip" ]] && break
    done
  fi
  
  # If still no IP, try getting from kubectl node info
  if [[ -z "$node_ip" ]]; then
    node_ip=$(kubectl get nodes -o wide --no-headers 2>/dev/null | awk '{print $6}' | head -1)
  fi
  
  if [[ -z "$node_ip" ]]; then
    log_warning "Could not determine node IP address - using hostname for node identification"
    local node_name=$(kubectl get nodes --no-headers -o custom-columns=NAME:.metadata.name 2>/dev/null | head -1)
  else
    # Get the node name using IP address
    local jsonpath="{.items[?(@.status.addresses[0].address == \"${node_ip}\")].metadata.name}"
    local node_name=$(kubectl get nodes -o jsonpath="$jsonpath" 2>/dev/null)
    
    # If node name lookup by IP failed, try getting first node
    if [[ -z "$node_name" ]]; then
      node_name=$(kubectl get nodes --no-headers -o custom-columns=NAME:.metadata.name 2>/dev/null | head -1)
    fi
  fi
  
  if [[ -z "$node_name" ]]; then
    log_error "Could not determine node name for taint removal"
    log_info "Manual fix: kubectl taint nodes --all node-role.kubernetes.io/control-plane-"
    return 1
  fi
  
  log_info "Node identified: $node_name (IP: ${node_ip:-unknown})"
  
  # Remove the taint to allow scheduling on master node
  local taint_output
  if taint_output=$(kubectl taint node "${node_name}" node-role.kubernetes.io/control-plane:NoSchedule- 2>&1); then
    log_success "Master node configured for pod scheduling"
    return 0
  else
    # Check if taint was already removed
    if [[ "$taint_output" == *"not found"* ]]; then
      log_info "Master node taint already removed - node ready for scheduling"
      return 0
    else
      log_warning "Master node taint removal failed: $taint_output"
      log_info "Manual fix: kubectl taint nodes --all node-role.kubernetes.io/control-plane-"
      return 1
    fi
  fi
}

k8sInst() {
  local k8s_type="${1:-kubernetes}"
  local verbose_mode="${GOK_VERBOSE:-false}"
  
  # Check for verbose flags in all arguments
  for arg in "$@"; do
    if [[ "$arg" == "--verbose" ]] || [[ "$arg" == "-v" ]]; then
      verbose_mode="true"
      break
    fi
  done
  
  # Also check if GOK_VERBOSE environment variable is set
  if [[ "${GOK_VERBOSE}" == "true" ]]; then
    verbose_mode="true"
  fi
  
  # Debug: Show verbose mode status
  if [[ "$verbose_mode" == "true" ]]; then
    echo -e "${COLOR_BRIGHT_GREEN}[DEBUG] k8sInst: Verbose mode is ENABLED${COLOR_RESET}"
  else
    echo -e "${COLOR_BRIGHT_YELLOW}[DEBUG] k8sInst: Verbose mode is DISABLED${COLOR_RESET}"
    echo -e "${COLOR_DIM}Arguments received: $@${COLOR_RESET}"
    echo -e "${COLOR_DIM}GOK_VERBOSE: ${GOK_VERBOSE}${COLOR_RESET}"
    echo -e "${COLOR_DIM}First arg: '$1', Second arg: '$2'${COLOR_RESET}"
  fi
  
  log_header "Kubernetes Cluster" "Starting ${k8s_type} Installation"
  
  echo -e "${COLOR_BRIGHT_CYAN}${COLOR_BOLD}ðŸš€ KUBERNETES CLUSTER INSTALLATION${COLOR_RESET}"
  echo -e "${COLOR_YELLOW}Installation Type: ${COLOR_BOLD}${k8s_type}${COLOR_RESET}"
  if [[ "$verbose_mode" == "true" ]]; then
    echo -e "${COLOR_DIM}Verbose mode: Enabled${COLOR_RESET}"
  fi
  echo
  
  # Step 1: System Prerequisites and Kernel Modules
  log_step "1" "Configuring system prerequisites and kernel modules"
  
  if ! validate_system_requirements; then
    log_error "System requirements validation failed"
    return 1
  fi
  
  log_info "Loading required kernel modules..."
  local mod_output
  if [[ "$verbose_mode" == "true" ]]; then
    echo -e "${COLOR_DIM}Executing: sudo modprobe overlay br_netfilter${COLOR_RESET}"
    if mod_output=$(sudo modprobe overlay && sudo modprobe br_netfilter 2>&1); then
      log_success "Kernel modules loaded successfully"
      [[ -n "$mod_output" ]] && echo -e "${COLOR_DIM}$mod_output${COLOR_RESET}"
    else
      log_error "Failed to load kernel modules"
      echo -e "${COLOR_RED}Error details: $mod_output${COLOR_RESET}"
      return 1
    fi
  else
    if mod_output=$(sudo modprobe overlay && sudo modprobe br_netfilter 2>&1); then
      log_success "Kernel modules loaded successfully"
    else
      log_error "Failed to load kernel modules"
      echo -e "${COLOR_RED}Error details: $mod_output${COLOR_RESET}"
      return 1
    fi
  fi
  
  # Configure persistent module loading
  log_info "Configuring persistent kernel modules..."
  sudo tee /etc/modules-load.d/containerd.conf <<EOF >/dev/null
overlay
br_netfilter
EOF
  log_success "Kernel modules configured for persistence"
  
  # Step 2: Network Configuration
  log_step "2" "Configuring network settings for Kubernetes"
  
  log_info "Setting up network bridge configurations..."
  sudo tee /etc/sysctl.d/kubernetes.conf <<EOF >/dev/null
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
net.ipv4.ip_forward = 1
EOF
  
  local sysctl_output
  if [[ "$verbose_mode" == "true" ]]; then
    echo -e "${COLOR_DIM}Executing: sudo sysctl --system${COLOR_RESET}"
    if sysctl_output=$(sudo sysctl --system 2>&1); then
      log_success "Network settings applied successfully"
      echo -e "${COLOR_DIM}$sysctl_output${COLOR_RESET}"
    else
      log_error "Failed to apply network settings"
      echo -e "${COLOR_RED}Error details: $sysctl_output${COLOR_RESET}"
      return 1
    fi
  else
    if sysctl_output=$(sudo sysctl --system 2>&1); then
      log_success "Network settings applied successfully"
    else
      log_error "Failed to apply network settings"
      echo -e "${COLOR_RED}Error details: $sysctl_output${COLOR_RESET}"
      return 1
    fi
  fi
  
  # Step 3: Container Runtime Configuration
  log_step "3" "Configuring containerd container runtime"
  
  log_info "Setting up containerd configuration..."
  sudo mkdir -p /etc/containerd
  
  local containerd_config_output
  if [[ "$verbose_mode" == "true" ]]; then
    echo -e "${COLOR_DIM}Creating /etc/containerd directory and configuration...${COLOR_RESET}"
    echo -e "${COLOR_DIM}Executing: containerd config default | sudo tee /etc/containerd/config.toml${COLOR_RESET}"
    if containerd_config_output=$(containerd config default 2>&1) && echo "$containerd_config_output" | sudo tee /etc/containerd/config.toml >/dev/null; then
      log_success "Default containerd configuration created"
      echo -e "${COLOR_DIM}Configuration written to /etc/containerd/config.toml${COLOR_RESET}"
    else
      log_error "Failed to create containerd configuration"
      echo -e "${COLOR_RED}Error details: $containerd_config_output${COLOR_RESET}"
      return 1
    fi
  else
    if containerd_config_output=$(containerd config default 2>&1) && echo "$containerd_config_output" | sudo tee /etc/containerd/config.toml >/dev/null; then
      log_success "Default containerd configuration created"
    else
      log_error "Failed to create containerd configuration"
      echo -e "${COLOR_RED}Error details: $containerd_config_output${COLOR_RESET}"
      return 1
    fi
  fi
  
  # Configure systemd cgroup driver
  log_info "Configuring systemd cgroup driver..."
  local sed_output
  if [[ "$verbose_mode" == "true" ]]; then
    echo -e "${COLOR_DIM}Updating containerd config: SystemdCgroup = false -> SystemdCgroup = true${COLOR_RESET}"
    echo -e "${COLOR_DIM}Executing: sudo sed -i 's/SystemdCgroup = false/SystemdCgroup = true/g' /etc/containerd/config.toml${COLOR_RESET}"
    if sed_output=$(sudo sed -i 's/SystemdCgroup = false/SystemdCgroup = true/g' /etc/containerd/config.toml 2>&1); then
      log_success "Systemd cgroup driver configured"
      echo -e "${COLOR_DIM}Verification:${COLOR_RESET}"
      local cgroup_check=$(grep -n "SystemdCgroup" /etc/containerd/config.toml || echo "SystemdCgroup setting not found")
      echo -e "${COLOR_DIM}$cgroup_check${COLOR_RESET}"
    else
      log_warning "Cgroup driver configuration may have failed"
      echo -e "${COLOR_YELLOW}Warning details: $sed_output${COLOR_RESET}"
    fi
  else
    if sed_output=$(sudo sed -i 's/SystemdCgroup = false/SystemdCgroup = true/g' /etc/containerd/config.toml 2>&1); then
      log_success "Systemd cgroup driver configured"
    else
      log_warning "Cgroup driver configuration may have failed"
      echo -e "${COLOR_YELLOW}Warning details: $sed_output${COLOR_RESET}"
    fi
  fi
  
  # Restart and enable containerd
  log_info "Starting containerd service..."
  local containerd_service_output
  if [[ "$verbose_mode" == "true" ]]; then
    echo -e "${COLOR_DIM}Executing: sudo systemctl restart containerd && sudo systemctl enable containerd${COLOR_RESET}"
    if containerd_service_output=$(sudo systemctl restart containerd 2>&1 && sudo systemctl enable containerd 2>&1); then
      log_success "Containerd service started and enabled"
      [[ -n "$containerd_service_output" ]] && echo -e "${COLOR_DIM}$containerd_service_output${COLOR_RESET}"
    else
      log_error "Failed to start containerd service"
      echo -e "${COLOR_RED}Error details: $containerd_service_output${COLOR_RESET}"
      echo -e "${COLOR_YELLOW}Try: ${COLOR_CYAN}sudo systemctl status containerd${COLOR_RESET} for more details"
      return 1
    fi
  else
    if containerd_service_output=$(sudo systemctl restart containerd 2>&1 && sudo systemctl enable containerd 2>&1); then
      log_success "Containerd service started and enabled"
    else
      log_error "Failed to start containerd service"
      echo -e "${COLOR_RED}Error details: $containerd_service_output${COLOR_RESET}"
      echo -e "${COLOR_YELLOW}Try: ${COLOR_CYAN}sudo systemctl status containerd${COLOR_RESET} for more details"
      return 1
    fi
  fi
  
  # Step 4: Kubernetes Repository Setup
  log_step "4" "Setting up Kubernetes package repository"
  
  log_info "Installing required packages..."
  local repo_packages_output
  if [[ "$verbose_mode" == "true" ]]; then
    echo -e "${COLOR_DIM}Executing: sudo apt-get update && sudo apt-get install -y apt-transport-https ca-certificates curl${COLOR_RESET}"
    if repo_packages_output=$(sudo apt-get update 2>&1 && sudo apt-get install -y apt-transport-https ca-certificates curl 2>&1); then
      log_success "Required packages installed"
      echo -e "${COLOR_DIM}$repo_packages_output${COLOR_RESET}"
    else
      log_error "Failed to install required packages"
      echo -e "${COLOR_RED}Error details: $repo_packages_output${COLOR_RESET}"
      return 1
    fi
  else
    if repo_packages_output=$(sudo apt-get update 2>&1 && sudo apt-get install -y apt-transport-https ca-certificates curl 2>&1); then
      log_success "Required packages installed"
    else
      log_error "Failed to install required packages"
      echo -e "${COLOR_RED}Error details: $repo_packages_output${COLOR_RESET}"
      return 1
    fi
  fi
  
  # Setup Kubernetes signing key
  log_info "Adding Kubernetes signing key..."
  sudo mkdir -p -m 755 /etc/apt/keyrings
  
  if [[ "$verbose_mode" == "true" ]]; then
    echo -e "${COLOR_DIM}Downloading Kubernetes signing key from pkgs.k8s.io...${COLOR_RESET}"
    if curl -fsSL --show-error https://pkgs.k8s.io/core:/stable:/v1.32/deb/Release.key | \
       sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg; then
      sudo chmod 644 /etc/apt/keyrings/kubernetes-apt-keyring.gpg
      log_success "Kubernetes signing key added"
      echo -e "${COLOR_DIM}Key location: /etc/apt/keyrings/kubernetes-apt-keyring.gpg${COLOR_RESET}"
    else
      log_error "Failed to add Kubernetes signing key"
      return 1
    fi
  else
    if curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.32/deb/Release.key 2>/dev/null | \
       sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg 2>/dev/null; then
      sudo chmod 644 /etc/apt/keyrings/kubernetes-apt-keyring.gpg
      log_success "Kubernetes signing key added"
    else
      log_error "Failed to add Kubernetes signing key"
      return 1
    fi
  fi
  
  # Add Kubernetes repository
  log_info "Adding Kubernetes repository..."
  if [[ "$verbose_mode" == "true" ]]; then
    echo -e "${COLOR_DIM}Creating repository configuration: /etc/apt/sources.list.d/kubernetes.list${COLOR_RESET}"
    echo 'deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.32/deb/ /' | \
      sudo tee /etc/apt/sources.list.d/kubernetes.list
  else
    echo 'deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.32/deb/ /' | \
      sudo tee /etc/apt/sources.list.d/kubernetes.list >/dev/null
  fi
  sudo chmod 644 /etc/apt/sources.list.d/kubernetes.list
  log_success "Kubernetes repository added"
  
  # Step 5: Kubernetes Components Installation
  log_step "5" "Installing Kubernetes components"
  
  log_info "Updating package lists..."
  local apt_update_output
  if [[ "$verbose_mode" == "true" ]]; then
    echo -e "${COLOR_DIM}Executing: sudo apt-get update${COLOR_RESET}"
    if apt_update_output=$(sudo apt-get update 2>&1); then
      log_success "Package lists updated"
      echo -e "${COLOR_DIM}$apt_update_output${COLOR_RESET}"
    else
      log_error "Failed to update package lists"
      echo -e "${COLOR_RED}Error details: $apt_update_output${COLOR_RESET}"
      return 1
    fi
  else
    if apt_update_output=$(sudo apt-get update 2>&1); then
      log_success "Package lists updated"
    else
      log_error "Failed to update package lists"
      echo -e "${COLOR_RED}Error details: $apt_update_output${COLOR_RESET}"
      return 1
    fi
  fi
  
  log_info "Installing kubectl, kubeadm, and kubelet..."
  local apt_install_output
  if [[ "$verbose_mode" == "true" ]]; then
    echo -e "${COLOR_DIM}Executing: sudo apt-get install -y kubectl kubeadm kubelet${COLOR_RESET}"
    if apt_install_output=$(sudo apt-get install -y kubectl kubeadm kubelet 2>&1); then
      log_success "Kubernetes components installed successfully"
      echo -e "${COLOR_DIM}$apt_install_output${COLOR_RESET}"
    else
      log_error "Failed to install Kubernetes components"
      echo -e "${COLOR_RED}Error details: $apt_install_output${COLOR_RESET}"
      return 1
    fi
  else
    if apt_install_output=$(sudo apt-get install -y kubectl kubeadm kubelet 2>&1); then
      log_success "Kubernetes components installed successfully"
    else
      log_error "Failed to install Kubernetes components"
      echo -e "${COLOR_RED}Error details: $apt_install_output${COLOR_RESET}"
      return 1
    fi
  fi
  
  # Show installed versions
  local kubectl_version=$(kubectl version --client 2>/dev/null | grep -oE 'v[0-9]+\.[0-9]+\.[0-9]+' | head -1 || echo "unknown")
  local kubeadm_version=$(kubeadm version -o short 2>/dev/null || echo "unknown")
  local kubelet_version=$(kubelet --version 2>/dev/null | grep -oE 'v[0-9]+\.[0-9]+\.[0-9]+' || echo "unknown")
  echo -e "  ${COLOR_GREEN}âœ“ kubectl: ${kubectl_version}${COLOR_RESET}"
  echo -e "  ${COLOR_GREEN}âœ“ kubeadm: ${kubeadm_version}${COLOR_RESET}"
  echo -e "  ${COLOR_GREEN}âœ“ kubelet: ${kubelet_version}${COLOR_RESET}"
  
  # Step 6: Cluster Initialization (Master) or Worker Setup
  if [ "$k8s_type" == "kubernetes" ]; then
    initialize_kubernetes_master "$verbose_mode"
  elif [ "$k8s_type" == "kubernetes-worker" ]; then
    setup_kubernetes_worker "$verbose_mode"
  else
    log_error "Unknown Kubernetes installation type: $k8s_type"
    return 1
  fi
}

# Validate system requirements for Kubernetes
validate_system_requirements() {
  log_info "Validating system requirements..."
  
  local validation_passed=true
  
  # Check available memory
  local mem_gb=$(free -g | awk '/^Mem:/{print $2}')
  if [ "$mem_gb" -lt 2 ]; then
    log_warning "System has ${mem_gb}GB RAM, minimum 2GB recommended"
  else
    log_success "Memory requirement satisfied (${mem_gb}GB available)"
  fi
  
  # Check available disk space
  local disk_gb=$(df / | awk 'NR==2 {print int($4/1024/1024)}')
  if [ "$disk_gb" -lt 10 ]; then
    log_warning "Available disk space: ${disk_gb}GB, minimum 20GB recommended"
  else
    log_success "Disk space requirement satisfied (${disk_gb}GB available)"
  fi
  
  # Check if Docker is running
  if systemctl is-active --quiet docker; then
    log_success "Docker service is running"
  else
    log_error "Docker service is not running - please install Docker first"
    validation_passed=false
  fi
  
  # Check if swap is disabled
  if [ "$(swapon --show | wc -l)" -gt 0 ]; then
    log_info "Swap is enabled - will be disabled during installation"
  else
    log_success "Swap is already disabled"
  fi
  
  return $([[ "$validation_passed" == "true" ]] && echo 0 || echo 1)
}

# Initialize Kubernetes master node
initialize_kubernetes_master() {
  local verbose_mode="${1:-false}"
  log_step "6" "Initializing Kubernetes master node"
  
  # Disable swap
  log_info "Disabling swap for Kubernetes..."
  sudo sed -i '/ swap / s/^\(.*\)$/#\1/g' /etc/fstab
  sudo swapoff -a
  log_success "Swap disabled successfully"
  
  # Enable kubelet service
  log_info "Enabling kubelet service..."
  if sudo systemctl enable kubelet >/dev/null 2>&1; then
    log_success "Kubelet service enabled"
  else
    log_warning "Kubelet service enable may have failed"
  fi
  
  # Pull container images
  log_info "Pulling required container images..."
  local pull_output
  if [[ "$verbose_mode" == "true" ]]; then
    echo -e "${COLOR_DIM}Executing: kubeadm config images pull${COLOR_RESET}"
    if pull_output=$(kubeadm config images pull 2>&1); then
      log_success "Container images pulled successfully"
      echo -e "${COLOR_DIM}$pull_output${COLOR_RESET}"
    else
      log_warning "Some container images may not have been pulled"
      echo -e "${COLOR_YELLOW}Warning details: $pull_output${COLOR_RESET}"
    fi
  else
    if pull_output=$(kubeadm config images pull 2>&1); then
      log_success "Container images pulled successfully"
    else
      log_warning "Some container images may not have been pulled"
      echo -e "${COLOR_YELLOW}Warning details: $pull_output${COLOR_RESET}"
    fi
  fi
  
  # Generate cluster configuration
  log_info "Generating cluster configuration..."
  if [ -f "$WORKING_DIR/cluster-config-master.yaml" ]; then
    envsubst <"$WORKING_DIR/cluster-config-master.yaml" >"$WORKING_DIR/config.yaml"
    log_success "Cluster configuration generated"
  else
    log_warning "Using default cluster configuration"
  fi
  
  # Initialize the cluster
  log_info "Initializing Kubernetes cluster (this may take several minutes)..."
  
  local init_cmd="kubeadm init"
  if [ -f "$WORKING_DIR/config.yaml" ]; then
    init_cmd="$init_cmd --config=$WORKING_DIR/config.yaml"
  fi
  init_cmd="$init_cmd --upload-certs"
  
  local init_output
  if [[ "$verbose_mode" == "true" ]]; then
    echo -e "${COLOR_DIM}Executing: sudo $init_cmd${COLOR_RESET}"
    echo -e "${COLOR_DIM}This will initialize the control plane and may take 5-10 minutes...${COLOR_RESET}"
    if init_output=$(sudo $init_cmd 2>&1); then
      log_success "Kubernetes cluster initialized successfully"
      echo -e "${COLOR_DIM}$init_output${COLOR_RESET}"
    else
      log_error "Kubernetes cluster initialization failed"
      echo -e "${COLOR_RED}Error details: $init_output${COLOR_RESET}"
      echo -e "${COLOR_YELLOW}Troubleshooting commands:${COLOR_RESET}"
      echo -e "  ${COLOR_CYAN}sudo journalctl -xeu kubelet${COLOR_RESET}"
      echo -e "  ${COLOR_CYAN}sudo kubeadm reset -f${COLOR_RESET} (to reset and try again)"
      return 1
    fi
  else
    if init_output=$(sudo $init_cmd 2>&1); then
      log_success "Kubernetes cluster initialized successfully"
    else
      log_error "Kubernetes cluster initialization failed"
      echo -e "${COLOR_RED}Error details: $init_output${COLOR_RESET}"
      echo -e "${COLOR_YELLOW}Troubleshooting commands:${COLOR_RESET}"
      echo -e "  ${COLOR_CYAN}sudo journalctl -xeu kubelet${COLOR_RESET}"
      echo -e "  ${COLOR_CYAN}sudo kubeadm reset -f${COLOR_RESET} (to reset and try again)"
      return 1
    fi
  fi
  
  # Setup kubectl configuration
  log_info "Setting up kubectl configuration..."
  export KUBECONFIG=/etc/kubernetes/admin.conf
  mkdir -p "$HOME/.kube"
  
  local config_output
  if [[ "$verbose_mode" == "true" ]]; then
    echo -e "${COLOR_DIM}Copying admin.conf to ~/.kube/config${COLOR_RESET}"
    echo -e "${COLOR_DIM}Setting proper ownership and permissions...${COLOR_RESET}"
    if config_output=$(sudo cp -i /etc/kubernetes/admin.conf "$HOME/.kube/config" 2>&1 && sudo chown $(id -u):$(id -g) "$HOME/.kube/config" 2>&1); then
      log_success "Kubectl configuration completed"
      echo -e "${COLOR_DIM}Config location: $HOME/.kube/config${COLOR_RESET}"
      [[ -n "$config_output" ]] && echo -e "${COLOR_DIM}$config_output${COLOR_RESET}"
    else
      log_error "Failed to setup kubectl configuration"
      echo -e "${COLOR_RED}Error details: $config_output${COLOR_RESET}"
      return 1
    fi
  else
    if config_output=$(sudo cp -i /etc/kubernetes/admin.conf "$HOME/.kube/config" 2>&1 && sudo chown $(id -u):$(id -g) "$HOME/.kube/config" 2>&1); then
      log_success "Kubectl configuration completed"
    else
      log_error "Failed to setup kubectl configuration"
      echo -e "${COLOR_RED}Error details: $config_output${COLOR_RESET}"
      return 1
    fi
  fi
  
  # Configure master node (remove taints for single-node scheduling)
  log_info "Configuring master node for pod scheduling..."
  
  # Integrate taintNode functionality directly
  if [[ "$verbose_mode" == "true" ]]; then
    echo -e "${COLOR_DIM}Removing NoSchedule taint from master node...${COLOR_RESET}"
  fi
  
  # Define a function to get IP address (improved version)
  local node_ip=""
  
  # Try multiple methods to get the node IP
  if command -v ip >/dev/null 2>&1; then
    # Modern approach using ip command
    node_ip=$(ip route get 8.8.8.8 2>/dev/null | awk '{print $7}' | head -1)
  fi
  
  # Fallback to ifconfig if ip command failed
  if [[ -z "$node_ip" ]] && command -v ifconfig >/dev/null 2>&1; then
    # Try common interface names
    for interface in eth1 enp1s0.100 eth0 enp0s3 enp0s8; do
      node_ip=$(ifconfig "$interface" 2>/dev/null | awk '/inet / {print $2}' | sed 's/addr://')
      [[ -n "$node_ip" ]] && break
    done
  fi
  
  # If still no IP, try getting from kubectl node info
  if [[ -z "$node_ip" ]]; then
    node_ip=$(kubectl get nodes -o wide --no-headers 2>/dev/null | awk '{print $6}' | head -1)
  fi
  
  if [[ -z "$node_ip" ]]; then
    log_warning "Could not determine node IP address - using hostname for node identification"
    local node_name=$(kubectl get nodes --no-headers -o custom-columns=NAME:.metadata.name 2>/dev/null | head -1)
  else
    # Get the node name using IP address
    local jsonpath="{.items[?(@.status.addresses[0].address == \"${node_ip}\")].metadata.name}"
    local node_name=$(kubectl get nodes -o jsonpath="$jsonpath" 2>/dev/null)
    
    # If node name lookup by IP failed, try getting first node
    if [[ -z "$node_name" ]]; then
      node_name=$(kubectl get nodes --no-headers -o custom-columns=NAME:.metadata.name 2>/dev/null | head -1)
    fi
  fi
  
  if [[ -z "$node_name" ]]; then
    log_error "Could not determine node name for taint removal"
    log_warning "You may need to manually remove taints: kubectl taint nodes --all node-role.kubernetes.io/control-plane-"
  else
    if [[ "$verbose_mode" == "true" ]]; then
      echo -e "${COLOR_DIM}Node identified: $node_name (IP: ${node_ip:-unknown})${COLOR_RESET}"
      echo -e "${COLOR_DIM}Executing: kubectl taint node ${node_name} node-role.kubernetes.io/control-plane:NoSchedule-${COLOR_RESET}"
    fi
    
    # Remove the taint to allow scheduling on master node
    local taint_output
    if taint_output=$(kubectl taint node "${node_name}" node-role.kubernetes.io/control-plane:NoSchedule- 2>&1); then
      log_success "Master node configured for pod scheduling"
      if [[ "$verbose_mode" == "true" ]] && [[ -n "$taint_output" ]]; then
        echo -e "${COLOR_DIM}Taint removal output: $taint_output${COLOR_RESET}"
      fi
    else
      # Check if taint was already removed
      if [[ "$taint_output" == *"not found"* ]]; then
        log_info "Master node taint already removed - node ready for scheduling"
      else
        log_warning "Master node taint removal failed: $taint_output"
        log_info "Master node may not schedule pods. Manual fix: kubectl taint nodes --all node-role.kubernetes.io/control-plane-"
      fi
    fi
  fi
  
  # Install Calico network plugin (integrated from calicoInst function)
  log_info "Installing Calico network plugin..."
  
  if [[ "$verbose_mode" == "true" ]]; then
    echo -e "${COLOR_DIM}Installing container networking for pod communication...${COLOR_RESET}"
  fi
  
  # Check if Calico is already installed
  if kubectl get pods -n kube-system -l k8s-app=calico-node --no-headers 2>/dev/null | grep -q "Running"; then
    log_info "Calico appears to be already running - skipping installation"
  else
    # Install Calico network plugin
    local calico_version="v3.25.1"
    local calico_manifest="https://raw.githubusercontent.com/projectcalico/calico/$calico_version/manifests/calico.yaml"
    
    log_substep "Downloading Calico manifest (version: $calico_version)..."
    if curl -s --connect-timeout 10 "$calico_manifest" >/dev/null 2>&1; then
      log_substep "Calico manifest is accessible"
    else
      log_warning "Cannot access Calico manifest - check internet connectivity"
      log_warning "Cluster networking may not work properly without CNI plugin"
    fi
    
    log_substep "Applying Calico network plugin..."
    local calico_apply_output
    if [[ "$verbose_mode" == "true" ]]; then
      echo -e "${COLOR_DIM}Executing: kubectl create -f $calico_manifest${COLOR_RESET}"
      if calico_apply_output=$(kubectl create -f "$calico_manifest" 2>&1); then
        log_success "Calico manifest applied successfully"
        echo -e "${COLOR_DIM}$calico_apply_output${COLOR_RESET}"
      else
        log_warning "Failed to apply Calico manifest"
        echo -e "${COLOR_YELLOW}Error details: $calico_apply_output${COLOR_RESET}"
        log_warning "You may need to install a network plugin manually"
      fi
    else
      if calico_apply_output=$(kubectl create -f "$calico_manifest" 2>&1); then
        log_success "Calico manifest applied successfully"
      else
        log_warning "Failed to apply Calico manifest"
        log_warning "You may need to install a network plugin manually"
      fi
    fi
    
    # Wait briefly for Calico pods to start
    log_substep "Waiting for Calico pods to start..."
    sleep 15
    
    # Get Calico pod status more reliably
    local pod_output
    pod_output=$(kubectl get pods -n kube-system -l k8s-app=calico-node --no-headers 2>/dev/null || true)
    
    local ready_pods=0
    local total_pods=0
    
    if [[ -n "$pod_output" ]]; then
      total_pods=$(echo "$pod_output" | wc -l)
      ready_pods=$(echo "$pod_output" | grep -c "Running" || echo "0")
      
      # Ensure we have valid integers
      [[ "$total_pods" =~ ^[0-9]+$ ]] || total_pods=0
      [[ "$ready_pods" =~ ^[0-9]+$ ]] || ready_pods=0
    fi
    
    if [[ "$ready_pods" -gt 0 ]]; then
      log_success "Calico network plugin is starting ($ready_pods/$total_pods pods running)"
    else
      log_info "Calico pods are initializing (this may take 1-2 minutes)"
    fi
  fi
  
  # Validate cluster is running
  log_info "Validating cluster status..."
  sleep 10  # Give cluster time to start
  
  if [[ "$verbose_mode" == "true" ]]; then
    echo -e "${COLOR_DIM}Checking cluster connectivity and status...${COLOR_RESET}"
    if kubectl cluster-info; then
      log_success "Kubernetes cluster is running and accessible"
      echo -e "${COLOR_DIM}Checking node status:${COLOR_RESET}"
      kubectl get nodes
      echo -e "${COLOR_DIM}Checking system pods:${COLOR_RESET}"
      kubectl get pods -n kube-system
      show_kubernetes_master_next_steps
    else
      log_error "Cluster validation failed"
      return 1
    fi
  else
    if kubectl cluster-info >/dev/null 2>&1; then
      log_success "Kubernetes cluster is running and accessible"
      show_kubernetes_master_next_steps
    else
      log_error "Cluster validation failed"
      return 1
    fi
  fi
}

# Show comprehensive Kubernetes installation summary
k8sSummary() {
  local verbose_mode="${1:-false}"
  
  echo -e "${COLOR_BRIGHT_BLUE}${COLOR_BOLD}"
  echo "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—"
  echo "â•‘                    KUBERNETES INSTALLATION SUMMARY                          â•‘"
  echo "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
  echo -e "${COLOR_RESET}"
  echo ""
  
  # Check if we can connect to cluster
  local cluster_accessible=false
  if kubectl cluster-info >/dev/null 2>&1; then
    cluster_accessible=true
  fi
  
  # 1. System Components Status
  log_step "1" "System Components"
  
  # Check component versions
  local kubectl_version=$(kubectl version --client 2>/dev/null | grep -oE 'v[0-9]+\.[0-9]+\.[0-9]+' | head -1 || echo "not-installed")
  local kubeadm_version=$(kubeadm version -o short 2>/dev/null || echo "not-installed")
  local kubelet_version=$(kubelet --version 2>/dev/null | grep -oE 'v[0-9]+\.[0-9]+\.[0-9]+' || echo "not-installed")
  local docker_version=$(docker --version 2>/dev/null | grep -oE '[0-9]+\.[0-9]+\.[0-9]+' | head -1 || echo "not-installed")
  
  if [[ "$kubectl_version" != "not-installed" ]]; then
    echo -e "  ${COLOR_GREEN}âœ“ kubectl: ${kubectl_version}${COLOR_RESET}"
  else
    echo -e "  ${COLOR_RED}âœ— kubectl: not installed${COLOR_RESET}"
  fi
  
  if [[ "$kubeadm_version" != "not-installed" ]]; then
    echo -e "  ${COLOR_GREEN}âœ“ kubeadm: ${kubeadm_version}${COLOR_RESET}"
  else
    echo -e "  ${COLOR_RED}âœ— kubeadm: not installed${COLOR_RESET}"
  fi
  
  if [[ "$kubelet_version" != "not-installed" ]]; then
    echo -e "  ${COLOR_GREEN}âœ“ kubelet: ${kubelet_version}${COLOR_RESET}"
  else
    echo -e "  ${COLOR_RED}âœ— kubelet: not installed${COLOR_RESET}"
  fi
  
  if [[ "$docker_version" != "not-installed" ]]; then
    echo -e "  ${COLOR_GREEN}âœ“ docker: ${docker_version}${COLOR_RESET}"
  else
    echo -e "  ${COLOR_RED}âœ— docker: not installed${COLOR_RESET}"
  fi
  
  echo ""
  
  # 2. Cluster Status
  log_step "2" "Cluster Status"
  
  if [[ "$cluster_accessible" == "true" ]]; then
    echo -e "  ${COLOR_GREEN}âœ“ Cluster: accessible${COLOR_RESET}"
    
    # Show cluster info
    local cluster_info=$(kubectl cluster-info 2>/dev/null)
    if [[ -n "$cluster_info" ]]; then
      echo -e "    ${COLOR_DIM}$(echo "$cluster_info" | head -2)${COLOR_RESET}"
    fi
    
    # Node status
    echo ""
    log_substep "Node Status:"
    if kubectl get nodes --no-headers 2>/dev/null | while read -r line; do
      local node_name=$(echo "$line" | awk '{print $1}')
      local node_status=$(echo "$line" | awk '{print $2}')
      local node_roles=$(echo "$line" | awk '{print $3}')
      local node_age=$(echo "$line" | awk '{print $4}')
      local node_version=$(echo "$line" | awk '{print $5}')
      
      if [[ "$node_status" == "Ready" ]]; then
        echo -e "    ${COLOR_GREEN}âœ“ ${node_name} (${node_roles}): ${node_status} - ${node_version} (${node_age})${COLOR_RESET}"
      else
        echo -e "    ${COLOR_YELLOW}âš  ${node_name} (${node_roles}): ${node_status} - ${node_version} (${node_age})${COLOR_RESET}"
      fi
    done; then
      :
    else
      echo -e "    ${COLOR_RED}âœ— Unable to get node status${COLOR_RESET}"
    fi
    
  else
    echo -e "  ${COLOR_RED}âœ— Cluster: not accessible${COLOR_RESET}"
    echo -e "    ${COLOR_DIM}Run: gok k8sInst to install Kubernetes${COLOR_RESET}"
  fi
  
  echo ""
  
  # 3. System Pods Status
  if [[ "$cluster_accessible" == "true" ]]; then
    log_step "3" "System Pods"
    
    # Check critical system pods
    local system_pods=$(kubectl get pods -n kube-system --no-headers 2>/dev/null)
    if [[ -n "$system_pods" ]]; then
      echo "$system_pods" | while read -r line; do
        local pod_name=$(echo "$line" | awk '{print $1}')
        local ready_status=$(echo "$line" | awk '{print $2}')
        local pod_status=$(echo "$line" | awk '{print $3}')
        local restarts=$(echo "$line" | awk '{print $4}')
        local age=$(echo "$line" | awk '{print $5}')
        
        if [[ "$pod_status" == "Running" ]]; then
          echo -e "    ${COLOR_GREEN}âœ“ ${pod_name}: ${pod_status} (${ready_status}) - ${age}${COLOR_RESET}"
        elif [[ "$pod_status" == "Pending" ]]; then
          echo -e "    ${COLOR_YELLOW}âš  ${pod_name}: ${pod_status} (${ready_status}) - ${age}${COLOR_RESET}"
        else
          echo -e "    ${COLOR_RED}âœ— ${pod_name}: ${pod_status} (${ready_status}) - ${age}${COLOR_RESET}"
        fi
      done
    else
      echo -e "    ${COLOR_RED}âœ— Unable to get system pods status${COLOR_RESET}"
    fi
    
    echo ""
  fi
  
  # 4. Network Configuration
  log_step "4" "Network Configuration"
  
  if [[ "$cluster_accessible" == "true" ]]; then
    # Check for Calico
    local calico_pods=$(kubectl get pods -n kube-system -l k8s-app=calico-node --no-headers 2>/dev/null | wc -l || echo "0")
    if [[ "$calico_pods" -gt 0 ]]; then
      local calico_running=$(kubectl get pods -n kube-system -l k8s-app=calico-node --no-headers 2>/dev/null | grep "Running" | wc -l || echo "0")
      echo -e "  ${COLOR_GREEN}âœ“ Calico CNI: installed (${calico_running}/${calico_pods} pods running)${COLOR_RESET}"
    else
      echo -e "  ${COLOR_RED}âœ— Calico CNI: not installed${COLOR_RESET}"
    fi
    
    # Check CoreDNS custom configuration
    local coredns_config=$(kubectl get configmap coredns -n kube-system -o jsonpath='{.data.Corefile}' 2>/dev/null)
    if [[ -n "$coredns_config" ]] && echo "$coredns_config" | grep -q "cloud.com\|gokcloud.com"; then
      echo -e "  ${COLOR_GREEN}âœ“ Custom DNS: configured${COLOR_RESET}"
      echo -e "    ${COLOR_DIM}â€¢ CoreDNS custom zones: cloud.com, gokcloud.com â†’ ${MASTER_HOST_IP:-<master-ip>}${COLOR_RESET}"
      echo -e "    ${COLOR_DIM}â€¢ Kubernetes internal DNS: cloud.uat domain with 30s TTL${COLOR_RESET}"
    else
      echo -e "  ${COLOR_YELLOW}âš  Custom DNS: default CoreDNS configuration${COLOR_RESET}"
    fi
  else
    echo -e "  ${COLOR_RED}âœ— Network status: cluster not accessible${COLOR_RESET}"
  fi
  
  echo ""
  
  # 5. Utility Pods
  log_step "5" "Utility Pods"
  
  if [[ "$cluster_accessible" == "true" ]]; then
    # Check dnsutils pod
    if kubectl get pod dnsutils -n default >/dev/null 2>&1; then
      local dns_status=$(kubectl get pod dnsutils -n default --no-headers 2>/dev/null | awk '{print $3}')
      if [[ "$dns_status" == "Running" ]]; then
        echo -e "  ${COLOR_GREEN}âœ“ DNS utilities: available${COLOR_RESET}"
        echo -e "    ${COLOR_DIM}â€¢ Pod: dnsutils (jessie-dnsutils:1.3) in default namespace${COLOR_RESET}"
        echo -e "    ${COLOR_DIM}â€¢ Usage: gok checkDns <domain> for DNS resolution testing${COLOR_RESET}"
      else
        echo -e "  ${COLOR_YELLOW}âš  DNS utilities: ${dns_status}${COLOR_RESET}"
      fi
    else
      echo -e "  ${COLOR_RED}âœ— DNS utilities: not installed${COLOR_RESET}"
    fi
    
    # Check curl pod
    if kubectl get pod curl -n default >/dev/null 2>&1; then
      local curl_status=$(kubectl get pod curl -n default --no-headers 2>/dev/null | awk '{print $3}')
      if [[ "$curl_status" == "Running" ]]; then
        echo -e "  ${COLOR_GREEN}âœ“ Curl utility: available${COLOR_RESET}"
        echo -e "    ${COLOR_DIM}â€¢ Pod: curl (curlimages/curl) in default namespace${COLOR_RESET}"
        echo -e "    ${COLOR_DIM}â€¢ Usage: gok checkCurl <url> for HTTP testing within cluster${COLOR_RESET}"
      else
        echo -e "  ${COLOR_YELLOW}âš  Curl utility: ${curl_status}${COLOR_RESET}"
      fi
    else
      echo -e "  ${COLOR_RED}âœ— Curl utility: not installed${COLOR_RESET}"
    fi
  else
    echo -e "  ${COLOR_RED}âœ— Utility pods: cluster not accessible${COLOR_RESET}"
  fi
  
  echo ""
  
  # 6. RBAC Configuration
  log_step "6" "RBAC Configuration"
  
  if [[ "$cluster_accessible" == "true" ]]; then
    # Check OAuth admin role binding
    if kubectl get clusterrolebinding oauth-cluster-admin >/dev/null 2>&1; then
      echo -e "  ${COLOR_GREEN}âœ“ OAuth admin: configured${COLOR_RESET}"
      echo -e "    ${COLOR_DIM}â€¢ ClusterRoleBinding: oauth-cluster-admin${COLOR_RESET}"
      echo -e "    ${COLOR_DIM}â€¢ Role: cluster-admin (full cluster access)${COLOR_RESET}"
      echo -e "    ${COLOR_DIM}â€¢ Subject: Group 'administrators' (OAuth group mapping)${COLOR_RESET}"
    else
      echo -e "  ${COLOR_RED}âœ— OAuth admin: not configured${COLOR_RESET}"
    fi
  else
    echo -e "  ${COLOR_RED}âœ— RBAC status: cluster not accessible${COLOR_RESET}"
  fi
  
  echo ""
  
  # 7. Next Steps
  log_step "7" "Available Commands"
  
  echo -e "  ${COLOR_CYAN}Cluster Management:${COLOR_RESET}"
  echo -e "    ${COLOR_DIM}gok k8sInst                     # Install/reinstall Kubernetes${COLOR_RESET}"
  echo -e "    ${COLOR_DIM}gok k8sSummary                  # Show installation summary (this command)${COLOR_RESET}"
  echo -e "    ${COLOR_DIM}gok calicoInst                  # Install Calico network plugin${COLOR_RESET}"
  echo -e "    ${COLOR_DIM}gok k8sReset                    # Reset cluster${COLOR_RESET}"
  
  echo -e "  ${COLOR_CYAN}Utilities:${COLOR_RESET}"
  echo -e "    ${COLOR_DIM}gok dnsUtils                    # Install DNS testing utilities${COLOR_RESET}"
  echo -e "    ${COLOR_DIM}gok kcurl                       # Install curl testing utilities${COLOR_RESET}"
  echo -e "    ${COLOR_DIM}gok checkDns <domain>           # Test DNS resolution${COLOR_RESET}"
  echo -e "    ${COLOR_DIM}gok checkCurl <url>             # Test HTTP connectivity${COLOR_RESET}"
  
  echo -e "  ${COLOR_CYAN}Configuration:${COLOR_RESET}"
  echo -e "    ${COLOR_DIM}gok customDns                   # Configure custom DNS zones${COLOR_RESET}"
  echo -e "    ${COLOR_DIM}gok oauthAdmin                  # Configure OAuth admin access${COLOR_RESET}"
  
  echo ""
  echo -e "${COLOR_BRIGHT_GREEN}${COLOR_BOLD}Summary complete! Use the commands above to manage your cluster.${COLOR_RESET}"
}

# Setup Kubernetes worker node
setup_kubernetes_worker() {
  local verbose_mode="${1:-false}"
  log_step "6" "Setting up Kubernetes worker node"
  
  log_info "Setting up CA certificates..."
  if [ -f "/export/certs/issuer.crt" ]; then
    sudo cp /export/certs/issuer.crt /usr/local/share/ca-certificates/issuer.crt
    sudo update-ca-certificates >/dev/null 2>&1
    log_success "CA certificates updated"
  else
    log_warning "CA certificate not found at /export/certs/issuer.crt"
  fi
  
  # Disable swap
  log_info "Disabling swap for Kubernetes..."
  sudo sed -i '/ swap / s/^\(.*\)$/#\1/g' /etc/fstab
  sudo swapoff -a
  log_success "Swap disabled successfully"
  
  show_kubernetes_worker_next_steps
}

# Show next steps after successful Kubernetes master installation
show_kubernetes_master_next_steps() {
  echo
  log_header "Kubernetes Master Ready" "Next Steps & Information"
  
  echo -e "${COLOR_BRIGHT_GREEN}${COLOR_BOLD}ðŸŽ‰ KUBERNETES MASTER NODE READY${COLOR_RESET}"
  echo
  
  # Cluster information
  echo -e "${COLOR_YELLOW}${COLOR_BOLD}ðŸ“Š Cluster Information:${COLOR_RESET}"
  local cluster_info=$(kubectl cluster-info 2>/dev/null | head -2)
  echo "$cluster_info" | while read line; do
    echo -e "  ${COLOR_GREEN}âœ“${COLOR_RESET} $line"
  done
  echo
  
  # Node status
  echo -e "${COLOR_YELLOW}${COLOR_BOLD}ðŸ–¥ï¸  Node Status:${COLOR_RESET}"
  local node_info=$(kubectl get nodes --no-headers 2>/dev/null | head -1)
  if [ -n "$node_info" ]; then
    local node_name=$(echo "$node_info" | awk '{print $1}')
    local node_status=$(echo "$node_info" | awk '{print $2}')
    local node_roles=$(echo "$node_info" | awk '{print $3}')
    echo -e "  ${COLOR_GREEN}âœ“${COLOR_RESET} Node: ${COLOR_BOLD}$node_name${COLOR_RESET}"
    echo -e "  ${COLOR_GREEN}âœ“${COLOR_RESET} Status: ${COLOR_BOLD}$node_status${COLOR_RESET}"
    echo -e "  ${COLOR_GREEN}âœ“${COLOR_RESET} Roles: ${COLOR_BOLD}$node_roles${COLOR_RESET}"
  fi
  echo
  
  # Get join command for worker nodes
  echo -e "${COLOR_YELLOW}${COLOR_BOLD}ðŸ”— Worker Node Join Command:${COLOR_RESET}"
  local join_cmd=$(kubeadm token create --print-join-command 2>/dev/null)
  if [ -n "$join_cmd" ]; then
    echo -e "${COLOR_CYAN}$join_cmd${COLOR_RESET}"
    echo -e "${COLOR_DIM}(Use this command on worker nodes to join the cluster)${COLOR_RESET}"
  else
    echo -e "${COLOR_YELLOW}Run: ${COLOR_BOLD}kubeadm token create --print-join-command${COLOR_RESET}"
  fi
  echo
  
  # Critical next steps
  echo -e "${COLOR_BRIGHT_YELLOW}${COLOR_BOLD}ðŸš€ CRITICAL NEXT STEPS:${COLOR_RESET}"
  echo
  
  echo -e "${COLOR_RED}${COLOR_BOLD}1. Install Network Plugin (REQUIRED):${COLOR_RESET}"
  echo -e "   ${COLOR_CYAN}The cluster needs a network plugin to function properly.${COLOR_RESET}"
  echo -e "   ${COLOR_GREEN}Run: ${COLOR_BOLD}calicoInst${COLOR_RESET} ${COLOR_DIM}# Install Calico networking${COLOR_RESET}"
  echo
  
  echo -e "${COLOR_YELLOW}${COLOR_BOLD}2. Install Base Services (RECOMMENDED):${COLOR_RESET}"
  echo -e "   ${COLOR_CYAN}Install essential cluster services and management tools.${COLOR_RESET}"
  echo -e "   ${COLOR_GREEN}Run: ${COLOR_BOLD}./gok install base-services${COLOR_RESET}"
  echo -e "   ${COLOR_DIM}   Includes: cert-manager, ingress-nginx, monitoring, vault${COLOR_RESET}"
  echo
  
  echo -e "${COLOR_YELLOW}${COLOR_BOLD}3. Verify Installation:${COLOR_RESET}"
  echo -e "   ${COLOR_GREEN}kubectl get nodes${COLOR_RESET}                    ${COLOR_DIM}# Check node status${COLOR_RESET}"
  echo -e "   ${COLOR_GREEN}kubectl get pods -A${COLOR_RESET}                  ${COLOR_DIM}# Check all pods${COLOR_RESET}"
  echo -e "   ${COLOR_GREEN}kubectl cluster-info${COLOR_RESET}                 ${COLOR_DIM}# Cluster information${COLOR_RESET}"
  echo
  
  echo -e "${COLOR_BRIGHT_CYAN}${COLOR_BOLD}ðŸŽ¯ RECOMMENDED INSTALLATION SEQUENCE:${COLOR_RESET}"
  echo -e "   ${COLOR_GREEN}1.${COLOR_RESET} ${COLOR_BOLD}calicoInst${COLOR_RESET}                        ${COLOR_DIM}# Network plugin (essential)${COLOR_RESET}"
  echo -e "   ${COLOR_GREEN}2.${COLOR_RESET} ${COLOR_BOLD}./gok install base-services${COLOR_RESET}       ${COLOR_DIM}# Core platform services${COLOR_RESET}"
  echo -e "   ${COLOR_GREEN}3.${COLOR_RESET} ${COLOR_BOLD}./gok install dashboard${COLOR_RESET}           ${COLOR_DIM}# Web dashboard${COLOR_RESET}"
  echo -e "   ${COLOR_GREEN}4.${COLOR_RESET} ${COLOR_BOLD}./gok install argocd${COLOR_RESET}              ${COLOR_DIM}# GitOps deployment${COLOR_RESET}"
  echo
  
  echo -e "${COLOR_BRIGHT_CYAN}${COLOR_BOLD}ðŸ“š USEFUL COMMANDS:${COLOR_RESET}"
  echo -e "   ${COLOR_GREEN}./gok status${COLOR_RESET}                         ${COLOR_DIM}# Check platform status${COLOR_RESET}"
  echo -e "   ${COLOR_GREEN}./gok install help${COLOR_RESET}                   ${COLOR_DIM}# See all installable components${COLOR_RESET}"
  echo -e "   ${COLOR_GREEN}kubectl get namespaces${COLOR_RESET}               ${COLOR_DIM}# List all namespaces${COLOR_RESET}"
  echo
}

# Show next steps for worker node setup
show_kubernetes_worker_next_steps() {
  echo
  log_header "Kubernetes Worker Setup" "Next Steps & Information"
  
  echo -e "${COLOR_BRIGHT_BLUE}${COLOR_BOLD}ðŸ”§ KUBERNETES WORKER NODE PREPARED${COLOR_RESET}"
  echo
  
  echo -e "${COLOR_YELLOW}${COLOR_BOLD}ðŸ“‹ Required Actions:${COLOR_RESET}"
  echo
  
  echo -e "${COLOR_RED}${COLOR_BOLD}1. Join the Cluster:${COLOR_RESET}"
  echo -e "   ${COLOR_CYAN}Get the join command from your master node:${COLOR_RESET}"
  echo -e "   ${COLOR_GREEN}kubeadm token create --print-join-command${COLOR_RESET}"
  echo -e "   ${COLOR_CYAN}Then run the output command on this worker node.${COLOR_RESET}"
  echo
  
  echo -e "${COLOR_YELLOW}${COLOR_BOLD}2. Label the Node (Optional):${COLOR_RESET}"
  echo -e "   ${COLOR_CYAN}After joining, label this node as a worker:${COLOR_RESET}"
  echo -e "   ${COLOR_GREEN}kubectl label node <node-name> node-role.kubernetes.io/worker=worker${COLOR_RESET}"
  echo
  
  echo -e "${COLOR_YELLOW}${COLOR_BOLD}3. Reboot System:${COLOR_RESET}"
  echo -e "   ${COLOR_CYAN}Reboot to ensure all changes take effect:${COLOR_RESET}"
  echo -e "   ${COLOR_GREEN}sudo reboot${COLOR_RESET}"
  echo
  
  echo -e "${COLOR_BRIGHT_CYAN}${COLOR_BOLD}ðŸ” VERIFICATION:${COLOR_RESET}"
  echo -e "   ${COLOR_CYAN}From the master node, verify this worker joined:${COLOR_RESET}"
  echo -e "   ${COLOR_GREEN}kubectl get nodes${COLOR_RESET}"
  echo
}

# Show comprehensive next steps after complete Kubernetes installation
show_kubernetes_complete_next_steps() {
  echo
  log_header "Kubernetes Platform Ready" "Complete Installation Next Steps"
  
  echo -e "${COLOR_BRIGHT_GREEN}${COLOR_BOLD}ðŸŽ‰ KUBERNETES PLATFORM INSTALLATION COMPLETE${COLOR_RESET}"
  echo
  
  # Cluster status overview
  echo -e "${COLOR_YELLOW}${COLOR_BOLD}ðŸ“Š Cluster Status:${COLOR_RESET}"
  local cluster_status=$(kubectl cluster-info 2>/dev/null | head -1 | grep -o "is running at.*" || echo "Status check failed")
  local node_status=$(kubectl get nodes --no-headers 2>/dev/null | head -1)
  
  if [ -n "$node_status" ]; then
    local node_name=$(echo "$node_status" | awk '{print $1}')
    local node_ready=$(echo "$node_status" | awk '{print $2}')
    echo -e "  ${COLOR_GREEN}âœ“${COLOR_RESET} Cluster: $cluster_status"
    echo -e "  ${COLOR_GREEN}âœ“${COLOR_RESET} Master Node: $node_name ($node_ready)"
  fi
  
  # Installed components
  echo -e "  ${COLOR_GREEN}âœ“${COLOR_RESET} Container Runtime: containerd"
  echo -e "  ${COLOR_GREEN}âœ“${COLOR_RESET} Network Plugin: Calico"
  echo -e "  ${COLOR_GREEN}âœ“${COLOR_RESET} Package Manager: Helm"
  echo
  
  # Critical next steps
  echo -e "${COLOR_BRIGHT_YELLOW}${COLOR_BOLD}ðŸš€ CRITICAL NEXT STEP:${COLOR_RESET}"
  echo
  
  echo -e "${COLOR_RED}${COLOR_BOLD}Install Base Services (HIGHLY RECOMMENDED):${COLOR_RESET}"
  echo -e "   ${COLOR_CYAN}Your cluster is ready, but needs essential services for production use.${COLOR_RESET}"
  echo
  
  echo -e "   ${COLOR_BRIGHT_GREEN}${COLOR_BOLD}Run this command to install all essential services:${COLOR_RESET}"
  echo -e "   ${COLOR_GREEN}${COLOR_BOLD}./gok install base-services${COLOR_RESET}"
  echo
  
  echo -e "   ${COLOR_YELLOW}Base services include:${COLOR_RESET}"
  echo -e "   ${COLOR_DIM}â€¢ Certificate Manager (cert-manager) - TLS/SSL automation${COLOR_RESET}"
  echo -e "   ${COLOR_DIM}â€¢ NGINX Ingress Controller - HTTP/HTTPS routing${COLOR_RESET}"
  echo -e "   ${COLOR_DIM}â€¢ Monitoring Stack - Prometheus & Grafana${COLOR_RESET}"
  echo -e "   ${COLOR_DIM}â€¢ Secrets Management - HashiCorp Vault${COLOR_RESET}"
  echo -e "   ${COLOR_DIM}â€¢ Identity Management - Keycloak${COLOR_RESET}"
  echo
  
  echo -e "${COLOR_BRIGHT_CYAN}${COLOR_BOLD}ðŸŽ¯ COMPLETE SETUP SEQUENCE:${COLOR_RESET}"
  echo -e "   ${COLOR_GREEN}${COLOR_BOLD}1. ./gok install base-services${COLOR_RESET}       ${COLOR_DIM}# Essential platform services${COLOR_RESET}"
  echo -e "   ${COLOR_GREEN}2. ./gok install dashboard${COLOR_RESET}           ${COLOR_DIM}# Kubernetes web dashboard${COLOR_RESET}"
  echo -e "   ${COLOR_GREEN}3. ./gok install argocd${COLOR_RESET}              ${COLOR_DIM}# GitOps continuous delivery${COLOR_RESET}"
  echo -e "   ${COLOR_GREEN}4. ./gok install jupyter${COLOR_RESET}             ${COLOR_DIM}# Data science environment${COLOR_RESET}"
  echo
  
  echo -e "${COLOR_BRIGHT_CYAN}${COLOR_BOLD}ðŸ” VERIFICATION COMMANDS:${COLOR_RESET}"
  echo -e "   ${COLOR_GREEN}kubectl get nodes${COLOR_RESET}                    ${COLOR_DIM}# Check node status${COLOR_RESET}"
  echo -e "   ${COLOR_GREEN}kubectl get pods -A${COLOR_RESET}                  ${COLOR_DIM}# Check all pods${COLOR_RESET}"
  echo -e "   ${COLOR_GREEN}kubectl get svc -A${COLOR_RESET}                   ${COLOR_DIM}# Check all services${COLOR_RESET}"
  echo -e "   ${COLOR_GREEN}./gok status${COLOR_RESET}                         ${COLOR_DIM}# GOK platform status${COLOR_RESET}"
  echo
  
  echo -e "${COLOR_BRIGHT_CYAN}${COLOR_BOLD}ðŸ“š USEFUL RESOURCES:${COLOR_RESET}"
  echo -e "   ${COLOR_GREEN}./gok install help${COLOR_RESET}                   ${COLOR_DIM}# See all installable components${COLOR_RESET}"
  echo -e "   ${COLOR_GREEN}./gok create help${COLOR_RESET}                    ${COLOR_DIM}# Create certificates, secrets${COLOR_RESET}"
  echo -e "   ${COLOR_GREEN}./gok generate help${COLOR_RESET}                  ${COLOR_DIM}# Generate microservices${COLOR_RESET}"
  echo
  
  echo -e "${COLOR_BRIGHT_YELLOW}${COLOR_BOLD}âš¡ QUICK START:${COLOR_RESET}"
  echo -e "   ${COLOR_CYAN}To get a fully functional production cluster:${COLOR_RESET}"
  echo -e "   ${COLOR_BOLD}./gok install base-services && ./gok install dashboard${COLOR_RESET}"
  echo
  
  # Wait message
  figlet "Ready for base-services!" 2>/dev/null || echo -e "${COLOR_BRIGHT_GREEN}${COLOR_BOLD}ðŸš€ Ready for base-services installation! ðŸš€${COLOR_RESET}"
}

calicoInst(){
  local verbose_mode="${GOK_VERBOSE:-false}"
  
  # Check for verbose flags in all arguments
  for arg in "$@"; do
    if [[ "$arg" == "--verbose" ]] || [[ "$arg" == "-v" ]]; then
      verbose_mode="true"
      break
    fi
  done
  
  # Also check if GOK_VERBOSE environment variable is set
  if [[ "${GOK_VERBOSE}" == "true" ]]; then
    verbose_mode="true"
  fi
  
  log_header "Calico Network Plugin" "Installing Container Networking"
  
  echo -e "${COLOR_BRIGHT_CYAN}${COLOR_BOLD}ðŸŒ CALICO NETWORK PLUGIN INSTALLATION${COLOR_RESET}"
  if [[ "$verbose_mode" == "true" ]]; then
    echo -e "${COLOR_DIM}Verbose mode: Enabled${COLOR_RESET}"
  fi
  echo
  
  # Step 1: Validate cluster is ready
  log_step "1" "Validating Kubernetes cluster readiness"
  
  if ! kubectl cluster-info >/dev/null 2>&1; then
    log_error "Kubernetes cluster is not accessible"
    log_error "Please ensure Kubernetes master is running and kubectl is configured"
    return 1
  fi
  log_success "Kubernetes cluster is accessible"
  
  # Check if nodes are ready (they won't be without networking)
  local node_count=$(kubectl get nodes --no-headers 2>/dev/null | wc -l)
  log_info "Found $node_count node(s) in cluster"
  
  # Step 2: Check if Calico is already installed
  log_step "2" "Checking for existing network plugins"
  
  if kubectl get pods -n kube-system -l k8s-app=calico-node --no-headers 2>/dev/null | grep -q "Running"; then
    log_warning "Calico appears to be already running"
    echo -e "  ${COLOR_YELLOW}Use: ${COLOR_BOLD}kubectl get pods -n kube-system -l k8s-app=calico-node${COLOR_RESET} to verify"
    return 0
  fi
  
  # Check for other CNI plugins
  if kubectl get pods -n kube-system --no-headers 2>/dev/null | grep -qE "flannel|weave|cilium"; then
    log_warning "Another CNI plugin may already be installed"
    echo -e "  ${COLOR_YELLOW}Check with: ${COLOR_BOLD}kubectl get pods -n kube-system${COLOR_RESET}"
  fi
  
  # Step 3: Download and apply Calico manifest
  log_step "3" "Installing Calico network plugin"
  
  local calico_version="v3.25.1"
  local calico_manifest="https://raw.githubusercontent.com/projectcalico/calico/$calico_version/manifests/calico.yaml"
  
  log_info "Downloading Calico manifest (version: $calico_version)..."
  if curl -s --connect-timeout 10 "$calico_manifest" >/dev/null 2>&1; then
    log_success "Calico manifest is accessible"
  else
    log_error "Cannot access Calico manifest - check internet connectivity"
    return 1
  fi
  
  log_info "Applying Calico network plugin..."
  local calico_apply_output
  if [[ "$verbose_mode" == "true" ]]; then
    echo -e "${COLOR_DIM}Executing: kubectl create -f $calico_manifest${COLOR_RESET}"
    if calico_apply_output=$(kubectl create -f "$calico_manifest" 2>&1); then
      log_success "Calico manifest applied successfully"
      echo -e "${COLOR_DIM}$calico_apply_output${COLOR_RESET}"
    else
      log_error "Failed to apply Calico manifest"
      echo -e "${COLOR_RED}Error details: $calico_apply_output${COLOR_RESET}"
      echo -e "${COLOR_YELLOW}Troubleshooting:${COLOR_RESET}"
      echo -e "  ${COLOR_CYAN}kubectl get nodes${COLOR_RESET} - Check node status"
      echo -e "  ${COLOR_CYAN}kubectl get pods -n kube-system${COLOR_RESET} - Check system pods"
      echo -e "  ${COLOR_CYAN}kubectl describe nodes${COLOR_RESET} - Check node details"
      return 1
    fi
  else
    if calico_apply_output=$(kubectl create -f "$calico_manifest" 2>&1); then
      log_success "Calico manifest applied successfully"
    else
      log_error "Failed to apply Calico manifest"
      echo -e "${COLOR_RED}Error details: $calico_apply_output${COLOR_RESET}"
      echo -e "${COLOR_YELLOW}Troubleshooting:${COLOR_RESET}"
      echo -e "  ${COLOR_CYAN}kubectl get nodes${COLOR_RESET} - Check node status"
      echo -e "  ${COLOR_CYAN}kubectl get pods -n kube-system${COLOR_RESET} - Check system pods"
      echo -e "  ${COLOR_CYAN}kubectl describe nodes${COLOR_RESET} - Check node details"
      return 1
    fi
  fi
  
  # Step 4: Wait for Calico pods to be ready
  log_step "4" "Waiting for Calico pods to become ready"
  
  log_info "Waiting for Calico system pods (this may take 2-3 minutes)..."
  local timeout=180  # 3 minutes
  local elapsed=0
  
  while [ $elapsed -lt $timeout ]; do
    local ready_pods=$(kubectl get pods -n kube-system -l k8s-app=calico-node --no-headers 2>/dev/null | grep -c "Running" || echo "0")
    local total_pods=$(kubectl get pods -n kube-system -l k8s-app=calico-node --no-headers 2>/dev/null | wc -l || echo "0")
    
    if [ "$ready_pods" -gt 0 ] && [ "$ready_pods" -eq "$total_pods" ]; then
      log_success "All Calico pods are running ($ready_pods/$total_pods)"
      break
    fi
    
    if [ $((elapsed % 30)) -eq 0 ]; then
      log_info "Calico pods status: $ready_pods/$total_pods ready (${elapsed}s elapsed)"
    fi
    
    sleep 5
    elapsed=$((elapsed + 5))
  done
  
  if [ $elapsed -ge $timeout ]; then
    log_warning "Calico pods took longer than expected to start"
    echo "  Current status:"
    kubectl get pods -n kube-system -l k8s-app=calico-node 2>/dev/null | head -5
  fi

echo "DEBUG: Reached line 5000" >&2
  
  # Step 5: Validate network plugin installation
  log_step "5" "Validating network plugin installation"
  
  # Check if nodes are now ready
  sleep 10  # Give nodes time to update status
  local ready_nodes=$(kubectl get nodes --no-headers 2>/dev/null | grep -c "Ready" || echo "0")
  local total_nodes=$(kubectl get nodes --no-headers 2>/dev/null | wc -l || echo "0")
  
  if [ "$ready_nodes" -eq "$total_nodes" ] && [ "$ready_nodes" -gt 0 ]; then
    log_success "All nodes are now Ready ($ready_nodes/$total_nodes)"
  else
    log_warning "Some nodes may not be Ready yet ($ready_nodes/$total_nodes)"
    echo "  This is normal immediately after installation"
  fi
  
  # Show Calico components status
  log_info "Calico components status:"
  kubectl get pods -n kube-system -l k8s-app=calico-node --no-headers 2>/dev/null | while read line; do
    local pod_name=$(echo "$line" | awk '{print $1}')
    local pod_status=$(echo "$line" | awk '{print $3}')
    if [ "$pod_status" = "Running" ]; then
      echo -e "  ${COLOR_GREEN}âœ“${COLOR_RESET} $pod_name"
    else
      echo -e "  ${COLOR_YELLOW}â³${COLOR_RESET} $pod_name ($pod_status)"
    fi
  done
  
  show_calico_next_steps
}

# Show next steps after Calico installation
show_calico_next_steps() {
  echo
  log_header "Calico Network Ready" "Next Steps & Verification"
  
  echo -e "${COLOR_BRIGHT_GREEN}${COLOR_BOLD}ðŸŒ CALICO NETWORK PLUGIN INSTALLED${COLOR_RESET}"
  echo
  
  echo -e "${COLOR_YELLOW}${COLOR_BOLD}ðŸ” Verification Commands:${COLOR_RESET}"
  echo -e "   ${COLOR_GREEN}kubectl get nodes${COLOR_RESET}                    ${COLOR_DIM}# All nodes should be 'Ready'${COLOR_RESET}"
  echo -e "   ${COLOR_GREEN}kubectl get pods -n kube-system${COLOR_RESET}      ${COLOR_DIM}# Check system pods${COLOR_RESET}"
  echo -e "   ${COLOR_GREEN}kubectl get pods -A${COLOR_RESET}                  ${COLOR_DIM}# Check all namespaces${COLOR_RESET}"
  echo
  
  echo -e "${COLOR_BRIGHT_CYAN}${COLOR_BOLD}ðŸš€ RECOMMENDED NEXT STEPS:${COLOR_RESET}"
  echo
  
  echo -e "${COLOR_YELLOW}${COLOR_BOLD}1. Install Base Services:${COLOR_RESET}"
  echo -e "   ${COLOR_CYAN}Install essential cluster services and management tools:${COLOR_RESET}"
  echo -e "   ${COLOR_GREEN}${COLOR_BOLD}./gok install base-services${COLOR_RESET}"
  echo -e "   ${COLOR_DIM}   â€¢ Certificate Manager (TLS automation)${COLOR_RESET}"
  echo -e "   ${COLOR_DIM}   â€¢ NGINX Ingress Controller (HTTP routing)${COLOR_RESET}"
  echo -e "   ${COLOR_DIM}   â€¢ Monitoring Stack (Prometheus & Grafana)${COLOR_RESET}"
  echo -e "   ${COLOR_DIM}   â€¢ Secrets Management (Vault)${COLOR_RESET}"
  echo
  
  echo -e "${COLOR_YELLOW}${COLOR_BOLD}2. Install Web Dashboard:${COLOR_RESET}"
  echo -e "   ${COLOR_GREEN}./gok install dashboard${COLOR_RESET}              ${COLOR_DIM}# Kubernetes web UI${COLOR_RESET}"
  echo
  
  echo -e "${COLOR_YELLOW}${COLOR_BOLD}3. Setup GitOps:${COLOR_RESET}"
  echo -e "   ${COLOR_GREEN}./gok install argocd${COLOR_RESET}                 ${COLOR_DIM}# GitOps continuous delivery${COLOR_RESET}"
  echo
  
  echo -e "${COLOR_BRIGHT_YELLOW}${COLOR_BOLD}âš¡ QUICK START SEQUENCE:${COLOR_RESET}"
  echo -e "   ${COLOR_GREEN}# Install everything needed for a production cluster:${COLOR_RESET}"
  echo -e "   ${COLOR_BOLD}./gok install base-services${COLOR_RESET}           ${COLOR_DIM}# Core services${COLOR_RESET}"
  echo -e "   ${COLOR_BOLD}./gok install dashboard${COLOR_RESET}               ${COLOR_DIM}# Web interface${COLOR_RESET}"
  echo -e "   ${COLOR_BOLD}./gok install argocd${COLOR_RESET}                  ${COLOR_DIM}# GitOps${COLOR_RESET}"
  echo
  
  echo -e "${COLOR_BRIGHT_CYAN}${COLOR_BOLD}ðŸ“Š Platform Status:${COLOR_RESET}"
  echo -e "   ${COLOR_GREEN}./gok status${COLOR_RESET}                         ${COLOR_DIM}# Check overall platform status${COLOR_RESET}"
  echo
}

helmInst() {
  log_header "Helm Package Manager" "Starting Installation"
  
  # Check if Helm is already installed
  if command -v helm >/dev/null 2>&1; then
    local current_version=$(helm version --short --client 2>/dev/null | grep -oE 'v[0-9]+\.[0-9]+\.[0-9]+' || echo "unknown")
    log_info "Helm is already installed (version: ${current_version})"
    
    # Test if Helm works
    if helm version --short >/dev/null 2>&1; then
      log_success "Helm is working correctly"
      show_helm_next_steps
      return 0
    else
      log_warning "Helm installed but not working correctly, attempting reinstall..."
    fi
  fi
  
  log_info "Installing Helm package manager..."
  
  # Method 1: Try snap first (recommended for Ubuntu)
  if command -v snap >/dev/null 2>&1; then
    log_info "Installing Helm via snap package manager..."
    if sudo snap install helm --classic >/dev/null 2>&1; then
      log_success "Helm installed successfully via snap"
      helm version --short
      show_helm_next_steps
      return 0
    else
      log_warning "Snap installation failed, trying official script method..."
    fi
  fi
  
  # Method 2: Official Helm installation script
  log_info "Installing Helm via official installation script..."
  if curl -fsSL -o get_helm.sh https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3 2>/dev/null; then
    chmod 700 get_helm.sh
    if ./get_helm.sh >/dev/null 2>&1; then
      rm -f get_helm.sh
      log_success "Helm installed successfully via official script"
      helm version --short
      show_helm_next_steps
      return 0
    else
      rm -f get_helm.sh
      log_error "Official script installation failed, trying APT method..."
    fi
  else
    log_warning "Could not download Helm installation script"
  fi
  
  # Method 3: APT with updated repository (fallback)
  log_info "Installing Helm via APT package manager..."
  if install_helm_via_apt; then
    log_success "Helm installed successfully via APT"
    helm version --short
    show_helm_next_steps
    return 0
  fi
  
  log_error "All Helm installation methods failed"
  
  # Check for network connectivity issues
  if ! ping -c 1 8.8.8.8 >/dev/null 2>&1; then
    log_error "Network connectivity issue detected"
    echo "  Please check your internet connection and try again"
    echo "  Or install Helm manually when online:"
  else
    log_error "Installation failed despite network connectivity"
    echo "  Please install Helm manually using one of these methods:"
  fi
  
  echo "  1. Snap: sudo snap install helm --classic"
  echo "  2. Script: curl https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3 | bash"
  echo "  3. GitHub Releases: https://github.com/helm/helm/releases"
  echo "  4. Direct binary: wget https://get.helm.sh/helm-v3.14.0-linux-amd64.tar.gz"
  return 1
}

# Helper function for APT installation
install_helm_via_apt() {
  # Clean up any old repositories first
  sudo rm -f /etc/apt/sources.list.d/helm*.list 2>/dev/null
  
  # Check network connectivity first
  if ! curl -s --connect-timeout 5 https://baltocdn.com >/dev/null 2>&1; then
    log_warning "Cannot reach baltocdn.com - network connectivity issue"
    return 1
  fi
  
  # Add official Helm APT repository with proper key management
  if curl -fsSL https://baltocdn.com/helm/signing.asc 2>/dev/null | gpg --dearmor | sudo tee /usr/share/keyrings/helm.gpg > /dev/null 2>&1; then
    echo "deb [arch=$(dpkg --print-architecture) signed-by=/usr/share/keyrings/helm.gpg] https://baltocdn.com/helm/stable/debian/ all main" | sudo tee /etc/apt/sources.list.d/helm-stable-debian.list >/dev/null
    
    if sudo apt-get update >/dev/null 2>&1 && sudo apt-get install -y helm >/dev/null 2>&1; then
      return 0
    fi
  fi
  return 1
}

# Show next steps after Helm installation
show_helm_next_steps() {
  echo
  log_header "Helm Next Steps" "Package Manager Ready"
  
  echo -e "${COLOR_BRIGHT_CYAN}${COLOR_BOLD}ðŸ”§ HELM PACKAGE MANAGER READY${COLOR_RESET}"
  echo
  
  echo -e "${COLOR_YELLOW}${COLOR_BOLD}Common Helm Commands:${COLOR_RESET}"
  echo -e "  ${COLOR_GREEN}helm repo add <name> <url>${COLOR_RESET}     - Add a chart repository"
  echo -e "  ${COLOR_GREEN}helm repo update${COLOR_RESET}               - Update repository information"
  echo -e "  ${COLOR_GREEN}helm search repo <keyword>${COLOR_RESET}     - Search for charts"
  echo -e "  ${COLOR_GREEN}helm install <name> <chart>${COLOR_RESET}    - Install a chart"
  echo -e "  ${COLOR_GREEN}helm list${COLOR_RESET}                      - List installed releases"
  echo
  
  echo -e "${COLOR_YELLOW}${COLOR_BOLD}Popular Chart Repositories:${COLOR_RESET}"
  echo -e "  ${COLOR_CYAN}Bitnami:${COLOR_RESET}        helm repo add bitnami https://charts.bitnami.com/bitnami"
  echo -e "  ${COLOR_CYAN}Jetstack:${COLOR_RESET}       helm repo add jetstack https://charts.jetstack.io"
  echo -e "  ${COLOR_CYAN}Ingress-NGINX:${COLOR_RESET}  helm repo add ingress-nginx https://kubernetes.github.io/ingress-nginx"
  echo -e "  ${COLOR_CYAN}Prometheus:${COLOR_RESET}     helm repo add prometheus-community https://prometheus-community.github.io/helm-charts"
  echo
  
  echo -e "${COLOR_YELLOW}${COLOR_BOLD}GOK Integration:${COLOR_RESET}"
  echo -e "  ${COLOR_GREEN}./gok install cert-manager${COLOR_RESET}    - Install cert-manager via Helm"
  echo -e "  ${COLOR_GREEN}./gok install ingress${COLOR_RESET}         - Install NGINX Ingress via Helm"
  echo -e "  ${COLOR_GREEN}./gok install monitoring${COLOR_RESET}      - Install Prometheus & Grafana"
  echo
}

certmanagerInst() {
  log_component_start "cert-manager" "Installing certificate management system"
  
  log_step "1" "Adding Jetstack Helm repository"
  execute_with_suppression helm repo add jetstack https://charts.jetstack.io
  execute_with_suppression helm repo update
  
  log_step "2" "Installing cert-manager via Helm"
  #kubectl apply -f https://github.com/cert-manager/cert-manager/releases/download/v1.14.5/cert-manager.crds.yaml

  #--set serviceAccount.automountServiceAccountToken=false \
  #--set webhook.timeoutSeconds=30
  #--set startupapicheck.timeout=10m
  # --debug
  if helm_install_with_summary "cert-manager" "cert-manager" \
    cert-manager jetstack/cert-manager \
    --namespace cert-manager \
    --create-namespace \
    --set installCRDs=true \
    --set global.leaderElection.namespace=cert-manager \
    --version v1.14.5 \
    --values https://github.com/sumitmaji/kubernetes/raw/master/install_k8s/cert-manager/values.yaml; then
    
    show_installation_summary "cert-manager" "cert-manager" "TLS certificate management system"
    log_component_success "cert-manager" "Certificate management system installed successfully"
  else
    log_error "cert-manager installation failed"
    return 1
  fi
}

subDomain(){
  if [ -z $1 ]; then
    echo "$(defaultSubdomain)"
  else
    echo "$1"
  fi
}

certificateRequestForNs() {
  NS=$1
  SUBDOMAIN=$(subDomain $2)
  cat <<EOF | kubectl apply -f -
apiVersion: cert-manager.io/v1
kind: Certificate
metadata:
  name: ${SUBDOMAIN}-$(sedRootDomain)-tls
  namespace: ${NS}
spec:
  secretName: ${SUBDOMAIN}-$(sedRootDomain)
  issuerRef:
    name: $(getClusterIssuerName)
    kind: ClusterIssuer
  commonName: ${SUBDOMAIN}.$(rootDomain)
  dnsNames:
    - ${SUBDOMAIN}.$(rootDomain)
  issuerRef:
    name: $(getClusterIssuerName)
    kind: ClusterIssuer
EOF
  echo "Certificate request for NS $NS created, executing below command to know current status"
  echo "kubectl get Issuers,ClusterIssuers,Certificates,CertificateRequests,Orders,Challenges --all-namespaces"
  kubectl get Issuers,ClusterIssuers,Certificates,CertificateRequests,Orders,Challenges --all-namespaces
  kubectl --timeout=10s -n ${NS} wait --for=condition=Ready certificates.cert-manager.io "${SUBDOMAIN}"-"$(sedRootDomain)"-tls
}

getLetsEncEnv(){
  echo "${LETS_ENCRYPT_ENV}"
}

getLetsEncryptUrl(){
  [[ $(getLetsEncEnv) == 'prod' ]] && echo "https://acme-v02.api.letsencrypt.org/directory " || echo "https://acme-staging-v02.api.letsencrypt.org/directory"
}

isProd(){
  [[ $(getLetsEncEnv) == 'prod' ]] && echo "true" || echo "false"
}

getClusterIssuerName(){
  case "$CERTMANAGER_CHALANGE_TYPE" in
   'dns') echo "letsencrypt-$(getLetsEncEnv)" ;;
   'http') echo "letsencrypt-$(getLetsEncEnv)" ;;
   'selfsigned') echo "gokselfsign-ca-cluster-issuer" ;;
  esac
}

#Godday api calls are disabled, hence going to remove this call.
godaddyWebhook() {
  replaceEnvVariable  https://github.com/sumitmaji/kubernetes/raw/master/install_k8s/godaddy-cert-webhook/webhook-all.yml | kubectl create -f - --validate=false
  echo "Provide godaddy apikey and secret <API_KEY:SECRET>"
  API_KEY=$(promptSecret "Provide godaddy apikey and secret <API_KEY:SECRET>")

  cat <<EOF | kubectl apply -f -
apiVersion: v1
kind: Secret
metadata:
  name: godaddy-api-key-secret
  namespace: cert-manager
type: Opaque
stringData:
  api-key: ${API_KEY}
EOF

  cat <<EOF | kubectl apply -f -
apiVersion: cert-manager.io/v1
kind: ClusterIssuer
metadata:
  name: $(getClusterIssuerName)
spec:
  acme:
    email: majisumitkumar@gmail.com
    server: $(getLetsEncryptUrl)
    privateKeySecretRef:
      name: letsencrypt-$(getLetsEncEnv)
    solvers:
    - dns01:
        webhook:
          config:
            apiKeySecretRef:
              name: godaddy-api-key-secret
              key: api-key
            production: $(isProd)
            ttl: 600
          groupName: $(rootDomain)
          solverName: godaddy
      selector:
       dnsNames:
       - '$(defaultSubdomain).$(rootDomain)'
       - '*.$(rootDomain)'
EOF

  cat <<EOF | kubectl apply -f -
apiVersion: cert-manager.io/v1
kind: Certificate
metadata:
  name: $(sedRootDomain)-tls
  namespace: default
spec:
  secretName: $(sedRootDomain)
  issuerRef:
    name: letsencrypt-prod
    kind: ClusterIssuer
  commonName: $(defaultSubdomain).$(rootDomain)
  dnsNames:
    - $(defaultSubdomain).$(rootDomain)
  issuerRef:
    name: letsencrypt-prod
    kind: ClusterIssuer
EOF

}



addLetsEncryptStagingCertificates(){
  wget https://letsencrypt.org/certs/staging/letsencrypt-stg-root-x1.pem
  sudo cp letsencrypt-stg-root-x1.pem /usr/local/share/ca-certificates/
  sudo update-ca-certificates
  echo "Added letsencrypt staging certificates, please reboot the system for it to effect"
}


godaddyWebhookReset() {
  kubectl delete -f https://github.com/sumitmaji/kubernetes/raw/master/install_k8s/godaddy-cert-webhook/webhook-all.yml
  kubectl delete secret godaddy-api-key-secret -n cert-manager
}

setupCertiIssuers() {
  log_component_start "cert-issuers" "Configuring certificate issuers and CA infrastructure"

if [ $CERTMANAGER_CHALANGE_TYPE == 'dns' ]; then
  log_step "1" "Creating Let's Encrypt DNS challenge cluster issuer"
  if execute_with_suppression kubectl apply -f - <<EOF
apiVersion: cert-manager.io/v1
kind: ClusterIssuer
metadata:
  name: $(getClusterIssuerName)
spec:
  acme:
    email: majisumitkumar@gmail.com
    server: $(getLetsEncryptUrl)
    privateKeySecretRef:
      name: letsencrypt-$(getLetsEncEnv)
    solvers:
    - dns01:
        webhook:
          config:
            apiKeySecretRef:
              name: godaddy-api-key-secret
              key: api-key
            production: $(isProd)
            ttl: 600
          groupName: $(rootDomain)
          solverName: godaddy
      selector:
       dnsNames:
       - '$(defaultSubdomain).$(rootDomain)'
       - '*.$(rootDomain)'
EOF
  then
    log_success "Let's Encrypt DNS challenge issuer created ($(getClusterIssuerName))"
  else
    log_error "Failed to create Let's Encrypt DNS challenge issuer"
    return 1
  fi
  
  log_step "2" "Installing GoDaddy DNS webhook"
  if godaddyWebhook; then
    log_success "GoDaddy DNS webhook installed"
  else
    log_error "Failed to install GoDaddy DNS webhook"
    return 1
  fi
  
  log_step "3" "Adding Let's Encrypt staging certificates"
  if addLetsEncryptStagingCertificates; then
    log_success "Let's Encrypt staging certificates configured"
  else
    log_error "Failed to configure Let's Encrypt staging certificates"
    return 1
  fi
elif [ $CERTMANAGER_CHALANGE_TYPE == 'http' ]; then
  log_step "1" "Creating Let's Encrypt HTTP challenge cluster issuer"
  if execute_with_suppression kubectl apply -f - <<EOF
apiVersion: cert-manager.io/v1
kind: ClusterIssuer
metadata:
  name: $(getClusterIssuerName)
spec:
  acme:
    email: majisumitkumar@gmail.com
    server: $(getLetsEncryptUrl)
    #preferredChain: "(STAGING) Pretend Pear X1"
    privateKeySecretRef:
      name: letsencrypt-$(getLetsEncEnv)
    solvers:
    - http01:
        ingress:
          ingressClassName: nginx
EOF
  then
    log_success "Let's Encrypt HTTP challenge issuer created ($(getClusterIssuerName))"
  else
    log_error "Failed to create Let's Encrypt HTTP challenge issuer"
    return 1
  fi
  
  log_step "2" "Adding Let's Encrypt staging certificates"
  if addLetsEncryptStagingCertificates; then
    log_success "Let's Encrypt staging certificates configured"
  else
    log_error "Failed to configure Let's Encrypt staging certificates"
    return 1
  fi
elif [ $CERTMANAGER_CHALANGE_TYPE == 'selfsigned' ]; then
  # https://medium.com/geekculture/a-simple-ca-setup-with-kubernetes-cert-manager-bc8ccbd9c2
  # https://gist.github.com/jakexks/c1de8238cbee247333f8c274dc0d6f0f
  
  log_step "1" "Creating self-signed cluster issuer"
  local retry_count=0
  local max_retries=5
  while [ $retry_count -lt $max_retries ]; do
    if execute_with_suppression kubectl apply -f - <<EOYAML
apiVersion: cert-manager.io/v1
kind: ClusterIssuer
metadata:
  name: selfsigned-cluster-issuer
spec:
  selfSigned: {}
EOYAML
    then
      log_success "Self-signed cluster issuer created"
      break
    else
      retry_count=$((retry_count + 1))
      if [ $retry_count -eq $max_retries ]; then
        log_error "Failed to create self-signed cluster issuer after $max_retries attempts"
        return 1
      fi
      sleep 1
    fi
  done

  if execute_with_suppression kubectl wait --timeout=10s --for=condition=Ready clusterissuers.cert-manager.io selfsigned-cluster-issuer; then
    log_success "Self-signed cluster issuer is ready"
  else
    log_error "Self-signed cluster issuer failed to become ready"
    return 1
  fi

  log_step "2" "Creating CA certificate authority"
  if execute_with_suppression kubectl apply -f - <<EOYAML
apiVersion: cert-manager.io/v1
kind: Certificate
metadata:
  name: gokselfsign-ca
  namespace: cert-manager
spec:
  isCA: true
  commonName: gokselfsign-ca
  secretName: gokselfsign-ca
  subject:
    organizations:
      - GOK Inc.
    organizationalUnits:
      - Widgets
  privateKey:
    algorithm: ECDSA
    size: 256
  issuerRef:
    name: selfsigned-cluster-issuer
    kind: ClusterIssuer
    group: cert-manager.io
EOYAML
  then
    log_success "CA certificate created (GOK Inc. authority)"
  else
    log_error "Failed to create CA certificate"
    return 1
  fi

  if execute_with_suppression kubectl wait --timeout=10s -n cert-manager --for=condition=Ready certificates.cert-manager.io gokselfsign-ca; then
    log_success "CA certificate is ready and issued"
  else
    log_error "CA certificate failed to become ready"
    return 1
  fi

  log_step "3" "Creating production CA cluster issuer"
  if execute_with_suppression kubectl apply -f - <<EOYAML
apiVersion: cert-manager.io/v1
kind: ClusterIssuer
metadata:
  name: $(getClusterIssuerName)
spec:
  ca:
    secretName: gokselfsign-ca
EOYAML
  then
    log_success "Production CA cluster issuer created ($(getClusterIssuerName))"
  else
    log_error "Failed to create production CA cluster issuer"
    return 1
  fi

  if execute_with_suppression kubectl wait --timeout=10s --for=condition=Ready clusterissuers.cert-manager.io "$(getClusterIssuerName)"; then
    log_success "Production CA cluster issuer is ready"
  else
    log_error "Production CA cluster issuer failed to become ready"
    return 1
  fi

  log_step "4" "Installing CA certificate to system trust store"
  # Extract CA certificate using proper shell command with redirection handling
  # Fixed: Wrap in 'bash -c' to ensure redirection works with execute_with_suppression 
  if execute_with_suppression bash -c 'kubectl get secrets -n cert-manager gokselfsign-ca -o json | jq -r ".data[\"tls.crt\"]" | base64 -d > /usr/local/share/ca-certificates/issuer.crt'; then
    log_success "CA certificate extracted from Kubernetes secret"
  else
    log_error "Failed to extract CA certificate"
    return 1
  fi

  # Add the issuer.crt to export directory so that the worker nodes can add the same to their trusted certificates.
  if execute_with_suppression mkdir -p /export/certs && execute_with_suppression cp /usr/local/share/ca-certificates/issuer.crt /export/certs/issuer.crt; then
    log_success "CA certificate copied to shared storage for worker nodes"
  else
    log_error "Failed to copy CA certificate to shared storage"
    return 1
  fi

  if execute_with_suppression update-ca-certificates; then
    log_success "System certificate store updated with GOK CA"
  else
    log_error "Failed to update system certificate store"
    return 1
  fi

  log_step "4b" "Refreshing certificates in Kubernetes cluster (avoiding system reboot)"
  log_info "âš ï¸ Self-signed certificate added to system CA store - applying cluster-wide refresh instead of system reboot"
  
  # Restart containerd/docker to pick up new CA certificates
  if systemctl is-active --quiet containerd; then
    log_info "ðŸ”„ Restarting containerd to refresh CA certificates"
    if execute_with_suppression systemctl restart containerd; then
      log_success "Containerd restarted successfully"
    else
      log_warning "Failed to restart containerd - certificates may not be fully effective"
    fi
  elif systemctl is-active --quiet docker; then
    log_info "ðŸ”„ Restarting Docker to refresh CA certificates"
    if execute_with_suppression systemctl restart docker; then
      log_success "Docker restarted successfully"
    else
      log_warning "Failed to restart Docker - certificates may not be fully effective"
    fi
  fi
  
  # Restart kubelet to refresh certificates
  log_info "ðŸ”„ Restarting kubelet to refresh CA certificates"
  if execute_with_suppression systemctl restart kubelet; then
    log_success "Kubelet restarted successfully"
  else
    log_warning "Failed to restart kubelet - certificates may not be fully effective"
  fi
  
  # Wait for kubelet to be ready
  log_info "â³ Waiting for kubelet to become ready..."
  sleep 10
  
  # Restart cert-manager pods to pick up new CA certificates
  log_info "ðŸ”„ Restarting cert-manager pods to refresh CA certificates"
  if execute_with_suppression kubectl rollout restart deployment cert-manager -n cert-manager; then
    log_success "Cert-manager deployment restarted"
  else
    log_warning "Failed to restart cert-manager deployment"
  fi
  
  if execute_with_suppression kubectl rollout restart deployment cert-manager-webhook -n cert-manager; then
    log_success "Cert-manager webhook restarted"
  else
    log_warning "Failed to restart cert-manager webhook"
  fi
  
  if execute_with_suppression kubectl rollout restart deployment cert-manager-cainjector -n cert-manager; then
    log_success "Cert-manager CA injector restarted"
  else
    log_warning "Failed to restart cert-manager CA injector"
  fi
  
  # Wait for cert-manager components to be ready
  log_info "â³ Waiting for cert-manager components to become ready..."
  if execute_with_suppression kubectl wait --for=condition=available --timeout=120s deployment/cert-manager -n cert-manager; then
    log_success "Cert-manager deployment is ready"
  else
    log_warning "Cert-manager deployment may not be fully ready"
  fi
  
  if execute_with_suppression kubectl wait --for=condition=available --timeout=120s deployment/cert-manager-webhook -n cert-manager; then
    log_success "Cert-manager webhook is ready"
  else
    log_warning "Cert-manager webhook may not be fully ready"
  fi
  
  # Restart kube-system components that use certificates
  log_info "ðŸ”„ Restarting core Kubernetes components to refresh certificates"
  
  # Restart kube-proxy daemonset
  if execute_with_suppression kubectl rollout restart daemonset kube-proxy -n kube-system; then
    log_success "Kube-proxy daemonset restarted"
  else
    log_warning "Failed to restart kube-proxy daemonset"
  fi
  
  # Restart CoreDNS
  if execute_with_suppression kubectl rollout restart deployment coredns -n kube-system; then
    log_success "CoreDNS deployment restarted"
  else
    log_warning "Failed to restart CoreDNS deployment"
  fi
  
  # If there's an ingress controller, restart it
  if kubectl get deployment -n ingress-nginx nginx-ingress-controller >/dev/null 2>&1; then
    log_info "ðŸ”„ Restarting NGINX ingress controller"
    if execute_with_suppression kubectl rollout restart deployment nginx-ingress-controller -n ingress-nginx; then
      log_success "NGINX ingress controller restarted"
    else
      log_warning "Failed to restart NGINX ingress controller"
    fi
  fi
  
  # Restart any other critical components that might use certificates
  if kubectl get deployment -n vault vault >/dev/null 2>&1; then
    log_info "ðŸ”„ Restarting Vault deployment"
    if execute_with_suppression kubectl rollout restart deployment vault -n vault; then
      log_success "Vault deployment restarted"
    else
      log_warning "Failed to restart Vault deployment"
    fi
  fi
  
  if kubectl get deployment -n rabbitmq rabbitmq >/dev/null 2>&1; then
    log_info "ðŸ”„ Restarting RabbitMQ deployment"
    if execute_with_suppression kubectl rollout restart deployment rabbitmq -n rabbitmq; then
      log_success "RabbitMQ deployment restarted"
    else
      log_warning "Failed to restart RabbitMQ deployment"
    fi
  fi
  
  # Restart HAProxy if it's running
  if docker ps --filter "name=master-proxy" --filter "status=running" -q | grep -q .; then
    log_info "ðŸ”„ Restarting HAProxy to refresh CA certificates"
    if startHa; then
      log_success "HAProxy restarted successfully"
    else
      log_warning "Failed to restart HAProxy - certificates may not be fully effective"
    fi
  elif [ -f /opt/haproxy.cfg ]; then
    log_info "ðŸ”„ HAProxy configuration found - starting HAProxy with new certificates"
    if startHa; then
      log_success "HAProxy started successfully with updated certificates"
    else
      log_warning "Failed to start HAProxy"
    fi
  fi
  
  # Final verification
  log_info "ðŸ” Verifying certificate refresh completion"
  
  # Test certificate validation
  if openssl verify -CAfile /etc/ssl/certs/ca-certificates.crt /usr/local/share/ca-certificates/issuer.crt >/dev/null 2>&1; then
    log_success "âœ… Self-signed CA certificate is properly trusted by the system"
  else
    log_warning "âš ï¸ Certificate validation test failed - some components may not recognize the new CA"
  fi
  
  # Check cluster health
  log_info "ðŸ¥ Checking cluster health status"
  if kubectl get nodes >/dev/null 2>&1; then
    log_success "âœ… Kubernetes cluster is responsive"
  else
    log_warning "âš ï¸ Kubernetes cluster may not be fully responsive"
  fi
  
  log_success "ðŸŽ‰ Certificate refresh completed - cluster-wide certificate update applied without system reboot"
  log_info "ðŸ“‹ Next steps: Verify your applications can validate certificates issued by the new CA"
  log_info "ðŸ’¡ Tip: If you encounter certificate validation issues, run 'kubectl rollout restart deployment <deployment-name>' for specific applications"
fi

  log_step "5" "Creating default domain certificate"
  if execute_with_suppression kubectl apply -f - <<EOF
apiVersion: cert-manager.io/v1
kind: Certificate
metadata:
  name: $(sedRootDomain)-tls
  namespace: default
spec:
  secretName: $(sedRootDomain)
  issuerRef:
    name: $(getClusterIssuerName)
    kind: ClusterIssuer
  commonName: $(defaultSubdomain).$(rootDomain)
  dnsNames:
    - $(defaultSubdomain).$(rootDomain)
  issuerRef:
    name: $(getClusterIssuerName)
    kind: ClusterIssuer
EOF
  then
    log_success "Default domain certificate created ($(defaultSubdomain).$(rootDomain))"
  else
    log_error "Failed to create default domain certificate"
    return 1
  fi

  log_warning "Self-signed certificate added to system CA store - system reboot recommended for full effect"
  log_component_success "cert-issuers" "Certificate issuers and CA infrastructure configured successfully"
}

certManagerReset() {
  log_component_start "cert-manager-reset" "Removing cert-manager and related resources"
  
  log_step "1" "Cleaning up cert-manager resources"
  if execute_with_suppression kubectl delete Issuers,ClusterIssuers,Certificates,CertificateRequests,Orders,Challenges --all --all-namespaces; then
    log_success "Cert-manager resources cleaned up"
  fi
  
  log_step "2" "Removing DNS challenge webhook (if configured)"
  if [[ $CERTMANAGER_CHALANGE_TYPE == 'dns' ]]; then 
    godaddyWebhookReset
    log_success "DNS challenge webhook removed"
  fi
  
  log_step "3" "Uninstalling cert-manager Helm release"
  if helm_uninstall_with_summary "cert-manager" "cert-manager" --namespace cert-manager cert-manager; then
    log_success "Cert-manager Helm release removed"
  fi
  
  log_step "4" "Removing cert-manager namespace"
  if kubectl_with_summary delete "namespace" cert-manager; then
    show_installation_summary "cert-manager" "cert-manager" "Certificate management system removed"
    log_component_success "cert-manager-reset" "cert-manager successfully removed from cluster"
  else
    log_error "Failed to remove cert-manager namespace"
    return 1
  fi
}

haInst() {
  log_section "High Availability Proxy Installation" "$EMOJI_NETWORK"
  
  # Pre-installation validation
  log_step "1" "Validating HA proxy requirements"
  
  # Check if Docker is installed and running
  if ! command -v docker >/dev/null 2>&1; then
    log_error "Docker is required for HA proxy but not installed"
    log_info "Install Docker first: gok install docker"
    return 1
  fi
  
  if ! docker info >/dev/null 2>&1; then
    log_error "Docker is installed but not running"
    log_info "Start Docker: systemctl start docker"
    return 1
  fi
  
  log_success "Docker is available and running"
  
  # Check if API_SERVERS is configured
  if [[ -z "$API_SERVERS" ]]; then
    log_error "API_SERVERS environment variable is not set"
    log_info "Set API_SERVERS with comma-separated master nodes: export API_SERVERS='192.168.1.10:master1,192.168.1.11:master2'"
    return 1
  fi
  
  # Validate API_SERVERS format and count
  local server_count=$(echo "$API_SERVERS" | tr ',' '\n' | wc -l)
  if [[ $server_count -lt 2 ]]; then
    log_warning "Only $server_count API server configured - HA proxy is recommended for 2+ servers"
    log_info "Current API_SERVERS: $API_SERVERS"
  else
    log_success "Multiple API servers detected ($server_count servers) - HA setup recommended"
    log_info "API servers: $API_SERVERS"
  fi
  
  # Set default HA proxy port if not specified
  if [[ -z "$HA_PROXY_PORT" ]]; then
    export HA_PROXY_PORT=8443
    log_info "Using default HA proxy port: $HA_PROXY_PORT"
  else
    log_info "Using configured HA proxy port: $HA_PROXY_PORT"
  fi
  
  # Step 2: Stop existing proxy if running
  log_step "2" "Cleaning up existing HA proxy container"
  
  if docker ps -a --format "table {{.Names}}" | grep -q "master-proxy"; then
    log_substep "Stopping existing master-proxy container"
    docker stop master-proxy >/dev/null 2>&1 || true
    
    log_substep "Removing existing master-proxy container"
    docker rm master-proxy >/dev/null 2>&1 || true
    
    log_success "Existing proxy container cleaned up"
  else
    log_info "No existing proxy container found"
  fi
  
  # Step 3: Generate HAProxy configuration
  log_step "3" "Generating HAProxy configuration"
  
  log_substep "Creating HAProxy configuration file at /opt/haproxy.cfg"
  
  if ! cat <<EOF >/opt/haproxy.cfg
global
        log 127.0.0.1 local0
        log 127.0.0.1 local1 notice
        maxconn 4096
        maxpipes 1024
        daemon
        user haproxy
        group haproxy

defaults
        log global
        mode tcp
        option tcplog
        option dontlognull
        option redispatch
        retries 3
        timeout connect 5000ms
        timeout client 50000ms
        timeout server 50000ms
        
frontend kubernetes-apiserver
        bind *:$HA_PROXY_PORT
        mode tcp
        option tcplog
        default_backend kubernetes-apiserver

backend kubernetes-apiserver
        mode tcp
        balance roundrobin
        option tcp-check
        option tcplog
$(
    # Generate backend servers from API_SERVERS
    IFS=','
    counter=0
    for worker in $API_SERVERS; do
      oifs=$IFS
      IFS=':'
      read -r ip node <<<"$worker"
      counter=$((counter + 1))
      echo "        server $node $ip:6443 check fall 3 rise 2"
      IFS=$oifs
    done
    unset IFS
)
EOF
  then
    log_error "Failed to create HAProxy configuration file"
    return 1
  fi
  
  log_success "HAProxy configuration generated"
  
  # Display configuration summary
  log_info "Configuration summary:"
  log_substep "Frontend port: $HA_PROXY_PORT"
  log_substep "Backend servers:"
  
  IFS=','
  counter=0
  for worker in $API_SERVERS; do
    oifs=$IFS
    IFS=':'
    read -r ip node <<<"$worker"
    counter=$((counter + 1))
    log_substep "  $counter. $node ($ip:6443)"
    IFS=$oifs
  done
  unset IFS
  
  # Step 4: Pull HAProxy image
  log_step "4" "Pulling HAProxy Docker image"
  
  if ! docker pull haproxy:latest; then
    log_error "Failed to pull HAProxy Docker image"
    return 1
  fi
  
  log_success "HAProxy image pulled successfully"
  
  # Step 5: Start HAProxy container
  log_step "5" "Starting HAProxy container"
  
  log_substep "Running HAProxy container with host networking"
  
  if ! docker run -d --name master-proxy \
    --restart=unless-stopped \
    -v /opt/haproxy.cfg:/usr/local/etc/haproxy/haproxy.cfg:ro \
    --net=host \
    haproxy:latest; then
    log_error "Failed to start HAProxy container"
    return 1
  fi
  
  log_success "HAProxy container started successfully"
  
  # Step 6: Validate installation
  log_step "6" "Validating HA proxy installation"
  
  # Wait a moment for container to start
  sleep 3
  
  if validate_haproxy_installation; then
    log_success "HA proxy installation validation passed"
  else
    log_error "HA proxy installation validation failed"
    return 1
  fi
  
  # Show next steps
  show_haproxy_next_steps
  
  return 0
}

# HAProxy installation validation
validate_haproxy_installation() {
  log_substep "Checking HAProxy container status"
  if ! docker ps --format "table {{.Names}}\t{{.Status}}" | grep -q "master-proxy.*Up"; then
    log_error "HAProxy container is not running"
    docker logs master-proxy 2>&1 | tail -5 | while read line; do
      log_error "Container log: $line"
    done
    return 1
  fi
  
  log_substep "Checking HAProxy port binding"
  if ! netstat -tlnp 2>/dev/null | grep -q ":$HA_PROXY_PORT.*LISTEN" && ! ss -tlnp 2>/dev/null | grep -q ":$HA_PROXY_PORT.*LISTEN"; then
    log_error "HAProxy is not listening on port $HA_PROXY_PORT"
    return 1
  fi
  
  log_substep "Testing HAProxy configuration"
  if ! docker exec master-proxy haproxy -c -f /usr/local/etc/haproxy/haproxy.cfg >/dev/null 2>&1; then
    log_error "HAProxy configuration validation failed"
    return 1
  fi
  
  log_substep "Checking backend server connectivity"
  local healthy_backends=0
  local total_backends=0
  
  IFS=','
  for worker in $API_SERVERS; do
    oifs=$IFS
    IFS=':'
    read -r ip node <<<"$worker"
    total_backends=$((total_backends + 1))
    
    if timeout 5 nc -z "$ip" 6443 2>/dev/null; then
      log_substep "  Backend $node ($ip:6443): ${EMOJI_SUCCESS} Reachable"
      healthy_backends=$((healthy_backends + 1))
    else
      log_substep "  Backend $node ($ip:6443): ${EMOJI_WARNING} Not reachable (may not be ready yet)"
    fi
    IFS=$oifs
  done
  unset IFS
  
  if [[ $healthy_backends -eq 0 ]]; then
    log_warning "No backend servers are currently reachable"
    log_info "This is normal if Kubernetes masters are not yet installed"
  else
    log_success "$healthy_backends out of $total_backends backend servers are reachable"
  fi
  
  return 0
}

# Show HAProxy next steps
show_haproxy_next_steps() {
  log_next_steps "HA Proxy Installation Complete" \
    "Check proxy status: docker ps | grep master-proxy" \
    "View proxy logs: docker logs master-proxy" \
    "Test connectivity: curl -k https://localhost:$HA_PROXY_PORT/healthz" \
    "Install Kubernetes masters using HA endpoint" \
    "Configure kubectl to use HA endpoint: https://localhost:$HA_PROXY_PORT"
  
  log_urls "HA Proxy Access Points" \
    "Load Balancer Endpoint: https://localhost:$HA_PROXY_PORT" \
    "HAProxy Stats: http://localhost:$HA_PROXY_PORT/stats (if enabled)" \
    "Configuration File: /opt/haproxy.cfg"
  
  log_credentials "HA Proxy Management" "docker" \
    "Container name: master-proxy" \
    "Restart: docker restart master-proxy" \
    "Stop: docker stop master-proxy"
  
  log_info "HA proxy is now load balancing across $(($(echo "$API_SERVERS" | tr ',' '\n' | wc -l))) Kubernetes API servers"
  
  log_troubleshooting "HA Proxy" \
    "Check container logs: docker logs master-proxy" \
    "Verify port availability: netstat -tlnp | grep $HA_PROXY_PORT" \
    "Test backend connectivity: nc -z <backend-ip> 6443" \
    "Validate configuration: docker exec master-proxy haproxy -c -f /usr/local/etc/haproxy/haproxy.cfg" \
    "Restart proxy: docker restart master-proxy"
  
  echo
  log_header "Kubernetes Installation with HA" "Ready for Multi-Master Setup"
  
  echo -e "${COLOR_BRIGHT_GREEN}${COLOR_BOLD}Your HA proxy is ready! Next steps:${COLOR_RESET}"
  echo
  echo -e "${COLOR_CYAN}1. ${COLOR_BOLD}Install first Kubernetes master:${COLOR_RESET}"
  echo -e "   ${COLOR_DIM}Use the HA proxy endpoint for --control-plane-endpoint${COLOR_RESET}"
  echo
  echo -e "${COLOR_CYAN}2. ${COLOR_BOLD}Join additional masters:${COLOR_RESET}"
  echo -e "   ${COLOR_DIM}Each additional master will connect through the HA proxy${COLOR_RESET}"
  echo
  echo -e "${COLOR_CYAN}3. ${COLOR_BOLD}Configure kubectl:${COLOR_RESET}"
  echo -e "   ${COLOR_DIM}Point kubectl to https://localhost:$HA_PROXY_PORT${COLOR_RESET}"
  echo
}

startKubelet() {
  # Stop kubelet service
  if systemctl is-active --quiet kubelet; then
    if ! systemctl stop kubelet >/dev/null 2>&1; then
      log_warning "Failed to stop kubelet service"
    fi
  fi
  
  # Start kubelet service
  if systemctl start kubelet >/dev/null 2>&1; then
    # Wait a moment for service to initialize
    sleep 2
    if systemctl is-active --quiet kubelet; then
      return 0
    else
      log_error "Kubelet service started but is not active"
      return 1
    fi
  else
    log_error "Failed to start kubelet service"
    systemctl status kubelet --no-pager -l || true
    return 1
  fi
}

startHa() {
  # Check if HAProxy configuration exists
  if [ ! -f /opt/haproxy.cfg ]; then
    log_error "HAProxy configuration file not found: /opt/haproxy.cfg"
    return 1
  fi
  
  # Stop existing container if running
  if docker ps -q -f name=master-proxy | grep -q .; then
    if ! docker stop master-proxy >/dev/null 2>&1; then
      log_warning "Failed to stop existing master-proxy container"
    fi
  fi
  
  # Remove existing container if exists
  if docker ps -a -q -f name=master-proxy | grep -q .; then
    if ! docker rm master-proxy >/dev/null 2>&1; then
      log_warning "Failed to remove existing master-proxy container"
    fi
  fi
  
  # Start new HAProxy container
  if docker run -d --name master-proxy \
    -v /opt/haproxy.cfg:/usr/local/etc/haproxy/haproxy.cfg:ro \
    --net=host haproxy >/dev/null 2>&1; then
    
    # Wait a moment and verify container is running
    sleep 2
    if docker ps -q -f name=master-proxy -f status=running | grep -q .; then
      return 0
    else
      log_error "HAProxy container started but is not running"
      docker logs master-proxy 2>/dev/null || true
      return 1
    fi
  else
    log_error "Failed to start HAProxy container"
    return 1
  fi
}

disableSwap() {
  sudo sed -i '/ swap / s/^\(.*\)$/#\1/g' /etc/fstab
  sudo swapoff -a
}

hostSecret() {
  openssl genrsa -out ${APP_HOST}.key 4096
  openssl req -new -key ${APP_HOST}.key -out ${APP_HOST}.csr -subj "/CN=${APP_HOST}" \
    -addext "subjectAltName = DNS:${APP_HOST}"
  openssl x509 -req -in ${APP_HOST}.csr -CA /etc/kubernetes/pki/ca.crt -CAkey /etc/kubernetes/pki/ca.key -CAcreateserial -out ${APP_HOST}.crt -days 7200

  kubectl create secret tls appingress-certificate --key ${APP_HOST}.key --cert ${APP_HOST}.crt -n default
}

dashboardInst() {
  helm repo add kubernetes-dashboard https://kubernetes.github.io/dashboard/
  kubectl delete ns kubernetes-dashboard
  helm install kubernetes-dashboard \
    kubernetes-dashboard/kubernetes-dashboard \
    --namespace kubernetes-dashboard \
    --create-namespace \
    -f "${MOUNT_PATH}"/kubernetes/install_k8s/dashboard/values2.yaml
}

prometheusGrafanaReset() {
  helm -n monitoring delete monitoring
  kubectl delete ns monitoring
  helm -n db delete postgres
  kubectl delete ns db
}

emptyLocalFsStorage() {
  local service=$1
  local pvName=$2
  local scName=$3
  local volumePath=$4
  local namespace=$5

  log_info "Cleaning up persistent storage for $service"
  
  # Clean up PVCs in namespace if specified
  if [[ -n $namespace ]]; then
    log_info "Removing persistent volume claims in namespace: $namespace"
    local pvc_count=$(kubectl get pvc -n $namespace --no-headers 2>/dev/null | wc -l)
    if [[ $pvc_count -gt 0 ]]; then
      if kubectl_with_summary delete "pvc" --all -n $namespace; then
        log_success "Persistent volume claims removed from $namespace namespace"
      else
        log_warning "Some PVCs may not have been removed from $namespace namespace"
      fi
    else
      log_info "No persistent volume claims found in $namespace namespace"
    fi
  fi

  # Clean up persistent volume
  log_info "Removing persistent volume: $pvName"
  if kubectl get pv $pvName >/dev/null 2>&1; then
    if kubectl_with_summary delete "pv" $pvName; then
      log_success "Persistent volume '$pvName' removed"
    else
      log_warning "Persistent volume '$pvName' removal had issues"
    fi
  else
    log_info "Persistent volume '$pvName' not found (may already be removed)"
  fi

  # Clean up storage class
  log_info "Removing storage class: $scName"
  if kubectl get sc $scName >/dev/null 2>&1; then
    if kubectl_with_summary delete "sc" $scName; then
      log_success "Storage class '$scName' removed"  
    else
      log_warning "Storage class '$scName' removal had issues"
    fi
  else
    log_info "Storage class '$scName' not found (may already be removed)"
  fi

  # Clean up local filesystem path
  if [[ -n "$volumePath" && -d "$volumePath" ]]; then
    log_info "Cleaning up local storage path: $volumePath"
    if execute_with_suppression rm -rf "$volumePath"; then
      log_success "Local storage path '$volumePath' cleaned up"
    else
      log_warning "Could not fully clean up local storage path '$volumePath'"
    fi
  else
    log_info "Local storage path '$volumePath' not found or already cleaned"
  fi
  
  log_success "$service persistent storage cleanup completed"
}

createLocalStorageClassAndPV() {
  local storageClassName=$1
  local pvName=$2
  local volumePath=$3

  if execute_with_suppression kubectl apply -f - << EOF
kind: StorageClass
apiVersion: storage.k8s.io/v1
metadata:
  name: ${storageClassName}
provisioner: kubernetes.io/no-provisioner
volumeBindingMode: WaitForFirstConsumer
EOF
  then
    log_success "Storage class created (${storageClassName})"
  else
    log_error "Failed to create storage class"
    return 1
  fi

  if execute_with_suppression mkdir -p "${volumePath}" && execute_with_suppression chmod 777 "${volumePath}"; then
    log_success "Local storage directory prepared (${volumePath})"
  else
    log_error "Failed to prepare local storage directory"
    return 1
  fi

  if execute_with_suppression kubectl apply -f - << EOF
apiVersion: v1
kind: PersistentVolume
metadata:
  name: ${pvName}
spec:
  capacity:
    storage: 10Gi
  accessModes:
  - ReadWriteOnce
  persistentVolumeReclaimPolicy: Retain
  storageClassName: ${storageClassName}
  local:
    path: ${volumePath}
  nodeAffinity:
    required:
      nodeSelectorTerms:
      - matchExpressions:
        - key: kubernetes.io/hostname
          operator: In
          values:
          - master.cloud.com
EOF
  then
    log_success "Persistent volume created (${pvName}, 10Gi)"
  else
    log_error "Failed to create persistent volume"
    return 1
  fi
}

prometheusGrafanaResetv2(){
  helm -n monitoring delete prometheus
  helm -n monitoring delete grafana
  kubectl delete ns monitoring

}

prometheusGrafanaInstv2(){
  CLIENT_SECRET=$(dataFromSecret oauth-secrets kube-system OIDC_CLIENT_SECRET)
  CLIENT_ID=$(dataFromSecret oauth-secrets kube-system OIDC_CLIENT_ID)
  URL=$(fullDefaultUrl)
  OAUTH2_HOST=$(fullKeycloakUrl)
  REALM=$(dataFromSecret oauth-secrets kube-system OIDC_REALM)

  kubectl create ns monitoring
  kubectl create secret generic kube-prometheus-stack-grafana-oauth \
    --from-literal GF_AUTH_KEYCLOAK_CLIENT_ID="${CLIENT_ID}" \
    --from-literal GF_AUTH_KEYCLOAK_CLIENT_SECRET="${CLIENT_SECRET}" \
    --from-literal=OAUTH_AUTH_URL="https://${OAUTH2_HOST}/realms/${REALM}/protocol/openid-connect/auth" \
    --from-literal=OAUTH_TOKEN_URL="https://${OAUTH2_HOST}/realms/${REALM}/protocol/openid-connect/token" \
    --from-literal=OAUTH_API_URL="https://${OAUTH2_HOST}/realms/${REALM}/protocol/openid-connect/userinfo" \
    --from-literal=GRAFANA_DOMAIN="${URL}" \
    --from-literal=GRAFANA_ROOT_URL="https://${URL}/grafana" \
    --namespace monitoring

  kubectl create configmap -n monitoring env-data \
    --from-literal=OAUTH_AUTH_URL="https://${OAUTH2_HOST}/realms/${REALM}/protocol/openid-connect/auth" \
    --from-literal=OAUTH_TOKEN_URL="https://${OAUTH2_HOST}/realms/${REALM}/protocol/openid-connect/token" \
    --from-literal=OAUTH_API_URL="https://${OAUTH2_HOST}/realms/${REALM}/protocol/openid-connect/userinfo" \
    --from-literal=GRAFANA_DOMAIN="${URL}" \
    --from-literal=GRAFANA_ROOT_URL="https://${URL}/grafana"

  kubectl get secrets -n cert-manager gokselfsign-ca -o json | jq -r '.data["tls.crt"]' | base64 -d > ~/issuer.crt
  kubectl get secrets -n keycloak keycloak-gokcloud-com -o json | jq -r '.data["tls.crt"]' | base64 -d > ~/keycloak.crt
  kubectl create configmap certs-configmap -n monitoring --from-file=/root/issuer.crt --from-file=/root/keycloak.crt
  rm /root/issuer.crt
  rm /root/keycloak.crt
  helm repo add prometheus-community https://prometheus-community.github.io/helm-charts
  helm repo add grafana https://grafana.github.io/helm-charts
  helm repo update
  helm install prometheus prometheus-community/prometheus \
    --set server.extraFlags=\{web.enable-lifecycle,web.route-prefix=/,web.external-url=prometheus\} \
    --values $MOUNT_PATH/kubernetes/install_k8s/prometheus-grafana/prometheus-values.yaml \
    --namespace monitoring \
    --create-namespace

  helm install grafana grafana/grafana \
    --set grafana.ini.server.root_url=https://$URL/grafana \
    --values $MOUNT_PATH/kubernetes/install_k8s/prometheus-grafana/grafana-values.yaml \
    --namespace monitoring

  kubectl -n kube-system get cm kube-proxy -o yaml | sed 's/metricsBindAddress: ""/metricsBindAddress: 0.0.0.0:10249/' | kubectl apply -f -

  kubectl -n kube-system patch ds kube-proxy -p \
      '{"spec":{"template":{"metadata":{"labels":{"updateTime":"'`date +'%s'`'"}}}}}'

}

prometheusGrafanaInst() {
  log_component_start "monitoring" "Installing Prometheus & Grafana monitoring stack"
  
  log_step "1" "Adding Prometheus Helm repository"
  execute_with_suppression helm repo add prometheus-community \
    https://prometheus-community.github.io/helm-charts
  execute_with_suppression helm repo update
  
  log_step "2" "Installing Prometheus monitoring stack"
  if helm_install_with_summary "monitoring" "monitoring" \
    monitoring prometheus-community/kube-prometheus-stack \
    --values https://github.com/sumitmaji/kubernetes/raw/master/install_k8s/prometheus-grafana/values.yaml \
    --version 39.6.0 \
    --namespace monitoring \
    --create-namespace; then
    
    log_step "3" "Configuring kube-proxy metrics"
#  kubectl -n kube-system get cm kube-proxy-config -o yaml | sed \
#    's/metricsBindAddress: 127.0.0.1:10249/metricsBindAddress: 0.0.0.0:10249' \
#    kubectl apply -f -

    if kubectl -n kube-system get cm kube-proxy -o yaml | sed 's/metricsBindAddress: ""/metricsBindAddress: 0.0.0.0:10249/' | kubectl_with_summary apply "configmap" -f -; then
      log_success "kube-proxy metrics configured"
    fi

    if execute_with_suppression kubectl -n kube-system patch ds kube-proxy -p \
      '{"spec":{"template":{"metadata":{"labels":{"updateTime":"'`date +'%s'`'"}}}}}'; then
      log_success "kube-proxy daemonset updated"
    fi

    log_step "4" "Installing PostgreSQL database"
    execute_with_suppression helm repo add bitnami https://charts.bitnami.com/bitnami
    execute_with_suppression helm repo update
    
    if helm_install_with_summary "postgres" "db" \
      postgres bitnami/postgresql \
      --values https://github.com/sumitmaji/kubernetes/raw/master/install_k8s/prometheus-grafana/postgres-values.yaml \
      --version 11.7.1 \
      --namespace db \
      --create-namespace; then
      
      show_installation_summary "monitoring" "monitoring" "Prometheus, Grafana & PostgreSQL stack"
      log_component_success "monitoring" "Complete monitoring stack installed successfully"
      
      # Call next module suggestion
      suggest_and_install_next_module "monitoring"
    else
      log_error "PostgreSQL installation failed"
      return 1
    fi
  else
    log_error "Prometheus monitoring stack installation failed"
    return 1
  fi
}

dashboardReset() {
  helm uninstall kubernetes-dashboard -n kubernetes-dashboard
  kubectl delete ns kubernetes-dashboard
}

#This creates a cluster-role-binding for admin user
adminRole() {
  WC=$(kubectl get clusterrolebinding cloud-cluster-admin 2>/dev/null | wc -l)
  if [ "$WC" == "0" ]; then

    cat <<EOF | kubectl apply -f -
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: cloud-cluster-admin
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: cluster-admin
subjects:
- apiGroup: rbac.authorization.k8s.io
  kind: Group
  name: cloud:masters
EOF
  fi

}

#Creating user role developers which would allow users authenticated with oauth and
#having developers role to connect with cluster
oauthDev(){
  cat <<EOF | kubectl create -f -
kind: Role
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  namespace: default
  name: pod-reader
rules:
- apiGroups: [""]
  resources: ["pods"]
  verbs: ["get", "watch", "list", "create", "update", "patch", "delete"]
- apiGroups: ["extensions", "apps"]
  resources: ["deployments"]
  verbs: ["get", "list", "watch", "create", "update", "patch", "delete"]
---
kind: RoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: read-pods
  namespace: default
subjects:
- kind: Group
  name: developers
  apiGroup: rbac.authorization.k8s.io
roleRef:
  kind: Role
  name: pod-reader
  apiGroup: rbac.authorization.k8s.io
EOF
}

# Run kubectl command on pod
runKubectlOnPod(){

  # The container will run will priveledges that are provided in the service account
  cat <<EOF | kubectl apply -f -
apiVersion: v1
kind: ServiceAccount
metadata:
  name: internal-deployer
  namespace: default
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: internal-deployer
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: cluster-admin
subjects:
  - kind: ServiceAccount
    name: internal-deployer
    namespace: default
EOF
  # Execute the container as job as it will execute the kubectl command and exit
  cat <<EOF | kubectl apply -f -
apiVersion: batch/v1
kind: Job
metadata:
  name: kubectl-executor
  namespace: default
spec:
  template:
    metadata:
      name: kubectl-executor
    spec:
      # The service account that will be used to run the container
      serviceAccountName: internal-deployer
      containers:
      - name: kubectl
        image: bitnami/kubectl:latest
        args: ["cluster-info"]
      restartPolicy: Never
EOF
  if [[ $? -eq 0 ]]; then
    log_success "Curl test pod created successfully"
  else
    log_error "Failed to create curl test pod"
    return 1
  fi
}

#Creating user role administrators which would allow users authenticated with oauth and
#having administrators role to connect with cluster
oauthAdmin(){
  cat <<EOF | kubectl create -f -
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: oauth-cluster-admin
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: cluster-admin
subjects:
- apiGroup: rbac.authorization.k8s.io
  kind: Group
  name: administrators
EOF
}

#Create kubeconfig file for oauth user used with kube-login
oauthUser(){
  kubectl config set-cluster cloud.com --certificate-authority=/etc/kubernetes/pki/ca.crt --embed-certs=true --server="$(apiUrl)" --kubeconfig=/root/oauth.conf
  kubectl config --kubeconfig=/root/oauth.conf set-context oauthuser@cloud.com --cluster=cloud.com --user=oauthuser
  kubectl config --kubeconfig=/root/oauth.conf use-context oauthuser@cloud.com

  echoSuccess "OAuth kubeconfig file create in /root/oauth.conf"
  echoSuccess "Use below command to use oauth.conf"
  echoSuccess "kubectl --kubeconfig=/root/oauth.conf --token=__USER_TOKEN__ rest of command"
  echoSuccess "alias kctl='kubectl --kubeconfig=/root/oauth.conf --token=\${__USER_TOKEN__}'"
  echoSuccess "alias kcd='kctl config set-context \$(kctl config current-context) --namespace'"
}

#This gives api server url
apiUrl(){
  kubectl config view -o json | jq -r '.clusters[] | .cluster.server'
}

opensearchReset(){
  helm uninstall opensearch -n opensearch
  emptyLocalFsStorage "Opensearch" "opensearch-pv" "opensearch-storage" "/data/volumes/pv5" "opensearch"
  kubectl delete ns opensearch
}

jenkinsReset(){
  helm uninstall jenkins -n jenkins
  emptyLocalFsStorage "Jenkins" "jenkins-pv" "jenkins-storage" "/data/volumes/jenkins"
  kubectl delete ns jenkins
}

opensearchDashReset(){
  helm uninstall opensearch-dashboard -n opensearch
}

opensearchDashInst(){
  helm repo add openSearch https://opensearch-project.github.io/helm-charts/
  helm repo update
  helm install opensearch-dashboard \
      --namespace opensearch \
      --values https://github.com/sumitmaji/kubernetes/raw/master/install_k8s/opensearch/values-dashboard.yaml \
      openSearch/opensearch-dashboards

  gok patch ingress opensearch-dashboard-opensearch-dashboards opensearch letsencrypt opensearch
  echo "Waiting for dashboard service to be up!!!!"
    kubectl --timeout=240s wait --for=condition=Ready pods --all --namespace opensearch
    [[ $? -eq 0 ]] && echoSuccess "Opensearch dashboard service now up!\n You can access the service using https://opensearch.gokcloud.com" || echoFailed "Opensearch dashboard service timed-out, plaese check!!"
}


callJenkinsApi(){

  # Jenkins API URL
  JENKINS_URL="https://jenkins.gokcloud.com/api/xml"
  QUERY_PARAMS="tree=jobs%5Bname%2ClastBuild%5Bactions%5BfailCount%2CskipCount%2CtotalCount%2CurlName%5D%2Cduration%2Cnumber%2Ctimestamp%2Cresult%2Cbuilding%2Curl%5D%2Cjobs%5Bname%2ClastBuild%5Bactions%5BfailCount%2CskipCount%2CtotalCount%2CurlName%5D%2Cduration%2Cnumber%2Ctimestamp%2Cresult%2Cbuilding%2Curl%5D%2Cjobs%5Bname%2ClastBuild%5Bactions%5BfailCount%2CskipCount%2CtotalCount%2CurlName%5D%2Cduration%2Cnumber%2Ctimestamp%2Cresult%2Cbuilding%2Curl%5D%2Cjobs%5Bname%2ClastBuild%5Bactions%5BfailCount%2CskipCount%2CtotalCount%2CurlName%5D%2Cduration%2Cnumber%2Ctimestamp%2Cresult%2Cbuilding%2Curl%5D%2Cjobs%5Bname%2ClastBuild%5Bactions%5BfailCount%2CskipCount%2CtotalCount%2CurlName%5D%2Cduration%2Cnumber%2Ctimestamp%2Cresult%2Cbuilding%2Curl%5D%2Cjobs%5Bname%2ClastBuild%5Bactions%5BfailCount%2CskipCount%2CtotalCount%2CurlName%5D%2Cduration%2Cnumber%2Ctimestamp%2Cresult%2Cbuilding%2Curl%5D%2Cjobs%5Bname%2ClastBuild%5Bactions%5BfailCount%2CskipCount%2CtotalCount%2CurlName%5D%2Cduration%2Cnumber%2Ctimestamp%2Cresult%2Cbuilding%2Curl%5D%2Cjobs%5Bname%2ClastBuild%5Bactions%5BfailCount%2CskipCount%2CtotalCount%2CurlName%5D%2Cduration%2Cnumber%2Ctimestamp%2Cresult%2Cbuilding%2Curl%5D%2Cjobs%5Bname%2ClastBuild%5Bactions%5BfailCount%2CskipCount%2CtotalCount%2CurlName%5D%2Cduration%2Cnumber%2Ctimestamp%2Cresult%2Cbuilding%2Curl%5D%5D%5D%5D%5D%5D%5D%5D%5D%5D"
  EXCLUDE_PARAMS="exclude=%2F*%2F*%2F*%2Faction%5Bnot%28totalCount%29%5D"

  # Jenkins credentials
  JENKINS_USER=$(promptUserInput "Enter Jenkins Username (skmaji1): " "skmaji1")
  JENKINS_API_TOKEN=$(promptSecret "Enter Jenkins API Token(Check README.md for steps): ")

  # Make the API call
  response=$(curl -s -u "$JENKINS_USER:$JENKINS_API_TOKEN" "$JENKINS_URL?$QUERY_PARAMS&$EXCLUDE_PARAMS")

  # Check if the response is empty
  if [ -z "$response" ]; then
    echo "Failed to fetch data from Jenkins API. Please check your credentials or URL."
    return 1
  fi

  # Print the response
  echo "Jenkins API Response:"
  echo "$response"
}

createExampleJenkinsPipeline() {
  # Prompt for Jenkins username and API token
  JENKINS_USER=$(promptUserInput "Enter Jenkins Username (skmaji1): " "skmaji1")
  JENKINS_API_TOKEN=$(promptSecret "Enter Jenkins API Token(Check README.md for steps): ")

  # Prompt for Jenkins URL and pipeline details
  JENKINS_URL=$(promptUserInput "Enter Jenkins URL (https://jenkins.gokcloud.com): " "https://jenkins.gokcloud.com")
  PIPELINE_NAME=$(promptUserInput "Enter Pipeline Name(Kaniko-Pipeline): " "Kaniko-Pipeline")

  pushd "$MOUNT_PATH/kubernetes/install_k8s/jenkins"
  # Make the API call to create the pipeline
  RESPONSE=$(curl -s -o /dev/null -w "%{http_code}" -X POST "$JENKINS_URL/createItem?name=$PIPELINE_NAME" \
    --user "$JENKINS_USER:$JENKINS_API_TOKEN" \
    -H "Content-Type: application/xml" \
    --data-binary @pipeline-config.xml)

  popd
  # Check the response
  if [ "$RESPONSE" -eq 200 ]; then
    echoSuccess "Pipeline '$PIPELINE_NAME' created successfully!"
  else
    echoFailed "Failed to create pipeline. HTTP Status Code: $RESPONSE"
  fi
}


createJenkinsPipeline() {
  # Prompt for Jenkins username and API token
  JENKINS_USER=$(promptUserInput "Enter Jenkins Username(skmaji1): " "skmaji1")
  JENKINS_API_TOKEN=$(promptSecret "Enter Jenkins API Token: ")

  # Prompt for Jenkins URL and pipeline details
  JENKINS_URL=$(promptUserInput "Enter Jenkins URL (e.g., https://jenkins.gokcloud.com): " "https://jenkins.gokcloud.com")
  PIPELINE_NAME=$(promptUserInput "Enter Pipeline Name: " "Git-Pipeline")
  GIT_REPO_URL=$(promptUserInput "Enter Git Repository URL (e.g., https://github.com/your-username/your-repo.git): ")
  GIT_BRANCH=$(promptUserInput "Enter Git Branch (e.g., main): " "main")
  CREDENTIALS_ID=$(promptUserInput "Enter Jenkins Credentials ID: ")

  # Create the pipeline configuration XML
  cat <<EOF > pipeline-config.xml
<flow-definition plugin="workflow-job">
  <description>Pipeline created automatically to refer to Jenkinsfile in Git</description>
  <keepDependencies>false</keepDependencies>
  <definition class="org.jenkinsci.plugins.workflow.cps.CpsScmFlowDefinition" plugin="workflow-cps">
    <scm class="hudson.plugins.git.GitSCM" plugin="git">
      <configVersion>2</configVersion>
      <userRemoteConfigs>
        <hudson.plugins.git.UserRemoteConfig>
          <url>${GIT_REPO_URL}</url>
          <credentialsId>${CREDENTIALS_ID}</credentialsId>
        </hudson.plugins.git.UserRemoteConfig>
      </userRemoteConfigs>
      <branches>
        <hudson.plugins.git.BranchSpec>
          <name>*/${GIT_BRANCH}</name>
        </hudson.plugins.git.BranchSpec>
      </branches>
    </scm>
    <scriptPath>Jenkinsfile</scriptPath>
    <lightweight>true</lightweight>
  </definition>
  <triggers/>
</flow-definition>
EOF

  # Make the API call to create the pipeline
  RESPONSE=$(curl -s -o /dev/null -w "%{http_code}" -X POST "$JENKINS_URL/createItem?name=$PIPELINE_NAME" \
    --user "$JENKINS_USER:$JENKINS_API_TOKEN" \
    -H "Content-Type: application/xml" \
    --data-binary @pipeline-config.xml)

  # Check the response
  if [ "$RESPONSE" -eq 200 ]; then
    echoSuccess "Pipeline '$PIPELINE_NAME' created successfully!"
  else
    echoFailed "Failed to create pipeline. HTTP Status Code: $RESPONSE"
  fi

  # Clean up the temporary XML file
  rm -f pipeline-config.xml
}

jenkinsInst() {
  log_component_start "jenkins" "Installing Jenkins CI/CD automation server"
  
  log_step "1" "Adding Jenkins Helm repository"
  execute_with_suppression helm repo add jenkins https://charts.jenkins.io
  execute_with_suppression helm repo update
  
  log_step "2" "Creating namespace and configuring secrets"
  kubectl_with_summary create "namespace" jenkins

  execute_with_suppression kubectl create configmap jenkins-logging-config \
  --from-file=${MOUNT_PATH}/kubernetes/install_k8s/jenkins/logging.properties \
  -n jenkins

  ADMIN_PASSWORD=$(promptSecret "Enter Jenkins Admin Password: ")
  execute_with_suppression kubectl create secret generic jenkins-admin-password \
    --from-literal=jenkins-admin-password="${ADMIN_PASSWORD}" -n jenkins

  CLIENT_SECRET=$(dataFromSecret oauth-secrets kube-system OIDC_CLIENT_SECRET)
  CLIENT_ID=$(dataFromSecret oauth-secrets kube-system OIDC_CLIENT_ID)
  keycloakUrl="https://$(fullKeycloakUrl)"
  REALM=$(dataFromSecret oauth-secrets kube-system OAUTH_REALM)

  execute_with_suppression kubectl create secret generic oic-auth \
  --from-literal=clientID="${CLIENT_ID}" \
  --from-literal=clientSecret="${CLIENT_SECRET}" \
  --from-literal=keycloakUrl=${keycloakUrl} \
  --from-literal=realm=${REALM} \
  --namespace jenkins
  
  log_step "3" "Configuring Docker build support"
  DOCKER_BUILD_ENABLED=true
  if [ "$DOCKER_BUILD_ENABLED" == "true" ]; then
    # kubectl create secret generic kaniko-docker-config \
    #   --from-file=/root/.docker/config.json \
    #   -n jenkins

    execute_with_suppression kubectl create secret generic docker-credentials \
    --from-file=.dockerconfigjson=/root/.docker/config.json \
    --type=kubernetes.io/dockerconfigjson \
    -n jenkins

    execute_with_suppression kubectl create secret generic registry-credentials \
    --from-file=config.json=/root/.docker/config.json \
    -n jenkins
    log_success "Docker build support configured"
  fi

  log_step "4" "Installing Jenkins via Helm"
  if helm_install_with_summary "jenkins" "jenkins" \
    jenkins \
    --namespace jenkins \
    --set dockerBuildEnabled=$DOCKER_BUILD_ENABLED \
    --values $MOUNT_PATH/kubernetes/install_k8s/jenkins/values-mod.yaml \
    jenkins/jenkins; then
    
    log_step "5" "Configuring storage and ingress"
    createLocalStorageClassAndPV "jenkins-storage" "jenkins-pv" "/data/volumes/jenkins"
    gok patch ingress jenkins jenkins letsencrypt jenkins
    
    log_step "6" "Waiting for Jenkins services to be ready"
    log_info "Waiting for Jenkins services to be up (timeout: 120s)..."
    if kubectl --timeout=120s wait --for=condition=Ready pods --all --namespace jenkins >/dev/null 2>&1; then
      log_success "Jenkins services are now up!"
      log_step "7" "Verifying Jenkins connectivity"
      checkCurl curl -XGET http://jenkins.jenkins.svc:8080 -u "admin:${ADMIN_PASSWORD}" --insecure
      
      show_installation_summary "jenkins" "jenkins" "CI/CD automation server with Docker build support"
      log_component_success "jenkins" "Jenkins CI/CD server installed successfully"
      
      # Call next module suggestion
      suggest_and_install_next_module "jenkins"
    else
      log_error "Jenkins services timed-out, please check the installation!"
      return 1
    fi
  else
    log_error "Jenkins installation failed"
    return 1
  fi
}

opensearchInst(){
  helm repo add openSearch https://opensearch-project.github.io/helm-charts/
  helm repo update
  kubectl create ns opensearch
  ADMIN_PASSWORD=$(promptSecret "Enter Admin Password: ")
  kubectl create secret generic opensearch-password \
        --from-literal=OPENSEARCH_INITIAL_ADMIN_PASSWORD="${ADMIN_PASSWORD}" -n opensearch
  helm install opensearch \
    --namespace opensearch \
    --create-namespace \
    --values $MOUNT_PATH/kubernetes/install_k8s/opensearch/values.yaml \
    openSearch/opensearch

  createLocalStorageClassAndPV "opensearch-storage" "opensearch-pv" "/data/volumes/pv5"
  echo "Waiting for services to be up!!!!"
  kubectl --timeout=120s wait --for=condition=Ready pods --all --namespace opensearch
  [[ $? -eq 0 ]] && echoSuccess "Opensearch services are now up!" || echoFailed "Opensearch services timed-out, plaese check!!"
  checkCurl curl -XGET https://opensearch-cluster-master.opensearch.svc:9200 -u "admin:${ADMIN_PASSWORD}" --insecure
  #gok patch ingress opensearch opensearch letsencrypt opensearch
  opensearchSecret $ADMIN_PASSWORD
}

fluentdReset(){
  helm uninstall fluentd -n fluentd
  kubectl delete ns fluentd
}

fluentdInst(){
  helm repo add fluent https://fluent.github.io/helm-charts
  helm repo update
  kubectl create ns fluentd

  # Workaround for now before permanent solution is identified, Begin
  ADMIN_PASSWORD=$(kubectl get secret opensearch-secrets -n kube-system -o=jsonpath='{.data.OPENSEARCH_INITIAL_ADMIN_PASSWORD}' | base64 -d)
  cat $MOUNT_PATH/kubernetes/install_k8s/fluentd/values.yaml | \
    sed "s|__PASSWORD__|${ADMIN_PASSWORD}|g" > $MOUNT_PATH/kubernetes/install_k8s/fluentd/values_temp.yaml
  # End

  helm install fluentd \
        --namespace fluentd \
        --create-namespace \
        --values $MOUNT_PATH/kubernetes/install_k8s/fluentd/values_temp.yaml \
        fluent/fluentd
  rm $MOUNT_PATH/kubernetes/install_k8s/fluentd/values_temp.yaml
  gok patch ingress fluentd fluentd letsencrypt fluentd
}

resetDockerRegistry(){
  log_component_start "registry-reset" "Removing container registry and related resources"
  
  log_step "1" "Checking registry installation status"
  local registry_installed=false
  if kubectl get namespace registry >/dev/null 2>&1; then
    registry_installed=true
    log_info "Registry namespace found - proceeding with removal"
  else
    log_warning "Registry namespace not found - may already be removed"
  fi
  
  if [[ "$registry_installed" == "true" ]]; then
    log_step "2" "Removing registry Helm release"
    if helm list -n registry 2>/dev/null | grep -q registry; then
      if helm_uninstall_with_summary "registry" "registry" --namespace registry registry; then
        log_success "Registry Helm release removed"
      else
        log_warning "Registry Helm release removal had issues, continuing..."
      fi
    else
      log_info "No registry Helm release found to remove"
    fi
    
    log_step "3" "Cleaning up registry persistent storage"
    emptyLocalFsStorage "Registry" "registry-pv" "registry-storage" "/data/volumes/pv4" "registry"
    log_success "Registry storage cleaned up"
    
    log_step "4" "Removing registry namespace and resources"
    if kubectl_with_summary delete "namespace" registry; then
      log_success "Registry namespace and all resources removed"
    else
      log_warning "Registry namespace removal had issues"
    fi
    
    log_step "5" "Cleaning up Docker trust store certificates"
    local cert_dir="/etc/docker/certs.d/$(registrySubdomain).$(sedRootDomain)"
    if [[ -d "$cert_dir" ]]; then
      if execute_with_suppression rm -rf "$cert_dir"; then
        log_success "Docker trust store certificates removed from $cert_dir"
      else
        log_warning "Could not remove Docker trust store certificates"
      fi
    else
      log_info "No Docker trust store certificates found to remove"
    fi
    
    show_installation_summary "registry" "registry" "Container registry system removed"
    log_component_success "registry-reset" "Container registry successfully removed from cluster"
    
    echo
    log_info "Registry reset completed. You can reinstall with: gok install registry"
  else
    log_info "Registry was not installed - nothing to reset"
    log_component_success "registry-reset" "Registry reset completed (nothing to remove)"
  fi
}

# createDevWorkspace and deleteDevWorkspace methods to prompt for namespace, username, workspace name, and manifest file, then call apply_devworkspace.py for create/delete actions.
createDevWorkspace() {
  echo "=== Create Che DevWorkspace ==="
  NAMESPACE=$(promptUserInput "Enter namespace (che-user): " "che-user")
  USERNAME=$(promptUserInput "Enter username (user1): " "user1")
  WORKSPACE=$(promptUserInput "Enter workspace name (devworkspace1): " "devworkspace1")
  MANIFEST_FILE=$(promptUserInput "Enter devworkspace manifest file path (devworkspace.yaml): " "devworkspace.yaml")
  export CHE_USER_NAMESPACE="$NAMESPACE"
  export CHE_USER_NAME="$USERNAME"
  export CHE_WORKSPACE_NAME="$WORKSPACE"
  export DW_FILE="$MANIFEST_FILE"
  export DW_DELETE="false"
  pushd "$MOUNT_PATH/kubernetes/install_k8s/eclipseche" || exit
  if ! python3 -c "import kubernetes" &>/dev/null; then
    apt-get install -y python3-kubernetes
  else
    echo "python3-kubernetes is already installed."
  fi

  if ! python3 -c "import yaml" &>/dev/null; then
    apt-get install -y python3-yaml
  else
    echo "python3-yaml is already installed."
  fi
  python3 "$WORKING_DIR/eclipseche/apply_devworkspace.py"
  popd || exit
}

deleteDevWorkspace() {
  echo "=== Delete Che DevWorkspace ==="
  NAMESPACE=$(promptUserInput "Enter namespace: " "che-user")
  USERNAME=$(promptUserInput "Enter username: " "user1")
  WORKSPACE=$(promptUserInput "Enter workspace name: " "devworkspace1")
  MANIFEST_FILE=$(promptUserInput "Enter devworkspace manifest file path: " "devworkspace.yaml")
  export CHE_USER_NAMESPACE="$NAMESPACE"
  export CHE_USER_NAME="$USERNAME"
  export CHE_WORKSPACE_NAME="$WORKSPACE"
  export DW_FILE="$MANIFEST_FILE"
  export DW_DELETE="true"
  pushd "$MOUNT_PATH/kubernetes/install_k8s/eclipseche" || exit
  python3 "$WORKING_DIR/eclipseche/apply_devworkspace.py"
  popd || exit
}

createDevWorkspaceV2() {
  echo "=== Create Che DevWorkspace ==="
  USERNAME=$(promptUserInput "Enter username (user1): " "user1")
  
  echo "Select workspace type:"
  echo "1 => core-java"
  echo "2 => spring-web"
  echo "3 => python-web"
  echo "4 => springboot-backend"
  echo "5 => tensorflow"
  echo "6 => microservice-study"
  echo "7 => javaparser"
  echo "8 => nlp"
  echo "9 => kubeauthentication"

  WORKSPACE_TYPE_INDEX=$(promptUserInput "Enter workspace type index (1): " "1")
  case "$WORKSPACE_TYPE_INDEX" in
    1) WORKSPACE_TYPE="core-java" ;;
    2) WORKSPACE_TYPE="springboot-web" ;;
    3) WORKSPACE_TYPE="python-web" ;;
    4) WORKSPACE_TYPE="springboot-backend" ;;
    5) WORKSPACE_TYPE="tensorflow" ;;
    6) WORKSPACE_TYPE="microservice-study" ;;
    7) WORKSPACE_TYPE="javaparser" ;;
    8) WORKSPACE_TYPE="nlp" ;;
    9) WORKSPACE_TYPE="kubeauthentication";;
    *) WORKSPACE_TYPE="core-java" ;;
  esac

  case "$WORKSPACE_TYPE" in
    core-java) WORKSPACE="java" ;;
    springboot-web) WORKSPACE="spring" ;;
    python-web) WORKSPACE="python" ;;
    springboot-backend) WORKSPACE="spring" ;;
    tensorflow) WORKSPACE="tensorflow" ;;
    microservice-study) WORKSPACE="microservice-study" ;;
    javaparser) WORKSPACE="javaparser" ;;
    nlp) WORKSPACE="nlp" ;;
    kubeauthentication) WORKSPACE="kubeauthentication" ;;
    *) WORKSPACE="java" ;;
  esac

  export CHE_USER_NAMESPACE="$USERNAME"
  export CHE_USER_NAME="$USERNAME"
  export CHE_WORKSPACE_NAME="$WORKSPACE"
  export WORKSPACE_TYPE="$WORKSPACE_TYPE"
  export DW_DELETE="false"
  pushd "$MOUNT_PATH/kubernetes/install_k8s/eclipseche" || exit
  if ! python3 -c "import kubernetes" &>/dev/null; then
    apt-get install -y python3-kubernetes
  else
    echo "python3-kubernetes is already installed."
  fi

  if ! python3 -c "import yaml" &>/dev/null; then
    apt-get install -y python3-yaml
  else
    echo "python3-yaml is already installed."
  fi
  python3 "$WORKING_DIR/eclipseche/create_devworkspace.py"
  popd || exit
}

deleteDevWorkspaceV2() {
  echo "=== Delete Che DevWorkspace ==="
  USERNAME=$(promptUserInput "Enter username (user1): " "user1")
  
  echo "Select workspace type:"
  echo "1 => core-java"
  echo "2 => springboot-web"
  echo "3 => python-web"
  echo "4 => springboot-backend"
  echo "5 => tensorflow"
  echo "6 => microservice-study"
  echo "7 => javaparser"
  echo "8 => nlp"
  echo "9 => kubeauthentication"
  WORKSPACE_TYPE_INDEX=$(promptUserInput "Enter workspace type index (1): " "1")
  case "$WORKSPACE_TYPE_INDEX" in
    1) WORKSPACE_TYPE="core-java" ;;
    2) WORKSPACE_TYPE="springboot-web" ;;
    3) WORKSPACE_TYPE="python-web" ;;
    4) WORKSPACE_TYPE="springboot-backend" ;;
    5) WORKSPACE_TYPE="tensorflow" ;;
    6) WORKSPACE_TYPE="microservice-study" ;;
    7) WORKSPACE_TYPE="javaparser" ;;
    8) WORKSPACE_TYPE="nlp" ;;
    9) WORKSPACE_TYPE="kubeauthentication" ;;
    *) WORKSPACE_TYPE="core-java" ;;
  esac

  case "$WORKSPACE_TYPE" in
    core-java) WORKSPACE="java" ;;
    springboot-web) WORKSPACE="spring" ;;
    python-web) WORKSPACE="python" ;;
    springboot-backend) WORKSPACE="spring" ;;
    tensorflow) WORKSPACE="tensorflow" ;;
    nlp) WORKSPACE="nlp" ;;
    microservice-study) WORKSPACE="microservice-study" ;;
    javaparser) WORKSPACE="javaparser" ;;
    kubeauthentication) WORKSPACE="kubeauthentication" ;;
    *) WORKSPACE="java" ;;
  esac

  export CHE_USER_NAMESPACE="$USERNAME"
  export CHE_USER_NAME="$USERNAME"
  export CHE_WORKSPACE_NAME="$WORKSPACE"
  export WORKSPACE_TYPE="$WORKSPACE_TYPE"
  export DW_DELETE="true"
  pushd "$MOUNT_PATH/kubernetes/install_k8s/eclipseche" || exit
  python3 "$WORKING_DIR/eclipseche/create_devworkspace.py"
  popd || exit
}

# https://itnext.io/error-x509-certificate-signed-by-unknown-authority-error-is-returned-f0e5d436f467
# Generate User & Password
genRegistryPassword(){
  export REGISTRY_USER="$1"
  export REGISTRY_PASS="$2"
  export DESTINATION_FOLDER=./registry-creds

  # Backup credentials to local files (in case you'll forget them later on)
  if execute_with_suppression mkdir -p ${DESTINATION_FOLDER} && \
     execute_with_suppression bash -c "echo '${REGISTRY_USER}' > ${DESTINATION_FOLDER}/registry-user.txt" && \
     execute_with_suppression bash -c "echo '${REGISTRY_PASS}' > ${DESTINATION_FOLDER}/registry-pass.txt"; then
    log_success "Registry credentials backed up to local files"
  else
    log_error "Failed to backup registry credentials"
    return 1
  fi

  if execute_with_suppression docker run --entrypoint htpasswd registry:2.7.0 \
      -Bbn ${REGISTRY_USER} ${REGISTRY_PASS} > ${DESTINATION_FOLDER}/htpasswd; then
    log_success "Registry htpasswd authentication file generated"
  else
    log_error "Failed to generate registry htpasswd file"
    return 1
  fi

  unset REGISTRY_USER REGISTRY_PASS DESTINATION_FOLDER
}

imagePullSecrets(){
  # Create a secret for the registry
  : "${DOCKER_USERNAME:=$(promptUserInput "Please enter docker user id: ")}"
  : "${DOCKER_PASSWORD:=$(promptSecret "Please enter your docker password: ")}"
  DOCKER_EMAIL="skmaji@outlook.com"

  # Delete the secret if it already exists
  if kubectl get secret regcred -n kube-system >/dev/null 2>&1; then
    if execute_with_suppression kubectl delete secret regcred -n kube-system; then
      log_success "Existing Docker registry secret removed"
    fi
  fi
  
  if execute_with_suppression kubectl create secret docker-registry regcred \
    --docker-server=$(fullRegistryUrl) \
    --docker-username=$DOCKER_USERNAME \
    --docker-password=$DOCKER_PASSWORD \
    --docker-email=$DOCKER_EMAIL -n kube-system; then
    log_success "Docker registry secret created (user: $DOCKER_USERNAME)"
  else
    log_error "Failed to create Docker registry secret"
    return 1
  fi

  if genRegistryPassword $DOCKER_USERNAME $DOCKER_PASSWORD; then
    log_success "Registry authentication credentials generated"
  else
    log_error "Failed to generate registry authentication credentials"
    return 1
  fi
}

# https://blog.zachinachshon.com/docker-registry/
# Verify access to the registry
verifyRegistryInst(){
  export DESTINATION_FOLDER=./registry-creds
  export USER=$(cat ${DESTINATION_FOLDER}/registry-user.txt)
  export PASSWORD=$(cat ${DESTINATION_FOLDER}/registry-pass.txt)

  curl -kiv -H \
    "Authorization: Basic $(echo -n "${USER}:${PASSWORD}" | base64)" \
    https://"$(registrySubdomain).$(rootDomain)"/v2/_catalog

  wget --no-check-certificate --header \
    "Authorization: Basic $(echo -n "${USER}:${PASSWORD}" | base64)" \
    https://"$(registrySubdomain).$(rootDomain)"/v2/_catalog

  unset USER PASSWORD
}

# https://itnext.io/error-x509-certificate-signed-by-unknown-authority-error-is-returned-f0e5d436f467
dockerRegistryInst(){
  log_component_start "docker-registry" "Setting up container registry with authentication"
  
  log_step "1" "Configuring Docker registry authentication secrets"
  if imagePullSecrets; then
    log_success "Docker registry authentication configured"
  else
    log_error "Failed to configure Docker registry authentication"
    return 1
  fi
  
  log_step "2" "Adding Twuni Helm repository"
  if execute_with_suppression helm repo add twuni https://helm.twun.io; then
    log_success "Twuni Helm repository added"
  else
    log_error "Failed to add Twuni Helm repository"
    return 1
  fi
  
  log_step "3" "Installing Docker registry via Helm"
  export DESTINATION_FOLDER=./registry-creds
  if helm_install_with_summary "registry" "registry" \
      registry twuni/docker-registry \
      --namespace registry \
      --create-namespace \
      --set replicaCount=1 \
      --set persistence.enabled=true \
      --set persistence.size=10Gi \
      --set persistence.deleteEnabled=true \
      --set persistence.storageClass=registry-storage \
      --set secrets.htpasswd="$(cat ${DESTINATION_FOLDER}/htpasswd)" \
      --values https://github.com/sumitmaji/kubernetes/raw/master/install_k8s/registry/values.yaml; then
    log_success "Docker registry deployed successfully"
  else
    log_error "Failed to deploy Docker registry"
    return 1
  fi
}

# It is used to generate both client/server certificate using
# Kubernetes ca certificate in /etc/kubernetes/pki/ca.crt
createCertificate(){
  if [[ -n $TRACE ]]; then
    set -x
  fi

  : ${INSTALL_PATH:=$MOUNT_PATH/kubernetes/install_k8s}

  while [[ $# -gt 0 ]]
  do
  key="$1"
  case $key in
   -i|--ip)
   NODE_IP="$2"
   shift
   shift
   ;;
   -h|--host)
   HOSTNAME="$2"
   shift
   shift
   ;;
   -f|--file)
   FILENAME="$2"
   shift
   shift
   ;;
   -t|--type)
   TYPE="$2"
   shift
   shift
   ;;
  esac
  done

  if [ -z "$NODE_IP" ]
  then
  	echo "Please provide node ip"
  	exit 0
  fi
  if [ -z "$HOSTNAME" ]
  then
          echo "Please provide node hostname"
          exit 0
  fi
  if [ -z "$FILENAME" ]
  then
          echo "Please provide node filename"
          exit 0
  fi
  if [ -z "$TYPE" ]
  then
          echo "Please provide file type"
          exit 0
  fi

  : ${COUNTRY:=IN}
  : ${STATE:=UP}
  : ${LOCALITY:=GN}
  : ${ORGANIZATION:=CloudInc}
  : ${ORGU:=IT}
  : ${EMAIL:=cloudinc.gmail.com}
  : ${COMMONNAME:=kube-system}

  mkdir -p $MOUNT_PATH/certs
  pushd $MOUNT_PATH/certs

  if [ $TYPE == 'server' ]
  then
   keyUsage='extendedKeyUsage = clientAuth,serverAuth'
   #HOSTNAME="${HOSTNAME}-${FILENAME}" # Need to see why I did that
  else
   keyUsage='extendedKeyUsage = clientAuth'
   FILENAME="${FILENAME}-client"
   #HOSTNAME="${HOSTNAME}-$FILENAME"
  fi

  cat <<EOF | sudo tee ${FILENAME}-openssl.cnf
[req]
req_extensions = v3_req
distinguished_name = req_distinguished_name
[req_distinguished_name]
[ v3_req ]
basicConstraints = CA:FALSE
keyUsage = nonRepudiation, digitalSignature, keyEncipherment
$keyUsage
`
if [ $TYPE == 'server' ]
then
echo "subjectAltName = IP:$NODE_IP, DNS:$HOSTNAME"
fi`
EOF

  #Create a private key
  openssl genrsa -out $HOSTNAME.key 2048

  #Create CSR for the node
  openssl req -new -key $HOSTNAME.key \
    -subj "/CN=$NODE_IP" \
    -subj "/C=$COUNTRY/ST=$STATE/L=$LOCALITY/O=$ORGANIZATION/OU=$ORGU/CN=$HOSTNAME/emailAddress=$EMAIL" \
    -out $HOSTNAME.csr -config ${FILENAME}-openssl.cnf

  #Create a self signed certificate
  openssl x509 -req -in $HOSTNAME.csr -CA /etc/kubernetes/pki/ca.crt -CAkey /etc/kubernetes/pki/ca.key -CAcreateserial \
    -out $HOSTNAME.crt -days 10000 -extensions v3_req -extfile ${FILENAME}-openssl.cnf

  #Copy ca.crt to crt
  cat /etc/kubernetes/pki/ca.crt >> $HOSTNAME.crt

  #Verify a Private Key Matches a Certificate
  openssl x509 -noout -text -in $HOSTNAME.crt

  popd
}

# It is used to generate client certificate
# using kubectl command
createClientCertificate(){
  USERNAME=$1
  GROUPNAME=$2
  echo + Creating private key: ${USERNAME}.key
  openssl genrsa -out ${USERNAME}.key 4096

  echo + Creating signing request: ${USERNAME}-csr
  openssl req -new -key ${USERNAME}.key -out ${USERNAME}.csr -subj "/CN=${USERNAME}/O=${GROUPNAME}" \
          -addext "subjectAltName = DNS:${USERNAME}"

  WC=$(kubectl get csr ${USERNAME}-csr 2>/dev/null | wc -l)
  if [ "$WC" != "0" ]; then
    kubectl delete csr ${USERNAME}-csr
  fi
  echo + Creating signing request in kubernetes
  cat <<EOF | kubectl create -f -
apiVersion: certificates.k8s.io/v1
kind: CertificateSigningRequest
metadata:
  name: ${USERNAME}-csr
spec:
  groups:
    - system:authenticated
    - ${GROUPNAME}
  request: $(cat ${USERNAME}.csr | base64 | tr -d '\n')
  signerName: kubernetes.io/kube-apiserver-client
  usages:
    - digital signature
    - key encipherment
    - client auth
EOF

  kubectl certificate approve ${USERNAME}-csr

  kubectl get csr ${USERNAME}-csr -o jsonpath='{.status.certificate}' | base64 -d >${USERNAME}.crt
}

# The method creates certificate for user having admin role and generate kube config file
# for login to api server
createKubeConfig() {
  USERNAME=$1
  : ${IP:=$(ifconfig eth0 2>/dev/null | awk '/inet / {print $2}' | sed 's/addr://')}
  if [ -z "$IP" ]; then
    : ${IP:=$(ifconfig enp0s3 2>/dev/null | awk '/inet / {print $2}' | sed 's/addr://')}
  fi
  adminRole
  echo + Creating private key: ${USERNAME}.key
  openssl genrsa -out ${USERNAME}.key 4096

  echo + Creating signing request: ${USERNAME}.csr
  openssl req -new -key ${USERNAME}.key -out ${USERNAME}.csr -subj "/CN=${USERNAME}/O=cloud:masters"
  WC=$(kubectl get csr ${USERNAME}-csr 2>/dev/null | wc -l)
  if [ "$WC" != "0" ]; then
    kubectl delete csr ${USERNAME}-csr
  fi
  echo + Creating signing request in kubernetes
  cat <<EOF | kubectl create -f -
apiVersion: certificates.k8s.io/v1
kind: CertificateSigningRequest
metadata:
  name: ${USERNAME}-csr
spec:
  groups:
    - system:authenticated
    - cloud:masters
  request: $(cat ${USERNAME}.csr | base64 | tr -d '\n')
  signerName: kubernetes.io/kube-apiserver-client
  usages:
    - digital signature
    - key encipherment
    - client auth
EOF

  kubectl certificate approve ${USERNAME}-csr

  kubectl get csr ${USERNAME}-csr -o jsonpath='{.status.certificate}' | base64 -d >${USERNAME}.crt

  echo "======Kubeconfig file user ${USERNAME}.conf generated"

  kubectl config set-cluster cloud.com --certificate-authority=/etc/kubernetes/pki/ca.crt --embed-certs=true --server=https://${IP}:6443 --kubeconfig=/root/${USERNAME}.conf
  kubectl config set-credentials ${USERNAME} --client-key=${USERNAME}.key --client-certificate=${USERNAME}.crt --embed-certs=true --kubeconfig=/root/${USERNAME}.conf
  kubectl config --kubeconfig=/root/${USERNAME}.conf set-context ${USERNAME}@cloud.com --cluster=cloud.com --user=${USERNAME}
  kubectl config --kubeconfig=/root/${USERNAME}.conf use-context ${USERNAME}@cloud.com

  rm ${USERNAME}.key ${USERNAME}.csr ${USERNAME}.crt

}

patchLdapSecure() {
  kubectl patch ing "$NAME" --patch "$(
    cat <<EOF
metadata:
  annotations:
    nginx.ingress.kubernetes.io/auth-signin: https://$(defaultSubdomain).$(rootDomain)/authenticate
    nginx.ingress.kubernetes.io/auth-url: https://$(defaultSubdomain).$(rootDomain)/check
EOF
  )" -n "$NS"
}

patchOauth2Secure() {
  NAME=$1
  NS=$2
  RD=$3
  kubectl patch ing "$NAME" --patch "$(
    cat <<EOF
metadata:
  annotations:
    nginx.ingress.kubernetes.io/auth-signin: https://$(defaultSubdomain).$(rootDomain)/oauth2/start?rd=${RD}
    nginx.ingress.kubernetes.io/auth-url: https://$(defaultSubdomain).$(rootDomain)/oauth2/auth
    nginx.ingress.kubernetes.io/auth-response-headers: Authorization
EOF
  )" -n "$NS"
}


patchLetsEncrypt() {
  NAME=$1
  NS=$2
  SUBDOMAIN=$(subDomain $3)

  kubectl patch ing "$NAME" --patch "$(
    cat <<EOF
metadata:
  annotations:
    #certmanager.k8s.io/cluster-issuer: $(getClusterIssuerName)
    cert-manager.io/cluster-issuer: $(getClusterIssuerName)
    nginx.ingress.kubernetes.io/ssl-redirect: "true"
spec:
  tls:
    - hosts:
        - ${SUBDOMAIN}.$(rootDomain)
      secretName: ${SUBDOMAIN}-$(sedRootDomain)
EOF
  )" -n "$NS"
  kubectl patch ing "$NAME" --type=json -p='[{"op": "replace", "path": "/spec/rules/0/host", "value":"'$SUBDOMAIN'.'$(rootDomain)'"}]' -n "$NS"

  kubectl --timeout=120s -n ${NS} wait --for=condition=Ready certificates.cert-manager.io ${SUBDOMAIN}-$(sedRootDomain)
}

# Is is used see the values that would be generated by helm
helmShowValues(){
  CHART_NAME=$1
  helm get values $CHART_NAME -a
}

# Is is used to see all the resources that would be generated by helm
helmShowAll(){
  CHART_NAME=$1
  helm get all $CHART_NAME
}

helmTemplate(){
  CHART_REPO=$1
  helm template $CHART_REPO -g
}

# Is is used see the template that would be generated by helm
helmShowTemplate(){
  CHART_REPO=$1
  helm install $CHART_REPO  -g \
    --dry-run
}

fetch_client_secret() {
  # Parameters
  local KEYCLOAK_URL=$1
  local REALM_NAME=$2
  local CLIENT_ID=$3
  local ADMIN_USERNAME=$4
  local ADMIN_PASSWORD=$5

  # Validate input
  if [[ -z "$KEYCLOAK_URL" || -z "$REALM_NAME" || -z "$CLIENT_ID" || -z "$ADMIN_USERNAME" || -z "$ADMIN_PASSWORD" ]]; then
    echo "Usage: fetch_client_secret <KEYCLOAK_URL> <REALM_NAME> <CLIENT_ID> <ADMIN_USERNAME> <ADMIN_PASSWORD>"
    return 1
  fi

  # Fetch admin access token
  ACCESS_TOKEN=$(curl -s -X POST https://"$KEYCLOAK_URL/realms/master/protocol/openid-connect/token" \
    -H "Content-Type: application/x-www-form-urlencoded" \
    -d "username=$ADMIN_USERNAME" \
    -d "password=$ADMIN_PASSWORD" \
    -d "grant_type=password" \
    -d "client_id=admin-cli" | jq -r '.access_token')

  if [ -z "$ACCESS_TOKEN" ]; then
    echo "Failed to fetch access token. Please check your credentials."
    return 1
  fi

  # Fetch client UUID from the realm
  CLIENT_UUID=$(curl -s -X GET https://"$KEYCLOAK_URL/admin/realms/$REALM_NAME/clients" \
    -H "Authorization: Bearer $ACCESS_TOKEN" \
    -H "Content-Type: application/json" | jq -r ".[] | select(.clientId==\"$CLIENT_ID\") | .id")

  if [ -z "$CLIENT_UUID" ]; then
    echo "Client '$CLIENT_ID' not found in realm '$REALM_NAME'."
    return 1
  fi

  # Fetch client secret
  CLIENT_SECRET=$(curl -s -X GET https://"$KEYCLOAK_URL/admin/realms/$REALM_NAME/clients/$CLIENT_UUID/client-secret" \
    -H "Authorization: Bearer $ACCESS_TOKEN" \
    -H "Content-Type: application/json" | jq -r '.value')

  if [ -z "$CLIENT_SECRET" ]; then
    echo "Failed to fetch client secret for client '$CLIENT_ID'."
    return 1
  fi

  # Return the client secret
  echo "$CLIENT_SECRET"
}

oauth2Secret(){
  CLIENT_ID=$(kubectl get secret keycloak-secrets -n keycloak -o jsonpath="{['data']['CLIENT_ID']}" | base64 --decode)
  REALM=$(kubectl get secret keycloak-secrets -n keycloak -o jsonpath="{['data']['OAUTH_REALM']}" | base64 --decode)
  KEYCLOAK_URL=$(fullKeycloakUrl)
  ADMIN_USERNAME=$(kubectl get secret keycloak-secrets -n keycloak -o jsonpath="{['data']['KEYCLOAK_ADMIN']}" | base64 --decode)
  ADMIN_PASSWORD=$(kubectl get secret keycloak-secrets -n keycloak -o jsonpath="{.data.KEYCLOAK_ADMIN_PASSWORD}" | base64 --decode)

  client_secret=$(fetch_client_secret "$KEYCLOAK_URL" "$REALM" "$CLIENT_ID" "$ADMIN_USERNAME" "$ADMIN_PASSWORD")
  
  # Use environment variables with fallbacks to predefined Keycloak configuration
  ACTIVE_PROFILE="${ACTIVE_PROFILE:-keycloak}"
  OIDC_ISSUE_URL="${OIDC_ISSUE_URL:-https://keycloak.gokcloud.com/realms/${REALM}}"
  OIDC_USERNAME_CLAIM="${OIDC_USERNAME_CLAIM:-sub}"
  OIDC_GROUPS_CLAIM="${OIDC_GROUPS_CLAIM:-groups}"
  AUTH0_DOMAIN="${AUTH0_DOMAIN:-keycloak.gokcloud.com}"
  APP_HOST="${APP_HOST:-kube.gokcloud.com}"
  JWKS_URL="${JWKS_URL:-${OIDC_ISSUE_URL}/protocol/openid-connect/certs}"
  OAUTH_SERVER_URI="${OAUTH_SERVER_URI:-https://$(fullKeycloakUrl)}"

  # If secret already exists then delete it
  kubectl get secret oauth-secrets -n kube-system 2>/dev/null && kubectl delete secret oauth-secrets -n kube-system
  kubectl create secret generic oauth-secrets \
    --from-literal=OAUTH_REALM="${REALM}" \
    --from-literal=ACTIVE_PROFILE="${ACTIVE_PROFILE}" \
    --from-literal=OIDC_CLIENT_ID="${CLIENT_ID}" \
    --from-literal=OIDC_ISSUE_URL="${OIDC_ISSUE_URL}" \
    --from-literal=OIDC_USERNAME_CLAIM="${OIDC_USERNAME_CLAIM}" \
    --from-literal=OIDC_GROUPS_CLAIM="${OIDC_GROUPS_CLAIM}" \
    --from-literal=AUTH0_DOMAIN="${AUTH0_DOMAIN}" \
    --from-literal=APP_HOST="${APP_HOST}" \
    --from-literal=JWKS_URL="${JWKS_URL}" \
    --from-literal=OAUTH_SERVER_URI="${OAUTH_SERVER_URI}" \
    --from-literal=OIDC_CLIENT_SECRET="${client_secret}" -n kube-system
}

opensearchSecret(){
  ADMIN_PASSWORD=$1
  # If secret already exists then delete it
  kubectl get secret opensearch-secrets -n kube-system 2>/dev/null && kubectl delete secret opensearch-secrets -n kube-system
  kubectl create secret generic opensearch-secrets \
    --from-literal=OPENSEARCH_INITIAL_ADMIN_PASSWORD="${ADMIN_PASSWORD}" -n kube-system
}

csiDriverInstall(){
  helm repo add secrets-store-csi-driver https://kubernetes-sigs.github.io/secrets-store-csi-driver/charts 
  helm repo update
  helm install csi-secrets-store secrets-store-csi-driver/secrets-store-csi-driver --namespace kube-system 

}

csiDriverUnInstall(){
  helm uninstall csi-secrets-store --namespace kube-system
}

vaultLogin(){
  ROOT_TOKEN=$(kubectl get secret vault-init-keys -n vault -o json | jq -r '.data["vault-init.json"]' | base64 -d | jq -r '.root_token')
  kubectl exec -it vault-0 -n vault -- vault login ${ROOT_TOKEN}
}

cleanExampleSecretStoreInVault(){
  kubectl delete pod vault-secret-pod -n default
  kubectl delete secretproviderclass vault-secret-provider -n default
  kubectl delete secret my-k8s-secret -n default >/dev/null 2>&1

  vaultLogin
  kubectl exec -it vault-0 -n vault -- vault kv delete secret/my-secret
  kubectl exec -it vault-0 -n vault -- vault delete auth/kubernetes/role/my-role
  kubectl exec -it vault-0 -n vault -- vault policy delete my-policy  
}

cleanDockerRegistrySecretStoreInVault(){
  kubectl delete pod docker-registry-secret-pod -n default
  kubectl delete secretproviderclass docker-registry-secret-provider -n default
  
}

verifyVault(){
  pushd $MOUNT_PATH/kubernetes/install_k8s/vault
  chmod +x *.sh
  ./verification.sh
  popd
}

verifyCertManater(){
  pushd $MOUNT_PATH/kubernetes/install_k8s/scripts
  chmod +x *.sh
  ./verify_cert_manager.sh
  popd
}

debugVault(){
  pushd $MOUNT_PATH/kubernetes/install_k8s/vault
  chmod +x *.sh
  ./debug-vault.sh
  popd
}

createVaultSecretStore() {
  # Initialize variables
  local secret_path=""
  local role_name=""
  local policy_name=""
  local secret_provider_class_name=""
  local prefix=""
  local namespace="default"
  local kv_pairs=()
  local secret_name=""

  # Parse command-line arguments
  while [[ $# -gt 0 ]]; do
    case $1 in
      -p|--secret-path)
        secret_path="$2"
        shift 2
        ;;
      -r|--role-name)
        role_name="$2"
        shift 2
        ;;
      -l|--policy-name)
        policy_name="$2"
        shift 2
        ;;
      -s|--secret-provider-class-name)
        secret_provider_class_name="$2"
        shift 2
        ;;
      -x|--prefix)
        prefix="$2"
        shift 2
        ;;
      -n|--namespace)
        namespace="$2"
        shift 2
        ;;
      -k|--key-value)
        kv_pairs+=("$2")
        shift 2
        ;;
      --secret-name)
        secret_name="$2"
        shift 2
        ;;
      *)
        echo "Unknown option: $1"
        echo "Usage: createVaultSecretStore -p <secret_path> -r <role_name> -l <policy_name> -s <secret_provider_class_name> -x <prefix> -n <namespace> -k <key=value> [--secret-name <secret_name>]"
        return 1
        ;;
    esac
  done

  # Validate required arguments
  if [[ -z "$secret_path" || -z "$role_name" || -z "$policy_name" || -z "$secret_provider_class_name" || -z "$prefix" || ${#kv_pairs[@]} -eq 0 ]]; then
    echo "Usage: createVaultSecretStore -p <secret_path> -r <role_name> -l <policy_name> -s <secret_provider_class_name> -x <prefix> -n <namespace> -k <key=value> [--secret-name <secret_name>]"
    return 1
  fi

  # Default secret name if not provided
  if [[ -z "$secret_name" ]]; then
    secret_name="${secret_provider_class_name}-secret"
  fi

  echo "SECRET_PATH: $secret_path"
  echo "ROLE_NAME: $role_name"
  echo "POLICY_NAME: $policy_name"
  echo "SECRET_PROVIDER_CLASS_NAME: $secret_provider_class_name"
  echo "PREFIX: $prefix"
  echo "NAMESPACE: $namespace"
  echo "KV_PAIRS: ${kv_pairs[*]}"
  echo "SECRET_NAME: $secret_name"

  vaultLogin

  # Create the secret in Vault with multiple key-value pairs
  if kubectl exec -it vault-0 -n vault -- vault kv get "$secret_path" >/dev/null 2>&1; then
    echo "Secret at path $secret_path already exists in Vault. Skipping creation."
  else
    local secret_command="vault kv put $secret_path"
    for kv_pair in "${kv_pairs[@]}"; do
      secret_command+=" $kv_pair"
    done
    kubectl exec -it vault-0 -n vault -- $secret_command
  fi

  # Create a policy for the secret
  if kubectl exec -it vault-0 -n vault -- vault policy read "$policy_name" >/dev/null 2>&1; then
    echo "Policy $policy_name already exists in Vault. Skipping creation."
  else
    kubectl exec -i vault-0 -n vault -- vault policy write "$policy_name" - <<EOF
path "$secret_path" {
  capabilities = ["read", "list"]
}
EOF
  fi

  # Create a role for Kubernetes authentication
  if kubectl exec -it vault-0 -n vault -- vault read auth/kubernetes/role/"$role_name" >/dev/null 2>&1; then
    echo "Role $role_name already exists in Vault. Skipping creation."
  else
    kubectl exec -it vault-0 -n vault -- vault write auth/kubernetes/role/"$role_name" \
      bound_service_account_names=* \
      bound_service_account_namespaces="$namespace" \
      policies="$policy_name" \
      ttl=24h
  fi

  # Generate the `objects` section dynamically
  local objects=""
  for kv_pair in "${kv_pairs[@]}"; do
    local key=$(echo "$kv_pair" | cut -d '=' -f 1)
    objects+="      - objectName: \"$prefix-$key\"\n"
    objects+="        objectType: \"kv\"\n"
    objects+="        secretPath: \"$secret_path\"\n"
    objects+="        secretKey: \"$key\"\n"
    objects+="        objectVersion: \"\"\n"
  done

  # Add the `secretObjects` section
  local secret_objects="  secretObjects:\n"
  secret_objects+="    - secretName: $secret_name\n"
  secret_objects+="      type: Opaque\n"
  secret_objects+="      data:\n"
  for kv_pair in "${kv_pairs[@]}"; do
    local key=$(echo "$kv_pair" | cut -d '=' -f 1)
    secret_objects+="        - objectName: \"$prefix-$key\"\n"
    secret_objects+="          key: \"$key\"\n"
  done

  # Generate the YAML for the SecretProviderClass
  local yaml=$(cat <<EOF
apiVersion: secrets-store.csi.x-k8s.io/v1
kind: SecretProviderClass
metadata:
  name: "$secret_provider_class_name"
  namespace: "$namespace"
spec:
  provider: vault
  parameters:
    vaultAddress: "https://vault.gokcloud.com"
    roleName: "$role_name"
    skipVerify: "true"
    vaultSkipTLSVerify: "true"
    objects: |
$objects
$secret_objects
EOF
)

  # Echo the generated YAML
  echo "Generated YAML:"
  printf "$yaml"

  # Apply the YAML
  printf "$yaml" | kubectl apply -f -
}

exampleSecretStoreInVaule(){
  vaultLogin
  kubectl exec -it vault-0 -n vault -- vault kv put secret/my-secret username="my-username" password="my-password"

  kubectl exec -it vault-0 -n vault -- vault write auth/kubernetes/role/my-role \
    bound_service_account_names=default \
    bound_service_account_namespaces=default \
    policies=my-policy \
    ttl=24h

  # For K/V v2 secrets engine
#   kubectl exec -i vault-0 -n vault -- vault policy write my-policy - <<EOF
# path "secret/data/my-secret" {
#   capabilities = ["read", "list"]
# }
# EOF

  # For K/V v1 secrets engine
  kubectl exec -i vault-0 -n vault -- vault policy write my-policy - <<EOF
path "secret/my-secret" {
  capabilities = ["read", "list"]
}
EOF

  cat <<EOF | kubectl apply -f -
apiVersion: secrets-store.csi.x-k8s.io/v1
kind: SecretProviderClass
metadata:
  name: vault-secret-provider
  namespace: default
spec:
  provider: vault
  parameters:
    vaultAddress: "http://vault.vault.svc.cloud.uat:8200"  
    roleName: "my-role"  
    skipVerify: "true"   
    vaultSkipTLSVerify: "true"                 
    objects: |
      - objectName: "my-username"
        objectType: "kv"
        secretPath: "secret/my-secret"
        objectVersion: ""
        secretKey: "username"
      - objectName: "my-password"
        objectType: "kv"
        secretPath: "secret/my-secret"
        objectVersion: ""
        secretKey: "password"
EOF

  cat <<EOF | kubectl apply -f -
apiVersion: v1
kind: Pod
metadata:
  name: vault-secret-pod
  namespace: default
spec:
  containers:
  - name: app
    image: busybox
    command: ["sleep", "3600"]
    volumeMounts:
    - name: secrets-store-inline
      mountPath: "/mnt/secrets-store"  # Path where the secret will be mounted
      readOnly: true
  volumes:
  - name: secrets-store-inline
    csi:
      driver: secrets-store.csi.k8s.io
      readOnly: true
      volumeAttributes:
        secretProviderClass: "vault-secret-provider"
EOF

}

dockerRegistrySecretStoreInVault(){
  vaultLogin
  kubectl exec -it vault-0 -n vault -- vault kv put secret/docker-registry \
    server="https://index.docker.io/v1/" \
    username="my-username" \
    password="my-password" \
    email="my-email@example.com"

  kubectl exec -i vault-0 -n vault -- vault write auth/kubernetes/role/docker-registry-role \
    bound_service_account_names=my-service-account \
    bound_service_account_namespaces=default \
    policies=docker-registry-policy \
    ttl=24h

  kubectl exec -i vault-0 -n vault -- vault policy write docker-registry-policy - <<EOF
path "secret/data/docker-registry" {
  capabilities = ["read"]
}
EOF


  cat <<EOF | kubectl apply -f -
apiVersion: secrets-store.csi.x-k8s.io/v1
kind: SecretProviderClass
metadata:
  name: docker-registry-secret-provider
  namespace: default
spec:
  provider: vault
  parameters:
    vaultAddress: "https://vault.vault.svc.cloud.uat"
    roleName: "docker-registry-role"
    objects: |
      - objectName: "docker-registry"
        objectType: "kv"
        objectVersion: ""
  secretObjects:          
    - secretName: docker-registry-secret
      type: kubernetes.io/dockerconfigjson
      data:
        - objectName: "docker-registry"
          key: ".dockerconfigjson"
EOF

}

vaultInstall() {
  csiDriverInstall
  # Create namespace for Vault
  kubectl create namespace vault

  # Prompt user for Vault storage configuration
  STORAGE_CLASS_NAME="vault-storage"
  PV_NAME="vault-pv"
  VOLUME_PATH="/data/volumes/vault"

  # Create local storage class and persistent volume
  createLocalStorageClassAndPV "$STORAGE_CLASS_NAME" "$PV_NAME" "$VOLUME_PATH"

  # kubectl apply -f $MOUNT_PATH/kubernetes/install_k8s/vault/vault-k8s-seal-role.yaml
  # Add HashiCorp Helm repository
  helm repo add hashicorp https://helm.releases.hashicorp.com
  helm repo update

  # Install Vault using Helm
  helm install vault hashicorp/vault \
    --namespace vault \
    --values $MOUNT_PATH/kubernetes/install_k8s/vault/values.yaml


  # Valut UI is disabled, so ingress is not created
  gok patch ingress vault vault letsencrypt vault
  if [[ $? -ne 0 ]]; then
    echoFailed "Failed to patch Vault ingress with Let's Encrypt!"
    return 1
  fi
  echoSuccess "Vault ingress patched successfully with Let's Encrypt!"
  
  echo "Vault installation started. This may take a few minutes..."

  kubectl apply -f $MOUNT_PATH/kubernetes/install_k8s/vault/vault-k8s-seal-role.yaml
  copySecret regcred default vault
  pushd $MOUNT_PATH/kubernetes/install_k8s/vault
  chmod +x *.sh
  ./ci.sh
  popd
  kubectl patch serviceaccount vault -p '{"imagePullSecrets": [{"name": "regcred"}]}' -n vault
  mkdir -p $MOUNT_PATH/vault-keys
  kubectl apply -f $MOUNT_PATH/kubernetes/install_k8s/vault/vault-init-unseal-job.yaml
  
  # Wait for the Job to complete
  if kubectl wait --for=condition=complete job.batch/vault-init-unseal -n vault --timeout=30s; then
    # Check if the Job completed successfully
    STATUS=$(kubectl get job vault-init-unseal -n vault -o jsonpath='{.status.succeeded}')
    if [[ "$STATUS" -eq 1 ]]; then
      echoSuccess "Vault unseal completed successfully."
      kubectl delete -f $MOUNT_PATH/kubernetes/install_k8s/vault/vault-init-unseal-job.yaml
    else
      echoFailed "JVault unsealob did not complete successfully."
      exit 1  # Return failure
    fi
  else
    echoSuccess "Vault unseal did not complete within the timeout."
    exit 1  # Return failure
  fi

  # Install Vault unseal service
  pushd $MOUNT_PATH/kubernetes/install_k8s/vault
  chmod +x *.sh
  ./install-vault-unseal-service.sh
  popd

  # Wait for Vault pods to be ready
  echo "Waiting for Vault services to be up!!!!"
  kubectl --timeout=120s wait --for=condition=Ready pods --all --namespace vault
  if [[ $? -eq 0 ]]; then
    echoSuccess "Vault services are now up!"
    echoSuccess "Vault installation completed!"
    echoSuccess "You can access Vault at: https://$(fullVaultUrl)"

    kubectl create clusterrolebinding vault-auth-delegator \
      --clusterrole=system:auth-delegator \
      --serviceaccount=vault:vault

    vaultLogin

    kubectl exec -it vault-0 -n vault -- vault auth enable kubernetes

    TOKEN=$(kubectl exec -it vault-0 -n vault -- cat /var/run/secrets/kubernetes.io/serviceaccount/token)

    kubectl exec -it vault-0 -n vault -- vault write auth/kubernetes/config \
      token_reviewer_jwt="$TOKEN" \
      kubernetes_host="https://11.0.0.1:6643" \
      kubernetes_ca_cert=@/var/run/secrets/kubernetes.io/serviceaccount/ca.crt
    
    kubectl exec -it vault-0 -n vault -- vault secrets enable -path=secret kv
    pushd $MOUNT_PATH/kubernetes/install_k8s/vault
    ./create_gok_vault_secrets.sh
    popd
    
  else
    echoFailed "Vault services timed-out, please check!"
  fi

}

gokAgentReset(){
  # Uninstall Gok Agent using Helm
  helm uninstall gok-agent -n gok-agent

  # Delete the Gok Agent namespace
  kubectl delete ns gok-agent
}


gokAgentInstall(){
  
  pushd ${MOUNT_PATH}/kubernetes/install_k8s/gok-cloud/agent
  ./build.sh
  ./tag_push.sh
  popd

  # Create namespace for gok-agent
  kubectl create namespace gok-agent

  # Moved the secret creation in create_gok_vault_secretes.sh
  # Moved the SecretProviderClass creation to helm chart
  # createVaultSecretStore \
  # -p "secret/gok-agent/config" \
  # -r "gok-agent" \
  # -l "gok-agent-policy" \
  # -s "gok-agent-provider" \
  # -x "gok-agent" \
  # -n "gok-agent" \
  # -k "oauth_client_id=adfasdfsasdfasdfasd" \
  # -k "static_config=asdfasdf"

  kubectl create configmap ca-cert --from-file=issuer.crt=/usr/local/share/ca-certificates/issuer.crt -n gok-agent

  # Install Vault using Helm
  helm install gok-agent ${MOUNT_PATH}/kubernetes/install_k8s/gok-cloud/agent/chart \
    --namespace gok-agent

}

# Function to delete old Docker images
# that are not tagged as "latest"
# Usage: deleteOldDockerImages <image_name>
deleteOldDockerImages() {
  IMAGE_NAME="$1"
  if [[ -z "$IMAGE_NAME" ]]; then
    echo "Usage: deleteOldDockerImages <image_name>"
    return 1
  fi
  docker images | grep "registry.gokcloud.com/${IMAGE_NAME}" | grep -v latest | awk '{print $3}' | xargs -r docker rmi
}

gokControllerReset(){
  # Uninstall Gok Controller using Helm
  helm uninstall gok-controller -n gok-controller

  # Delete the Gok Controller namespace
  kubectl delete ns gok-controller
}


gokControllerInstall(){
  pushd ${MOUNT_PATH}/kubernetes/install_k8s/gok-cloud/controller
  ./build.sh
  ./tag_push.sh
  popd

  # Create namespace for gok-controller
  kubectl create namespace gok-controller

  # Moved the secret creation in create_gok_vault_secretes.sh
  # Moved the SecretProviderClass creation to helm chart  
  # createVaultSecretStore \
  # -p "secret/gok-controller" \
  # -r "gok-controller" \
  # -l "gok-controller-policy" \
  # -s "gok-controller-provider" \
  # -x "gok-controller" \
  # -n "gok-controller" \
  # -k "api-token=adfasdfasdfasdfasd"
  
  kubectl create configmap ca-cert --from-file=issuer.crt=/usr/local/share/ca-certificates/issuer.crt -n gok-controller

  # Install Vault using Helm
  helm install gok-controller ${MOUNT_PATH}/kubernetes/install_k8s/gok-cloud/controller/chart \
    --namespace gok-controller

  # Patch the ingress with Let's Encrypt
  gok patch ingress gok-controller gok-controller letsencrypt controller
  patchControllerWithOauth
  if [[ $? -ne 0 ]]; then
    echoFailed "Failed to patch Gok Controller ingress with Let's Encrypt!"
    return 1
  fi
  echoSuccess "Gok Controller ingress patched successfully with Let's Encrypt!"
  echoSuccess "Access Gok Controller at: https://controller.$(rootDomain)"
}

vaultReset() {
  # Uninstall Vault using Helm
  helm uninstall vault -n vault

  # Delete all PersistentVolumeClaims in the Vault namespace
  kubectl delete pvc --all -n vault

  # Clean up the local persistent volume and storage class
  emptyLocalFsStorage "Vault" "vault-pv" "vault-storage" "/data/volumes/vault"

  # Delete the Vault namespace
  kubectl delete ns vault

  csiDriverUnInstall

  kubectl delete clusterrolebinding vault-auth-delegator
  systemctl stop vault-unseal.service
  systemctl disable vault-unseal.service
  rm -f /etc/systemd/system/vault-unseal.service
  systemctl daemon-reload

  # Provide feedback to the user
  echo "Vault has been reset successfully!"
}


keycloakReset(){
  local start_time=$(date +%s)
  
  log_component_start "keycloak-reset" "Resetting Keycloak identity and access management"
  
  log_step "1" "Checking Keycloak installation status"
  local keycloak_exists=false
  local namespace_exists=false
  
  # Check if Keycloak helm release exists
  if helm list -n keycloak 2>/dev/null | grep -q "keycloak"; then
    keycloak_exists=true
    log_success "Keycloak helm release found"
  else
    log_info "Keycloak helm release not found (may already be uninstalled)"
  fi
  
  # Check if Keycloak namespace exists
  if kubectl get namespace keycloak >/dev/null 2>&1; then
    namespace_exists=true
    log_success "Keycloak namespace found"
  else
    log_info "Keycloak namespace not found (may already be deleted)"
  fi
  
  # If nothing exists, inform user and exit
  if [[ "$keycloak_exists" == false && "$namespace_exists" == false ]]; then
    log_info "Keycloak appears to already be reset or was never installed"
    log_component_success "keycloak-reset" "Keycloak reset completed (nothing to reset)"
    return 0
  fi
  
  log_step "2" "Uninstalling Keycloak helm release"
  if [[ "$keycloak_exists" == true ]]; then
    if execute_with_suppression helm uninstall keycloak -n keycloak; then
      log_success "Keycloak helm release uninstalled successfully"
      
      # Wait for pods to terminate
      log_substep "Waiting for Keycloak pods to terminate..."
      local wait_count=0
      while kubectl get pods -n keycloak 2>/dev/null | grep -q "keycloak" && [[ $wait_count -lt 30 ]]; do
        printf "."
        sleep 2
        wait_count=$((wait_count + 1))
      done
      printf "\n"
      
      if [[ $wait_count -lt 30 ]]; then
        log_success "Keycloak pods terminated successfully"
      else
        log_warning "Keycloak pods taking longer than expected to terminate"
      fi
    else
      log_error "Failed to uninstall Keycloak helm release"
      if is_verbose_mode; then
        log_info "Run with --verbose for detailed error information"
      fi
      return 1
    fi
  else
    log_info "Skipping helm uninstall (release not found)"
  fi
  
  log_step "3" "Cleaning up PostgreSQL database"
  if [[ "$namespace_exists" == true ]]; then
    log_substep "Removing PostgreSQL deployment..."
    if execute_with_suppression kubectl delete deployment postgres -n keycloak --timeout=60s; then
      log_success "PostgreSQL deployment removed"
    else
      log_warning "PostgreSQL deployment may not exist or failed to delete"
    fi
    
    log_substep "Removing PostgreSQL service..."
    if execute_with_suppression kubectl delete service keycloak-postgresql -n keycloak --timeout=30s; then
      log_success "PostgreSQL service removed"
    else
      log_warning "PostgreSQL service may not exist or failed to delete"
    fi
    
    log_substep "Removing PostgreSQL secrets and config..."
  execute_with_suppression kubectl delete configmap postgres-config -n keycloak --timeout=30s || true
    log_success "PostgreSQL configuration cleaned up"
  fi

  log_step "4" "Cleaning up persistent volumes and storage"
  if [[ "$namespace_exists" == true ]]; then
    # Clean up PVCs
    log_substep "Removing persistent volume claims..."
    if execute_with_suppression kubectl delete pvc --all -n keycloak --timeout=60s; then
      log_success "Persistent volume claims removed"
    else
      log_warning "Some PVCs may remain or are taking time to delete"
    fi
    
    # Clean up local storage
    log_substep "Cleaning up local filesystem storage..."
    if emptyLocalFsStorage "Keycloak" "keycloak-pv" "keycloak-storage" "/data/volumes/pv3"; then
      log_success "Local filesystem storage cleaned up"
    else
      log_warning "Local storage cleanup had issues but continuing"
    fi
  else
    log_info "Skipping storage cleanup (namespace not found)"
  fi
  
  log_step "5" "Removing Keycloak namespace and resources"
  if [[ "$namespace_exists" == true ]]; then
    if execute_with_suppression kubectl delete namespace keycloak --timeout=120s; then
      log_success "Keycloak namespace deleted successfully"
    else
      log_warning "Keycloak namespace deletion had issues but may still complete"
      
      # Try to force delete if needed
      log_substep "Attempting force cleanup..."
      if execute_with_suppression kubectl delete namespace keycloak --force --grace-period=0 2>/dev/null; then
        log_success "Keycloak namespace force-deleted successfully"
      else
        log_warning "Force deletion also had issues - manual cleanup may be needed"
      fi
    fi
  else
    log_info "Skipping namespace deletion (namespace not found)"
  fi
  
  log_step "6" "Cleaning up Keycloak-related resources"
  
  # Clean up any remaining Keycloak-related ingress resources
  local ingress_cleaned=false
  if kubectl get ingress -A 2>/dev/null | grep -q "keycloak"; then
    log_substep "Removing Keycloak ingress resources..."
    if execute_with_suppression kubectl delete ingress -l app.kubernetes.io/name=keycloak --all-namespaces 2>/dev/null; then
      ingress_cleaned=true
      log_success "Keycloak ingress resources cleaned up"
    else
      log_warning "Some Keycloak ingress resources may remain"
    fi
  fi
  
  # Clean up OAuth secrets that may reference Keycloak
  local oauth_secrets_cleaned=false
  if kubectl get secret oauth-secrets -n kube-system >/dev/null 2>&1; then
    log_substep "Checking OAuth secrets configuration..."
    log_info "OAuth secrets found - may need manual reconfiguration"
    oauth_secrets_cleaned=true
  fi
  
  # Clean up any remaining Keycloak-related certificates
  local certs_cleaned=false
  if kubectl get certificates -A 2>/dev/null | grep -q "keycloak"; then
    log_substep "Removing Keycloak TLS certificates..."
    if execute_with_suppression kubectl delete certificates -l app.kubernetes.io/name=keycloak --all-namespaces 2>/dev/null; then
      certs_cleaned=true
      log_success "Keycloak certificates cleaned up"
    else
      log_warning "Some Keycloak certificates may remain"
    fi
  fi
  
  if [[ "$ingress_cleaned" == false && "$oauth_secrets_cleaned" == false && "$certs_cleaned" == false ]]; then
    log_info "No additional Keycloak resources found to clean up"
  fi
  
  log_step "7" "Validating Keycloak reset completion"
  validate_keycloak_reset
  
  local end_time=$(date +%s)
  local duration=$((end_time - start_time))
  
  log_component_success "keycloak-reset" "Keycloak identity management reset completed"
  log_success "Keycloak reset completed in ${duration}s"
  
  # Show reset summary and next steps
  show_keycloak_reset_summary
}

# Validate that Keycloak reset was successful
validate_keycloak_reset() {
  local validation_success=true
  
  # Check that helm release is gone
  if helm list -n keycloak 2>/dev/null | grep -q "keycloak"; then
    log_error "Keycloak helm release still exists"
    validation_success=false
  else
    log_success "Keycloak helm release successfully removed"
  fi
  
  # Check that namespace is gone
  if kubectl get namespace keycloak >/dev/null 2>&1; then
    log_warning "Keycloak namespace still exists (may be terminating)"
  else
    log_success "Keycloak namespace successfully removed"
  fi
  
  # Check that pods are gone
  if kubectl get pods -n keycloak 2>/dev/null | grep -q "keycloak"; then
    log_warning "Some Keycloak pods may still be terminating"
  else
    log_success "All Keycloak pods removed"
  fi
  
  # Check that services are gone
  if kubectl get services -n keycloak 2>/dev/null | grep -q "keycloak"; then
    log_warning "Some Keycloak services may still exist"
  else
    log_success "All Keycloak services removed"
  fi
  
  # Check that PVCs are gone
  if kubectl get pvc -n keycloak 2>/dev/null | grep -q -v "No resources found"; then
    log_warning "Some Keycloak persistent volume claims may still exist"
  else
    log_success "All Keycloak persistent volumes removed"
  fi
  
  return $([[ "$validation_success" == true ]] && echo 0 || echo 1)
}

# Show Keycloak reset summary and recommendations
show_keycloak_reset_summary() {
  echo
  echo -e "${COLOR_BRIGHT_BLUE}${COLOR_BOLD}ðŸ”„ Keycloak Reset Summary${COLOR_RESET}"
  echo -e "${COLOR_CYAN}â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”${COLOR_RESET}"
  
  echo -e "${COLOR_YELLOW}${COLOR_BOLD}Reset Actions Completed:${COLOR_RESET}"
  echo -e "  âœ… ${COLOR_GREEN}Keycloak helm release uninstalled${COLOR_RESET}"
  echo -e "  âœ… ${COLOR_GREEN}PostgreSQL database and data removed${COLOR_RESET}"
  echo -e "  âœ… ${COLOR_GREEN}Keycloak namespace and pods cleaned up${COLOR_RESET}"
  echo -e "  âœ… ${COLOR_GREEN}Persistent volumes and storage cleared${COLOR_RESET}"
  echo -e "  âœ… ${COLOR_GREEN}Keycloak ingress and certificates removed${COLOR_RESET}"
  echo -e "  âœ… ${COLOR_GREEN}Identity provider configuration reset${COLOR_RESET}"
  echo
  
  echo -e "${COLOR_YELLOW}${COLOR_BOLD}Impact of Keycloak Reset:${COLOR_RESET}"
  echo -e "  ðŸ” ${COLOR_RED}All identity and access management data removed${COLOR_RESET}"
  echo -e "  ðŸ‘¥ ${COLOR_RED}User accounts, roles, and realm configurations deleted${COLOR_RESET}"
  echo -e "  ðŸ”‘ ${COLOR_RED}OAuth2/OIDC clients and tokens invalidated${COLOR_RESET}"
  echo -e "  ðŸŒ ${COLOR_RED}Keycloak admin console and APIs unavailable${COLOR_RESET}"
  echo -e "  ðŸ”— ${COLOR_YELLOW}Applications using Keycloak authentication will fail${COLOR_RESET}"
  echo
  
  echo -e "${COLOR_BRIGHT_YELLOW}${COLOR_BOLD}âš ï¸  Post-Reset Considerations:${COLOR_RESET}"
  echo -e "${COLOR_CYAN}â€¢ OAuth2 proxy and other services depending on Keycloak will need reconfiguration${COLOR_RESET}"
  echo -e "${COLOR_CYAN}â€¢ LDAP user federation settings will need to be recreated${COLOR_RESET}"
  echo -e "${COLOR_CYAN}â€¢ Application OAuth2/OIDC clients will need to be re-registered${COLOR_RESET}"
  echo -e "${COLOR_CYAN}â€¢ Custom authentication flows and security policies will be lost${COLOR_RESET}"
  echo
  
  echo -e "${COLOR_BRIGHT_GREEN}${COLOR_BOLD}ðŸš€ Next Steps:${COLOR_RESET}"
  echo -e "${COLOR_GREEN}â€¢ To reinstall Keycloak: ${COLOR_BOLD}gok install keycloak${COLOR_RESET}"
  echo -e "${COLOR_GREEN}â€¢ To reset dependent services: ${COLOR_BOLD}gok reset oauth2${COLOR_RESET}"
  echo -e "${COLOR_GREEN}â€¢ To check system status: ${COLOR_BOLD}gok status${COLOR_RESET}"
  echo -e "${COLOR_GREEN}â€¢ To see available components: ${COLOR_BOLD}gok help reset${COLOR_RESET}"
  echo
}

# Deploy PostgreSQL for Keycloak
deploy_keycloak_postgresql() {
  log_substep "Creating PostgreSQL deployment for Keycloak..."
  
  # Clean up any existing PostgreSQL resources first
  log_substep "Cleaning up any existing PostgreSQL resources..."
  execute_with_suppression kubectl delete pvc postgres-pvc -n keycloak --ignore-not-found=true
  execute_with_suppression kubectl delete deployment postgres -n keycloak --ignore-not-found=true
  execute_with_suppression kubectl delete service keycloak-postgresql -n keycloak --ignore-not-found=true
  execute_with_suppression kubectl delete configmap postgres-config -n keycloak --ignore-not-found=true
  
  # First ensure storage class and persistent volume exist
  log_substep "Setting up storage for PostgreSQL..."
  if createLocalStorageClassAndPV "keycloak-storage" "keycloak-pv" "/data/volumes/pv3"; then
    log_success "Keycloak storage configured"
  else
    log_error "Failed to create storage class and persistent volume"
    return 1
  fi
  
  # Create PostgreSQL deployment YAML
  cat > /tmp/keycloak-postgres.yaml << 'EOF'
apiVersion: v1
kind: ConfigMap
metadata:
  name: postgres-config
  namespace: keycloak
data:
  POSTGRES_DB: keycloak
---
apiVersion: v1
kind: Secret
metadata:
  name: postgres-secret
  namespace: keycloak
type: Opaque
data:
  # POSTGRES_PASSWORD now comes from keycloak-postgresql secret
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: postgres-pvc
  namespace: keycloak
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 5Gi
  storageClassName: keycloak-storage
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: postgres
  namespace: keycloak
spec:
  replicas: 1
  selector:
    matchLabels:
      app: postgres
  template:
    metadata:
      labels:
        app: postgres
    spec:
      containers:
      - name: postgres
        image: postgres:15
        ports:
        - containerPort: 5432
        envFrom:
        - configMapRef:
            name: postgres-config
        - secretRef:
            name: keycloak-postgresql
        volumeMounts:
        - name: postgres-storage
          mountPath: /var/lib/postgresql/data
        resources:
          requests:
            memory: "256Mi"
            cpu: "250m"
          limits:
            memory: "512Mi"
            cpu: "500m"
        readinessProbe:
          exec:
            command:
              - /bin/sh
              - -c
              - exec pg_isready -U postgres -h 127.0.0.1 -p 5432
          initialDelaySeconds: 30
          periodSeconds: 10
          timeoutSeconds: 5
        livenessProbe:
          exec:
            command:
              - /bin/sh
              - -c
              - exec pg_isready -U postgres -h 127.0.0.1 -p 5432
          initialDelaySeconds: 30
          periodSeconds: 10
          timeoutSeconds: 5
      volumes:
      - name: postgres-storage
        persistentVolumeClaim:
          claimName: postgres-pvc
---
apiVersion: v1
kind: Service
metadata:
  name: keycloak-postgresql
  namespace: keycloak
spec:
  ports:
  - port: 5432
    targetPort: 5432
  selector:
    app: postgres
  type: ClusterIP
EOF

  # Apply PostgreSQL deployment
  if execute_with_suppression kubectl apply -f /tmp/keycloak-postgres.yaml; then
    log_success "PostgreSQL deployment created"
  else
    log_error "Failed to create PostgreSQL deployment"
    return 1
  fi

  # Wait for PostgreSQL to be ready
  log_substep "Waiting for PostgreSQL to be ready..."
  if execute_with_suppression kubectl wait --for=condition=ready pod -l app=postgres -n keycloak --timeout=120s; then
    log_success "PostgreSQL is ready"
    return 0
  else
    log_error "PostgreSQL failed to become ready"
    return 1
  fi
}

keycloakInst(){
  log_substep "Installing core Keycloak identity management service"
  
  # Create namespace
  if kubectl_with_summary create "namespace" keycloak; then
    log_info "Keycloak namespace created"
  else
    log_info "Keycloak namespace already exists"
  fi
  
  # Collect configuration parameters (always prompt)
  KEYCLOAK_ADMIN_USERNAME=$(promptUserInput "Please enter keycloak admin username (admin): " "admin")
  KEYCLOAK_ADMIN_PASSWORD=$(promptSecret "Please enter keycloak admin password: ")
  POSTGRESQL_USERNAME=$(promptUserInput "Please enter postgresql username (postgres): " "postgres")
  POSTGRESQL_PASSWORD=$(promptSecret "Please enter postgresql password: ")
  OIDC_CLIENT_ID=$(promptUserInput "Please enter OIDC client id (${OIDC_CLIENT_ID:-gok-developers-client}): " "${OIDC_CLIENT_ID:-gok-developers-client}")
  REALM=$(promptUserInput "Please enter realm name (${REALM:-GokDevelopers}): " "${REALM:-GokDevelopers}")
  
  # Calculate Keycloak hostname
  local keycloak_hostname="$(subDomain 'keycloak').$(rootDomain)"

  if is_verbose_mode "$@"; then
    kubectl create secret generic keycloak-secrets \
  --from-literal=KEYCLOAK_LOG_LEVEL="TRACE" \
  --from-literal=KC_LOG_LEVEL="TRACE" \
  --from-literal=KEYCLOAK_ADMIN="${KEYCLOAK_ADMIN_USERNAME}" \
  --from-literal=CLIENT_ID="${OIDC_CLIENT_ID}" \
  --from-literal=OAUTH_REALM="${REALM}" \
  --from-literal=KEYCLOAK_ADMIN_PASSWORD="${KEYCLOAK_ADMIN_PASSWORD}" \
  --from-literal=admin-user="${KEYCLOAK_ADMIN_USERNAME}" \
  --from-literal=admin-password="${KEYCLOAK_ADMIN_PASSWORD}" \
  --from-literal=hostname="${keycloak_hostname}" -n keycloak || true

    kubectl create secret generic keycloak-postgresql \
      --from-literal=username="${POSTGRESQL_USERNAME}" \
      --from-literal=POSTGRES_USER="${POSTGRESQL_USERNAME}" \
      --from-literal=password="${POSTGRESQL_PASSWORD}" \
      --from-literal=POSTGRES_PASSWORD="${POSTGRESQL_PASSWORD}" \
      --from-literal=postgres-password="${POSTGRESQL_PASSWORD}" -n keycloak || true
  else
    # Calculate Keycloak hostname
    local keycloak_hostname="$(subDomain 'keycloak').$(rootDomain)"
    
    execute_with_suppression kubectl create secret generic keycloak-secrets \
  --from-literal=KEYCLOAK_LOG_LEVEL="TRACE" \
  --from-literal=KC_LOG_LEVEL="TRACE" \
  --from-literal=KEYCLOAK_ADMIN="${KEYCLOAK_ADMIN_USERNAME}" \
  --from-literal=CLIENT_ID="${OIDC_CLIENT_ID}" \
  --from-literal=OAUTH_REALM="${REALM}" \
  --from-literal=KEYCLOAK_ADMIN_PASSWORD="${KEYCLOAK_ADMIN_PASSWORD}" \
  --from-literal=admin-user="${KEYCLOAK_ADMIN_USERNAME}" \
  --from-literal=admin-password="${KEYCLOAK_ADMIN_PASSWORD}" \
  --from-literal=hostname="${keycloak_hostname}" -n keycloak || true

    execute_with_suppression kubectl create secret generic keycloak-postgresql \
      --from-literal=username="${POSTGRESQL_USERNAME}" \
      --from-literal=POSTGRES_USER="${POSTGRESQL_USERNAME}" \
      --from-literal=password="${POSTGRESQL_PASSWORD}" \
      --from-literal=POSTGRES_PASSWORD="${POSTGRESQL_PASSWORD}" \
      --from-literal=postgres-password="${POSTGRESQL_PASSWORD}" -n keycloak || true
  fi

  # Deploy PostgreSQL for Keycloak
  log_substep "Deploying PostgreSQL database for Keycloak..."
  if deploy_keycloak_postgresql; then
    log_success "PostgreSQL database deployed successfully"
  else
    log_error "PostgreSQL deployment failed"
    return 1
  fi

  # Add codecentric helm repository
  log_substep "Adding codecentric helm repository..."
  if execute_with_suppression helm repo add codecentric https://codecentric.github.io/helm-charts; then
    log_success "Codecentric helm repository added"
  else
    log_warning "Codecentric repository may already exist"
  fi
  
  log_substep "Updating helm repositories..."
  if execute_with_suppression helm repo update; then
    log_success "Helm repositories updated"
  else
    log_warning "Helm repository update had issues but continuing"
  fi

  # Install Keycloak via codecentric/keycloakx Helm chart
  local hostname="$(subDomain 'keycloak').$(rootDomain)"

  if is_verbose_mode "$@"; then
    helm upgrade --install keycloak codecentric/keycloakx \
      --namespace keycloak \
      --values "${MOUNT_PATH}"/kubernetes/install_k8s/keycloak/values-kcx.yaml
    if [[ $? -eq 0 ]]; then
      return 0
    else
      log_error "Keycloak helm installation failed"
      return 1
    fi
  else
    if helm_install_with_summary "keycloak" "keycloak" \
      keycloak codecentric/keycloakx \
      --namespace keycloak \
      --values "${MOUNT_PATH}"/kubernetes/install_k8s/keycloak/values-kcx.yaml; then
      return 0
    else
      log_error "Keycloak helm installation failed"
      return 1
    fi
  fi
}

# Enhanced Keycloak build with detailed progress tracking
build_keycloak_with_progress() {
  local keycloak_admin_username="$1"
  local keycloak_admin_password="$2"
  local postgresql_username="$3"
  local postgresql_password="$4"
  local oidc_client_id="$5"
  local realm="$6"
  local start_time=$(date +%s)
  
  # Get hostname configuration
  local hostname="$(subDomain 'keycloak').$(rootDomain)"
  
  log_substep "Identity Provider: ${COLOR_CYAN}Keycloak${COLOR_RESET}"
  log_substep "Hostname: ${COLOR_CYAN}${hostname}${COLOR_RESET}"
  log_substep "Admin User: ${COLOR_CYAN}${keycloak_admin_username}${COLOR_RESET}"
  log_substep "Database: ${COLOR_CYAN}PostgreSQL${COLOR_RESET}"
  log_substep "OIDC Client: ${COLOR_CYAN}${oidc_client_id}${COLOR_RESET}"
  log_substep "Realm: ${COLOR_CYAN}${realm}${COLOR_RESET}"
  
  # Step 1: Create Kubernetes secrets with progress
  log_info "ðŸ” Creating Keycloak and PostgreSQL secrets"
  
  local temp_secret_log=$(mktemp)
  local temp_secret_error=$(mktemp)
  
  # Create secrets in background with progress tracking
  (
    kubectl create secret generic keycloak-secrets \
  --from-literal=KEYCLOAK_LOG_LEVEL="TRACE" \
  --from-literal=KC_LOG_LEVEL="TRACE" \
      --from-literal=KEYCLOAK_ADMIN="${keycloak_admin_username}" \
      --from-literal=CLIENT_ID="${oidc_client_id}" \
      --from-literal=OAUTH_REALM="${realm}" \
      --from-literal=KEYCLOAK_ADMIN_PASSWORD="${keycloak_admin_password}" \
      --from-literal=admin-user="${keycloak_admin_username}" \
      --from-literal=admin-password="${keycloak_admin_password}" \
      --from-literal=hostname="${hostname}" \
      -n keycloak >"$temp_secret_log" 2>"$temp_secret_error"
    
    kubectl create secret generic keycloak-postgresql \
      --from-literal=username="${postgresql_username}" \
      --from-literal=POSTGRES_USER="${postgresql_username}" \
      --from-literal=password="${postgresql_password}" \
      --from-literal=POSTGRES_PASSWORD="${postgresql_password}" \
      --from-literal=postgres-password="${postgresql_password}" \
      -n keycloak >>"$temp_secret_log" 2>>"$temp_secret_error"
  ) &
  local secret_pid=$!
  
  # Show secret creation progress
  local secret_progress=0
  local spinner_chars="|/-\\"
  local spinner_idx=0
  local secret_stage="Creating authentication secrets"
  
  while kill -0 $secret_pid 2>/dev/null; do
    local char=${spinner_chars:spinner_idx:1}
    secret_progress=$(( (secret_progress + 1) % 40 ))
    local progress_percent=$(( secret_progress * 100 / 40 ))
    
    # Update stage based on progress
    if [[ $secret_progress -lt 20 ]]; then
      secret_stage="Creating Keycloak admin secrets"
    else
      secret_stage="Creating PostgreSQL database secrets"
    fi
    
    printf "\r${COLOR_BLUE}  Creating secrets [%c] ${COLOR_CYAN}%d%%${COLOR_RESET} - ${COLOR_DIM}%s${COLOR_RESET}" "$char" "$progress_percent" "$secret_stage"
    
    spinner_idx=$(( (spinner_idx + 1) % 4 ))
    sleep 0.3
  done
  
  wait $secret_pid
  local secret_exit_code=$?
  
  if [[ $secret_exit_code -eq 0 ]]; then
    printf "\r${COLOR_GREEN}  âœ“ Keycloak secrets created [100%%]${COLOR_RESET}\n"
    log_success "Keycloak and PostgreSQL secrets created successfully"
  else
    printf "\r${COLOR_RED}  âœ— Secret creation failed${COLOR_RESET}\n"
    log_error "Keycloak secret creation failed - error details:"
    if [[ -s "$temp_secret_error" ]]; then
      cat "$temp_secret_error" >&2
    fi
    rm -f "$temp_secret_log" "$temp_secret_error"
    return 1
  fi
  
  # Step 2: Add codecentric helm repository and install Keycloak
  log_info "ðŸ“¦ Adding codecentric helm repository"
  log_substep "Repository: ${COLOR_CYAN}https://codecentric.github.io/helm-charts${COLOR_RESET}"
  
  if execute_with_suppression helm repo add codecentric https://codecentric.github.io/helm-charts >/dev/null 2>&1; then
    log_success "Codecentric helm repository added"
  else
    log_info "Codecentric repository already exists, continuing..."
  fi
  
  log_substep "Updating helm repositories..."
  if execute_with_suppression helm repo update >/dev/null 2>&1; then
    log_success "Helm repositories updated"
  else
    log_warning "Helm repository update had issues but continuing"
  fi

  # Step 3: Install Keycloak via codecentric/keycloakx Helm chart with enhanced progress
  log_info "â˜¸ï¸  Installing Keycloak via codecentric/keycloakx chart"
  log_substep "Chart: ${COLOR_CYAN}codecentric/keycloakx${COLOR_RESET}"
  log_substep "Values: ${COLOR_CYAN}${MOUNT_PATH}/kubernetes/install_k8s/keycloak/values-kcx.yaml${COLOR_RESET}"
  
  local temp_helm_log=$(mktemp)
  local temp_helm_error=$(mktemp)
  
  # Start Helm installation in background with codecentric/keycloakx
  helm upgrade --install keycloak codecentric/keycloakx \
    --namespace keycloak \
    --values "${MOUNT_PATH}"/kubernetes/install_k8s/keycloak/values-kcx.yaml \
    >"$temp_helm_log" 2>"$temp_helm_error" &
  local helm_pid=$!
  
  # Show Helm installation progress with Keycloak-specific stages
  local helm_progress=0
  local helm_steps=8
  local helm_stage="Preparing"
  
  while kill -0 $helm_pid 2>/dev/null; do
    local char=${spinner_chars:spinner_idx:1}
    helm_progress=$(( (helm_progress + 1) % (helm_steps * 15) ))
    local progress_percent=$(( helm_progress * 100 / (helm_steps * 15) ))
    
    # Update stage based on progress for codecentric/keycloakx
    case $((helm_progress / 15)) in
      0) helm_stage="Downloading keycloakx chart" ;;
      1) helm_stage="Creating Keycloak StatefulSet" ;;
      2) helm_stage="Configuring Keycloak service" ;;
      3) helm_stage="Setting up identity providers" ;;
      4) helm_stage="Configuring authentication flows" ;;
      5) helm_stage="Setting up ingress routing" ;;
      6) helm_stage="Waiting for Keycloak pods" ;;
      7) helm_stage="Finalizing keycloakx installation" ;;
    esac
    
    printf "\r${COLOR_BLUE}  Installing Keycloak [%c] ${COLOR_CYAN}%d%%${COLOR_RESET} - ${COLOR_DIM}%s${COLOR_RESET}" "$char" "$progress_percent" "$helm_stage"
    
    spinner_idx=$(( (spinner_idx + 1) % 4 ))
    sleep 0.5
  done
  
  wait $helm_pid
  local helm_exit_code=$?
  
  if [[ $helm_exit_code -eq 0 ]]; then
    printf "\r${COLOR_GREEN}  âœ“ Keycloak keycloakx installation completed [100%%]${COLOR_RESET}\n"
    log_success "Keycloak deployed successfully via codecentric/keycloakx Helm chart"
  else
    printf "\r${COLOR_RED}  âœ— Keycloak keycloakx installation failed${COLOR_RESET}\n"
    log_error "Keycloak keycloakx Helm installation failed - error details:"
    if [[ -s "$temp_helm_error" ]]; then
      if is_verbose_mode; then
        cat "$temp_helm_error" >&2
      else
        tail -10 "$temp_helm_error" >&2
        log_info "Use --verbose flag to see full installation logs"
      fi
    fi
    rm -f "$temp_secret_log" "$temp_secret_error" "$temp_helm_log" "$temp_helm_error"
    return 1
  fi
  
  # Step 3: Wait for Keycloak to be ready
  log_info "â³ Waiting for Keycloak services to be ready"
  
  local wait_progress=0
  local wait_steps=6
  local wait_stage="Starting"
  local ready=false
  local max_wait=180  # 3 minutes
  local wait_count=0
  
  while [[ $wait_count -lt $max_wait ]]; do
    if kubectl wait --for=condition=available --timeout=10s deployment/keycloak -n keycloak >/dev/null 2>&1; then
      ready=true
      break
    fi
    
    local char=${spinner_chars:spinner_idx:1}
    wait_progress=$(( (wait_progress + 1) % (wait_steps * 10) ))
    local progress_percent=$(( wait_progress * 100 / (wait_steps * 10) ))
    
    # Update stage based on progress
    case $((wait_progress / 10)) in
      0) wait_stage="Starting PostgreSQL" ;;
      1) wait_stage="Initializing Keycloak" ;;
      2) wait_stage="Loading identity providers" ;;
      3) wait_stage="Setting up authentication" ;;
      4) wait_stage="Configuring realms" ;;
      5) wait_stage="Finalizing services" ;;
    esac
    
    printf "\r${COLOR_YELLOW}  Waiting for Keycloak [%c] ${COLOR_CYAN}%d%%${COLOR_RESET} - ${COLOR_DIM}%s${COLOR_RESET}" "$char" "$progress_percent" "$wait_stage"
    
    spinner_idx=$(( (spinner_idx + 1) % 4 ))
    sleep 2
    wait_count=$((wait_count + 2))
  done
  
  if [[ "$ready" == "true" ]]; then
    printf "\r${COLOR_GREEN}  âœ“ Keycloak services ready [100%%]${COLOR_RESET}\n"
    log_success "Keycloak is ready and accepting connections"
    
    # Step 4: Create permanent admin account
    log_info "ðŸ‘¤ Creating permanent admin account"
    if create_permanent_keycloak_admin; then
      log_success "Permanent admin account created successfully"
    else
      log_warning "Could not create permanent admin account automatically (may need manual setup)"
    fi
  else
    printf "\r${COLOR_YELLOW}  âš  Keycloak taking longer than expected${COLOR_RESET}\n"
    log_warning "Keycloak deployment may still be starting (check with: kubectl get pods -n keycloak)"
  fi
  
  # Clean up temporary files
  rm -f "$temp_secret_log" "$temp_secret_error" "$temp_helm_log" "$temp_helm_error"
  
  # Installation completion summary
  local end_time=$(date +%s)
  local duration=$((end_time - start_time))
  log_success "Keycloak deployment completed in ${duration}s"
  
  # Show Keycloak summary
  show_keycloak_summary "$hostname"
  
  return 0
}

# Enhanced validation for Keycloak installation
validate_keycloak_installation() {
  log_info "Validating Keycloak installation..."
  
  # Check if Keycloak namespace exists
  if kubectl get namespace keycloak >/dev/null 2>&1; then
    log_success "Keycloak namespace found"
  else
    log_error "Keycloak namespace not found"
    return 1
  fi
  
  # Check if Keycloak StatefulSet is ready
  if kubectl get statefulset keycloak -n keycloak >/dev/null 2>&1; then
    log_success "Keycloak StatefulSet found"
    
    # Check StatefulSet status
    local ready_replicas=$(kubectl get statefulset keycloak -n keycloak -o jsonpath='{.status.readyReplicas}' 2>/dev/null || echo "0")
    local desired_replicas=$(kubectl get statefulset keycloak -n keycloak -o jsonpath='{.spec.replicas}' 2>/dev/null || echo "1")
    
    if [[ "$ready_replicas" == "$desired_replicas" ]]; then
      log_success "Keycloak StatefulSet is ready (${ready_replicas}/${desired_replicas} replicas)"
    else
      log_warning "Keycloak StatefulSet is scaling (${ready_replicas}/${desired_replicas} replicas ready)"
    fi
  else
    log_error "Keycloak StatefulSet not found"
    return 1
  fi
  
  # Check if PostgreSQL is ready
  if kubectl get statefulset keycloak-postgresql -n keycloak >/dev/null 2>&1; then
    log_success "PostgreSQL database found"
    
    local pg_ready=$(kubectl get statefulset keycloak-postgresql -n keycloak -o jsonpath='{.status.readyReplicas}' 2>/dev/null || echo "0")
    if [[ "$pg_ready" == "1" ]]; then
      log_success "PostgreSQL database is ready"
    else
      log_warning "PostgreSQL database is starting"
    fi
  else
    log_warning "PostgreSQL statefulset not found (may use external database)"
  fi
  
  # Check if Keycloak service exists
  if kubectl get service keycloak-http -n keycloak >/dev/null 2>&1; then
    log_success "Keycloak service found"
  else
    log_warning "Keycloak service not found"
  fi
  
  # Check if ingress exists
  if kubectl get ingress keycloak -n keycloak >/dev/null 2>&1; then
    local hostname=$(kubectl get ingress keycloak -n keycloak -o jsonpath='{.spec.rules[0].host}' 2>/dev/null || echo "unknown")
    log_success "Keycloak ingress found (${hostname})"
  else
    log_info "Keycloak ingress not found (may be configured separately)"
  fi
  
  return 0
}

# Show Keycloak-specific summary
show_keycloak_summary() {
  local hostname="$1"
  
  log_info "ðŸ“‹ Keycloak Identity Management Summary"
  echo
  
  echo -e "${COLOR_BRIGHT_BLUE}${COLOR_BOLD}ðŸ” Keycloak Identity & Access Management Details${COLOR_RESET}"
  echo -e "${COLOR_CYAN}â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”${COLOR_RESET}"
  
  echo -e "${COLOR_YELLOW}${COLOR_BOLD}Identity Provider Information:${COLOR_RESET}"
  echo -e "  ðŸ¢ ${COLOR_GREEN}Keycloak Server${COLOR_RESET} - Enterprise identity and access management"
  echo -e "  ðŸ—„ï¸  ${COLOR_GREEN}PostgreSQL Database${COLOR_RESET} - Reliable data persistence and sessions"
  echo -e "  ðŸŒ ${COLOR_GREEN}Admin Console${COLOR_RESET} - Web-based identity management interface"
  echo -e "  ðŸ”— ${COLOR_GREEN}OAuth2/OIDC Provider${COLOR_RESET} - Modern authentication protocols"
  echo
  
  echo -e "${COLOR_YELLOW}${COLOR_BOLD}Access Information:${COLOR_RESET}"
  echo -e "  ðŸŒ ${COLOR_CYAN}Admin Console${COLOR_RESET}: https://${hostname}/admin/"
  echo -e "  ðŸ” ${COLOR_CYAN}OIDC Endpoint${COLOR_RESET}: https://${hostname}/realms/{realm}/.well-known/openid_configuration"
  echo -e "  ðŸ”‘ ${COLOR_CYAN}Token Endpoint${COLOR_RESET}: https://${hostname}/realms/{realm}/protocol/openid-connect/token"
  echo -e "  ðŸ‘¤ ${COLOR_CYAN}User Account${COLOR_RESET}: https://${hostname}/realms/{realm}/account/"
  echo
  
  echo -e "${COLOR_YELLOW}${COLOR_BOLD}Authentication Features:${COLOR_RESET}"
  echo -e "  âœ… ${COLOR_GREEN}Single Sign-On (SSO)${COLOR_RESET} - Unified authentication across applications"
  echo -e "  âœ… ${COLOR_GREEN}Multi-Factor Authentication${COLOR_RESET} - Enhanced security with 2FA/MFA"
  echo -e "  âœ… ${COLOR_GREEN}Social Login Integration${COLOR_RESET} - Google, GitHub, Facebook, etc."
  echo -e "  âœ… ${COLOR_GREEN}LDAP/Active Directory${COLOR_RESET} - Enterprise directory integration"
  echo -e "  âœ… ${COLOR_GREEN}Role-Based Access Control${COLOR_RESET} - Fine-grained permissions"
  echo -e "  âœ… ${COLOR_GREEN}OAuth2 & OIDC Support${COLOR_RESET} - Modern authentication standards"
  echo
  
  echo -e "${COLOR_BRIGHT_GREEN}${COLOR_BOLD}ðŸ’¡ Keycloak Benefits:${COLOR_RESET}"
  echo -e "${COLOR_GREEN}â€¢ Centralized identity and access management for all platform services${COLOR_RESET}"
  echo -e "${COLOR_GREEN}â€¢ Seamless integration with existing LDAP directory services${COLOR_RESET}"
  echo -e "${COLOR_GREEN}â€¢ Modern OAuth2/OIDC protocols for secure API authentication${COLOR_RESET}"
  echo -e "${COLOR_GREEN}â€¢ Comprehensive admin console for user and role management${COLOR_RESET}"
  echo -e "${COLOR_GREEN}â€¢ High availability and scalability for enterprise environments${COLOR_RESET}"
  echo
}

# Show Keycloak next steps and recommend OAuth2
show_keycloak_next_steps() {
  echo
  echo -e "${COLOR_BRIGHT_YELLOW}${COLOR_BOLD}ðŸš€ Keycloak Post-Installation Steps${COLOR_RESET}"
  echo
  
  echo -e "${COLOR_MAGENTA}${COLOR_BOLD}Immediate Next Steps:${COLOR_RESET}"
  echo -e "${COLOR_CYAN}  1. Access Keycloak admin console to configure realms and clients${COLOR_RESET}"
  echo -e "${COLOR_CYAN}  2. Set up LDAP integration for user federation${COLOR_RESET}"
  echo -e "${COLOR_CYAN}  3. Create OAuth2/OIDC clients for your applications${COLOR_RESET}"
  echo -e "${COLOR_CYAN}  4. Configure authentication flows and security policies${COLOR_RESET}"
  echo
  
  echo -e "${COLOR_BRIGHT_MAGENTA}${COLOR_BOLD}ðŸŽ¯ Recommended Next Installation: OAuth2 Proxy${COLOR_RESET}"
  echo -e "${COLOR_CYAN}Keycloak provides identity management - OAuth2 proxy adds application protection!${COLOR_RESET}"
  echo
  echo -e "${COLOR_YELLOW}${COLOR_BOLD}Why install OAuth2 proxy next?${COLOR_RESET}"
  echo -e "${COLOR_GREEN}â€¢ ðŸ›¡ï¸  Protects applications with Keycloak authentication${COLOR_RESET}"
  echo -e "${COLOR_GREEN}â€¢ ðŸ” Seamless integration with Keycloak OIDC provider${COLOR_RESET}"
  echo -e "${COLOR_GREEN}â€¢ ðŸŒ Single sign-on experience across all platform services${COLOR_RESET}"
  echo -e "${COLOR_GREEN}â€¢ ðŸ“Š Protects dashboards and admin interfaces${COLOR_RESET}"
  echo -e "${COLOR_GREEN}â€¢ ðŸš€ Ready-to-use authentication proxy for any HTTP service${COLOR_RESET}"
  echo
  echo -e "${COLOR_CYAN}You will be prompted to install OAuth2 proxy after Keycloak setup completes.${COLOR_RESET}"
  echo
}


addPolicyToSyncSecrets(){
  log_step "9" "Creating Kyverno policy for registry credential synchronization"
  log_info "Installing cluster-wide policy to automatically sync registry credentials"
  
  local apply_result=""
  local exit_code=0
  
  if [ "$GOK_VERBOSE" = "1" ]; then
    # In verbose mode, show kubectl output
    apply_result=$(cat <<EOF | kubectl apply -f - 2>&1
apiVersion: kyverno.io/v1
kind: ClusterPolicy
metadata:
  name: sync-regcred
spec:
  rules:
    - name: sync-regcred-secret
      match:
        any:
          - resources:
              kinds:
                - Namespace
      generate:
        apiVersion: v1
        kind: Secret
        name: regcred
        namespace: "{{request.object.metadata.name}}"
        synchronize: true
        clone:
          namespace: kube-system
          name: regcred
    - name: patch-serviceaccount
      match:
        any:
          - resources:
              kinds:
                - ServiceAccount
              namespaces:
                - "*"
      mutate:
        patchStrategicMerge:
          spec:
            imagePullSecrets:
              - name: regcred
EOF
)
    exit_code=$?
    log_debug "Kubectl apply result: $apply_result"
  else
    # In normal mode, suppress kubectl output
    apply_result=$(cat <<EOF | kubectl apply -f - 2>/dev/null
apiVersion: kyverno.io/v1
kind: ClusterPolicy
metadata:
  name: sync-regcred
spec:
  rules:
    - name: sync-regcred-secret
      match:
        any:
          - resources:
              kinds:
                - Namespace
      generate:
        apiVersion: v1
        kind: Secret
        name: regcred
        namespace: "{{request.object.metadata.name}}"
        synchronize: true
        clone:
          namespace: kube-system
          name: regcred
    - name: patch-serviceaccount
      match:
        any:
          - resources:
              kinds:
                - ServiceAccount
              namespaces:
                - "*"
      mutate:
        patchStrategicMerge:
          spec:
            imagePullSecrets:
              - name: regcred
EOF
)
    exit_code=$?
  fi
  
  if [ $exit_code -eq 0 ]
  then
    log_success "Kyverno cluster policy 'sync-regcred' created successfully"
    log_info "âœ“ Registry credentials will auto-sync to all new namespaces"
    log_info "âœ“ Service accounts will automatically get imagePullSecrets configured"
    log_info "âœ“ Seamless container image pulling from private registry enabled"
  else
    log_warning "Kyverno policy creation had issues"
    log_info "Manual registry credential configuration may be needed for some namespaces"
  fi
}

installRegistryWithCertMgr(){
  log_component_start "registry-install" "Installing container registry with certificate management"
  
  log_step "1" "Creating storage infrastructure"
  if createLocalStorageClassAndPV "registry-storage" "registry-pv" "/data/volumes/pv4"; then
    log_success "Storage class and persistent volume created (10Gi)"
  else
    log_error "Failed to create storage infrastructure"
    return 1
  fi
  
  log_step "2" "Deploying Docker registry"
  if dockerRegistryInst; then
    log_success "Docker registry deployment completed"
  else
    log_error "Failed to deploy Docker registry"
    return 1
  fi
  
  log_step "3" "Configuring TLS certificate and ingress"
  # No need to generate the certificate manually.
  # The certificate and secret will be directly issued via the ingress annotation cert-manager.io/cluster-issuer
  if execute_with_suppression gok patch ingress registry-docker-registry registry letsencrypt $(registrySubdomain); then
    log_success "TLS certificate and ingress configured ($(registrySubdomain).$(rootDomain))"
  else
    log_error "Failed to configure TLS certificate and ingress"
    return 1
  fi

  log_step "4" "Installing registry certificates for Docker trust"
  # Let docker trust the self-signed certificates
  # https://itnext.io/error-x509-certificate-signed-by-unknown-authority-error-is-returned-f0e5d436f467
  local cert_dir="/etc/docker/certs.d/$(registrySubdomain).$(rootDomain)"
  
  if execute_with_suppression rm -f "$cert_dir/ca.crt" && execute_with_suppression mkdir -p "$cert_dir"; then
    log_success "Docker certificate directory prepared"
  else
    log_error "Failed to prepare Docker certificate directory"
    return 1
  fi
  
  # Extract CA certificate from cert-manager secret
  # Use mktemp for proper temporary file handling with permissions
  local secret_name="$(registrySubdomain)-$(sedRootDomain)"
  local temp_cert=$(mktemp)
  if kubectl get secret "$secret_name" -n registry -o jsonpath='{.data.ca\.crt}' 2>/dev/null | base64 --decode > "$temp_cert" && \
     [[ -s "$temp_cert" ]] && \
     execute_with_suppression cp "$temp_cert" "$cert_dir/ca.crt"; then
    log_success "Registry CA certificate installed to Docker trust store ($(stat -c%s "$cert_dir/ca.crt") bytes)"
    rm -f "$temp_cert"
  else
    rm -f "$temp_cert"
    log_warning "Failed to extract CA certificate from secret, trying alternative approach..."
    # Fallback: look for certificate in ingress controller or containerd
    local alt_cert_path=""
    for path in /var/lib/containerd/io.containerd.snapshotter.v1.overlayfs/snapshots/*/fs/etc/ingress-controller/ssl/*registry*gokcloud*.pem; do
      if [[ -f "$path" ]]; then
        alt_cert_path="$path"
        break
      fi
    done
    
    if [[ -n "$alt_cert_path" ]] && [[ -f "$alt_cert_path" ]]; then
      log_info "Found alternative certificate path: $alt_cert_path"
      # Extract CA from certificate chain (usually the second certificate)
      local temp_alt_cert=$(mktemp)
      if tail -n +$(($(grep -n 'END CERTIFICATE' "$alt_cert_path" | head -1 | cut -d: -f1) + 1)) "$alt_cert_path" > "$temp_alt_cert" 2>/dev/null && \
         [[ -s "$temp_alt_cert" ]] && \
         execute_with_suppression cp "$temp_alt_cert" "$cert_dir/ca.crt"; then
        log_success "Registry CA certificate extracted from alternative source ($(stat -c%s "$cert_dir/ca.crt") bytes)"
        rm -f "$temp_alt_cert"
      else
        rm -f "$temp_alt_cert"
        log_error "Failed to extract CA certificate from alternative source"
        return 1
      fi
    else
      log_error "No alternative certificate source found"
      return 1
    fi
  fi
  
  log_step "5" "Restarting Docker service with new certificates"
  if execute_with_suppression systemctl restart docker; then
    log_success "Docker service restarted with registry trust"
  else
    log_error "Failed to restart Docker service"
    return 1
  fi

  log_step "6" "Restarting load balancer proxy"
  # Restarting docker stops the haproxy, need to start it again
  if execute_with_suppression gok start proxy; then
    log_success "Load balancer proxy restarted"
  else
    log_warning "Load balancer proxy restart may have failed"
  fi
  
  log_step "7" "Verifying registry authentication"
  
  # Use stored credentials for non-interactive login
  local DESTINATION_FOLDER=./registry-creds
  if [[ -f "${DESTINATION_FOLDER}/registry-user.txt" && -f "${DESTINATION_FOLDER}/registry-pass.txt" ]]; then
    local registry_user=$(cat "${DESTINATION_FOLDER}/registry-user.txt")
    local registry_pass=$(cat "${DESTINATION_FOLDER}/registry-pass.txt")
    
    log_info "Authenticating with registry using stored credentials (user: $registry_user)"
    if echo "$registry_pass" | execute_with_suppression docker login $(registrySubdomain).$(rootDomain) --username "$registry_user" --password-stdin; then
      log_success "Registry authentication verified - ready for use"
      log_component_success "registry-install" "Container registry installation completed successfully"
    else
      log_error "Registry authentication failed with stored credentials"
      return 1
    fi
  else
    log_warning "Registry credentials not found, attempting interactive login"
    log_info "Please enter your registry credentials when prompted"
    # Allow interactive login without suppression
    if docker login $(registrySubdomain).$(rootDomain); then
      log_success "Registry authentication verified - ready for use"
      log_component_success "registry-install" "Container registry installation completed successfully"
    else
      log_error "Registry authentication failed"
      return 1
    fi
  fi
  log_step "8" "Verifying TLS certificate chain and trust"
  log_info "Testing SSL/TLS connection to $(registrySubdomain).$(rootDomain):443"
  
  # Test SSL/TLS connection with suppressed output
  local ssl_result=""
  if [ "$GOK_VERBOSE" = "1" ]; then
    # In verbose mode, show the openssl output
    ssl_result=$(openssl s_client -connect $(registrySubdomain).$(rootDomain):443 -showcerts </dev/null 2>&1)
    log_debug "OpenSSL connection test output:"
    log_debug "$ssl_result"
  else
    # In normal mode, suppress openssl output
    ssl_result=$(openssl s_client -connect $(registrySubdomain).$(rootDomain):443 -showcerts </dev/null 2>/dev/null)
  fi
  
  if echo "$ssl_result" | grep 'Verify return code: 0 (ok)' >/dev/null; then
    log_success "TLS certificate verification successful"
    log_info "âœ“ Certificate chain is valid and trusted"
    log_info "âœ“ Registry is accessible via secure HTTPS connection"
  else
    log_warning "TLS certificate verification had issues"
    log_info "Registry may still be functional but certificate trust needs attention"
    if [ "$GOK_VERBOSE" = "1" ]; then
      log_debug "SSL verification details available in verbose output above"
    fi
  fi
  addPolicyToSyncSecrets
}



create_sub_scope() {
  # Parameters
  local KEYCLOAK_URL=$1
  local REALM_NAME=$2
  local ADMIN_USERNAME=$3
  local ADMIN_PASSWORD=$4
  local CLIENT_ID=$5

  # Validate input
  if [[ -z "$KEYCLOAK_URL" || -z "$REALM_NAME" || -z "$ADMIN_USERNAME" || -z "$ADMIN_PASSWORD" || -z "$CLIENT_ID" ]]; then
    echo "Usage: create_sub_scope <KEYCLOAK_URL> <REALM_NAME> <ADMIN_USERNAME> <ADMIN_PASSWORD> <CLIENT_ID>"
    return 1
  fi

  # Fetch admin access token
  ACCESS_TOKEN=$(curl -s -X POST "$KEYCLOAK_URL/realms/master/protocol/openid-connect/token" \
    -H "Content-Type: application/x-www-form-urlencoded" \
    -d "username=$ADMIN_USERNAME" \
    -d "password=$ADMIN_PASSWORD" \
    -d "grant_type=password" \
    -d "client_id=admin-cli" | jq -r '.access_token')

  if [ -z "$ACCESS_TOKEN" ]; then
    echo "Failed to fetch access token. Please check your credentials."
    return 1
  fi

  # Create the `sub` client scope
  CLIENT_SCOPE_ID=$(curl -s -X POST "$KEYCLOAK_URL/admin/realms/$REALM_NAME/client-scopes" \
    -H "Authorization: Bearer $ACCESS_TOKEN" \
    -H "Content-Type: application/json" \
    -d '{
          "name": "sub",
          "description": "Custom scope for the sub claim",
          "protocol": "openid-connect"
        }' | jq -r '.id')

  if [ -z "$CLIENT_SCOPE_ID" ]; then
    echo "Failed to create client scope 'sub'. It may already exist."
    CLIENT_SCOPE_ID=$(curl -s -X GET "$KEYCLOAK_URL/admin/realms/$REALM_NAME/client-scopes" \
      -H "Authorization: Bearer $ACCESS_TOKEN" \
      -H "Content-Type: application/json" | jq -r '.[] | select(.name=="sub") | .id')
  fi

  echo "Client Scope ID for 'sub': $CLIENT_SCOPE_ID"

  # Add a protocol mapper for the `sub` claim
  curl -s -X POST "$KEYCLOAK_URL/admin/realms/$REALM_NAME/client-scopes/$CLIENT_SCOPE_ID/protocol-mappers/models" \
    -H "Authorization: Bearer $ACCESS_TOKEN" \
    -H "Content-Type: application/json" \
    -d '{
          "name": "sub-mapper",
          "protocol": "openid-connect",
          "protocolMapper": "oidc-usermodel-property-mapper",
          "consentRequired": false,
          "config": {
            "user.attribute": "sub",
            "claim.name": "sub",
            "jsonType.label": "String",
            "id.token.claim": "true",
            "access.token.claim": "true",
            "userinfo.token.claim": "true"
          }
        }'

  echo "Protocol mapper for 'sub' added successfully."

  # Fetch client UUID
  CLIENT_UUID=$(curl -s -X GET "$KEYCLOAK_URL/admin/realms/$REALM_NAME/clients" \
    -H "Authorization: Bearer $ACCESS_TOKEN" \
    -H "Content-Type: application/json" | jq -r ".[] | select(.clientId==\"$CLIENT_ID\") | .id")

  if [ -z "$CLIENT_UUID" ]; then
    echo "Client '$CLIENT_ID' not found in realm '$REALM_NAME'."
    return 1
  fi

  # Assign the `sub` scope to the client
  curl -s -X PUT "$KEYCLOAK_URL/admin/realms/$REALM_NAME/clients/$CLIENT_UUID/optional-client-scopes/$CLIENT_SCOPE_ID" \
    -H "Authorization: Bearer $ACCESS_TOKEN" \
    -H "Content-Type: application/json"

  echo "Client scope 'sub' assigned to client '$CLIENT_ID'."

  # Verify the configuration
  echo "Verifying the configuration..."
  curl -s -X GET "$KEYCLOAK_URL/admin/realms/$REALM_NAME/clients/$CLIENT_UUID" \
    -H "Authorization: Bearer $ACCESS_TOKEN" \
    -H "Content-Type: application/json" | jq

  echo "Automation for 'sub' scope creation completed successfully."
}

debugScope(){
  # This function is used to debug the scope of the keycloak client
  # It will fetch the client secret and print it
  KEYCLOAK_URL=$(fullKeycloakUrl)
  REALM_NAME=$(kubectl get secret keycloak-secrets -n keycloak -o jsonpath="{['data']['OAUTH_REALM']}" | base64 --decode)
  CLIENT_ID=$(kubectl get secret keycloak-secrets -n keycloak -o jsonpath="{['data']['CLIENT_ID']}" | base64 --decode)
  ADMIN_USERNAME=$(kubectl get secret keycloak-secrets -n keycloak -o jsonpath="{['data']['KEYCLOAK_ADMIN']}" | base64 --decode)
  ADMIN_PASSWORD=$(kubectl get secret keycloak-secrets -n keycloak -o jsonpath="{.data.KEYCLOAK_ADMIN_PASSWORD}" | base64 --decode)

  CLIENT_SECRET=$(fetch_client_secret "$KEYCLOAK_URL" "$REALM_NAME" "$CLIENT_ID" "$ADMIN_USERNAME" "$ADMIN_PASSWORD")
  echo "Client Secret: $CLIENT_SECRET"
  if [[ -z "$CLIENT_SECRET" ]]; then
    echoFailed "Failed to fetch client secret, please check the logs!"
    return 1
  fi
  echoSuccess "Client Secret fetched successfully!"
  generateAccessToken "$KEYCLOAK_URL" "$REALM_NAME" "$CLIENT_ID" "$CLIENT_SECRET" "openid profile email"
  if [[ $? -ne 0 ]]; then
    echoFailed "Failed to generate access token, please check the logs!"
    return 1
  fi  
}

generateAccessToken() {
    local KEYCLOAK_URL=$1
    local REALM_NAME=$2
    local CLIENT_ID=$3
    local CLIENT_SECRET=$4
    local SCOPE=$5

    if [[ -z "$KEYCLOAK_URL" || -z "$REALM_NAME" || -z "$CLIENT_ID" || -z "$CLIENT_SECRET" || -z "$SCOPE" ]]; then
      echo "Usage: generateAccessToken <KEYCLOAK_URL> <REALM_NAME> <CLIENT_ID> <CLIENT_SECRET> <SCOPE>"
      return 1
    fi

    curl -s -X POST https://"$KEYCLOAK_URL/realms/$REALM_NAME/protocol/openid-connect/token" \
      -H "Content-Type: application/x-www-form-urlencoded" \
      -d "client_id=$CLIENT_ID" \
      -d "client_secret=$CLIENT_SECRET" \
      -d "grant_type=client_credentials" \
      -d "scope=$SCOPE" | jq -r '.access_token'
  }

installKeycloakWithCertMgr(){
  local start_time=$(date +%s)
  
  log_component_start "keycloak-install" "Installing Keycloak identity and access management"
  
  log_step "1" "Updating system packages with smart caching"
  if ! updateSys; then
    log_error "Failed to update system packages"
    return 1
  fi
  
  log_step "2" "Installing dependencies with smart caching"
  if ! installDeps; then
    log_error "Failed to install Keycloak dependencies"
    return 1
  fi
  
  log_step "3" "Preparing Keycloak installation directory"
  local keycloak_dir="$MOUNT_PATH/kubernetes/install_k8s/keycloak"
  if [[ ! -d "$keycloak_dir" ]]; then
    log_error "Keycloak installation directory not found: $keycloak_dir"
    return 1
  fi
  
  if execute_with_suppression pushd "$keycloak_dir"; then
    log_success "Keycloak installation directory prepared"
  else
    log_error "Failed to access Keycloak installation directory"
    return 1
  fi

  log_step "4" "Installing core Keycloak services"
  if ! keycloakInst; then
    log_error "Keycloak core installation failed"
    popd || true
    return 1
  fi
  log_success "Keycloak core services installed"

  log_step "5" "Setting up persistent storage and certificates"
  log_substep "Creating storage class and persistent volume..."
  if createLocalStorageClassAndPV "keycloak-storage" "keycloak-pv" "/data/volumes/pv3"; then
    log_success "Keycloak storage configured"
  else
    log_warning "Storage configuration had issues but continuing"
  fi
  
  log_substep "Configuring ingress and SSL certificates..."
  if execute_with_suppression gok patch ingress keycloak keycloak letsencrypt $(keycloakSubdomain); then
    log_success "Keycloak ingress and certificates configured"
  else
    log_warning "Ingress configuration had issues but continuing"
  fi

  log_step "6" "Waiting for Keycloak services to be ready"
  if wait_for_keycloak_services; then
    log_success "Keycloak services are ready and accepting connections"
    
    # Create permanent admin account
    log_step "7" "Creating permanent admin account"
    if create_permanent_keycloak_admin; then
      log_success "Permanent admin account created successfully"
    else
      log_warning "Could not create permanent admin account automatically (may need manual setup)"
    fi
  else
    log_error "Keycloak services failed to start properly"
    popd || true
    return 1
  fi

  log_step "8" "Installing Python dependencies for configuration"
  log_substep "Installing Python packages for Keycloak client management..."
  if execute_with_suppression apt install python3-dotenv python3-requests python3-jose -y; then
    log_success "Python dependencies installed"
  else
    log_warning "Python dependency installation had issues but continuing"
  fi

  log_step "8" "Configuring Keycloak clients and realms"
  if setup_keycloak_clients; then
    log_success "Keycloak clients and realms configured"
    
    log_step "9" "Setting up OAuth2 integration"
    if execute_with_suppression oauth2Secret; then
      log_success "OAuth2 integration configured"
    else
      log_warning "OAuth2 integration had issues but continuing"
    fi
  else
    log_error "Keycloak client configuration failed"
    popd || true
    return 1
  fi

  log_step "10" "Configuring LDAP user federation"
  if setup_ldap_federation; then
    log_success "LDAP user federation configured"
  else
    log_warning "LDAP federation configuration had issues but core Keycloak is working"
  fi

  log_step "11" "Finalizing Keycloak installation"
  if execute_with_suppression popd; then
    log_success "Keycloak installation directory cleanup completed"
  else
    log_warning "Directory cleanup had issues but installation completed"
  fi

  log_step "12" "Validating complete Keycloak installation"
  if validate_keycloak_installation; then
    log_success "Keycloak installation validation completed"
  else
    log_warning "Keycloak validation had issues but installation may still work"
  fi

  local end_time=$(date +%s)
  local duration=$((end_time - start_time))
  
  show_installation_summary "keycloak" "keycloak" "Identity and access management system with LDAP integration"
  log_component_success "keycloak-install" "Keycloak identity management system installed successfully"
  log_success "Complete Keycloak installation completed in ${duration}s"
  
  # Show Keycloak-specific next steps and recommend OAuth2
  show_keycloak_next_steps
  
  # Suggest and install OAuth2 as the next step (only after complete installation)
  suggest_and_install_next_module "keycloak"
}

# Enhanced waiting for Keycloak services with immediate issue detection
wait_for_keycloak_services() {
  log_info "â³ Waiting for Keycloak services to be ready with enhanced diagnostics (timeout: 4 minutes)"
  
  # First, check for immediate deployment issues
  log_substep "Performing initial deployment health check..."
  sleep 10  # Give pods time to start creating
  
  # Check for immediate image pull issues
  if ! check_image_pull_issues "keycloak" "keycloak"; then
    log_error "âŒ Keycloak has Docker image pull issues - aborting wait"
    return 1
  fi
  
  # Check for resource constraint issues
  if ! check_resource_constraints "keycloak" "keycloak"; then
    log_error "âŒ Keycloak has resource constraint issues - aborting wait"
    return 1
  fi
  
  # Use enhanced pod waiting with detailed diagnostics
  log_substep "Waiting for Keycloak pods with detailed monitoring..."
  if wait_for_pods_ready "keycloak" "240" "keycloak"; then
    log_success "âœ“ Keycloak pods are ready"
  else
    log_error "âŒ Keycloak pods failed to become ready"
    return 1
  fi
  
  # Verify StatefulSet is actually ready (Keycloak uses StatefulSet, not Deployment)
  log_substep "Verifying Keycloak StatefulSet status..."
  if check_statefulset_readiness "keycloak" "keycloak"; then
    log_success "âœ“ Keycloak StatefulSet is healthy"
  else
    log_error "âŒ Keycloak StatefulSet has issues"
    return 1
  fi
  
  # Check service connectivity
  log_substep "Verifying Keycloak service connectivity..."
  if check_service_connectivity "keycloak-http" "keycloak"; then
    log_success "âœ“ Keycloak service is accessible"
  else
    log_warning "âš ï¸  Keycloak service connectivity issues detected"
  fi
  
  # Final verification
  local keycloak_url="https://$(keycloakSubdomain).$(rootDomain)/"
  log_success "âœ… Keycloak is ready and accessible at: ${COLOR_CYAN}${keycloak_url}${COLOR_RESET}"
  
  # Brief pause for services to fully initialize
  log_substep "Allowing services to fully initialize..."
  sleep 10
  return 0
}

# Setup Keycloak clients and realms
setup_keycloak_clients() {
  log_info "ðŸ”§ Configuring Keycloak clients and realms"
  
  # Retrieve credentials from secrets
  local admin_id=$(kubectl get secret keycloak-secrets -n keycloak -o jsonpath="{.data.KEYCLOAK_ADMIN}" 2>/dev/null | base64 --decode || echo "")
  local admin_pwd=$(kubectl get secret keycloak-secrets -n keycloak -o jsonpath="{.data.KEYCLOAK_ADMIN_PASSWORD}" 2>/dev/null | base64 --decode || echo "")
  local client_id=$(kubectl get secret keycloak-secrets -n keycloak -o jsonpath="{.data.CLIENT_ID}" 2>/dev/null | base64 --decode || echo "")
  local realm=$(kubectl get secret keycloak-secrets -n keycloak -o jsonpath="{.data.OAUTH_REALM}" 2>/dev/null | base64 --decode || echo "")
  
  if [[ -z "$admin_id" || -z "$admin_pwd" || -z "$client_id" || -z "$realm" ]]; then
    log_error "Failed to retrieve Keycloak credentials from secrets"
    return 1
  fi
  
  log_substep "Admin User: ${COLOR_CYAN}${admin_id}${COLOR_RESET}"
  log_substep "Client ID: ${COLOR_CYAN}${client_id}${COLOR_RESET}"
  log_substep "Realm: ${COLOR_CYAN}${realm}${COLOR_RESET}"
  
  # Show what will be created
  log_substep "ðŸ“‹ Configuration Details:"
  log_substep "  â€¢ Realm: ${COLOR_CYAN}${realm}${COLOR_RESET} (will be created)"
  log_substep "  â€¢ Client: ${COLOR_CYAN}${client_id}${COLOR_RESET} (OIDC client for automation)"
  log_substep "  â€¢ Groups: ${COLOR_CYAN}administrators, developers${COLOR_RESET} (user groups)"
  log_substep "  â€¢ Sample User: ${COLOR_CYAN}skmaji1${COLOR_RESET} (with admin/developer roles)"
  log_substep "  â€¢ Scopes: ${COLOR_CYAN}groups${COLOR_RESET} (OIDC group membership claims)"
  log_substep "  â€¢ Token Lifespan: ${COLOR_CYAN}24 hours${COLOR_RESET} (access token validity)"
  
  # Run Keycloak client configuration
  log_substep "ðŸš€ Running Keycloak client configuration script..."
  if execute_with_suppression python3 "$MOUNT_PATH/kubernetes/install_k8s/keycloak/keycloak-client.py" all "$admin_id" "$admin_pwd" "$client_id" "$realm"; then
    log_success "Keycloak client configuration completed"
  else
    log_error "Keycloak client configuration failed"
    return 1
  fi
  
  # Create sub scope
  local keycloak_url=$(fullKeycloakUrl)
  log_substep "Creating groups scope for OIDC claims..."
  if execute_with_suppression create_sub_scope "https://${keycloak_url}" "${realm}" "${admin_id}" "${admin_pwd}" "${client_id}"; then
    log_success "Groups scope created for OIDC integration"
  else
    log_warning "Groups scope creation had issues but continuing"
  fi
  
  return 0
}

# Setup LDAP federation
setup_ldap_federation() {
  log_info "ðŸ”— Setting up LDAP user federation"
  
  # Check if LDAP service is running
  log_substep "Checking LDAP service availability..."
  local ldap_status=$(kubectl get svc ldap -n ldap 2>/dev/null | grep ldap | wc -l)
  
  if [[ "$ldap_status" -eq 0 ]]; then
    log_warning "LDAP service is not running - skipping user federation setup"
    log_info "To set up LDAP federation later, ensure LDAP is installed and run the federation scripts manually"
    return 0
  else
    log_success "LDAP service is running - proceeding with user federation"
  fi
  
  # Get credentials
  local admin_id=$(kubectl get secret keycloak-secrets -n keycloak -o jsonpath="{.data.KEYCLOAK_ADMIN}" 2>/dev/null | base64 --decode || echo "")
  local admin_pwd=$(kubectl get secret keycloak-secrets -n keycloak -o jsonpath="{.data.KEYCLOAK_ADMIN_PASSWORD}" 2>/dev/null | base64 --decode || echo "")
  
  : "${LDAP_PASSWORD:=$(promptSecret "Please enter LDAP password for admin: ")}"
  
  # Create audience scope
  log_substep "Setting up Kubernetes audience scope..."
  if execute_with_suppression pushd "$MOUNT_PATH/kubernetes/install_k8s/keycloak" && execute_with_suppression chmod +x setup_kubernetes_audience.sh && execute_with_suppression ./setup_kubernetes_audience.sh "$admin_id" "$admin_pwd" && execute_with_suppression popd; then
    log_success "Kubernetes audience scope created"
  else
    log_error "Audience scope creation failed"
    popd || true
    return 1
  fi
  
  # Create user federation
  log_substep "Setting up LDAP user federation..."
  if execute_with_suppression chmod +x setup_user_federation.sh && execute_with_suppression ./setup_user_federation.sh "$admin_id" "$admin_pwd" "$LDAP_PASSWORD"; then
    log_success "LDAP user federation created"
  else
    log_error "User federation creation failed"
    return 1
  fi
  
  # Create group mappers
  log_substep "Setting up Keycloak group mappers..."
  if execute_with_suppression chmod +x setup_group_mappers.sh && execute_with_suppression ./setup_group_mappers.sh "$admin_id" "$admin_pwd"; then
    log_success "Keycloak group mappers created"
  else
    log_error "Group mapper creation failed"
    return 1
  fi
  
  return 0
}


installLdap(){
  local start_time=$(date +%s)
  
  log_component_start "ldap-install" "Installing LDAP directory service and authentication"
  
  log_step "1" "Preparing LDAP installation directory"
  local ldap_dir="$MOUNT_PATH/kubernetes/install_k8s/ldap"
  if [[ ! -d "$ldap_dir" ]]; then
    log_error "LDAP installation directory not found: $ldap_dir"
    return 1
  fi
  
  if execute_with_suppression pushd "$ldap_dir"; then
    log_success "LDAP installation directory prepared"
  else
    log_error "Failed to access LDAP installation directory"
    return 1
  fi

  log_step "2" "Collecting LDAP configuration parameters"
  : "${LDAP_PASSWORD:=${1:-$(promptSecret "Please enter LDAP password for admin: ")} }"
  : "${KERBEROS_PASSWORD:=${1:-$(promptSecret "Please enter Kerberos password: ")} }"
  : "${KERBEROS_KDC_PASSWORD:=${1:-$(promptSecret "Please enter Kerberos kdc password: ")} }"
  : "${KERBEROS_ADM_PASSWORD:=${1:-$(promptSecret "Please enter Kerberos adm password: ")} }"
  
  log_success "LDAP configuration parameters collected"

  log_step "3" "Building and deploying LDAP directory service"
  
  # Enhanced LDAP build with progress tracking
  build_ldap_with_progress "$LDAP_PASSWORD" "$KERBEROS_PASSWORD" "$KERBEROS_KDC_PASSWORD" "$KERBEROS_ADM_PASSWORD"
  if [[ $? -ne 0 ]]; then
    log_error "LDAP installation failed"
    popd || true
    return 1
  fi

  log_step "4" "Configuring LDAP ingress and networking"
  if execute_with_suppression gok patch ingress ldap ldap letsencrypt $(defaultSubdomain); then
    log_success "LDAP ingress configured successfully"
  else
    log_warning "LDAP ingress configuration had issues but installation may still work"
  fi

  log_step "5" "Validating LDAP installation"
  if validate_ldap_installation; then
    log_success "LDAP installation validation completed"
  else
    log_warning "LDAP validation had issues but installation may still work"
  fi

  if execute_with_suppression popd; then
    log_success "LDAP installation directory cleanup completed"
  else
    log_warning "Directory cleanup had issues but installation completed"
  fi

  local end_time=$(date +%s)
  local duration=$((end_time - start_time))
  
  show_installation_summary "ldap" "ldap" "LDAP directory service and authentication"
  log_component_success "ldap-install" "LDAP directory service installed successfully"
  log_success "LDAP installation completed in ${duration}s"
  
  # Show LDAP-specific next steps and recommend Keycloak
  show_ldap_next_steps
}

# Enhanced LDAP build with detailed progress tracking
build_ldap_with_progress() {
  local ldap_password="$1"
  local kerberos_password="$2"
  local kerberos_kdc_password="$3"
  local kerberos_adm_password="$4"
  local start_time=$(date +%s)
  
  # Source configuration to get image details
  if [[ ! -f "configuration" ]]; then
    log_error "Configuration file not found in LDAP directory"
    return 1
  fi
  
  source configuration
  source "$MOUNT_PATH/kubernetes/install_k8s/util" 2>/dev/null || true
  
  # Get registry and image information
  local registry_url=$(fullRegistryUrl 2>/dev/null || echo "localhost:5000")
  local image_name="${IMAGE_NAME:-sumit/ldap}"
  local repo_name="${REPO_NAME:-ldap}"
  local full_image_url="${registry_url}/${repo_name}"
  
  log_substep "Registry: ${COLOR_CYAN}${registry_url}${COLOR_RESET}"
  log_substep "Image: ${COLOR_CYAN}${image_name}${COLOR_RESET}"
  log_substep "Target: ${COLOR_CYAN}${full_image_url}${COLOR_RESET}"
  
  # Get domain configuration
  source config/config 2>/dev/null || true
  local domain_name="${DOMAIN_NAME:-default.svc.cloud.uat}"
  local ldap_hostname="${LDAP_HOSTNAME:-ldap.${domain_name}}"
  local base_dn="${DC:-dc=default,dc=svc,dc=cloud,dc=uat}"
  
  log_substep "LDAP Domain: ${COLOR_CYAN}${domain_name}${COLOR_RESET}"
  log_substep "LDAP Hostname: ${COLOR_CYAN}${ldap_hostname}${COLOR_RESET}"
  log_substep "Base DN: ${COLOR_CYAN}${base_dn}${COLOR_RESET}"
  
  # Step 1: Docker Build with Progress
  log_info "ðŸ³ Building LDAP Docker image: ${COLOR_BOLD}${image_name}${COLOR_RESET}"
  
  local temp_build_log=$(mktemp)
  local temp_build_error=$(mktemp)
  
  # Start Docker build in background with enhanced arguments
  docker build \
    --build-arg LDAP_DOMAIN="$domain_name" \
    --build-arg REGISTRY="$registry_url" \
    --build-arg LDAP_HOSTNAME="$ldap_hostname" \
    --build-arg BASE_DN="$base_dn" \
    --build-arg LDAP_PASSWORD="$ldap_password" \
    -t "$image_name" . >"$temp_build_log" 2>"$temp_build_error" &
  local build_pid=$!
  
  # Show build progress with LDAP-specific stages
  local build_progress=0
  local build_steps=12
  local spinner_chars="|/-\\"
  local spinner_idx=0
  local build_stage="Initializing"
  
  while kill -0 $build_pid 2>/dev/null; do
    local char=${spinner_chars:spinner_idx:1}
    build_progress=$(( (build_progress + 1) % (build_steps * 8) ))
    local progress_percent=$(( build_progress * 100 / (build_steps * 8) ))
    
    # Update stage based on progress
    case $((build_progress / 8)) in
      0) build_stage="Preparing base image" ;;
      1) build_stage="Installing LDAP packages" ;;
      2) build_stage="Configuring OpenLDAP" ;;
      3) build_stage="Setting up phpLDAPadmin" ;;
      4) build_stage="Installing Kerberos" ;;
      5) build_stage="Configuring authentication" ;;
      6) build_stage="Setting up SSL certificates" ;;
      7) build_stage="Installing utilities" ;;
      8) build_stage="Configuring permissions" ;;
      9) build_stage="Installing NTP services" ;;
      10) build_stage="Cleaning up packages" ;;
      11) build_stage="Finalizing LDAP image" ;;
    esac
    
    printf "\r${COLOR_BLUE}  Building LDAP image [%c] ${COLOR_CYAN}%d%%${COLOR_RESET} - ${COLOR_DIM}%s${COLOR_RESET}" "$char" "$progress_percent" "$build_stage"
    
    spinner_idx=$(( (spinner_idx + 1) % 4 ))
    sleep 0.4
  done
  
  wait $build_pid
  local build_exit_code=$?
  
  if [[ $build_exit_code -eq 0 ]]; then
    printf "\r${COLOR_GREEN}  âœ“ LDAP Docker build completed [100%%]${COLOR_RESET}\n"
    log_success "LDAP Docker image built successfully: ${image_name}"
    
    # Show warnings if present but don't fail
    if is_verbose_mode && [[ -s "$temp_build_log" ]]; then
      if grep -q "warning" "$temp_build_log"; then
        log_info "Build warnings (non-critical):"
        grep -i "warning" "$temp_build_log" | head -5 | while read line; do
          log_warning "  $line"
        done
      fi
    fi
  else
    printf "\r${COLOR_RED}  âœ— LDAP Docker build failed${COLOR_RESET}\n"
    log_error "LDAP Docker build failed - error details:"
    if [[ -s "$temp_build_error" ]]; then
      cat "$temp_build_error" >&2
    fi
    rm -f "$temp_build_log" "$temp_build_error"
    return 1
  fi
  
  # Step 2: Docker Tag
  log_info "ðŸ·ï¸  Tagging LDAP image for registry: ${COLOR_BOLD}${full_image_url}${COLOR_RESET}"
  if docker tag "$image_name" "$full_image_url" >/dev/null 2>&1; then
    log_success "LDAP image tagged successfully"
  else
    log_error "Failed to tag LDAP Docker image"
    rm -f "$temp_build_log" "$temp_build_error"
    return 1
  fi
  
  # Step 3: Docker Push with Progress
  log_info "ðŸ“¤ Pushing LDAP image to registry: ${COLOR_BOLD}${registry_url}${COLOR_RESET}"
  log_substep "Target repository: ${COLOR_CYAN}${repo_name}${COLOR_RESET}"
  
  # Start Docker push in background
  docker push "$full_image_url" >"$temp_build_log" 2>"$temp_build_error" &
  local push_pid=$!
  
  # Show push progress with LDAP-specific stages
  local push_progress=0
  local push_steps=8
  local push_stage="Preparing"
  
  while kill -0 $push_pid 2>/dev/null; do
    local char=${spinner_chars:spinner_idx:1}
    push_progress=$(( (push_progress + 1) % (push_steps * 12) ))
    local progress_percent=$(( push_progress * 100 / (push_steps * 12) ))
    
    # Update stage based on progress
    case $((push_progress / 12)) in
      0) push_stage="Preparing LDAP layers" ;;
      1) push_stage="Uploading base system" ;;
      2) push_stage="Uploading LDAP packages" ;;
      3) push_stage="Uploading configurations" ;;
      4) push_stage="Uploading certificates" ;;
      5) push_stage="Uploading utilities" ;;
      6) push_stage="Uploading final layers" ;;
      7) push_stage="Finalizing push" ;;
    esac
    
    printf "\r${COLOR_MAGENTA}  Pushing LDAP to registry [%c] ${COLOR_CYAN}%d%%${COLOR_RESET} - ${COLOR_DIM}%s${COLOR_RESET}" "$char" "$progress_percent" "$push_stage"
    
    spinner_idx=$(( (spinner_idx + 1) % 4 ))
    sleep 0.4
  done
  
  wait $push_pid
  local push_exit_code=$?
  
  if [[ $push_exit_code -eq 0 ]]; then
    printf "\r${COLOR_GREEN}  âœ“ LDAP push completed [100%%] - Image available at ${COLOR_BOLD}${full_image_url}${COLOR_RESET}\n"
    log_success "LDAP image pushed successfully to registry"
  else
    printf "\r${COLOR_RED}  âœ— LDAP push failed${COLOR_RESET}\n"
    log_error "LDAP Docker push failed - error details:"
    if [[ -s "$temp_build_error" ]]; then
      cat "$temp_build_error" >&2
    fi
    rm -f "$temp_build_log" "$temp_build_error"
    return 1
  fi
  
  # Step 4: Deploy to Kubernetes
  log_info "â˜¸ï¸  Deploying LDAP to Kubernetes cluster"
  
  # Execute the deployment script with suppressed output unless verbose
  local temp_deploy_log=$(mktemp)
  local temp_deploy_error=$(mktemp)
  
  ./run_ldap.sh "$ldap_password" "$kerberos_password" "$kerberos_kdc_password" "$kerberos_adm_password" >"$temp_deploy_log" 2>"$temp_deploy_error" &
  local deploy_pid=$!
  
  # Show deployment progress
  local deploy_progress=0
  local deploy_steps=6
  local deploy_stage="Preparing"
  
  while kill -0 $deploy_pid 2>/dev/null; do
    local char=${spinner_chars:spinner_idx:1}
    deploy_progress=$(( (deploy_progress + 1) % (deploy_steps * 10) ))
    local progress_percent=$(( deploy_progress * 100 / (deploy_steps * 10) ))
    
    # Update stage based on progress
    case $((deploy_progress / 10)) in
      0) deploy_stage="Creating namespace" ;;
      1) deploy_stage="Deploying LDAP service" ;;
      2) deploy_stage="Setting up ingress" ;;
      3) deploy_stage="Configuring certificates" ;;
      4) deploy_stage="Waiting for pods" ;;
      5) deploy_stage="Verifying services" ;;
    esac
    
    printf "\r${COLOR_BLUE}  Deploying LDAP service [%c] ${COLOR_CYAN}%d%%${COLOR_RESET} - ${COLOR_DIM}%s${COLOR_RESET}" "$char" "$progress_percent" "$deploy_stage"
    
    spinner_idx=$(( (spinner_idx + 1) % 4 ))
    sleep 0.5
  done
  
  wait $deploy_pid
  local deploy_exit_code=$?
  
  if [[ $deploy_exit_code -eq 0 ]]; then
    printf "\r${COLOR_GREEN}  âœ“ LDAP deployment completed [100%%]${COLOR_RESET}\n"
    log_success "LDAP service deployed successfully to Kubernetes"
    
    # Step 5: Enhanced diagnostic verification after deployment
    log_info "ðŸ” Performing enhanced deployment verification"
    
    # Give the deployment a moment to start creating pods
    log_substep "Allowing deployment to initialize..."
    sleep 15
    
    # Check for immediate image pull issues
    log_substep "Checking for Docker image pull issues..."
    if ! check_image_pull_issues "ldap" "ldap"; then
      log_error "âŒ LDAP has Docker image pull issues"
      rm -f "$temp_build_log" "$temp_build_error" "$temp_deploy_log" "$temp_deploy_error"
      return 1
    fi
    
    # Check for resource constraint issues
    log_substep "Checking for resource constraints..."
    if ! check_resource_constraints "ldap" "ldap"; then
      log_error "âŒ LDAP has resource constraint issues"
      rm -f "$temp_build_log" "$temp_build_error" "$temp_deploy_log" "$temp_deploy_error"
      return 1
    fi
    
    # Use enhanced pod waiting with detailed diagnostics
    log_substep "Waiting for LDAP pods with enhanced monitoring..."
    if wait_for_pods_ready "ldap" "300" "ldap"; then
      log_success "âœ… LDAP pods are ready and healthy"
    else
      log_error "âŒ LDAP pods failed to become ready"
      rm -f "$temp_build_log" "$temp_build_error" "$temp_deploy_log" "$temp_deploy_error"
      return 1
    fi
    
  else
    printf "\r${COLOR_RED}  âœ— LDAP deployment failed${COLOR_RESET}\n"
    log_error "LDAP deployment failed - error details:"
    if [[ -s "$temp_deploy_error" ]]; then
      if is_verbose_mode; then
        cat "$temp_deploy_error" >&2
      else
        tail -10 "$temp_deploy_error" >&2
        log_info "Use --verbose flag to see full deployment logs"
      fi
    fi
    rm -f "$temp_build_log" "$temp_build_error" "$temp_deploy_log" "$temp_deploy_error"
    return 1
  fi
  
  # Clean up temporary files
  rm -f "$temp_build_log" "$temp_build_error" "$temp_deploy_log" "$temp_deploy_error"
  
  # Build completion summary
  local end_time=$(date +%s)
  local duration=$((end_time - start_time))
  log_success "LDAP build and deployment completed in ${duration}s"
  
  # Show LDAP image and deployment summary
  show_ldap_summary
  
  return 0
}

# Validate LDAP installation with comprehensive checks
validate_ldap_installation() {
  log_info "Validating LDAP installation with enhanced diagnostics..."
  local validation_passed=true
  
  log_step "1" "Checking LDAP namespace"
  if kubectl get namespace ldap >/dev/null 2>&1; then
    log_success "LDAP namespace found"
  else
    log_error "LDAP namespace not found"
    return 1
  fi
  
  log_step "2" "Checking LDAP deployment status"
  if check_deployment_readiness "ldap" "ldap"; then
    log_success "LDAP deployment is ready"
  else
    log_error "LDAP deployment has issues"
    validation_passed=false
  fi
  
  log_step "3" "Checking LDAP pods with detailed diagnostics"
  if ! wait_for_pods_ready "ldap" "300" "ldap"; then
    log_error "LDAP pods not ready"
    validation_passed=false
  else
    log_success "LDAP pods are ready"
  fi
  
  log_step "4" "Checking LDAP service connectivity"
  if check_service_connectivity "ldap" "ldap"; then
    log_success "LDAP service is accessible"
  else
    log_warning "LDAP service connectivity issues detected"
  fi
  
  log_step "5" "Checking LDAP persistent volumes"
  local ldap_pvcs=$(kubectl get pvc -n ldap --no-headers 2>/dev/null | wc -l)
  if [[ $ldap_pvcs -gt 0 ]]; then
    log_success "LDAP persistent volume claims found ($ldap_pvcs PVCs)"
    
    # Check for pending PVCs
    local pending_pvcs=$(kubectl get pvc -n ldap --no-headers 2>/dev/null | grep "Pending" | wc -l)
    if [[ $pending_pvcs -gt 0 ]]; then
      log_warning "$pending_pvcs LDAP PVC(s) are in Pending state"
    fi
  else
    log_info "No persistent volume claims found for LDAP (may be using ephemeral storage)"
  fi
  
  log_step "6" "Testing LDAP connectivity (if accessible)"
  local ldap_pod=$(kubectl get pods -n ldap --no-headers 2>/dev/null | grep "Running" | head -1 | awk '{print $1}')
  if [[ -n "$ldap_pod" ]]; then
    if kubectl exec -n ldap "$ldap_pod" -- ldapsearch -x -b "" -s base "(objectclass=*)" >/dev/null 2>&1; then
      log_success "LDAP server is responding to queries"
    else
      log_warning "LDAP server may not be fully initialized yet"
    fi
  fi
  
  return $([[ "$validation_passed" == "true" ]] && echo 0 || echo 1)
}

# Show LDAP-specific summary and next steps
show_ldap_summary() {
  log_info "ðŸ“‹ LDAP Directory Service Summary"
  echo
  
  echo -e "${COLOR_BRIGHT_BLUE}${COLOR_BOLD}ðŸ—‚ï¸  LDAP Directory Service Details${COLOR_RESET}"
  echo -e "${COLOR_CYAN}â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”${COLOR_RESET}"
  
  echo -e "${COLOR_YELLOW}${COLOR_BOLD}Service Information:${COLOR_RESET}"
  echo -e "  ðŸŒ ${COLOR_GREEN}LDAP Server${COLOR_RESET} - OpenLDAP directory service"
  echo -e "  ðŸ–¥ï¸  ${COLOR_GREEN}phpLDAPadmin${COLOR_RESET} - Web-based LDAP administration interface"
  echo -e "  ðŸ” ${COLOR_GREEN}Kerberos Integration${COLOR_RESET} - Single sign-on authentication"
  echo -e "  ðŸ—‚ï¸  ${COLOR_GREEN}Directory Structure${COLOR_RESET} - Hierarchical user and group management"
  echo
  
  echo -e "${COLOR_YELLOW}${COLOR_BOLD}Access Information:${COLOR_RESET}"
  local subdomain=$(defaultSubdomain 2>/dev/null || echo "kube")
  local domain=$(rootDomain 2>/dev/null || echo "gokcloud.com")
  echo -e "  ðŸŒ ${COLOR_CYAN}Web Interface${COLOR_RESET}: https://${subdomain}.${domain}/phpldapadmin/"
  echo -e "  ðŸ”Œ ${COLOR_CYAN}LDAP Protocol${COLOR_RESET}: ldap://ldap.default.svc.cloud.uat:389"
  echo -e "  ðŸ”’ ${COLOR_CYAN}LDAPS (SSL)${COLOR_RESET}: ldaps://ldap.default.svc.cloud.uat:636"
  echo
  
  echo -e "${COLOR_YELLOW}${COLOR_BOLD}Authentication Features:${COLOR_RESET}"
  echo -e "  âœ… ${COLOR_GREEN}User Management${COLOR_RESET} - Create, modify, and delete users"
  echo -e "  âœ… ${COLOR_GREEN}Group Management${COLOR_RESET} - Organize users into groups"
  echo -e "  âœ… ${COLOR_GREEN}Password Policies${COLOR_RESET} - Enforce security requirements"
  echo -e "  âœ… ${COLOR_GREEN}SSL/TLS Support${COLOR_RESET} - Encrypted connections"
  echo -e "  âœ… ${COLOR_GREEN}Kerberos SSO${COLOR_RESET} - Single sign-on integration"
  echo
  
  echo -e "${COLOR_BRIGHT_GREEN}${COLOR_BOLD}ðŸ’¡ LDAP Benefits:${COLOR_RESET}"
  echo -e "${COLOR_GREEN}â€¢ Centralized user and group management across all services${COLOR_RESET}"
  echo -e "${COLOR_GREEN}â€¢ Foundation for enterprise identity and access management${COLOR_RESET}"
  echo -e "${COLOR_GREEN}â€¢ Integration with Keycloak, OAuth2, and other auth services${COLOR_RESET}"
  echo -e "${COLOR_GREEN}â€¢ Support for complex organizational hierarchies${COLOR_RESET}"
  echo -e "${COLOR_GREEN}â€¢ High availability and scalability for enterprise environments${COLOR_RESET}"
  echo
}

# Show LDAP next steps and recommend Keycloak
show_ldap_next_steps() {
  echo
  echo -e "${COLOR_BRIGHT_YELLOW}${COLOR_BOLD}ðŸš€ LDAP Post-Installation Steps${COLOR_RESET}"
  echo
  
  echo -e "${COLOR_MAGENTA}${COLOR_BOLD}Immediate Next Steps:${COLOR_RESET}"
  echo -e "${COLOR_CYAN}  1. Access phpLDAPadmin web interface to configure users and groups${COLOR_RESET}"
  echo -e "${COLOR_CYAN}  2. Create organizational units (OUs) for your directory structure${COLOR_RESET}"
  echo -e "${COLOR_CYAN}  3. Add initial users and groups to the directory${COLOR_RESET}"
  echo -e "${COLOR_CYAN}  4. Test LDAP authentication and directory queries${COLOR_RESET}"
  echo
  
  echo -e "${COLOR_BRIGHT_MAGENTA}${COLOR_BOLD}ðŸŽ¯ Recommended Next Installation: Keycloak${COLOR_RESET}"
  echo -e "${COLOR_CYAN}LDAP provides the directory foundation - now add modern identity management with Keycloak!${COLOR_RESET}"
  echo
  echo -e "${COLOR_YELLOW}${COLOR_BOLD}Why install Keycloak next?${COLOR_RESET}"
  echo -e "${COLOR_GREEN}â€¢ ðŸ” Modern OAuth2/OIDC identity provider${COLOR_RESET}"
  echo -e "${COLOR_GREEN}â€¢ ðŸ”— Seamless LDAP integration for user authentication${COLOR_RESET}"
  echo -e "${COLOR_GREEN}â€¢ ðŸŒ Single sign-on (SSO) across all GOK platform services${COLOR_RESET}"
  echo -e "${COLOR_GREEN}â€¢ ðŸ‘¥ Advanced user federation and social login support${COLOR_RESET}"
  echo -e "${COLOR_GREEN}â€¢ ðŸ›¡ï¸  Enterprise-grade security and compliance features${COLOR_RESET}"
  echo
  echo -e "${COLOR_BRIGHT_GREEN}${COLOR_BOLD}Install Keycloak now?${COLOR_RESET}"
  echo -e "${COLOR_CYAN}  Command: ${COLOR_BOLD}gok install keycloak${COLOR_RESET}"
  echo
  
  # Suggest and install Keycloak as the next step
  suggest_and_install_next_module "ldap"
}

updateLdapConfig(){

  # Usage: ./update_configmap.sh <LDAP_HOSTNAME> <LDAP_ADMIN_DN> <BASE_DN>
  LDAP_HOSTNAME=$1
  BASE_DN=$2

  if [[ -z "$LDAP_HOSTNAME" || -z "$BASE_DN" ]]; then
    echo "Usage: $0 <LDAP_HOSTNAME> <BASE_DN>"
    exit 1
  fi

  # Create or update the ConfigMap
  kubectl create configmap ldap-env-config \
    --from-literal=LDAP_HOSTNAME="$LDAP_HOSTNAME" \
    --from-literal=BASE_DN="$BASE_DN" \
    --namespace=default \
    --dry-run=client -o yaml | kubectl apply -f -
}

updateUserData(){

  # Usage: ./update_user_data.sh <USERNAME> <PASSWORD> <EMAIL> <FIRST_NAME> <LAST_NAME> <GROUP_NAME>
  USERNAME=$1
  PASSWORD=$2
  EMAIL=$3
  FIRST_NAME=$4
  LAST_NAME=$5
  GROUP_NAME=$6

  # Prepare the `kubectl create configmap` command
  CONFIGMAP_CMD="kubectl create configmap ldap-user-data --namespace=default"

  # Add non-empty parameters to the ConfigMap
  [[ -n "$USERNAME" ]] && CONFIGMAP_CMD+=" --from-literal=USERNAME=\"$USERNAME\""
  [[ -n "$PASSWORD" ]] && CONFIGMAP_CMD+=" --from-literal=USER_PASSWORD=\"$PASSWORD\""
  [[ -n "$EMAIL" ]] && CONFIGMAP_CMD+=" --from-literal=EMAIL=\"$EMAIL\""
  [[ -n "$FIRST_NAME" ]] && CONFIGMAP_CMD+=" --from-literal=FIRST_NAME=\"$FIRST_NAME\""
  [[ -n "$LAST_NAME" ]] && CONFIGMAP_CMD+=" --from-literal=LAST_NAME=\"$LAST_NAME\""
  [[ -n "$GROUP_NAME" ]] && CONFIGMAP_CMD+=" --from-literal=GROUP_NAME=\"$GROUP_NAME\""

  # Add dry-run and apply options
  CONFIGMAP_CMD+=" --dry-run=client -o yaml | kubectl apply -f -"

  # Execute the command
  eval "$CONFIGMAP_CMD"
}

copySecret(){
  # Usage: ./copy_secret.sh <secret-name> <source-namespace> <target-namespace>
  SECRET_NAME=$1
  SOURCE_NAMESPACE=$2
  TARGET_NAMESPACE=$3

  if [[ -z "$SECRET_NAME" || -z "$SOURCE_NAMESPACE" || -z "$TARGET_NAMESPACE" ]]; then
    echo "Usage: $0 <secret-name> <source-namespace> <target-namespace>"
    exit 1
  fi

  # Export the secret from the source namespace
  kubectl get secret "$SECRET_NAME" -n "$SOURCE_NAMESPACE" -o yaml | \
  # Update the namespace in the YAML and apply it to the target namespace
  sed "s/namespace: $SOURCE_NAMESPACE/namespace: $TARGET_NAMESPACE/" | \
  kubectl apply -n "$TARGET_NAMESPACE" -f -
}

# createUserGroup -u skmaji -p sumit -e skm@outlook.com -f Sumit -l Maji -g hadoop -s create_ldap_user.sh
# createUserGroup -g sqa -s create_ldap_group.sh
# Function to create an LDAP user and group
createUserGroup(){

  local script_name=""
  while [[ $# -gt 0 ]]; do
    case $1 in
      -u|--username)
        local username="$2"
        shift 2
        ;;
      -p|--password)
        local password="$2"
        shift 2
        ;;
      -e|--email)
        local email="$2"
        shift 2
        ;;
      -f|--first-name)
        local first_name="$2"
        shift 2
        ;;
      -l|--last-name)
        local last_name="$2"
        shift 2
        ;;
      -g|--group-name)
        local group_name="$2"
        shift 2
        ;;
      -s|--script-name)
        script_name="$2"
        shift 2
        ;;
      *)
        echo "Unknown option: $1"
        exit 1
        ;;
    esac
  done

  if [[ -z "$script_name" ]]; then
    echo "Error: Script name is required. Use -s or --script-name to specify it."
    exit 1
  fi
  
  source $MOUNT_PATH/kubernetes/install_k8s/ldap/config/config
  
  BASE_DN=${DC}
  updateUserData "$username" "$password" "$email" "$first_name" "$last_name" "$group_name"
  copySecret ldapsecret ldap default
  updateLdapConfig "ldap://ldap.ldap.svc.cloud.uat" "$BASE_DN"

  local temp_script=$(mktemp)
  cp ${MOUNT_PATH}/kubernetes/install_k8s/ldap/${script_name} "$temp_script"

  kubectl create configmap user-script --from-file="$temp_script" -n default
  rm -f "$temp_script"
  
  # Apply the Job
  kubectl apply -f ${MOUNT_PATH}/kubernetes/install_k8s/gokutil/job.yaml

  # Wait for the Job to complete
  kubectl wait --for=condition=complete job/gokclient-runtime-job --timeout=300s

  # Get the logs from the Job
  kubectl logs job/gokclient-runtime-job -n default


  { 
    kubectl delete job gokclient-runtime-job -n default;
    kubectl delete configmap ldap-user-data -n default; 
    kubectl delete configmap user-script -n default; 
    kubectl delete configmap ldap-env-config -n default; 
    kubectl delete secret ldapsecret -n default;
  } >> /dev/null 2>&1
}

ldapReset(){
  local start_time=$(date +%s)
  
  log_component_start "ldap-reset" "Resetting LDAP directory service"
  
  log_step "1" "Checking LDAP installation status"
  local ldap_exists=false
  local namespace_exists=false
  
  # Check if LDAP helm release exists
  if helm list -n ldap 2>/dev/null | grep -q "ldap"; then
    ldap_exists=true
    log_success "LDAP helm release found"
  else
    log_info "LDAP helm release not found (may already be uninstalled)"
  fi
  
  # Check if LDAP namespace exists
  if kubectl get namespace ldap >/dev/null 2>&1; then
    namespace_exists=true
    log_success "LDAP namespace found"
  else
    log_info "LDAP namespace not found (may already be deleted)"
  fi
  
  # If nothing exists, inform user and exit
  if [[ "$ldap_exists" == false && "$namespace_exists" == false ]]; then
    log_info "LDAP appears to already be reset or was never installed"
    log_component_success "ldap-reset" "LDAP reset completed (nothing to reset)"
    return 0
  fi
  
  log_step "2" "Uninstalling LDAP helm release"
  if [[ "$ldap_exists" == true ]]; then
    if execute_with_suppression helm uninstall ldap -n ldap; then
      log_success "LDAP helm release uninstalled successfully"
      
      # Wait for pods to terminate
      log_substep "Waiting for LDAP pods to terminate..."
      local wait_count=0
      while kubectl get pods -n ldap 2>/dev/null | grep -q "ldap" && [[ $wait_count -lt 30 ]]; do
        printf "."
        sleep 2
        wait_count=$((wait_count + 1))
      done
      printf "\n"
      
      if [[ $wait_count -lt 30 ]]; then
        log_success "LDAP pods terminated successfully"
      else
        log_warning "LDAP pods taking longer than expected to terminate"
      fi
    else
      log_error "Failed to uninstall LDAP helm release"
      if is_verbose_mode; then
        log_info "Run with --verbose for detailed error information"
      fi
      return 1
    fi
  else
    log_info "Skipping helm uninstall (release not found)"
  fi
  
  log_step "3" "Removing LDAP namespace and resources"
  if [[ "$namespace_exists" == true ]]; then
    if execute_with_suppression kubectl delete namespace ldap --timeout=120s; then
      log_success "LDAP namespace deleted successfully"
    else
      log_warning "LDAP namespace deletion had issues but may still complete"
      
      # Try to force delete if needed
      log_substep "Attempting force cleanup..."
      if execute_with_suppression kubectl delete namespace ldap --force --grace-period=0 2>/dev/null; then
        log_success "LDAP namespace force-deleted successfully"
      else
        log_warning "Force deletion also had issues - manual cleanup may be needed"
      fi
    fi
  else
    log_info "Skipping namespace deletion (namespace not found)"
  fi
  
  log_step "4" "Cleaning up LDAP-related resources"
  
  # Clean up any remaining LDAP-related ingress resources
  local ingress_cleaned=false
  if kubectl get ingress -A 2>/dev/null | grep -q "ldap"; then
    log_substep "Removing LDAP ingress resources..."
    if execute_with_suppression kubectl delete ingress -l app=ldap --all-namespaces 2>/dev/null; then
      ingress_cleaned=true
      log_success "LDAP ingress resources cleaned up"
    else
      log_warning "Some LDAP ingress resources may remain"
    fi
  fi
  
  # Clean up any remaining LDAP-related secrets
  local secrets_cleaned=false
  if kubectl get secrets -A 2>/dev/null | grep -q "ldap"; then
    log_substep "Removing LDAP-related secrets..."
    if execute_with_suppression kubectl delete secrets -l app=ldap --all-namespaces 2>/dev/null; then
      secrets_cleaned=true
      log_success "LDAP secrets cleaned up"
    else
      log_warning "Some LDAP secrets may remain"
    fi
  fi
  
  # Clean up any remaining LDAP-related persistent volumes
  local pv_cleaned=false
  if kubectl get pv 2>/dev/null | grep -q "ldap"; then
    log_substep "Removing LDAP persistent volumes..."
    if execute_with_suppression kubectl delete pv -l app=ldap 2>/dev/null; then
      pv_cleaned=true
      log_success "LDAP persistent volumes cleaned up"
    else
      log_warning "Some LDAP persistent volumes may remain"
    fi
  fi
  
  if [[ "$ingress_cleaned" == false && "$secrets_cleaned" == false && "$pv_cleaned" == false ]]; then
    log_info "No additional LDAP resources found to clean up"
  fi
  
  log_step "5" "Validating LDAP reset completion"
  validate_ldap_reset
  
  local end_time=$(date +%s)
  local duration=$((end_time - start_time))
  
  log_component_success "ldap-reset" "LDAP directory service reset completed"
  log_success "LDAP reset completed in ${duration}s"
  
  # Show reset summary and next steps
  show_ldap_reset_summary
}

# Validate that LDAP reset was successful
validate_ldap_reset() {
  local validation_success=true
  
  # Check that helm release is gone
  if helm list -n ldap 2>/dev/null | grep -q "ldap"; then
    log_error "LDAP helm release still exists"
    validation_success=false
  else
    log_success "LDAP helm release successfully removed"
  fi
  
  # Check that namespace is gone
  if kubectl get namespace ldap >/dev/null 2>&1; then
    log_warning "LDAP namespace still exists (may be terminating)"
  else
    log_success "LDAP namespace successfully removed"
  fi
  
  # Check that pods are gone
  if kubectl get pods -n ldap 2>/dev/null | grep -q "ldap"; then
    log_warning "Some LDAP pods may still be terminating"
  else
    log_success "All LDAP pods removed"
  fi
  
  # Check that services are gone
  if kubectl get services -n ldap 2>/dev/null | grep -q "ldap"; then
    log_warning "Some LDAP services may still exist"
  else
    log_success "All LDAP services removed"
  fi
  
  return $([[ "$validation_success" == true ]] && echo 0 || echo 1)
}

# Show LDAP reset summary and recommendations
show_ldap_reset_summary() {
  echo
  echo -e "${COLOR_BRIGHT_BLUE}${COLOR_BOLD}ðŸ”„ LDAP Reset Summary${COLOR_RESET}"
  echo -e "${COLOR_CYAN}â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”${COLOR_RESET}"
  
  echo -e "${COLOR_YELLOW}${COLOR_BOLD}Reset Actions Completed:${COLOR_RESET}"
  echo -e "  âœ… ${COLOR_GREEN}LDAP helm release uninstalled${COLOR_RESET}"
  echo -e "  âœ… ${COLOR_GREEN}LDAP namespace removed${COLOR_RESET}"
  echo -e "  âœ… ${COLOR_GREEN}LDAP pods and services cleaned up${COLOR_RESET}"
  echo -e "  âœ… ${COLOR_GREEN}LDAP ingress resources removed${COLOR_RESET}"
  echo -e "  âœ… ${COLOR_GREEN}LDAP secrets and certificates cleaned up${COLOR_RESET}"
  echo -e "  âœ… ${COLOR_GREEN}LDAP persistent volumes removed${COLOR_RESET}"
  echo
  
  echo -e "${COLOR_YELLOW}${COLOR_BOLD}Impact of LDAP Reset:${COLOR_RESET}"
  echo -e "  ðŸ—‚ï¸  ${COLOR_RED}All LDAP directory data removed${COLOR_RESET}"
  echo -e "  ðŸ‘¥ ${COLOR_RED}User accounts and groups deleted${COLOR_RESET}"
  echo -e "  ðŸ” ${COLOR_RED}LDAP authentication unavailable${COLOR_RESET}"
  echo -e "  ðŸŒ ${COLOR_RED}phpLDAPadmin web interface removed${COLOR_RESET}"
  echo -e "  ðŸ”— ${COLOR_YELLOW}Dependent services may need reconfiguration${COLOR_RESET}"
  echo
  
  echo -e "${COLOR_BRIGHT_YELLOW}${COLOR_BOLD}âš ï¸  Post-Reset Considerations:${COLOR_RESET}"
  echo -e "${COLOR_CYAN}â€¢ Services depending on LDAP authentication may fail${COLOR_RESET}"
  echo -e "${COLOR_CYAN}â€¢ Keycloak user federation will need reconfiguration if LDAP is reinstalled${COLOR_RESET}"
  echo -e "${COLOR_CYAN}â€¢ OAuth2 proxy configurations may need updates${COLOR_RESET}"
  echo -e "${COLOR_CYAN}â€¢ Any custom applications using LDAP will need alternative auth${COLOR_RESET}"
  echo
  
  echo -e "${COLOR_BRIGHT_GREEN}${COLOR_BOLD}ðŸš€ Next Steps:${COLOR_RESET}"
  echo -e "${COLOR_GREEN}â€¢ To reinstall LDAP: ${COLOR_BOLD}gok install ldap${COLOR_RESET}"
  echo -e "${COLOR_GREEN}â€¢ To check system status: ${COLOR_BOLD}gok status${COLOR_RESET}"
  echo -e "${COLOR_GREEN}â€¢ To reset other auth components: ${COLOR_BOLD}gok reset keycloak${COLOR_RESET}"
  echo -e "${COLOR_GREEN}â€¢ To see available components: ${COLOR_BOLD}gok help reset${COLOR_RESET}"
  echo
}

kcAdminPwd(){
  kubectl get secret keycloak -n keycloak -o jsonpath="{.data.admin-password}" | base64 --decode
}

argocdAdminPwd(){
  kubectl get secret argocd-secret -n argocd -o jsonpath='{.data.admin\.password}' | base64 -d
}

installPrometheusGrafanaWithCertMgr(){

  createLocalStorageClassAndPV "prometheus-storage" "prometheus-pv" "/data/volumes/pv1"
  createLocalStorageClassAndPV "alertmanager-storage" "alertmanager-pv" "/data/volumes/pv2"
  prometheusGrafanaInstv2

  # No need to generate the certificate.
  # The certificate and secret will be directory issued the ingress annotation cert-manager.io/cluster-issuer
  #gok create certificate monitoring kube
  #No need for prometheus to be externalized, removing the ingress as well.
  gok patch ingress prometheus-server monitoring letsencrypt $(defaultSubdomain)
  gok patch ingress grafana monitoring letsencrypt $(defaultSubdomain)

  echo "Waiting for services to be up!!!!"
  kubectl --timeout=120s wait --for=condition=Ready pods --all --namespace monitoring
  [[ $? -eq 0 ]] && echoSuccess "Prometheus and Grafana services are now up!\nAccess it using \nprometheus: https://$(defaultSubdomain).$(rootDomain)/prometheus\ngrafana: https://$(defaultSubdomain).$(rootDomain)/grafana" || echoFailed "Keycloak services timed-out, plaese check!!"
}

patchDashboardWithOauth(){
  patchOauth2Secure kubernetes-dashboard kubernetes-dashboard https://kube.gokcloud.com/dashboard/
}

patchJupyterWithOauth() {
  patchOauth2Secure jupyterhub jupyterhub https://$(defaultSubdomain).$(rootDomain)/hub/
}

patchTtydWithOauth() {
  patchOauth2Secure ttyd ttyd https://ttyd.$(rootDomain)
}

patchCloudshellWithOauth() {
  patchOauth2Secure cloudshell-cloudshell cloudshell https://$(defaultSubdomain).$(rootDomain)
}

patchControllerWithOauth() {
  patchOauth2Secure gok-controller gok-controller https://controller.$(rootDomain)
}

checkAndPatchDashboard() {
  # Check if the oauth2-proxy service is running
  echo "Checking if oauth2-proxy service is running..."
  if kubectl get pods -n oauth2 -l app.kubernetes.io/name=oauth2-proxy 2>/dev/null | grep -q "Running"; then
    echo "oauth2-proxy service is running. Proceeding to patch the dashboard..."
    patchDashboardWithOauth
  else
    echoFailed "oauth2-proxy service is not running. Please ensure it is running before patching the dashboard."
  fi
}

checkAndPatchHub() {
  # Check if the oauth2-proxy service is running
  echo "Checking if oauth2-proxy service is running..."
  if kubectl get pods -n oauth2 -l app.kubernetes.io/name=oauth2-proxy 2>/dev/null | grep -q "Running"; then
    echo "oauth2-proxy service is running. Proceeding to patch JupyterHub..."
    patchJupyterWithOauth
  else
    echo "oauth2-proxy service is not running. Please ensure it is running before patching JupyterHub."
  fi
}

cloudshellReset() {
  helm uninstall cloudshell -n cloudshell
  kubectl delete ns cloudshell
  echo "cloudshell has been uninstalled and the namespace deleted."
}


gokLoginInst() {
  pushd ${MOUNT_PATH}/kubernetes/install_k8s/gok-login
  chmod +x build.sh tag_push.sh
  ./build.sh
  ./tag_push.sh
  popd

  # Create namespace for gok-login
  kubectl create namespace gok-login || echo "Namespace gok-login already exists"

  # # Copy image pull secret if needed
  # copySecret regcred kube-system gok-login

  kubectl create configmap ca-cert --from-file=issuer.crt=/usr/local/share/ca-certificates/issuer.crt -n gok-login

  # Install gok-login using Helm chart (adjust path if needed)
  helm install gok-login ${MOUNT_PATH}/kubernetes/install_k8s/gok-login/chart -n gok-login \
  --set oidc.clientSecret="$(dataFromSecret oauth-secrets kube-system OIDC_CLIENT_SECRET)"

  # Patch ingress with letsencrypt and oauth if needed
  gok patch ingress gok-login-gok-login gok-login letsencrypt gok-login
  # No need for oauth integrtion with ingress as user will provide username and password
  # patchOauth2Secure gok-login-gok-login gok-login https://gok-login.$(rootDomain)

  echo "Waiting for gok-login to be ready..."
  kubectl --namespace gok-login wait --for=condition=Ready pods --all --timeout=120s
  if [[ $? -eq 0 ]]; then
    echoSuccess "gok-login is now up! Access it at: https://gok-login.$(rootDomain)/"
  else
    echoFailed "gok-login setup timed out. Please check the logs."
    return 1
  fi
}

cloudshellInst() {
  pushd ${MOUNT_PATH}/kubernetes/install_k8s/cloud-shell/gok
  ./build.sh
  ./tag_push.sh
  popd

  # Create namespace for cloudshell
  kubectl create namespace cloudshell || echo "Namespace cloudshell already exists"

  copySecret regcred kube-system cloudshell

  kubectl create configmap ca-cert --from-file=issuer.crt=/usr/local/share/ca-certificates/issuer.crt -n cloudshell

  helm install cloudshell ${MOUNT_PATH}/kubernetes/install_k8s/cloud-shell/gok/chart -n cloudshell

  # Patch ingress with letsencrypt and oauth if needed
  gok patch ingress cloudshell-cloudshell cloudshell letsencrypt $(defaultSubdomain)
  patchCloudshellWithOauth

  echo "Waiting for cloudshell to be ready..."
  kubectl --namespace cloudshell wait --for=condition=Ready pods --all --timeout=120s
  if [[ $? -eq 0 ]]; then
    echoSuccess "cloudshell is now up! Access it at: https://$(defaultSubdomain).$(rootDomain)/cloudshell/home"
  else
    echoFailed "cloudshell setup timed out. Please check the logs."
    return 1
  fi
}

consoleReset() {
  helm uninstall console -n console
  kubectl delete ns console
  echo "console has been uninstalled and the namespace deleted."
}

patchConsoleWithOauth() {
  patchOauth2Secure console-console console https://console.$(rootDomain)/
}

consoleInst() {
  pushd ${MOUNT_PATH}/kubernetes/install_k8s/console/app
  ./build.sh
  ./tag_push.sh
  popd


  # Create namespace for console
  kubectl create namespace console || echo "Namespace console already exists"

  # Copy image pull secret if needed
  copySecret regcred kube-system console

  kubectl create configmap ca-cert --from-file=issuer.crt=/usr/local/share/ca-certificates/issuer.crt -n console

  # Install console using Helm chart (adjust path if needed)
  helm install console ${MOUNT_PATH}/kubernetes/install_k8s/console/app/chart -n console

  # Patch ingress with letsencrypt and oauth if needed
  gok patch ingress console-console console letsencrypt console
  # Optionally patch with oauth if you use oauth2-proxy or similar
  patchConsoleWithOauth

  echo "Waiting for console to be ready..."
  kubectl --namespace console wait --for=condition=Ready pods --all --timeout=120s
  if [[ $? -eq 0 ]]; then
    echoSuccess "console is now up! Access it at: https://console.$(rootDomain)/"
  else
    echoFailed "console setup timed out. Please check the logs."
    return 1
  fi
}

installDashboardwithCertManager(){
  dashboardInst
  # No need to generate the certificate.
  # The certificate and secret will be directory issued the ingress annotation cert-manager.io/cluster-issuer
  #gok create certificate kubernetes-dashboard kube
  gok patch ingress kubernetes-dashboard kubernetes-dashboard letsencrypt $(defaultSubdomain)
  checkAndPatchDashboard
  echo "Waiting for services to be up!!!!"
  kubectl --timeout=120s wait --for=condition=Ready pods --all --namespace kubernetes-dashboard
  [[ $? -eq 0 ]] && echoSuccess "Dashboard services are now up!\nAccess it using https://$(defaultSubdomain).$(rootDomain)/dashboard/" || echoFailed "Keycloak services timed-out, plaese check!!"
}

# add_audience_mapper_to_groups_scope \
#   "https://keycloak.gokcloud.com" \
#   "GokDevelopers" \
#   "admin" \
#   "your-admin-password" \
#   "gok-developers-client"

add_audience_mapper_to_groups_scope() {
  # Required inputs
  local KEYCLOAK_URL=$1         # e.g. https://keycloak.gokcloud.com
  local REALM=$2                # e.g. GokDevelopers
  local ADMIN_USER=$3           # e.g. admin
  local ADMIN_PASS=$4           # e.g. <admin-password>
  local CLIENT_ID=$5            # e.g. gok-developers-client

  # Get admin access token
  local ADMIN_TOKEN=$(curl -s -X POST "$KEYCLOAK_URL/realms/master/protocol/openid-connect/token" \
    -d "username=$ADMIN_USER" \
    -d "password=$ADMIN_PASS" \
    -d "grant_type=password" \
    -d "client_id=admin-cli" \
    | jq -r .access_token)

  if [[ -z "$ADMIN_TOKEN" || "$ADMIN_TOKEN" == "null" ]]; then
    echo "Failed to get admin token"
    return 1
  fi

  # Get client UUID
  local CLIENT_UUID=$(curl -s -X GET "$KEYCLOAK_URL/admin/realms/$REALM/clients" \
    -H "Authorization: Bearer $ADMIN_TOKEN" \
    | jq -r ".[] | select(.clientId==\"$CLIENT_ID\") | .id")

  if [[ -z "$CLIENT_UUID" ]]; then
    echo "Client $CLIENT_ID not found"
    return 1
  fi

  # Add Audience protocol mapper to client
  curl -s -X POST "$KEYCLOAK_URL/admin/realms/$REALM/clients/$CLIENT_UUID/protocol-mappers/models" \
    -H "Authorization: Bearer $ADMIN_TOKEN" \
    -H "Content-Type: application/json" \
    -d "{
      \"name\": \"audience-groups\",
      \"protocol\": \"openid-connect\",
      \"protocolMapper\": \"oidc-audience-mapper\",
      \"consentRequired\": false,
      \"config\": {
        \"included.client.audience\": \"$CLIENT_ID\",
        \"id.token.claim\": \"true\",
        \"access.token.claim\": \"true\"
      }
    }"

  echo "Audience protocol mapper added to client $CLIENT_ID."

  # (Optional) Assign groups scope to client if not already assigned
  # Get groups client scope ID
  local GROUPS_SCOPE_ID=$(curl -s -X GET "$KEYCLOAK_URL/admin/realms/$REALM/client-scopes" \
    -H "Authorization: Bearer $ADMIN_TOKEN" \
    | jq -r '.[] | select(.name=="groups") | .id')

  if [[ -n "$GROUPS_SCOPE_ID" ]]; then
    # Assign as default client scope if not already
    curl -s -X PUT "$KEYCLOAK_URL/admin/realms/$REALM/clients/$CLIENT_UUID/default-client-scopes/$GROUPS_SCOPE_ID" \
      -H "Authorization: Bearer $ADMIN_TOKEN"
    echo "Assigned 'groups' scope to client $CLIENT_ID."
  else
    echo "No 'groups' client scope found, skipping scope assignment."
  fi
}

generate_kubeconfig() {
  read -p "Enter your Kubernetes API server URL (e.g., https://kubernetes.default.svc): " KUBE_API
  read -p "Enter your Kubernetes namespace: " KUBE_NAMESPACE
  read -p "Enter your Bearer token: " KUBE_TOKEN

  # Optional: set a context name
  CONTEXT_NAME="user-ttyd"

  cat > kubeconfig <<EOF
apiVersion: v1
kind: Config
clusters:
- cluster:
    server: ${KUBE_API}
    insecure-skip-tls-verify: true
  name: cluster
users:
- name: user
  user:
    token: ${KUBE_TOKEN}
contexts:
- context:
    cluster: cluster
    user: user
    namespace: ${KUBE_NAMESPACE}
  name: ${CONTEXT_NAME}
current-context: ${CONTEXT_NAME}
EOF

  echo "Kubeconfig written to ./kubeconfig"
  echo "To use it, run:"
  echo "  export KUBECONFIG=$(pwd)/kubeconfig"
}

get_keycloak_token() {
  read -p "Keycloak URL (default: https://keycloak.gokcloud.com/realms/GokDevelopers/protocol/openid-connect/token): " KEYCLOAK_URL
  KEYCLOAK_URL=${KEYCLOAK_URL:-https://keycloak.gokcloud.com/realms/GokDevelopers/protocol/openid-connect/token}

  read -p "Client ID (default: gok-developers-client): " CLIENT_ID
  CLIENT_ID=${CLIENT_ID:-gok-developers-client}
  read -p "Client Secret: " CLIENT_SECRET
  read -p "Username: " USERNAME
  read -s -p "Password: " PASSWORD
  echo

  TOKEN_RESPONSE=$(curl -s -X POST "$KEYCLOAK_URL" \
    -d "client_id=$CLIENT_ID" \
    -d "client_secret=$CLIENT_SECRET" \
    -d "grant_type=password" \
    -d "username=$USERNAME" \
    -d "password=$PASSWORD")

  ACCESS_TOKEN=$(echo "$TOKEN_RESPONSE" | grep -o '"access_token":"[^"]*' | grep -o '[^"]*$')
  if [ -n "$ACCESS_TOKEN" ]; then
    echo "Your access token:"
    echo "$ACCESS_TOKEN"
  else
    echo "Failed to get token. Response:"
    echo "$TOKEN_RESPONSE"
  fi
}

oauth2ProxyReset(){
  local start_time=$(date +%s)
  
  log_component_start "oauth2-proxy-reset" "Resetting OAuth2 Proxy authentication system"
  
  log_step "1" "Checking OAuth2 Proxy installation status"
  local oauth2_exists=false
  local namespace_exists=false
  
  # Check if OAuth2 helm release exists (try both possible names)
  if helm list -n oauth2 2>/dev/null | grep -q "oauth2proxy\|oauth2-proxy"; then
    oauth2_exists=true
    log_success "OAuth2 Proxy helm release found"
  else
    log_info "OAuth2 Proxy helm release not found (may already be uninstalled)"
  fi
  
  # Check if OAuth2 namespace exists
  if kubectl get namespace oauth2 >/dev/null 2>&1; then
    namespace_exists=true
    log_success "OAuth2 Proxy namespace found"
  else
    log_info "OAuth2 Proxy namespace not found (may already be deleted)"
  fi
  
  # If nothing exists, inform user and exit
  if [[ "$oauth2_exists" == false && "$namespace_exists" == false ]]; then
    log_info "OAuth2 Proxy appears to already be reset or was never installed"
    log_component_success "oauth2-proxy-reset" "OAuth2 Proxy reset completed (nothing to reset)"
    return 0
  fi
  
  log_step "2" "Uninstalling OAuth2 Proxy helm release"
  if [[ "$oauth2_exists" == true ]]; then
    # Try both possible release names
    local release_name=""
    if helm list -n oauth2 2>/dev/null | grep -q "oauth2proxy"; then
      release_name="oauth2proxy"
    fi
    
    if [[ -n "$release_name" ]]; then
      if execute_with_suppression helm uninstall "$release_name" -n oauth2; then
        log_success "OAuth2 Proxy helm release ($release_name) uninstalled successfully"
        
        # Wait for pods to terminate
        log_substep "Waiting for OAuth2 Proxy pods to terminate..."
        local wait_count=0
        while kubectl get pods -n oauth2 2>/dev/null | grep -q "oauth2" && [[ $wait_count -lt 30 ]]; do
          printf "."
          sleep 2
          wait_count=$((wait_count + 1))
        done
        printf "\n"
        
        if [[ $wait_count -lt 30 ]]; then
          log_success "OAuth2 Proxy pods terminated successfully"
        else
          log_warning "OAuth2 Proxy pods taking longer than expected to terminate"
        fi
      else
        log_error "Failed to uninstall OAuth2 Proxy helm release"
        if is_verbose_mode; then
          log_info "Run with --verbose for detailed error information"
        fi
        return 1
      fi
    else
      log_warning "Could not determine OAuth2 Proxy release name"
    fi
  else
    log_info "Skipping helm uninstall (release not found)"
  fi
  
  log_step "3" "Cleaning up persistent volumes and storage"
  if [[ "$namespace_exists" == true ]]; then
    # Clean up local storage
    log_substep "Cleaning up local filesystem storage..."
    if emptyLocalFsStorage "OAuth2" "oauth-pv" "oauth-storage" "/data/volumes/pv6" "oauth2"; then
      log_success "Local filesystem storage cleaned up"
    else
      log_warning "Local storage cleanup had issues but continuing"
    fi
  else
    log_info "Skipping storage cleanup (namespace not found)"
  fi
  
  log_step "4" "Removing OAuth2 Proxy namespace and resources"
  if [[ "$namespace_exists" == true ]]; then
    if execute_with_suppression kubectl delete namespace oauth2 --timeout=120s; then
      log_success "OAuth2 Proxy namespace deleted successfully"
    else
      log_warning "OAuth2 Proxy namespace deletion had issues but may still complete"
      
      # Try to force delete if needed
      log_substep "Attempting force cleanup..."
      if execute_with_suppression kubectl delete namespace oauth2 --force --grace-period=0 2>/dev/null; then
        log_success "OAuth2 Proxy namespace force-deleted successfully"
      else
        log_warning "Force deletion also had issues - manual cleanup may be needed"
      fi
    fi
  else
    log_info "Skipping namespace deletion (namespace not found)"
  fi
  
  log_step "5" "Cleaning up OAuth2-related resources"
  
  # Clean up any remaining OAuth2-related ingress resources
  local ingress_cleaned=false
  if kubectl get ingress -A 2>/dev/null | grep -q "oauth2"; then
    log_substep "Removing OAuth2 ingress resources..."
    if execute_with_suppression kubectl delete ingress -l app.kubernetes.io/name=oauth2-proxy --all-namespaces 2>/dev/null; then
      ingress_cleaned=true
      log_success "OAuth2 ingress resources cleaned up"
    else
      log_warning "Some OAuth2 ingress resources may remain"
    fi
  fi
  
  # Clean up any remaining OAuth2-related certificates
  local certs_cleaned=false
  if kubectl get certificates -A 2>/dev/null | grep -q "oauth2"; then
    log_substep "Removing OAuth2 TLS certificates..."
    if execute_with_suppression kubectl delete certificates -l app.kubernetes.io/name=oauth2-proxy --all-namespaces 2>/dev/null; then
      certs_cleaned=true
      log_success "OAuth2 certificates cleaned up"
    else
      log_warning "Some OAuth2 certificates may remain"
    fi
  fi
  
  # Clean up OAuth2-related storage classes and persistent volumes
  local storage_cleaned=false
  if kubectl get storageclass oauth-storage >/dev/null 2>&1; then
    log_substep "Removing OAuth2 storage resources..."
    if execute_with_suppression kubectl delete storageclass oauth-storage 2>/dev/null; then
      storage_cleaned=true
      log_success "OAuth2 storage class cleaned up"
    fi
  fi
  
  if kubectl get pv oauth-pv >/dev/null 2>&1; then
    if execute_with_suppression kubectl delete pv oauth-pv 2>/dev/null; then
      storage_cleaned=true
      log_success "OAuth2 persistent volume cleaned up"
    fi
  fi
  
  if [[ "$ingress_cleaned" == false && "$certs_cleaned" == false && "$storage_cleaned" == false ]]; then
    log_info "No additional OAuth2 resources found to clean up"
  fi
  
  log_step "6" "Validating OAuth2 Proxy reset completion"
  validate_oauth2_reset
  
  local end_time=$(date +%s)
  local duration=$((end_time - start_time))
  
  log_component_success "oauth2-proxy-reset" "OAuth2 Proxy authentication system reset completed"
  log_success "OAuth2 Proxy reset completed in ${duration}s"
  
  # Show reset summary and next steps
  show_oauth2_reset_summary
}

# Validate that OAuth2 Proxy reset was successful
validate_oauth2_reset() {
  local validation_success=true
  
  # Check that helm release is gone
  if helm list -n oauth2 2>/dev/null | grep -q "oauth2"; then
    log_error "OAuth2 Proxy helm release still exists"
    validation_success=false
  else
    log_success "OAuth2 Proxy helm release successfully removed"
  fi
  
  # Check that namespace is gone
  if kubectl get namespace oauth2 >/dev/null 2>&1; then
    log_warning "OAuth2 Proxy namespace still exists (may be terminating)"
  else
    log_success "OAuth2 Proxy namespace successfully removed"
  fi
  
  # Check that pods are gone
  if kubectl get pods -n oauth2 2>/dev/null | grep -q "oauth2"; then
    log_warning "Some OAuth2 Proxy pods may still be terminating"
  else
    log_success "All OAuth2 Proxy pods removed"
  fi
  
  # Check that services are gone
  if kubectl get services -n oauth2 2>/dev/null | grep -q "oauth2"; then
    log_warning "Some OAuth2 Proxy services may still exist"
  else
    log_success "All OAuth2 Proxy services removed"
  fi
  
  # Check that storage is cleaned up
  if kubectl get storageclass oauth-storage >/dev/null 2>&1; then
    log_warning "OAuth2 storage class still exists"
  else
    log_success "OAuth2 storage resources removed"
  fi
  
  return $([[ "$validation_success" == true ]] && echo 0 || echo 1)
}

# Show OAuth2 Proxy reset summary and recommendations
show_oauth2_reset_summary() {
  echo
  echo -e "${COLOR_BRIGHT_BLUE}${COLOR_BOLD}ðŸ”„ OAuth2 Proxy Reset Summary${COLOR_RESET}"
  echo -e "${COLOR_CYAN}â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”${COLOR_RESET}"
  
  echo -e "${COLOR_YELLOW}${COLOR_BOLD}Reset Actions Completed:${COLOR_RESET}"
  echo -e "  âœ… ${COLOR_GREEN}OAuth2 Proxy helm release uninstalled${COLOR_RESET}"
  echo -e "  âœ… ${COLOR_GREEN}OAuth2 Proxy namespace and pods cleaned up${COLOR_RESET}"
  echo -e "  âœ… ${COLOR_GREEN}Persistent volumes and storage cleared${COLOR_RESET}"
  echo -e "  âœ… ${COLOR_GREEN}OAuth2 ingress and certificates removed${COLOR_RESET}"
  echo -e "  âœ… ${COLOR_GREEN}Authentication proxy configuration reset${COLOR_RESET}"
  echo -e "  âœ… ${COLOR_GREEN}Storage classes and volumes cleaned up${COLOR_RESET}"
  echo
  
  echo -e "${COLOR_YELLOW}${COLOR_BOLD}Impact of OAuth2 Proxy Reset:${COLOR_RESET}"
  echo -e "  ðŸ›¡ï¸  ${COLOR_RED}HTTP authentication proxy removed${COLOR_RESET}"
  echo -e "  ðŸª ${COLOR_RED}User sessions and authentication cookies invalidated${COLOR_RESET}"
  echo -e "  ðŸ” ${COLOR_RED}Protected services no longer secured by OAuth2${COLOR_RESET}"
  echo -e "  ðŸŒ ${COLOR_RED}OAuth2 proxy endpoints and callbacks unavailable${COLOR_RESET}"
  echo -e "  ðŸ”— ${COLOR_YELLOW}Applications relying on OAuth2 proxy will be unprotected${COLOR_RESET}"
  echo
  
  echo -e "${COLOR_BRIGHT_YELLOW}${COLOR_BOLD}âš ï¸  Post-Reset Considerations:${COLOR_RESET}"
  echo -e "${COLOR_CYAN}â€¢ Services protected by OAuth2 proxy are now publicly accessible${COLOR_RESET}"
  echo -e "${COLOR_CYAN}â€¢ Dashboards and admin interfaces may need alternative protection${COLOR_RESET}"
  echo -e "${COLOR_CYAN}â€¢ Session cookies for authenticated users are now invalid${COLOR_RESET}"
  echo -e "${COLOR_CYAN}â€¢ Applications may need reconfiguration for direct Keycloak integration${COLOR_RESET}"
  echo
  
  echo -e "${COLOR_BRIGHT_GREEN}${COLOR_BOLD}ðŸš€ Next Steps:${COLOR_RESET}"
  echo -e "${COLOR_GREEN}â€¢ To reinstall OAuth2 Proxy: ${COLOR_BOLD}gok install oauth2${COLOR_RESET}"
  echo -e "${COLOR_GREEN}â€¢ To check protected services: ${COLOR_BOLD}gok status${COLOR_RESET}"
  echo -e "${COLOR_GREEN}â€¢ To install next component: ${COLOR_BOLD}gok install rabbitmq${COLOR_RESET}"
  echo -e "${COLOR_GREEN}â€¢ To see available components: ${COLOR_BOLD}gok help reset${COLOR_RESET}"
  echo
}

#export NODE_TLS_REJECT_UNAUTHORIZED=0
jupyterHubInst() {
  # Add the Helm repository for JupyterHub
  helm repo add jupyterhub https://jupyterhub.github.io/helm-chart/
  helm repo update

  # Create the storage class for JupyterHub using the generic method
  createLocalStorageClassAndPV "jupyter-storage" "jupyter-pv" "/data/volumes/jupyter"
  createLocalStorageClassAndPV "jupyter-user-storage" "jupyter-user-pv" "/data/volumes/jupyter-user"
  # Create a namespace for JupyterHub
  kubectl create namespace jupyterhub || echo "Namespace jupyterhub already exists"

  # Generate a random secret token for JupyterHub
  JUPYTERHUB_SECRET=$(openssl rand -hex 32)

  # Create a Kubernetes secret for the JupyterHub proxy
  kubectl create secret generic hub-secret \
    --from-literal=hub-secret-key="${JUPYTERHUB_SECRET}" \
    --namespace jupyterhub || echo "Secret hub-secret already exists"

  CLIENT_SECRET=$(dataFromSecret oauth-secrets kube-system OIDC_CLIENT_SECRET)
  CLIENT_ID=$(dataFromSecret oauth-secrets kube-system OIDC_CLIENT_ID)
  keycloakUrl="https://$(fullKeycloakUrl)"
  REALM=$(dataFromSecret oauth-secrets kube-system OAUTH_REALM)
  OAUTH2_HOST="$(fullKeycloakUrl)"
  ENABLE_OAUTH2="true"

  # Install JupyterHub using Helm with the provided values.yaml
  helm install jupyterhub jupyterhub/jupyterhub \
    --namespace jupyterhub \
    --set proxy.secretToken="${JUPYTERHUB_SECRET}" \
    --values "${MOUNT_PATH}/kubernetes/install_k8s/jupyter/values.yaml" \
    $(if [[ "$ENABLE_OAUTH2" == "true" ]]; then
      echo "--set hub.config.GenericOAuthenticator.client_id=${CLIENT_ID} \
      --set hub.config.GenericOAuthenticator.client_secret=${CLIENT_SECRET} \
      --set hub.config.GenericOAuthenticator.oauth_callback_url=https://$(jupyterHubSubdomain).$(rootDomain)/oauth_callback \
      --set hub.config.GenericOAuthenticator.authorize_url=https://${OAUTH2_HOST}/realms/${REALM}/protocol/openid-connect/auth \
      --set hub.config.GenericOAuthenticator.token_url=https://${OAUTH2_HOST}/realms/${REALM}/protocol/openid-connect/token \
      --set hub.config.GenericOAuthenticator.userdata_url=https://${OAUTH2_HOST}/realms/${REALM}/protocol/openid-connect/userinfo \
      --set hub.config.GenericOAuthenticator.login_service=keycloak \
      --set hub.config.GenericOAuthenticator.username_claim=preferred_username \
      --set hub.config.GenericOAuthenticator.userdata_params.state=state \
      --set hub.config.GenericOAuthenticator.allow_all=true \
      --set hub.config.GenericOAuthenticator.tls_verify=false \
      --set hub.config.GenericOAuthenticator.admin_users[0]=admin \
      --set hub.config.GenericOAuthenticator.admin_users[1]=skmaji1 \
      --set hub.config.JupyterHub.authenticator_class=generic-oauth"
    fi)

  # Wait for the JupyterHub pods to be ready
  echo "Waiting for JupyterHub services to be up..."
  kubectl --namespace jupyterhub wait --for=condition=Ready pods --all --timeout=300s

  gok patch ingress jupyterhub jupyterhub letsencrypt $(jupyterHubSubdomain)
  # checkAndPatchHub

  if [[ $? -eq 0 ]]; then
    echoSuccess "JupyterHub services are now up!"
    echoSuccess "Access it using https://master.cloud.com"
  else
    echoFailed "JupyterHub services timed out. Please check the logs!"
  fi
}

jupyterHubReset() {
  helm uninstall jupyterhub -n jupyterhub
  kubectl delete secret hub-secret -n jupyterhub
  kubectl delete pod --all -n jupyterhub
  kubectl delete pvc --all -n jupyterhub
  emptyLocalFsStorage "JupyterHub" "jupyter-pv" "jupyter-storage" "/data/volumes/jupyter"
  emptyLocalFsStorage "JupyterHub User" "jupyter-user-pv" "jupyter-user-storage" "/data/volumes/jupyter-user"
  kubectl delete ns jupyterhub
}

installKubectlClient(){
  # Install kubectl client
  if ! command -v kubectl &> /dev/null; then
    echo "kubectl not found, installing..."
    curl -LO "https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl"
    chmod +x kubectl
    sudo mv kubectl /usr/local/bin/
    echoSuccess "kubectl installed successfully!"
  else
    echo "kubectl is already installed."
  fi
}   

installDocker(){
  # Install Docker
  if ! command -v docker &> /dev/null; then
    echo "Docker not found, installing..."
    sudo apt-get update
    sudo apt-get install -y docker.io
    sudo systemctl start docker
    sudo systemctl enable docker
    echoSuccess "Docker installed successfully!"
  else
    echo "Docker is already installed."
  fi

  # Add current user to the docker group
  sudo usermod -aG docker $USER
  echoSuccess "Current user added to the docker group. Please log out and log back in for changes to take effect."
}

# Method to remove claimRef from a specific Persistent Volume
# removeClaimRefFromPV "eclipse-che-pv"
removeClaimRefFromPV() {
  local pv_name=$1

  if [ -z "$pv_name" ]; then
    echo "Error: No PV name provided."
    return 1
  fi

  # Check if the PV exists and is in the "Released" state
  pv_state=$(kubectl get pv "$pv_name" --no-headers | awk '{print $5}')
  if [ "$pv_state" != "Released" ]; then
    echo "Error: PV '$pv_name' is not in the 'Released' state. Current state: $pv_state"
    return 1
  fi

  echo "Processing PV: $pv_name"

  # Patch the PV to remove the claimRef
  kubectl patch pv "$pv_name" --type=json -p='[{"op": "remove", "path": "/spec/claimRef"}]'

  if [ $? -eq 0 ]; then
    echo "Successfully removed claimRef from PV: $pv_name"
    return 0
  else
    echo "Failed to remove claimRef from PV: $pv_name"
    return 1
  fi
}


eclipseCheInst(){

  # Fetch OIDC client secret and other required parameters from the secret
  CLIENT_SECRET=$(dataFromSecret oauth-secrets kube-system OIDC_CLIENT_SECRET)
  CLIENT_ID=$(dataFromSecret oauth-secrets kube-system OIDC_CLIENT_ID)
  REALM_NAME=$(dataFromSecret oauth-secrets kube-system OAUTH_REALM)

  createLocalStorageClassAndPV "eclipse-che-storage" "eclipse-che-pv" "/data/volumes/eclipse-che"
  
  # Install Chectl
  wget https://che-incubator.github.io/chectl/install.sh
  chmod +x install.sh
  ./install.sh
  [[ $? -ne 0 ]] && echoFailed "Chectl installation failed, please check the logs!" && return 1
  echoSuccess "Chectl installed successfully!"

  # Create a patch file for CheCluster
  cat > che-patch.yaml << EOF
kind: CheCluster
apiVersion: org.eclipse.che/v2
spec:
  devEnvironments:
    secondsOfInactivityBeforeIdling: -1
    storage:
      perUserStrategyPvcConfig:
        claimSize: 2Gi 
        storageClass: eclipse-che-storage
      pvcStrategy: per-user 
    
  networking:
    auth:
      oAuthClientName: ${CLIENT_ID}
      oAuthSecret: ${CLIENT_SECRET}
      identityProviderURL: "https://$(fullKeycloakUrl)/realms/${REALM_NAME}"
      gateway:
        oAuthProxy:
          cookieExpireSeconds: 300
  components:
    cheServer:
      extraVolumes:
        - name: keycloak-ca
          configMap:
            name: keycloak-certs
      extraVolumeMounts:
        - name: keycloak-ca
          mountPath: /etc/ssl/certs/keycloak-ca.crt
          subPath: keycloak-ca.crt
          readOnly: true
      extraProperties:
        CHE_OIDC_USERNAME__CLAIM: email
        CHE_OIDC_SKIP_CERTIFICATE_VERIFICATION: "true"
        REQUESTS_CA_BUNDLE: /etc/ssl/certs/keycloak-ca.crt
EOF

  kubectl create ns eclipse-che || echo "Namespace eclipse-che already exists"

  # Create a secret for Keycloak CA certificate
  kubectl get secret keycloak-gokcloud-com -n keycloak -o jsonpath="{.data['ca\.crt']}" | base64 -d > keycloak-ca.crt
  kubectl create configmap keycloak-certs --from-file=keycloak-ca.crt=keycloak-ca.crt -n eclipse-che
  kubectl label configmap keycloak-certs app.kubernetes.io/part-of=che.eclipse.org app.kubernetes.io/component=ca-bundle -n eclipse-che

  # Deploy Eclipse Che using chectl
  chectl server:deploy --platform k8s --domain che.gokcloud.com --che-operator-cr-patch-yaml che-patch.yaml --skip-cert-manager
  [[ $? -eq 0 ]] && echoSuccess "Eclipse Che is now installed and running!" || echoFailed "Eclipse Che installation failed, please check the logs!"
  rm -f keycloak-ca.crt che-patch.yaml install.sh
}

resetEclipseChe(){
  # Uninstall Eclipse Che
  kubectl delete deployment --all -n eclipse-che
  chectl server:delete --delete-namespace --delete-all
  [[ $? -ne 0 ]] && echoFailed "Eclipse Che uninstallation failed, please check the logs!" && return 1
  kubectl delete configmap keycloak-certs -n eclipse-che || echo "ConfigMap keycloak-certs already deleted or does not exist"
  emptyLocalFsStorage "Eclipse Che" "eclipse-che-pv" "eclipse-che-storage" "/data/volumes/eclipse-che"
  echoSuccess "Eclipse Che has been uninstalled successfully!"
}

oauth2ProxyInst(){
  local start_time=$(date +%s)
  
  log_component_start "oauth2-proxy-install" "Installing OAuth2 Proxy authentication system"
  
  log_step "1" "Updating system packages with smart caching"
  if ! updateSys; then
    log_error "Failed to update system packages"
    return 1
  fi
  
  log_step "2" "Installing dependencies with smart caching"
  if ! installDeps; then
    log_error "Failed to install OAuth2 Proxy dependencies"
    return 1
  fi
  
  log_step "3" "Preparing OAuth2 Proxy installation directory"
  local oauth2_dir="$MOUNT_PATH/kubernetes/install_k8s/oauth2-proxy"
  if [[ ! -d "$oauth2_dir" ]]; then
    log_error "OAuth2 Proxy installation directory not found: $oauth2_dir"
    return 1
  fi
  
  if execute_with_suppression pushd "$oauth2_dir"; then
    log_success "OAuth2 Proxy installation directory prepared"
  else
    log_error "Failed to access OAuth2 Proxy installation directory"
    return 1
  fi

  log_step "4" "Creating namespace and persistent storage"
  if createLocalStorageClassAndPV "oauth-storage" "oauth-pv" "/data/volumes/pv6"; then
    log_success "OAuth2 Proxy storage created"
  else
    log_warning "OAuth2 Proxy storage creation had issues but continuing"
  fi
  
  if kubectl get namespace oauth2 >/dev/null 2>&1 || kubectl create namespace oauth2 >/dev/null 2>&1; then
    log_success "OAuth2 Proxy namespace created or already exists"
  else
    log_error "Failed to create OAuth2 Proxy namespace"
    return 1
  fi

  log_step "5" "Collecting OAuth2 configuration from Keycloak"
  log_substep "Retrieving OAuth2 configuration parameters"
  
  local oauth2_client_id=$(dataFromSecret oauth-secrets kube-system OIDC_CLIENT_ID 2>/dev/null || echo "")
  local realm=$(dataFromSecret oauth-secrets kube-system OAUTH_REALM 2>/dev/null || echo "")
  local oauth2_host="$(fullKeycloakUrl 2>/dev/null || echo "")"
  local oidc_issuer_url=$(dataFromSecret oauth-secrets kube-system OIDC_ISSUE_URL 2>/dev/null || echo "")
  
  if [[ -z "$oauth2_client_id" || -z "$realm" || -z "$oauth2_host" || -z "$oidc_issuer_url" ]]; then
    log_error "Failed to retrieve OAuth2 configuration from Keycloak secrets"
    log_error "Please ensure Keycloak is installed and configured properly"
    popd || true
    return 1
  fi
  
  log_substep "Client ID: ${COLOR_CYAN}${oauth2_client_id}${COLOR_RESET}"
  log_substep "Realm: ${COLOR_CYAN}${realm}${COLOR_RESET}"
  log_substep "Keycloak Host: ${COLOR_CYAN}${oauth2_host}${COLOR_RESET}"
  log_substep "OIDC Issuer: ${COLOR_CYAN}${oidc_issuer_url}${COLOR_RESET}"
  
  log_success "OAuth2 configuration parameters collected"

  log_step "6" "Building and deploying OAuth2 Proxy services"
  
  # Add upstream OAuth2 proxy Helm repository
  log_substep "Adding upstream OAuth2 proxy Helm repository..."
  if execute_with_suppression helm repo add oauth2-proxy https://oauth2-proxy.github.io/manifests; then
    log_success "Upstream OAuth2 proxy Helm repository added"
  else
    log_warning "OAuth2 proxy repository may already exist"
  fi
  
  log_substep "Updating Helm repositories..."
  if execute_with_suppression helm repo update; then
    log_success "Helm repositories updated"
  else
    log_warning "Helm repository update had issues but continuing"
  fi
  
  # Enhanced OAuth2 build with progress tracking
  build_oauth2_with_progress "$oauth2_client_id" "$realm" "$oauth2_host" "$oidc_issuer_url"
  if [[ $? -ne 0 ]]; then
    log_error "OAuth2 Proxy installation failed"
    popd || true
    return 1
  fi

  log_step "7" "Configuring OAuth2 Proxy networking and access"
  log_substep "Creating OAuth2 Proxy ingress manually..."
  
  # Create the ingress resource manually since upstream chart ingress has bugs
  if execute_with_suppression kubectl apply -f - <<EOF
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: oauth2-proxy
  namespace: oauth2
spec:
  ingressClassName: nginx
  rules:
  - host: $(defaultSubdomain).$(rootDomain)
    http:
      paths:
      - path: /oauth2(.*)
        pathType: ImplementationSpecific
        backend:
          service:
            name: oauth2-proxy
            port:
              number: 80
EOF
  then
    log_success "OAuth2 Proxy ingress created successfully"
  else
    log_warning "OAuth2 Proxy ingress creation had issues but continuing"
  fi
  
  log_substep "Configuring TLS for OAuth2 Proxy ingress..."
  if execute_with_suppression kubectl patch ingress oauth2-proxy -n oauth2 --type=merge -p '{"spec":{"tls":[{"hosts":["'$(defaultSubdomain).$(rootDomain)'"],"secretName":"'$(defaultSubdomain)-$(sedRootDomain)-tls'"}]}}'; then
    log_success "OAuth2 Proxy TLS configured successfully"
  else
    log_warning "OAuth2 Proxy TLS configuration had issues but continuing"
  fi
  
  log_substep "Setting SSL and protocol annotations..."
  if execute_with_suppression kubectl annotate ingress oauth2-proxy -n oauth2 nginx.ingress.kubernetes.io/ssl-redirect='true' ingress.kubernetes.io/ssl-passthrough='true' nginx.ingress.kubernetes.io/backend-protocol='HTTP' kubernetes.io/ingress.allow-http='false' nginx.ingress.kubernetes.io/rewrite-target='/$1'; then
    log_success "OAuth2 Proxy SSL annotations configured successfully"
  else
    log_warning "OAuth2 Proxy SSL annotations had issues but continuing"
  fi
  
  log_substep "Applying Let's Encrypt certificate configuration..."
  if execute_with_suppression gok patch ingress oauth2-proxy oauth2 letsencrypt $(defaultSubdomain); then
    log_success "OAuth2 Proxy Let's Encrypt configuration applied successfully"
  else
    log_warning "OAuth2 Proxy Let's Encrypt configuration had issues but installation may still work"
  fi

  log_step "8" "Validating OAuth2 Proxy installation"
  if validate_oauth2_installation; then
    log_success "OAuth2 Proxy installation validation completed"
  else
    log_warning "OAuth2 Proxy validation had issues but installation may still work"
  fi

  if execute_with_suppression popd; then
    log_success "OAuth2 Proxy installation directory cleanup completed"
  else
    log_warning "Directory cleanup had issues but installation completed"
  fi

  local end_time=$(date +%s)
  local duration=$((end_time - start_time))
  
  show_installation_summary "oauth2-proxy" "oauth2" "Authentication proxy with Keycloak integration"
  log_component_success "oauth2-proxy-install" "OAuth2 Proxy authentication system installed successfully"
  log_success "OAuth2 Proxy installation completed in ${duration}s"
  
  # Show OAuth2-specific next steps and recommend RabbitMQ
  show_oauth2_next_steps
}

# Enhanced OAuth2 Proxy build with detailed progress tracking
build_oauth2_with_progress() {
  local oauth2_client_id="$1"
  local realm="$2"
  local oauth2_host="$3"
  local oidc_issuer_url="$4"
  local start_time=$(date +%s)
  
  log_substep "Authentication Provider: ${COLOR_CYAN}OAuth2 + OIDC${COLOR_RESET}"
  log_substep "Keycloak Integration: ${COLOR_CYAN}${oauth2_host}${COLOR_RESET}"
  log_substep "Client ID: ${COLOR_CYAN}${oauth2_client_id}${COLOR_RESET}"
  log_substep "Realm: ${COLOR_CYAN}${realm}${COLOR_RESET}"
  log_substep "OIDC Issuer: ${COLOR_CYAN}${oidc_issuer_url}${COLOR_RESET}"
  
  # Step 1: Create OAuth2 configuration with progress
  log_info "ðŸ”§ Creating OAuth2 Proxy configuration"
  
  local temp_config_log=$(mktemp)
  local temp_config_error=$(mktemp)
  
  # Create configuration in background with progress tracking
  (
    kubectl create configmap oauth2-proxy-config \
      --from-literal=clientID="${oauth2_client_id}" \
      --from-literal=oidcIssuerUrl="${oidc_issuer_url}" \
      --from-literal=redirectUrl=oauth2/callback \
      -n oauth2 >"$temp_config_log" 2>"$temp_config_error"
    
    kubectl create configmap oauth2-ca-cert \
      --from-file=issuer.crt=/usr/local/share/ca-certificates/issuer.crt \
      -n oauth2 >>"$temp_config_log" 2>>"$temp_config_error"
  ) &
  local config_pid=$!
  
  # Show configuration creation progress
  local config_progress=0
  local spinner_chars="|/-\\"
  local spinner_idx=0
  local config_stage="Creating OAuth2 configuration"
  
  while kill -0 $config_pid 2>/dev/null; do
    local char=${spinner_chars:spinner_idx:1}
    config_progress=$(( (config_progress + 1) % 40 ))
    local progress_percent=$(( config_progress * 100 / 40 ))
    
    # Update stage based on progress
    if [[ $config_progress -lt 20 ]]; then
      config_stage="Creating OAuth2 proxy config"
    else
      config_stage="Setting up CA certificates"
    fi
    
    printf "\r${COLOR_BLUE}  Creating configuration [%c] ${COLOR_CYAN}%d%%${COLOR_RESET} - ${COLOR_DIM}%s${COLOR_RESET}" "$char" "$progress_percent" "$config_stage"
    
    spinner_idx=$(( (spinner_idx + 1) % 4 ))
    sleep 0.3
  done
  
  wait $config_pid
  local config_exit_code=$?
  
  if [[ $config_exit_code -eq 0 ]]; then
    printf "\r${COLOR_GREEN}  âœ“ OAuth2 configuration created [100%%]${COLOR_RESET}\n"
    log_success "OAuth2 Proxy configuration created successfully"
  else
    printf "\r${COLOR_RED}  âœ— Configuration creation failed${COLOR_RESET}\n"
    log_error "OAuth2 configuration creation failed - error details:"
    if [[ -s "$temp_config_error" ]]; then
      cat "$temp_config_error" >&2
    fi
    rm -f "$temp_config_log" "$temp_config_error"
    return 1
  fi
  
  # Step 2: Install OAuth2 Proxy via upstream Helm chart (default)
  log_info "â˜¸ï¸  Installing OAuth2 Proxy via upstream Helm chart"
  log_substep "Chart: ${COLOR_CYAN}oauth2-proxy/oauth2-proxy${COLOR_RESET}"
  log_substep "Values: ${COLOR_CYAN}${MOUNT_PATH}/kubernetes/install_k8s/oauth2-proxy/values.yaml${COLOR_RESET}"
  
  local temp_helm_log=$(mktemp)
  local temp_helm_error=$(mktemp)
  
  # Start Helm installation in background with upstream oauth2-proxy chart
  helm upgrade --install oauth2-proxy oauth2-proxy/oauth2-proxy \
    --namespace oauth2 \
    --values "${MOUNT_PATH}"/kubernetes/install_k8s/oauth2-proxy/values.yaml \
    --set-string config.clientID="${oauth2_client_id}" \
    --set-string config.clientSecret="$(dataFromSecret oauth-secrets kube-system OIDC_CLIENT_SECRET)" \
    --set-string config.cookieSecret=bXljb29raWVzdW1pdDEyMzQ1Njc4OTAx \
    --set-string config.oidcIssuerUrl="${oidc_issuer_url}" \
    --set-string config.loginHost="${oauth2_host}" \
    --set-string config.realm="${realm}" \
    --set ingress.enabled=false \
    --set 'extraArgs[0]=--provider=oidc' \
    --set "extraArgs[1]=--keycloak-group=administrators" \
    --set "extraArgs[2]=--keycloak-group=developers" \
    --set "extraArgs[3]=--allowed-group=administrators" \
    --set "extraArgs[4]=--allowed-group=developers" \
    --set "extraArgs[5]=--scope=openid email profile groups sub offline_access" \
    --set "extraArgs[6]=--ssl-insecure-skip-verify=false" \
    --set "extraArgs[7]=--set-authorization-header=true" \
    --set "extraArgs[8]=--whitelist-domain=.gokcloud.com" \
    --set "extraArgs[9]=--oidc-groups-claim=groups" \
    --set "extraArgs[10]=--user-id-claim=sub" \
    --set "extraArgs[11]=--cookie-domain=.gokcloud.com" \
    --set "extraArgs[12]=--cookie-secure=true" \
    --set "extraArgs[13]=--pass-access-token=true" \
    --set "extraArgs[14]=--pass-authorization-header=true" \
    --set "extraArgs[15]=--oidc-issuer-url=${oauth2_oidc_issuer_url}" \
    --set "extraArgs[16]=--standard-logging=true" \
    --set "extraArgs[17]=--auth-logging=true" \
    --set "extraArgs[18]=--request-logging=true" \
    --set "extraArgs[19]=--cookie-refresh=1h" \
    --set "extraArgs[20]=--cookie-expire=8h" \
    --set "extraArgs[21]=--set-xauthrequest=true" \
    --set "extraArgs[22]=--skip-jwt-bearer-tokens=true" \
    --set "extraArgs[23]=--email-domain=*" \
    --set "extraArgs[24]=--oidc-issuer-url=https://${oauth2_host}/realms/${realm}/" \
    >"$temp_helm_log" 2>"$temp_helm_error" &
  local helm_pid=$!
  
  # Show Helm installation progress with OAuth2-specific stages
  local helm_progress=0
  local helm_steps=8
  local helm_stage="Preparing"
  
  while kill -0 $helm_pid 2>/dev/null; do
    local char=${spinner_chars:spinner_idx:1}
    helm_progress=$(( (helm_progress + 1) % (helm_steps * 15) ))
    local progress_percent=$(( helm_progress * 100 / (helm_steps * 15) ))
    
    # Update stage based on progress
    case $((helm_progress / 15)) in
      0) helm_stage="Downloading OAuth2 chart" ;;
      1) helm_stage="Installing OAuth2 proxy" ;;
      2) helm_stage="Configuring authentication" ;;
      3) helm_stage="Setting up OIDC integration" ;;
      4) helm_stage="Configuring Keycloak provider" ;;
      5) helm_stage="Setting up cookie management" ;;
      6) helm_stage="Configuring ingress routing" ;;
      7) helm_stage="Finalizing installation" ;;
    esac
    
    printf "\r${COLOR_BLUE}  Installing OAuth2 Proxy [%c] ${COLOR_CYAN}%d%%${COLOR_RESET} - ${COLOR_DIM}%s${COLOR_RESET}" "$char" "$progress_percent" "$helm_stage"
    
    spinner_idx=$(( (spinner_idx + 1) % 4 ))
    sleep 0.5
  done
  
  wait $helm_pid
  local helm_exit_code=$?
  
  if [[ $helm_exit_code -eq 0 ]]; then
    printf "\r${COLOR_GREEN}  âœ“ OAuth2 Proxy installation completed [100%%]${COLOR_RESET}\n"
    log_success "OAuth2 Proxy deployed successfully via Helm"
  else
    printf "\r${COLOR_RED}  âœ— OAuth2 Proxy installation failed${COLOR_RESET}\n"
    log_error "OAuth2 Proxy Helm installation failed - error details:"
    if [[ -s "$temp_helm_error" ]]; then
      if is_verbose_mode; then
        cat "$temp_helm_error" >&2
      else
        tail -10 "$temp_helm_error" >&2
        log_info "Use --verbose flag to see full installation logs"
      fi
    fi
    rm -f "$temp_config_log" "$temp_config_error" "$temp_helm_log" "$temp_helm_error"
    return 1
  fi
  
  # Step 3: Enhanced OAuth2 Proxy readiness verification with immediate issue detection
  log_info "ðŸ” Waiting for OAuth2 Proxy services with enhanced diagnostics"
  
  # Give deployment time to start creating pods
  log_substep "Allowing OAuth2 deployment to initialize..."
  sleep 10
  
  # Check for immediate image pull issues
  log_substep "Checking for Docker image pull issues..."
  if ! check_image_pull_issues "oauth2" "oauth2-proxy"; then
    log_error "âŒ OAuth2 Proxy has Docker image pull issues"
    rm -f "$temp_config_log" "$temp_config_error" "$temp_helm_log" "$temp_helm_error"
    return 1
  fi
  
  # Check for resource constraint issues
  log_substep "Checking for resource constraints..."
  if ! check_resource_constraints "oauth2" "oauth2-proxy"; then
    log_error "âŒ OAuth2 Proxy has resource constraint issues"
    rm -f "$temp_config_log" "$temp_config_error" "$temp_helm_log" "$temp_helm_error"
    return 1
  fi
  
  # Use enhanced pod waiting with detailed diagnostics
  log_substep "Waiting for OAuth2 Proxy pods with enhanced monitoring..."
  if wait_for_pods_ready "oauth2" "180" "oauth2-proxy"; then
    log_success "âœ… OAuth2 Proxy pods are ready and healthy"
  else
    log_error "âŒ OAuth2 Proxy pods failed to become ready"
    rm -f "$temp_config_log" "$temp_config_error" "$temp_helm_log" "$temp_helm_error"
    return 1
  fi
  
  # Verify deployment is actually ready
  local deployment_name=""
  if kubectl get deployment oauth2proxy-oauth2-proxy -n oauth2 >/dev/null 2>&1; then
    deployment_name="oauth2proxy-oauth2-proxy"
  elif kubectl get deployment oauth2proxy -n oauth2 >/dev/null 2>&1; then
    deployment_name="oauth2proxy"
  fi
  
  if [[ -n "$deployment_name" ]]; then
    log_substep "Verifying OAuth2 deployment status..."
    if check_deployment_readiness "$deployment_name" "oauth2"; then
      log_success "âœ… OAuth2 Proxy deployment is healthy"
    else
      log_error "âŒ OAuth2 Proxy deployment has issues"
      rm -f "$temp_config_log" "$temp_config_error" "$temp_helm_log" "$temp_helm_error"
      return 1
    fi
  fi
  
  if [[ "$ready" == "true" ]]; then
    printf "\r${COLOR_GREEN}  âœ“ OAuth2 Proxy services ready [100%%]${COLOR_RESET}\n"
    log_success "OAuth2 Proxy is ready and accepting connections"
  else
    printf "\r${COLOR_YELLOW}  âš  OAuth2 Proxy taking longer than expected${COLOR_RESET}\n"
    log_warning "OAuth2 Proxy deployment may still be starting (check with: kubectl get pods -n oauth2)"
  fi
  
  # Clean up temporary files
  rm -f "$temp_config_log" "$temp_config_error" "$temp_helm_log" "$temp_helm_error"
  
  # Installation completion summary
  local end_time=$(date +%s)
  local duration=$((end_time - start_time))
  log_success "OAuth2 Proxy deployment completed in ${duration}s"
  
  # Show OAuth2 summary
  show_oauth2_summary "$oauth2_host" "$realm"
  
  return 0
}

# Enhanced validation for OAuth2 Proxy installation with comprehensive checks
validate_oauth2_installation() {
  log_info "Validating OAuth2 Proxy installation with enhanced diagnostics..."
  local validation_passed=true
  
  log_step "1" "Checking OAuth2 Proxy namespace"
  if kubectl get namespace oauth2 >/dev/null 2>&1; then
    log_success "OAuth2 Proxy namespace found"
  else
    log_error "OAuth2 Proxy namespace not found"
    return 1
  fi
  
  log_step "2" "Checking OAuth2 Proxy deployment status"
  # Try different possible deployment names
  local deployment_name=""
  if kubectl get deployment oauth2proxy-oauth2-proxy -n oauth2 >/dev/null 2>&1; then
    deployment_name="oauth2proxy-oauth2-proxy"
  elif kubectl get deployment oauth2proxy -n oauth2 >/dev/null 2>&1; then
    deployment_name="oauth2proxy"
  elif kubectl get deployment oauth2-proxy -n oauth2 >/dev/null 2>&1; then
    deployment_name="oauth2-proxy"
  fi
  
  if [[ -n "$deployment_name" ]]; then
    if check_deployment_readiness "$deployment_name" "oauth2"; then
      log_success "OAuth2 Proxy deployment ($deployment_name) is ready"
    else
      log_error "OAuth2 Proxy deployment ($deployment_name) has issues"
      validation_passed=false
    fi
  else
    log_error "OAuth2 Proxy deployment not found (checked multiple naming patterns)"
    validation_passed=false
  fi
  
  log_step "3" "Checking OAuth2 Proxy pods with detailed diagnostics"
  if ! wait_for_pods_ready "oauth2" "300" "oauth2-proxy"; then
    log_error "OAuth2 Proxy pods not ready"
    validation_passed=false
  else
    log_success "OAuth2 Proxy pods are ready"
  fi
  
  log_step "4" "Checking OAuth2 Proxy service connectivity"
  local service_name=""
  if kubectl get service oauth2proxy-oauth2-proxy -n oauth2 >/dev/null 2>&1; then
    service_name="oauth2proxy-oauth2-proxy"
  elif kubectl get service oauth2proxy -n oauth2 >/dev/null 2>&1; then
    service_name="oauth2proxy"
  elif kubectl get service oauth2-proxy -n oauth2 >/dev/null 2>&1; then
    service_name="oauth2-proxy"
  fi
  
  if [[ -n "$service_name" ]]; then
    if check_service_connectivity "$service_name" "oauth2"; then
      log_success "OAuth2 Proxy service ($service_name) is accessible"
    else
      log_warning "OAuth2 Proxy service ($service_name) connectivity issues detected"
    fi
  else
    log_warning "OAuth2 Proxy service not found (checked multiple naming patterns)"
  fi
  
  log_step "5" "Checking OAuth2 Proxy ingress configuration"
  local ingress_name=""
  if kubectl get ingress oauth2proxy-oauth2-proxy -n oauth2 >/dev/null 2>&1; then
    ingress_name="oauth2proxy-oauth2-proxy"
  elif kubectl get ingress oauth2proxy -n oauth2 >/dev/null 2>&1; then
    ingress_name="oauth2proxy"
  elif kubectl get ingress oauth2-proxy -n oauth2 >/dev/null 2>&1; then
    ingress_name="oauth2-proxy"
  fi
  
  if [[ -n "$ingress_name" ]]; then
    if check_ingress_status "$ingress_name" "oauth2"; then
      log_success "OAuth2 Proxy ingress ($ingress_name) is configured and ready"
    else
      log_warning "OAuth2 Proxy ingress ($ingress_name) has configuration issues"
    fi
  else
    log_info "OAuth2 Proxy ingress not found (may be using NodePort/LoadBalancer or configured separately)"
  fi
  
  log_step "6" "Checking OAuth2 Proxy configuration"
  if kubectl get secret oauth2-proxy-config -n oauth2 >/dev/null 2>&1; then
    log_success "OAuth2 Proxy configuration secret found"
  elif kubectl get configmap oauth2-proxy-config -n oauth2 >/dev/null 2>&1; then
    log_success "OAuth2 Proxy configuration configmap found"
  else
    log_warning "OAuth2 Proxy configuration not found (may use different naming)"
  fi
  
  return $([[ "$validation_passed" == "true" ]] && echo 0 || echo 1)
}

# Show OAuth2-specific summary
show_oauth2_summary() {
  local oauth2_host="$1"
  local realm="$2"
  
  log_info "ðŸ“‹ OAuth2 Proxy Authentication Summary"
  echo
  
  echo -e "${COLOR_BRIGHT_BLUE}${COLOR_BOLD}ðŸ›¡ï¸  OAuth2 Proxy Authentication Details${COLOR_RESET}"
  echo -e "${COLOR_CYAN}â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”${COLOR_RESET}"
  
  echo -e "${COLOR_YELLOW}${COLOR_BOLD}Authentication Proxy Information:${COLOR_RESET}"
  echo -e "  ðŸ” ${COLOR_GREEN}OAuth2 Proxy${COLOR_RESET} - HTTP reverse proxy for authentication"
  echo -e "  ðŸ”— ${COLOR_GREEN}Keycloak Integration${COLOR_RESET} - OIDC provider integration"
  echo -e "  ðŸª ${COLOR_GREEN}Session Management${COLOR_RESET} - Secure cookie-based sessions"
  echo -e "  ðŸ›¡ï¸  ${COLOR_GREEN}Access Control${COLOR_RESET} - Group-based authorization"
  echo
  
  echo -e "${COLOR_YELLOW}${COLOR_BOLD}Integration Configuration:${COLOR_RESET}"
  echo -e "  ðŸ¢ ${COLOR_CYAN}Identity Provider${COLOR_RESET}: ${oauth2_host}"
  echo -e "  ðŸŒ ${COLOR_CYAN}Keycloak Realm${COLOR_RESET}: ${realm}"
  echo -e "  ðŸ‘¥ ${COLOR_CYAN}Allowed Groups${COLOR_RESET}: administrators, developers"
  echo -e "  ðŸª ${COLOR_CYAN}Cookie Domain${COLOR_RESET}: .gokcloud.com"
  echo -e "  â° ${COLOR_CYAN}Session Duration${COLOR_RESET}: 8 hours (refresh: 1 hour)"
  echo
  
  echo -e "${COLOR_YELLOW}${COLOR_BOLD}Authentication Features:${COLOR_RESET}"
  echo -e "  âœ… ${COLOR_GREEN}OIDC Integration${COLOR_RESET} - OpenID Connect with Keycloak"
  echo -e "  âœ… ${COLOR_GREEN}Group-based Access${COLOR_RESET} - Role-based authorization"
  echo -e "  âœ… ${COLOR_GREEN}Secure Cookies${COLOR_RESET} - HTTPs-only secure session cookies"
  echo -e "  âœ… ${COLOR_GREEN}Token Passing${COLOR_RESET} - Access token forwarding to backends"
  echo -e "  âœ… ${COLOR_GREEN}Domain Whitelisting${COLOR_RESET} - Restricted to trusted domains"
  echo -e "  âœ… ${COLOR_GREEN}Request Logging${COLOR_RESET} - Comprehensive authentication audit logs"
  echo
  
  echo -e "${COLOR_BRIGHT_GREEN}${COLOR_BOLD}ðŸ’¡ OAuth2 Proxy Benefits:${COLOR_RESET}"
  echo -e "${COLOR_GREEN}â€¢ Protects any HTTP service with Keycloak authentication${COLOR_RESET}"
  echo -e "${COLOR_GREEN}â€¢ Seamless single sign-on experience across all platform services${COLOR_RESET}"
  echo -e "${COLOR_GREEN}â€¢ Group-based access control integrated with LDAP directory${COLOR_RESET}"
  echo -e "${COLOR_GREEN}â€¢ Secure session management with configurable timeouts${COLOR_RESET}"
  echo -e "${COLOR_GREEN}â€¢ Production-ready authentication proxy for enterprise environments${COLOR_RESET}"
  echo
}

# Show OAuth2 next steps and recommend RabbitMQ
show_oauth2_next_steps() {
  echo
  echo -e "${COLOR_BRIGHT_YELLOW}${COLOR_BOLD}ðŸš€ OAuth2 Proxy Post-Installation Steps${COLOR_RESET}"
  echo
  
  echo -e "${COLOR_MAGENTA}${COLOR_BOLD}Immediate Next Steps:${COLOR_RESET}"
  echo -e "${COLOR_CYAN}  1. Test OAuth2 authentication with protected services${COLOR_RESET}"
  echo -e "${COLOR_CYAN}  2. Configure additional applications to use OAuth2 proxy${COLOR_RESET}"
  echo -e "${COLOR_CYAN}  3. Set up group-based access policies in Keycloak${COLOR_RESET}"
  echo -e "${COLOR_CYAN}  4. Monitor authentication logs and session management${COLOR_RESET}"
  echo
  
  echo -e "${COLOR_BRIGHT_MAGENTA}${COLOR_BOLD}ðŸŽ¯ Recommended Next Installation: RabbitMQ${COLOR_RESET}"
  echo -e "${COLOR_CYAN}OAuth2 provides authentication - now add RabbitMQ for messaging and communication!${COLOR_RESET}"
  echo
  echo -e "${COLOR_YELLOW}${COLOR_BOLD}Why install RabbitMQ next?${COLOR_RESET}"
  echo -e "${COLOR_GREEN}â€¢ ðŸ“¨ Reliable message broker for asynchronous communication${COLOR_RESET}"
  echo -e "${COLOR_GREEN}â€¢ ðŸ”— Decouples services for better scalability and resilience${COLOR_RESET}"
  echo -e "${COLOR_GREEN}â€¢ ðŸ“Š Enables event-driven architecture patterns${COLOR_RESET}"
  echo -e "${COLOR_GREEN}â€¢ ðŸš€ High-performance messaging for microservices${COLOR_RESET}"
  echo -e "${COLOR_GREEN}â€¢ ðŸ›¡ï¸  Secure messaging with authentication and authorization${COLOR_RESET}"
  echo
  echo -e "${COLOR_BRIGHT_GREEN}${COLOR_BOLD}Install RabbitMQ now?${COLOR_RESET}"
  echo -e "${COLOR_CYAN}  Command: ${COLOR_BOLD}gok install rabbitmq${COLOR_RESET}"
  echo
  
  # Suggest and install RabbitMQ as the next step
  suggest_and_install_next_module "oauth2"
}


waitForServiceAvailable(){
  NS=$1
  # Suppress verbose output, only show on error
  if ! kubectl --timeout=120s wait --for=condition=Ready pods --all --namespace "$NS" >/dev/null 2>&1; then
    echoFailed "Service timed-out, please check!!"
    return 1
  fi
}

resolveDns() {
  local domain=$1
  if [ -z "$domain" ]; then
    echo "Usage: resolveDns <domain>"
    return 1
  fi

  kubectl run -it --rm dnsutils --image=busybox:1.28 --restart=Never -- nslookup "$domain"
}

ttydReset() {
  helm uninstall ttyd -n ttyd
  kubectl delete ns ttyd
  echo "ttyd has been uninstalled and the namespace deleted."
}

ttydInst() {
  # Create namespace for ttyd
  kubectl create namespace ttyd || echo "Namespace ttyd already exists"

  # Add Helm repo if needed (ttyd uses Docker image directly, so no repo needed)
  # Prepare a minimal values.yaml if you want customization

  # Install ttyd using a local chart directory or a published chart
  helm install ttyd ${MOUNT_PATH}/kubernetes/install_k8s/ttyd/chart \
    --namespace ttyd \
    --set ingress.enabled=true \
    --set ingress.host="ttyd.$(rootDomain)"

  gok patch ingress ttyd ttyd letsencrypt ttyd
  patchTtydWithOauth

  echo "Waiting for ttyd to be ready..."
  kubectl --namespace ttyd wait --for=condition=Ready pods --all --timeout=120s
  if [[ $? -eq 0 ]]; then
    echoSuccess "ttyd is now up! Access it at: https://ttyd.$(rootDomain)"
  else
    echoFailed "ttyd setup timed out. Please check the logs."
    return 1
  fi
}


enableJenkins(){
  USERNAME=$(promptUserInput "Enter Jenkins Username (skmaji1): " "skmaji1")
  PASSWORD=$(promptSecret "Enter Jenkins Password: ")

  docker exec -it halyard hal config ci jenkins enable

  docker exec -it halyard hal config ci jenkins master add gok-jenkins-master \
    --address https://jenkins.gokcloud.com \
    --username ${USERNAME} \
    --password ${PASSWORD}

  docker exec -it halyard hal config ci jenkins master edit gok-jenkins-master --csrf true

  docker exec -it halyard hal deploy apply
}

spinnakerInst(){
  CLIENT_SECRET=$(dataFromSecret oauth-secrets kube-system OIDC_CLIENT_SECRET)
  CLIENT_ID=$(dataFromSecret oauth-secrets kube-system OIDC_CLIENT_ID)
  OIDC_ISSUE_URL=$(dataFromSecret oauth-secrets kube-system OIDC_ISSUE_URL)

  useradd -m -g root spinnaker
  mkdir -p /home/spinnaker/.hal && chmod -R 777 /home/spinnaker/.hal
  mkdir -p /home/spinnaker/.kube/ && cp ~/.kube/config /home/spinnaker/.kube/config && chmod 755 /home/spinnaker/.kube/config
  docker run --name halyard -v /home/spinnaker/.hal:/home/spinnaker/.hal -v /home/spinnaker/.kube/config:/home/spinnaker/.kube/config \
    -d gcr.io/spinnaker-marketplace/halyard:stable

  echo "Waiting for the service to be up for 30 Seconds"
  sleep 30
  docker exec -it halyard kubectl cluster-info
  docker exec -it halyard hal config provider kubernetes enable
  docker exec -it halyard hal config provider kubernetes account add gok-k8s --provider-version v2 \
    --context $(kubectl config current-context)
  docker exec -it halyard hal config features edit --artifacts true
  docker exec -it halyard hal config deploy edit --type distributed --account-name gok-k8s
  kubectl create ns spinnaker
  helm repo add stable https://charts.helm.sh/stable
  helm install minio --namespace spinnaker --set accessKey="myaccesskey" --set secretKey="mysecretkey" \
    --set persistence.enabled=false stable/minio
  docker exec -it halyard bash -c 'mkdir /home/spinnaker/.hal/default/profiles'
  docker exec -it halyard echo "spinnaker.s3.versioning: false" > /home/spinnaker/.hal/default/profiles/front50-local.yml

  cat <<EOF > /home/spinnaker/.hal/default/profiles/front50-local.yml
kubernetes:
  volumes:
  - id: internal-trust-store
    mountPath: /etc/ssl/certs/java
    type: secret
EOF

  ##https://github.com/spinnaker/spinnaker/issues/6498
  cat <<EOF > /home/spinnaker/.hal/permission.yml
users:
 - username: skmaji1
   roles:
   - admin
EOF

  cat <<EOF > /home/spinnaker/.hal/default/profiles/spinnaker-local.yml
logging:
  level:
    # Enable debug logging by changing level to DEBUG
    root: TRACE  # default
EOF

  #https://stackoverflow.com/questions/78087469/how-to-apply-custom-profiles-setting-to-spinnaker-to-make-it-deploy-with-one-com
  cat <<EOF > /home/spinnaker/.hal/default/profiles/fiat-local.yml
fiat.restrictApplicationCreation: true #Allows to restrict permissions
auth.permissions.provider.application: aggregate
auth.permissions.source.application.prefix: #Allows to work with
  enabled: true                          # applications prefixes
  prefixes:
  - prefix: "*" # All applications
    permissions:
      READ:
      - "administrators"
      - "admin"
      WRITE:
      - "administrators"
      - "admin"
      EXECUTE:
      - "administrators"
      - "admin"
      CREATE:
      - "administrators"
      - "admin"
EOF

  docker exec -it halyard hal config storage s3 edit --endpoint http://minio:9000 --access-key-id "myaccesskey" --secret-access-key "mysecretkey"
  docker exec -it halyard hal config storage s3 edit --path-style-access true
  docker exec -it halyard hal config storage edit --type s3
  docker exec -it halyard hal version list
  docker exec -it halyard hal config version edit --version 1.34.2

  cat <<EOF > /home/spinnaker/.hal/default/profiles/gate-local.yml
kubernetes:
  volumes:
  - id: internal-trust-store
    mountPath: /etc/ssl/certs
    type: secret
server:
  tomcat:
    protocolHeader: X-Forwarded-Proto
    remoteIpHeader: X-Forwarded-For
    internalProxies: .*
    httpsServerPort: X-Forwarded-Port
security:
  oauth2:
    enabled: true
    client:
      clientId: gok-developers-client
      clientSecret: $CLIENT_SECRET
      userAuthorizationUri: $OIDC_ISSUE_URL/protocol/openid-connect/auth
      accessTokenUri: $OIDC_ISSUE_URL/protocol/openid-connect/token
      scope: openid,email,profile,groups
      preEstablishedRedirectUri: https://spin-gate.gokcloud.com/login
      clientAuthenticationScheme: form
    resource:
      userInfoUri: $OIDC_ISSUE_URL/protocol/openid-connect/userinfo
    userInfoMapping:
      email: email
      firstName: given_name
      lastName: family_name
      username: preferred_username
      roles: groups
EOF

  cat <<EOF | kubectl apply -f -
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  annotations:
    kubernetes.io/ingress.allow-http: "false"
    nginx.ingress.kubernetes.io/ssl-redirect: "true"
    ingress.kubernetes.io/ssl-passthrough: "true"
    nginx.ingress.kubernetes.io/backend-protocol: "HTTP"
  name: spin-deck
  namespace: spinnaker
spec:
  ingressClassName: nginx
  rules:
  - host: spinnaker.cloud.com
    http:
      paths:
      - backend:
          service:
            name: spin-deck
            port:
              number: 9000
        path: /
        pathType: ImplementationSpecific
  tls:
    - secretName: appingress-certificate
      hosts:
        - spinnaker.cloud.com
---
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  annotations:
    kubernetes.io/ingress.allow-http: "false"
    nginx.ingress.kubernetes.io/ssl-redirect: "true"
    ingress.kubernetes.io/ssl-passthrough: "true"
    nginx.ingress.kubernetes.io/backend-protocol: "HTTP"
  name: spin-gate
  namespace: spinnaker
spec:
  ingressClassName: nginx
  rules:
  - host: spin-gate.cloud.com
    http:
      paths:
      - backend:
          service:
            name: spin-gate
            port:
              number: 8084
        path: /
        pathType: ImplementationSpecific
  tls:
    - secretName: appingress-certificate
      hosts:
        - spin-gate.cloud.com
EOF

  gok patch ingress spin-deck spinnaker letsencrypt spinnaker
  gok patch ingress spin-gate spinnaker letsencrypt spin-gate

  docker exec -it halyard hal config security ui edit --override-base-url "https://spinnaker.gokcloud.com"
  docker exec -it halyard hal config security api edit --override-base-url "https://spin-gate.gokcloud.com"
  docker exec -it halyard hal config security api edit --cors-access-pattern "https://spinnaker.gokcloud.com"

  #Authentication
  docker exec -it halyard hal config security authn oauth2 edit --client-id $CLIENT_ID --client-secret $CLIENT_SECRET --provider OTHER --pre-established-redirect-uri https://spin-gate.gokcloud.com/login
  docker exec -it halyard hal config security authn oauth2 enable

  #Authorization
  #Roles will be fetched from keycloak
  #https://github.com/spinnaker/spinnaker/issues/6498
  #docker exec -it halyard hal config security authz file edit --file-path /home/spinnaker/.hal/permission.yml
  docker exec -it halyard hal config security authz enable

  docker exec -it halyard hal config artifact github account add sumitmaji

  kubectl get secrets -n cert-manager gokselfsign-ca -o json | jq -r '.data["tls.crt"]' | base64 -d > /home/spinnaker/.hal/issuer.crt
  kubectl get secrets -n keycloak keycloak-gokcloud-com -o json | jq -r '.data["tls.crt"]' | base64 -d > /home/spinnaker/.hal/keycloak.crt
  kubectl get secrets -n jenkins jenkins-gokcloud-com -o json | jq -r '.data["tls.crt"]' | base64 -d > /home/spinnaker/.hal/jenkins.crt
  
  docker exec -it halyard  cp /etc/ssl/certs/java/cacerts /home/spinnaker/.hal/
  chmod +w /home/spinnaker/.hal/cacerts
  #password is changeit
  docker exec -it halyard keytool -import -noprompt -trustcacerts -alias ca -file /home/spinnaker/.hal/issuer.crt -keystore /home/spinnaker/.hal/cacerts
  docker exec -it halyard keytool -import -noprompt -trustcacerts -alias keycloak -file /home/spinnaker/.hal/keycloak.crt -keystore /home/spinnaker/.hal/cacerts
  docker exec -it halyard keytool -import -noprompt -trustcacerts -alias jenkins -file /home/spinnaker/.hal/jenkins.crt -keystore /home/spinnaker/.hal/cacerts

  kubectl create secret generic -n spinnaker internal-trust-store \
  	--from-file /home/spinnaker/.hal/cacerts

  #https://spinnaker.io/docs/reference/halyard/custom/
  mkdir -p /home/spinnaker/.hal/default/service-settings/
  cat <<EOF > /home/spinnaker/.hal/default/service-settings/gate.yml
kubernetes:
  volumes:
  - id: internal-trust-store
    mountPath: /etc/ssl/certs/java
    type: secret
EOF

  cat <<EOF > /home/spinnaker/.hal/default/service-settings/igor.yml
kubernetes:
  volumes:
  - id: internal-trust-store
    mountPath: /etc/ssl/certs/java
    type: secret
EOF

  docker exec -it halyard hal config artifact github enable
  docker exec -it halyard hal config artifact github account add gok-github

  docker exec -it halyard hal config artifact helm enable
  docker exec -it halyard hal config artifact helm account add gok-helm \
    --repository https://chart.gokcloud.com --username sumit --password abcdef


  docker exec -it halyard hal deploy apply
}

rabbitmqInst(){
  echo "Installing RabbitMQ using Cluster Operator..."
  
  kubectl create namespace rabbitmq || true
  createLocalStorageClassAndPV "rabbitmq-storage" "rabbitmq-pv" "/data/volumes/rabbitmq"
  
  # Install RabbitMQ Cluster Operator
  kubectl apply -f "https://github.com/rabbitmq/cluster-operator/releases/latest/download/cluster-operator.yml"
  
  # Wait for operator to be ready
  kubectl wait --for=condition=available deployment/rabbitmq-cluster-operator --timeout=300s -n rabbitmq-system
  
  # Create RabbitMQ cluster
  cat <<EOF | kubectl apply -f -
apiVersion: rabbitmq.com/v1beta1
kind: RabbitmqCluster
metadata:
  name: rabbitmq
  namespace: rabbitmq
spec:
  replicas: 1
  image: rabbitmq:3.12-management
  resources:
    requests:
      cpu: 256m
      memory: 1Gi
    limits:
      cpu: 1
      memory: 2Gi
  rabbitmq:
    additionalConfig: |
      log.console.level = info
      channel_max = 1700
      default_user_tags.administrator = true
      management.tcp.port = 15672
  persistence:
    storageClassName: rabbitmq-storage
    storage: 10Gi
  service:
    type: ClusterIP
  override:
    statefulSet:
      spec:
        template:
          spec:
            containers:
            - name: rabbitmq
              ports:
              - containerPort: 5672
                name: amqp
              - containerPort: 15672
                name: management
EOF

  # Create ingress for management UI
  cat <<EOF | kubectl apply -f -
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: rabbitmq-management
  namespace: rabbitmq
  annotations:
    nginx.ingress.kubernetes.io/rewrite-target: /
spec:
  ingressClassName: nginx
  rules:
  - host: rabbitmq.$(rootDomain)
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: rabbitmq
            port:
              number: 15672
EOF

  # Wait for RabbitMQ to be ready - use the correct condition
  echo "Waiting for RabbitMQ cluster to be ready..."
  kubectl wait --for=condition=AllReplicasReady rabbitmqcluster/rabbitmq -n rabbitmq --timeout=600s
  
  # Alternative: Wait for the StatefulSet to be ready as well
  kubectl wait --for=condition=ready pod -l app.kubernetes.io/name=rabbitmq -n rabbitmq --timeout=300s
  
  # Get default credentials
  echo "RabbitMQ default credentials:"
  echo "Username: $(kubectl get secret rabbitmq-default-user -n rabbitmq -o jsonpath='{.data.username}' | base64 --decode)"
  echo "Password: $(kubectl get secret rabbitmq-default-user -n rabbitmq -o jsonpath='{.data.password}' | base64 --decode)"
  
  gok patch ingress rabbitmq-management rabbitmq letsencrypt rabbitmq
}

rabbitmqReset(){
  # Delete RabbitMQ cluster
  kubectl delete rabbitmqcluster rabbitmq -n rabbitmq --ignore-not-found=true
  
  # Delete operator (optional - might affect other clusters)
  kubectl delete -f "https://github.com/rabbitmq/cluster-operator/releases/latest/download/cluster-operator.yml"
  
  # Clean up namespace and storage
  kubectl delete namespace rabbitmq --ignore-not-found=true
  emptyLocalFsStorage "RabbitMQ" "rabbitmq-pv" "rabbitmq-storage" "/data/volumes/rabbitmq"
}


rabbitmqResetLegacy(){
  kubectl delete all --all -n rabbitmq
  kubectl delete ns rabbitmq
  emptyLocalFsStorage "RabbitMQ" "rabbitmq-pv" "rabbitmq-storage" "/data/volumes/rabbitmq" "rabbitmq"
}

rabbitmqInstLegacy(){
  helm repo add bitnami https://charts.bitnami.com/bitnami
  helm repo update

  kubectl create namespace rabbitmq || true
  createLocalStorageClassAndPV "rabbitmq-storage" "rabbitmq-pv" "/data/volumes/rabbitmq"
  
  helm install rabbitmq bitnami/rabbitmq \
    --namespace rabbitmq \
    --create-namespace \
    --values "${MOUNT_PATH}"/kubernetes/install_k8s/rabbitmq/values.yaml

  kubectl get secret --namespace rabbitmq rabbitmq -o jsonpath="{.data.rabbitmq-password}" | base64 --decode
  echo
}


argocdReset(){
  helm uninstall argocd -n argocd
  kubectl delete ns argocd
  # emptyLocalFsStorage "ArgoCD" "argocd-pv" "argocd-storage" "/data/volumes/argocd" "argocd"
}

argocdInst(){
  CLIENT_SECRET=$(dataFromSecret oauth-secrets kube-system OIDC_CLIENT_SECRET)
  CLIENT_ID=$(dataFromSecret oauth-secrets kube-system OIDC_CLIENT_ID)
  REALM_NAME=$(dataFromSecret oauth-secrets kube-system OAUTH_REALM)
  OAUTH2_HOST="$(fullKeycloakUrl)"
  OAUTH2_OIDC_ISSUE_URL="$(dataFromSecret oauth-secrets kube-system OIDC_ISSUE_URL)"

  kubectl create ns argocd
  helm repo add argo https://argoproj.github.io/argo-helm
  helm repo update
  helm install argocd argo/argo-cd --namespace argocd \
    --set configs.secret.extra."oidc\.keycloak\.clientSecret"="${CLIENT_SECRET}" \
    --set configs.secret.argocdServerAdminPassword="sumit" \
    --values "${MOUNT_PATH}"/kubernetes/install_k8s/argocd/values.yaml

  gok patch ingress argocd-server argocd letsencrypt $(argocdSubdomain)
 
  #kubectl create secret generic argocd-secret -n argocd \
  #--from-literal=oidc.clientId=$CLIENT_ID \
  #--from-literal=oidc.clientSecret=$CLIENT_SECRET \
  #--from-literal=oidc.issuerUrl=$OAUTH2_HOST \
  #--from-literal=oidc.scopes="openid email profile groups" \
  #--from-literal=oidc.usernameClaim=email \
  #--from-literal=oidc.groupsClaim=groups
}

getUserInfo(){
  curl https://keycloak.gokcloud.com/realms/GokDevelopers/protocol/openid-connect/auth?client_id=gok-developers-client&redirect_uri=https://localhost/login&response_type=code&scope=openid%20email%20profile&state=eLUwT2

  TOKEN=$(curl --location --request POST 'https://keycloak.gokcloud.com/realms/GokDevelopers/protocol/openid-connect/token' \
     --header 'Content-Type: application/x-www-form-urlencoded' \
     --data-urlencode 'grant_type=authorization_code' \
     --data-urlencode 'client_id=gok-developers-client' \
     --data-urlencode 'client_secret=sC1hHm9c2qHjwYtfumcQnEwyH7NOqkaV' \
     --data-urlencode 'redirect_uri=https://localhost/login' \
     --data-urlencode 'scope=roles email profile openid' \
     --data-urlencode 'code=2a34daeb-4bba-48fb-9531-69f55104ab62.69c4c286-f2c9-4455-8368-7a8723ad60cf.f79ef798-e122-403f-8626-ad3793bf3f44' | jq -r '.["access_token"]')

  curl -kv 'https://keycloak.gokcloud.com/realms/GokDevelopers/protocol/openid-connect/userinfo' \
  --header 'Content-Type: application/x-www-form-urlencoded' \
  --header "Authorization: Bearer $TOKEN"

}

spinnakerReset(){
  kubectl delete all --all -n spinnaker
  kubectl delete ns spinnaker
  docker stop halyard
  docker rm halyard
  rm -rf /home/spinnaker/.hal
  rm -rf /home/spinnaker/.kube
}

patchLocalTls() {
  NAME=$1
  NS=$2
  kubectl patch ing "$NAME" --patch "$(
    cat <<EOF
spec:
  tls:
    - hosts:
        - $APP_HOST
      secretName: appingress-certificate
EOF
  )" -n "$NS"
  kubectl patch ing "$NAME" --type=json -p='[{"op": "replace", "path": "/spec/rules/0/host", "value":"master.cloud.com"}]' -n "$NS"
  kubectl patch ing "$NAME" --type=json -p='[{"op": "add", "path": "/metadata/annotations", "value":{"nginx.ingress.kubernetes.io/rewrite-target": "/", "kubernetes.io/ingress.class": "nginx"}}]' -n "$NS"
}

#Enable debug logs for Api Server and Kubelet
debugLog(){
  kcd default
  cat << EOT | kubectl apply -f -
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: edit-debug-flags-v
rules:
- apiGroups:
  - ""
  resources:
  - nodes/proxy
  verbs:
  - update
- nonResourceURLs:
  - /debug/flags/v
  verbs:
  - put
EOT

  cat << EOT | kubectl apply -f -
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: edit-debug-flags-v
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: edit-debug-flags-v
subjects:
- kind: ServiceAccount
  name: default
  namespace: default
EOT
  TOKEN=$(kubectl create token default)

  # Change the log level to 5 (trace) for Api Server
  APISERVER=$(apiUrl)
  curl -s -X PUT -d '5' $APISERVER/debug/flags/v --header "Authorization: Bearer $TOKEN" -k

  # Change the log level to 5 (trace) for Kubelet : Currently not working
  docker exec kind-control-plane curl -s -X PUT -d '5' https://localhost:10250/debug/flags/v --header "Authorization: Bearer $TOKEN" -k

}

kubeloginInst(){
  useradd -m -g root linuxbrew
  cd /home/linuxbrew
  su - linuxbrew
  /bin/bash -c "$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)"
  echo 'eval "$(/home/linuxbrew/.linuxbrew/bin/brew shellenv)"' >> /home/linuxbrew/.profile
  eval "$(/home/linuxbrew/.linuxbrew/bin/brew shellenv)"
  brew install kubelogin
  echo 'export PATH="/home/linuxbrew/.linuxbrew/bin:$PATH"' >> /home/linuxbrew/.profile
  echo 'export PATH="/home/linuxbrew/.linuxbrew/sbin:$PATH"' >> /home/linuxbrew/.profile
  brew doctor
  brew install Azure/kubelogin/kubelogin
  # upgrade
  brew update
  brew upgrade Azure/kubelogin/kubelogin
}

kyvernoInst(){
  log_component_start "kyverno" "Installing Kyverno policy engine for Kubernetes governance"
  
  log_step "1" "Adding Kyverno Helm repository"
  execute_with_suppression helm repo add kyverno https://kyverno.github.io/kyverno/
  execute_with_suppression helm repo update
  
  log_step "2" "Installing Kyverno via Helm"
  if helm_install_with_summary "kyverno" "kyverno" \
    kyverno --namespace kyverno kyverno/kyverno --create-namespace; then
    
    log_step "3" "Waiting for Kyverno services to be ready"
    waitForServiceAvailable kyverno
    log_success "Kyverno services are now operational"
    
    log_step "4" "Configuring cluster-admin permissions"
    cat <<EOF | kubectl apply -f - >/dev/null 2>&1
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: kyverno:cloud-cluster-admin
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: cluster-admin
subjects:
- kind: ServiceAccount
  name: kyverno-admission-controller
  namespace: kyverno
- kind: ServiceAccount
  name: kyverno-background-controller
  namespace: kyverno
EOF
    if [[ $? -eq 0 ]]; then
      log_success "Cluster-admin permissions configured for Kyverno"
    else
      log_warning "Cluster-admin permissions may already exist"
    fi

    log_step "5" "Creating secret synchronization policies"
    cat <<EOF | kubectl apply -f - >/dev/null 2>&1
apiVersion: kyverno.io/v1
kind: ClusterPolicy
metadata:
  name: sync-secrets
spec:
  rules:
  - name: sync-image-pull-secret
    match:
      any:
      - resources:
          kinds:
          - Namespace
    generate:
      apiVersion: v1
      kind: Secret
      name: regcred
      namespace: "{{request.object.metadata.name}}"
      synchronize: true
      clone:
        namespace: kube-system
        name: regcred
  - name: sync-oauth-secrets
    match:
      any:
      - resources:
          kinds:
          - Namespace
    generate:
      apiVersion: v1
      kind: Secret
      name: oauth-secrets
      namespace: "{{request.object.metadata.name}}"
      synchronize: true
      clone:
        namespace: kube-system
        name: oauth-secrets
  - name: sync-opensearch-secrets
    match:
      any:
      - resources:
          kinds:
          - Namespace
    generate:
      apiVersion: v1
      kind: Secret
      name: opensearch-secrets
      namespace: "{{request.object.metadata.name}}"
      synchronize: true
      clone:
        namespace: kube-system
        name: opensearch-secrets
EOF
    if [[ $? -eq 0 ]]; then
      log_success "Secret synchronization policies created"
    else
      log_warning "Secret synchronization policies may already exist"
    fi
    
    show_installation_summary "kyverno" "kyverno" "Policy engine with secret synchronization and governance policies"
    log_component_success "kyverno" "Kyverno policy engine installed successfully"
    
    # Call next module suggestion
    suggest_and_install_next_module "kyverno"
  else
    log_error "Kyverno installation failed"
    return 1
  fi
}

kyvernoReset(){
  log_component_start "kyverno-reset" "Removing Kyverno policy engine and policies"
  
  log_step "1" "Removing cluster policies"
  if execute_with_suppression kubectl delete clusterpolicy --all; then
    log_success "All cluster policies removed"
  fi
  
  log_step "2" "Uninstalling Kyverno Helm release"
  if helm_uninstall_with_summary "kyverno" "kyverno" kyverno -n kyverno; then
    log_success "Kyverno Helm release removed"
  fi
  
  log_step "3" "Removing Kyverno namespace"
  if kubectl_with_summary delete "namespace" kyverno; then
    show_installation_summary "kyverno" "kyverno" "Policy engine and governance system removed"
    log_component_success "kyverno-reset" "Kyverno has been completely removed from the cluster"
  else
    log_error "Failed to remove Kyverno namespace"
    return 1
  fi
}

help(){
  IFS='' read -r -d '' HELP <<"EOF"
gok install kubernetes => Installing Kubernetes
gok install ingress => Installing Ingress
gok install dashboard => Installing Dashboard
gok install cert-manager => Installing Cert-Manager
gok install keycloak => Installing Keycloak
gok install registry => Install registry
python3 keycloak-setup.py all => Setup keycloak
helmTemplateDebug $char_name $char_repo = Debug helm template
apiUrl => view url for kubernetes api server
oauthUser => Configure Kubeconfig for OAuth
oauthDev => Create developers role for oauth
oauthAdmin => Create administrators role for oauth

kcd $namespace => switch namespace
k => short form for kubectl
current => Show current namespace
pods => show pods in current namespace
secrets => show secrets in current namespace
viewcert => View certificate in secret in current namespace
decode => Base64 decode content in secret in current namespace
ns => Show all namespaces
edit $resouce => Edit resource in current namespace
logs => View logs of pod in current namespace
bash => Get terminal in pod
all => Show all resource in current namespace
kctl => Execute kubectl command with oauth authentication, create dev and admin role
EOF
echo "$HELP"
}

function bashCmd() {
  pod=$(getpod)
  echo "Opening terminal on $pod"
  kubectl exec -it "$pod" -- /bin/bash
}

function helpCmd() {
    log_header "GOK Platform - Kubernetes Operations Toolkit" "Your Complete Cloud-Native Development & Operations Platform"
    
    echo -e "${COLOR_BRIGHT_BLUE}${COLOR_BOLD}ðŸŽ¯ WHAT IS GOK?${COLOR_RESET}"
    echo -e "${COLOR_CYAN}GOK is an enterprise-grade, comprehensive toolkit for managing Kubernetes clusters"
    echo -e "and deploying cloud-native applications with integrated security, monitoring, and DevOps tools."
    echo -e "It provides a unified interface to deploy and manage 35+ components across your infrastructure.${COLOR_RESET}"
    echo
    
    echo -e "${COLOR_BRIGHT_GREEN}${COLOR_BOLD}ðŸ“‹ CORE COMMANDS${COLOR_RESET}"
    echo
    echo -e "${COLOR_YELLOW}${COLOR_BOLD}Lifecycle Management:${COLOR_RESET}"
    echo -e "  ${COLOR_GREEN}install${COLOR_RESET} <component>     ${COLOR_CYAN}Install and configure 35+ components${COLOR_RESET}"
    echo -e "  ${COLOR_GREEN}reset${COLOR_RESET} <component>       ${COLOR_CYAN}Reset and uninstall components safely${COLOR_RESET}"
    echo -e "  ${COLOR_GREEN}start${COLOR_RESET} <component>       ${COLOR_CYAN}Start system services and processes${COLOR_RESET}"
    echo -e "  ${COLOR_GREEN}deploy${COLOR_RESET} <component>      ${COLOR_CYAN}Deploy applications and services${COLOR_RESET}"
    echo
    echo -e "${COLOR_YELLOW}${COLOR_BOLD}Resource Management:${COLOR_RESET}"
    echo -e "  ${COLOR_GREEN}create${COLOR_RESET} <resource>       ${COLOR_CYAN}Create Kubernetes resources and configurations${COLOR_RESET}"
    echo -e "  ${COLOR_GREEN}generate${COLOR_RESET} <type>         ${COLOR_CYAN}Generate microservice templates and code${COLOR_RESET}"
    echo -e "  ${COLOR_GREEN}patch${COLOR_RESET} <resource>        ${COLOR_CYAN}Patch and modify existing resources${COLOR_RESET}"
    echo
    echo -e "${COLOR_YELLOW}${COLOR_BOLD}Cluster Operations:${COLOR_RESET}"
    echo -e "  ${COLOR_GREEN}bash${COLOR_RESET}                    ${COLOR_CYAN}Open interactive terminal in pods${COLOR_RESET}"
    echo -e "  ${COLOR_GREEN}desc${COLOR_RESET} [methods]          ${COLOR_CYAN}Describe pod details or GOK methods${COLOR_RESET}"
    echo -e "  ${COLOR_GREEN}logs${COLOR_RESET}                    ${COLOR_CYAN}View pod logs and diagnostics${COLOR_RESET}"
    echo -e "  ${COLOR_GREEN}status${COLOR_RESET}                  ${COLOR_CYAN}Check service and Helm release status${COLOR_RESET}"
    echo -e "  ${COLOR_GREEN}taint-node${COLOR_RESET}              ${COLOR_CYAN}Configure node taints and scheduling${COLOR_RESET}"
    echo -e "  ${COLOR_GREEN}completion${COLOR_RESET}              ${COLOR_CYAN}Setup bash tab completion for GOK commands${COLOR_RESET}"
    echo
    
    echo -e "${COLOR_BRIGHT_YELLOW}${COLOR_BOLD}ðŸš€ QUICK START EXAMPLES${COLOR_RESET}"
    echo
    echo -e "${COLOR_MAGENTA}${COLOR_BOLD}Complete Production Cluster Setup:${COLOR_RESET}"
    echo -e "${COLOR_CYAN}  gok install kubernetes        ${COLOR_DIM}# Core Kubernetes cluster with HA${COLOR_RESET}"
    echo -e "${COLOR_CYAN}  gok install cert-manager      ${COLOR_DIM}# Automated TLS certificate management${COLOR_RESET}"
    echo -e "${COLOR_CYAN}  gok install ingress           ${COLOR_DIM}# NGINX ingress controller${COLOR_RESET}"
    echo -e "${COLOR_CYAN}  gok install monitoring        ${COLOR_DIM}# Prometheus + Grafana stack${COLOR_RESET}"
    echo
    echo -e "${COLOR_MAGENTA}${COLOR_BOLD}Security & Identity Management:${COLOR_RESET}"
    echo -e "${COLOR_CYAN}  gok install vault             ${COLOR_DIM}# HashiCorp Vault for secrets${COLOR_RESET}"
    echo -e "${COLOR_CYAN}  gok install keycloak          ${COLOR_DIM}# Identity and access management${COLOR_RESET}"
    echo -e "${COLOR_CYAN}  gok install oauth2            ${COLOR_DIM}# OAuth2 proxy for authentication${COLOR_RESET}"
    echo
    echo -e "${COLOR_MAGENTA}${COLOR_BOLD}Development & DevOps:${COLOR_RESET}"
    echo -e "${COLOR_CYAN}  gok install argocd            ${COLOR_DIM}# GitOps continuous delivery${COLOR_RESET}"
    echo -e "${COLOR_CYAN}  gok install jenkins           ${COLOR_DIM}# CI/CD automation server${COLOR_RESET}"
    echo -e "${COLOR_CYAN}  gok install jupyter           ${COLOR_DIM}# Data science development${COLOR_RESET}"
    echo
    echo -e "${COLOR_MAGENTA}${COLOR_BOLD}Microservice Generation:${COLOR_RESET}"
    echo -e "${COLOR_CYAN}  gok generate python-api user-service    ${COLOR_DIM}# Python REST API template${COLOR_RESET}"
    echo -e "${COLOR_CYAN}  gok generate python-reactjs webapp      ${COLOR_DIM}# Full-stack React+Python app${COLOR_RESET}"
    echo
    echo -e "${COLOR_MAGENTA}${COLOR_BOLD}Resource Creation:${COLOR_RESET}"
    echo -e "${COLOR_CYAN}  gok create certificate production       ${COLOR_DIM}# Production TLS certificates${COLOR_RESET}"
    echo -e "${COLOR_CYAN}  gok create kubeconfig developer         ${COLOR_DIM}# Developer access configuration${COLOR_RESET}"
    echo -e "${COLOR_CYAN}  gok create secret apphost               ${COLOR_DIM}# Application host secrets${COLOR_RESET}"
    echo
    
    echo -e "${COLOR_BRIGHT_MAGENTA}${COLOR_BOLD}ðŸ“š DETAILED HELP & DOCUMENTATION${COLOR_RESET}"
    echo
    echo -e "${COLOR_YELLOW}For comprehensive information about any command, use:${COLOR_RESET}"
    echo
    echo -e "  ${COLOR_GREEN}${COLOR_BOLD}gok install help${COLOR_RESET}        ${COLOR_CYAN}View all 35+ installable components with descriptions${COLOR_RESET}"
    echo -e "  ${COLOR_GREEN}${COLOR_BOLD}gok reset help${COLOR_RESET}          ${COLOR_CYAN}View all 32+ resettable components${COLOR_RESET}"
    echo -e "  ${COLOR_GREEN}${COLOR_BOLD}gok create help${COLOR_RESET}         ${COLOR_CYAN}View resource creation options and templates${COLOR_RESET}"
    echo -e "  ${COLOR_GREEN}${COLOR_BOLD}gok generate help${COLOR_RESET}       ${COLOR_CYAN}View microservice template options${COLOR_RESET}"
    echo -e "  ${COLOR_GREEN}${COLOR_BOLD}gok status${COLOR_RESET}              ${COLOR_CYAN}View complete platform status overview${COLOR_RESET}"
    echo
    
    echo -e "${COLOR_BRIGHT_CYAN}${COLOR_BOLD}â­ PLATFORM FEATURES & CAPABILITIES${COLOR_RESET}"
    echo
    echo -e "${COLOR_GREEN}${EMOJI_SUCCESS} ${COLOR_BOLD}Core Infrastructure:${COLOR_RESET}"
    echo -e "  ${COLOR_CYAN}â€¢ Complete Kubernetes cluster management with HA${COLOR_RESET}"
    echo -e "  ${COLOR_CYAN}â€¢ Automated TLS certificate management (Let's Encrypt + Custom CA)${COLOR_RESET}"
    echo -e "  ${COLOR_CYAN}â€¢ Production-ready ingress with NGINX${COLOR_RESET}"
    echo -e "  ${COLOR_CYAN}â€¢ Comprehensive monitoring (Prometheus/Grafana/AlertManager)${COLOR_RESET}"
    echo
    echo -e "${COLOR_GREEN}${EMOJI_SHIELD} ${COLOR_BOLD}Security & Identity:${COLOR_RESET}"
    echo -e "  ${COLOR_CYAN}â€¢ Enterprise identity management (Keycloak/OAuth2)${COLOR_RESET}"
    echo -e "  ${COLOR_CYAN}â€¢ Secrets management (HashiCorp Vault integration)${COLOR_RESET}"
    echo -e "  ${COLOR_CYAN}â€¢ LDAP directory services${COLOR_RESET}"
    echo -e "  ${COLOR_CYAN}â€¢ RBAC and policy enforcement (Kyverno)${COLOR_RESET}"
    echo
    echo -e "${COLOR_GREEN}${EMOJI_TOOLS} ${COLOR_BOLD}Development & DevOps:${COLOR_RESET}"
    echo -e "  ${COLOR_CYAN}â€¢ GitOps deployment (ArgoCD)${COLOR_RESET}"
    echo -e "  ${COLOR_CYAN}â€¢ CI/CD automation (Jenkins/Spinnaker)${COLOR_RESET}"
    echo -e "  ${COLOR_CYAN}â€¢ Development environments (JupyterHub/Eclipse Che)${COLOR_RESET}"
    echo -e "  ${COLOR_CYAN}â€¢ Microservice code generation${COLOR_RESET}"
    echo
    echo -e "${COLOR_GREEN}${EMOJI_NETWORK} ${COLOR_BOLD}Service Mesh & Networking:${COLOR_RESET}"
    echo -e "  ${COLOR_CYAN}â€¢ Service mesh ready (Istio integration)${COLOR_RESET}"
    echo -e "  ${COLOR_CYAN}â€¢ Message broker (RabbitMQ)${COLOR_RESET}"
    echo -e "  ${COLOR_CYAN}â€¢ Container registry${COLOR_RESET}"
    echo -e "  ${COLOR_CYAN}â€¢ Advanced networking and traffic management${COLOR_RESET}"
    echo
    
    echo -e "${COLOR_BRIGHT_BLUE}${COLOR_BOLD}ðŸŒ SUPPORTED PLATFORMS & REQUIREMENTS${COLOR_RESET}"
    echo
    echo -e "${COLOR_YELLOW}${COLOR_BOLD}Operating Systems:${COLOR_RESET}   ${COLOR_CYAN}Ubuntu 18.04+, Debian 9+, CentOS 7+${COLOR_RESET}"
    echo -e "${COLOR_YELLOW}${COLOR_BOLD}Cloud Providers:${COLOR_RESET}     ${COLOR_CYAN}AWS, GCP, Azure, On-premises, Edge computing${COLOR_RESET}"
    echo -e "${COLOR_YELLOW}${COLOR_BOLD}Container Runtime:${COLOR_RESET}   ${COLOR_CYAN}Docker, containerd, CRI-O${COLOR_RESET}"
    echo -e "${COLOR_YELLOW}${COLOR_BOLD}Kubernetes:${COLOR_RESET}          ${COLOR_CYAN}1.20+ (supports latest versions)${COLOR_RESET}"
    echo -e "${COLOR_YELLOW}${COLOR_BOLD}Hardware:${COLOR_RESET}            ${COLOR_CYAN}Minimum 4GB RAM, 2 CPU cores (8GB+ recommended)${COLOR_RESET}"
    echo
    
    echo -e "${COLOR_BRIGHT_GREEN}${COLOR_BOLD}ðŸŽ¯ COMPLETE SETUP GUIDE${COLOR_RESET}"
    echo
    echo -e "${COLOR_MAGENTA}${COLOR_BOLD}Phase 1 - Core Infrastructure:${COLOR_RESET}"
    echo -e "${COLOR_CYAN}  1. ${COLOR_BOLD}gok install docker${COLOR_RESET}              ${COLOR_DIM}# Container runtime${COLOR_RESET}"
    echo -e "${COLOR_CYAN}  2. ${COLOR_BOLD}gok install kubernetes${COLOR_RESET}          ${COLOR_DIM}# Core K8s cluster${COLOR_RESET}"
    echo -e "${COLOR_CYAN}  3. ${COLOR_BOLD}gok install cert-manager${COLOR_RESET}        ${COLOR_DIM}# Certificate management${COLOR_RESET}"
    echo -e "${COLOR_CYAN}  4. ${COLOR_BOLD}gok install ingress${COLOR_RESET}             ${COLOR_DIM}# Traffic routing${COLOR_RESET}"
    echo
    echo -e "${COLOR_MAGENTA}${COLOR_BOLD}Phase 2 - Monitoring & Security:${COLOR_RESET}"
    echo -e "${COLOR_CYAN}  5. ${COLOR_BOLD}gok install monitoring${COLOR_RESET}          ${COLOR_DIM}# Observability stack${COLOR_RESET}"
    echo -e "${COLOR_CYAN}  6. ${COLOR_BOLD}gok install vault${COLOR_RESET}               ${COLOR_DIM}# Secrets management${COLOR_RESET}"
    echo -e "${COLOR_CYAN}  7. ${COLOR_BOLD}gok install keycloak${COLOR_RESET}            ${COLOR_DIM}# Identity management${COLOR_RESET}"
    echo
    echo -e "${COLOR_MAGENTA}${COLOR_BOLD}Phase 3 - Development & DevOps:${COLOR_RESET}"
    echo -e "${COLOR_CYAN}  8. ${COLOR_BOLD}gok install argocd${COLOR_RESET}              ${COLOR_DIM}# GitOps deployment${COLOR_RESET}"
    echo -e "${COLOR_CYAN}  9. ${COLOR_BOLD}gok install jupyter${COLOR_RESET}             ${COLOR_DIM}# Development environment${COLOR_RESET}"
    echo -e "${COLOR_CYAN} 10. ${COLOR_BOLD}gok install gok-controller${COLOR_RESET}      ${COLOR_DIM}# GOK platform services${COLOR_RESET}"
    echo
    
    log_next_steps "After Installation" \
        "Check platform status: ${COLOR_BOLD}gok status${COLOR_RESET}" \
        "Access services via ingress URLs (check gok status for URLs)" \
        "Configure authentication and RBAC as needed" \
        "Generate your first microservice: ${COLOR_BOLD}gok generate python-api my-service${COLOR_RESET}" \
        "Deploy applications using ArgoCD or direct kubectl"
    
    echo -e "${COLOR_BRIGHT_CYAN}${COLOR_BOLD}ðŸ“ž SUPPORT & RESOURCES${COLOR_RESET}"
    echo -e "${COLOR_CYAN}â€¢ Documentation: ${COLOR_BOLD}https://github.com/sumitmaji/kubernetes${COLOR_RESET}"
    echo -e "${COLOR_CYAN}â€¢ Issues & Support: ${COLOR_BOLD}https://github.com/sumitmaji/kubernetes/issues${COLOR_RESET}"
    echo -e "${COLOR_CYAN}â€¢ Community: Join our Kubernetes community for help and discussions${COLOR_RESET}"
    echo
    
    echo -e "${COLOR_BRIGHT_MAGENTA}${COLOR_BOLD}âš¡ SMART CACHING SYSTEM${COLOR_RESET}"
    echo -e "${COLOR_CYAN}GOK intelligently caches system updates and dependency installations to avoid redundant operations.${COLOR_RESET}"
    echo
    echo -e "${COLOR_YELLOW}${COLOR_BOLD}Cache Management:${COLOR_RESET}"
    echo -e "${COLOR_CYAN}  gok cache status      ${COLOR_DIM}# Check cache age for updates & dependencies${COLOR_RESET}"
    echo -e "${COLOR_CYAN}  gok cache clear       ${COLOR_DIM}# Clear all caches (force fresh installs)${COLOR_RESET}"
    echo 
    echo -e "${COLOR_YELLOW}${COLOR_BOLD}Installation Flags:${COLOR_RESET}"
    echo -e "${COLOR_CYAN}  gok install <component>                ${COLOR_DIM}# Smart caching (default)${COLOR_RESET}"
    echo -e "${COLOR_CYAN}  gok install <component> --force-update ${COLOR_DIM}# Force system update${COLOR_RESET}"
    echo -e "${COLOR_CYAN}  gok install <component> --skip-update  ${COLOR_DIM}# Skip system update${COLOR_RESET}"
    echo -e "${COLOR_CYAN}  gok install <component> --force-deps   ${COLOR_DIM}# Force dependency installation${COLOR_RESET}"
    echo -e "${COLOR_CYAN}  gok install <component> --skip-deps    ${COLOR_DIM}# Skip dependency installation${COLOR_RESET}"
    echo
    echo -e "${COLOR_YELLOW}${COLOR_BOLD}Configuration Options:${COLOR_RESET}"
    echo -e "${COLOR_CYAN}  GOK_UPDATE_CACHE_HOURS=6  ${COLOR_DIM}# Cache system updates for 6 hours (default)${COLOR_RESET}"
    echo -e "${COLOR_CYAN}  GOK_DEPS_CACHE_HOURS=12   ${COLOR_DIM}# Cache dependencies for 12 hours${COLOR_RESET}"
    echo -e "${COLOR_CYAN}  GOK_UPDATE_CACHE_HOURS=1  ${COLOR_DIM}# Cache updates for 1 hour${COLOR_RESET}"
    echo -e "${COLOR_CYAN}  GOK_CACHE_DIR=/custom/path ${COLOR_DIM}# Custom cache directory${COLOR_RESET}"
    echo
    echo -e "${COLOR_YELLOW}${COLOR_BOLD}Examples:${COLOR_RESET}"
    echo -e "${COLOR_CYAN}  # Development - Skip all installations for speed${COLOR_RESET}"
    echo -e "${COLOR_CYAN}  gok install base --skip-update --skip-deps${COLOR_RESET}"
    echo
    echo -e "${COLOR_CYAN}  # Production - Force fresh installations${COLOR_RESET}"
    echo -e "${COLOR_CYAN}  gok install kubernetes --force-update --force-deps${COLOR_RESET}"
    echo
    echo -e "${COLOR_CYAN}  # Custom cache durations${COLOR_RESET}"
    echo -e "${COLOR_CYAN}  GOK_UPDATE_CACHE_HOURS=24 GOK_DEPS_CACHE_HOURS=48 gok install docker${COLOR_RESET}"
    echo
    echo -e "${COLOR_YELLOW}${COLOR_BOLD}Performance Impact:${COLOR_RESET}"
    echo -e "${COLOR_GREEN}â€¢ First install: Full system update (~8s) + dependencies (~10s) = ~18s${COLOR_RESET}"
    echo -e "${COLOR_GREEN}â€¢ Subsequent installs: Skip both (instant, saves ~18s per component)${COLOR_RESET}"
    echo -e "${COLOR_GREEN}â€¢ Installing 5 components: Saves ~72 seconds with smart caching${COLOR_RESET}"
    echo
    
    echo -e "${COLOR_BRIGHT_YELLOW}${COLOR_BOLD}ðŸ’¡ PRO TIPS${COLOR_RESET}"
    echo -e "${COLOR_YELLOW}â€¢ Use ${COLOR_BOLD}gok completion setup${COLOR_RESET} for tab completion${COLOR_RESET}"
    echo -e "${COLOR_YELLOW}â€¢ Run ${COLOR_BOLD}gok status${COLOR_RESET} regularly to monitor your platform${COLOR_RESET}"
    echo -e "${COLOR_YELLOW}â€¢ Install components in the recommended order for best results${COLOR_RESET}"
    echo -e "${COLOR_YELLOW}â€¢ Use ${COLOR_BOLD}gok logs${COLOR_RESET} for troubleshooting issues${COLOR_RESET}"
    echo -e "${COLOR_YELLOW}â€¢ Add ${COLOR_BOLD}--verbose${COLOR_RESET} or ${COLOR_BOLD}-v${COLOR_RESET} flag to show detailed system logs and command output${COLOR_RESET}"
    echo -e "${COLOR_YELLOW}â€¢ Use ${COLOR_BOLD}gok cache status${COLOR_RESET} to check when your last system update was performed${COLOR_RESET}"
    echo -e "${COLOR_YELLOW}â€¢ Set ${COLOR_BOLD}GOK_UPDATE_CACHE_HOURS=1${COLOR_RESET} for frequent updates or ${COLOR_BOLD}GOK_UPDATE_CACHE_HOURS=24${COLOR_RESET} for daily updates${COLOR_RESET}"
    echo
}

function descCmd() {
  TARGET=$1
  
  if [ "$TARGET" == "methods" ] || [ "$TARGET" == "functions" ] || [ "$TARGET" == "all" ]; then
    describeAllMethods
  elif [ "$TARGET" == "help" ] || [ "$TARGET" == "--help" ]; then
    echo "gok desc - Describe pods or GOK methods"
    echo ""
    echo "Usage:"
    echo "  gok desc              # Describe a selected pod (interactive)"
    echo "  gok desc methods      # Describe all GOK methods and functions"
    echo "  gok desc functions    # Same as methods"
    echo "  gok desc all          # Same as methods"
    echo ""
    echo "Examples:"
    echo "  gok desc              # Interactive pod description"
    echo "  gok desc methods      # Technical method documentation"
    return 0
  else
    # Traditional pod description functionality
    pod=$(getpod)
    echo "Describing pod $pod"
    kubectl describe po "$pod"
  fi
}

function describeAllMethods() {
  echo "GOK - Method Reference Documentation"
  echo "===================================="
  echo ""
  echo "This document describes all methods, functions, and their parameters"
  echo "available in the GOK Kubernetes Operations Toolkit."
  echo ""
  echo "COMMAND INTERFACE METHODS:"
  echo "========================="
  echo ""
  
  echo "helpCmd()"
  echo "â”œâ”€ Purpose: Display main help and documentation"
  echo "â”œâ”€ Parameters: None"
  echo "â”œâ”€ Usage: gok help | gok --help | gok -h"
  echo "â””â”€ Returns: Comprehensive toolkit overview and usage guide"
  echo ""
  
  echo "installCmd(\$COMPONENT)"
  echo "â”œâ”€ Purpose: Install and configure Kubernetes components"
  echo "â”œâ”€ Parameters:"
  echo "â”‚  â””â”€ \$COMPONENT: Component name to install"
  echo "â”œâ”€ Supported Components: 35+ including:"
  echo "â”‚  â”œâ”€ Core: docker, kubernetes, kubernetes-worker, cert-manager, ingress, dashboard"
  echo "â”‚  â”œâ”€ Monitoring: monitoring, fluentd, opensearch"
  echo "â”‚  â”œâ”€ Security: keycloak, oauth2, vault, ldap"
  echo "â”‚  â”œâ”€ Development: jupyter, devworkspace, workspace, che, ttyd, cloudshell, console"
  echo "â”‚  â”œâ”€ CI/CD: argocd, jenkins, spinnaker, registry"
  echo "â”‚  â”œâ”€ Networking: istio, rabbitmq"
  echo "â”‚  â”œâ”€ Policy: kyverno"
  echo "â”‚  â”œâ”€ GOK Platform: gok-agent, gok-controller, controller, gok-login, chart"
  echo "â”‚  â””â”€ Solutions: base, base-services"
  echo "â”œâ”€ Usage: gok install <component>"
  echo "â””â”€ Returns: Installation status and configuration"
  echo ""
  
  echo "resetCmd(\$COMPONENT)"
  echo "â”œâ”€ Purpose: Reset and uninstall Kubernetes components"
  echo "â”œâ”€ Parameters:"
  echo "â”‚  â””â”€ \$COMPONENT: Component name to reset/uninstall"
  echo "â”œâ”€ Supported Components: 32+ (same as install minus docker)"
  echo "â”œâ”€ Usage: gok reset <component>"
  echo "â”œâ”€ Warning: Destructive operation - permanent data loss"
  echo "â””â”€ Returns: Reset status and cleanup confirmation"
  echo ""
  
  echo "createCmd(\$RESOURCE, \$NAME, \$ADDITIONAL)"
  echo "â”œâ”€ Purpose: Create Kubernetes resources and configurations"
  echo "â”œâ”€ Parameters:"
  echo "â”‚  â”œâ”€ \$RESOURCE: Resource type (secret, certificate, kubeconfig)"
  echo "â”‚  â”œâ”€ \$NAME: Resource name or identifier"
  echo "â”‚  â””â”€ \$ADDITIONAL: Optional additional parameters"
  echo "â”œâ”€ Resource Types:"
  echo "â”‚  â”œâ”€ secret: Creates Kubernetes secrets (apphost supported)"
  echo "â”‚  â”œâ”€ certificate: Creates TLS certificates for namespaces"
  echo "â”‚  â””â”€ kubeconfig: Creates kubeconfig files for users"
  echo "â”œâ”€ Usage: gok create <resource> <name> [additional]"
  echo "â””â”€ Returns: Resource creation status and details"
  echo ""
  
  echo "generateCmd(\$SERVICE_TYPE, \$SERVICE_NAME, \$SERVICE_DESC, \$SERVICE_NS, \$SERVICE_HOST)"
  echo "â”œâ”€ Purpose: Generate microservice templates from predefined patterns"
  echo "â”œâ”€ Parameters:"
  echo "â”‚  â”œâ”€ \$SERVICE_TYPE: Template type (python-api, python-reactjs)"
  echo "â”‚  â”œâ”€ \$SERVICE_NAME: Name of the service to generate"
  echo "â”‚  â”œâ”€ \$SERVICE_DESC: Optional service description"
  echo "â”‚  â”œâ”€ \$SERVICE_NS: Optional Kubernetes namespace"
  echo "â”‚  â””â”€ \$SERVICE_HOST: Optional ingress hostname"
  echo "â”œâ”€ Service Types:"
  echo "â”‚  â”œâ”€ python-api: Python Flask REST API (backend-only)"
  echo "â”‚  â””â”€ python-reactjs: Python Flask + React.js (full-stack)"
  echo "â”œâ”€ Generated Features: OAuth2, RBAC, TLS, health checks, API docs"
  echo "â”œâ”€ Usage: gok generate <type> <name> [description] [namespace] [hostname]"
  echo "â””â”€ Returns: Complete microservice with deployment configuration"
  echo ""
  
  echo "startCmd(\$COMPONENT)"
  echo "â”œâ”€ Purpose: Start system services and components"
  echo "â”œâ”€ Parameters:"
  echo "â”‚  â””â”€ \$COMPONENT: Service name (kubernetes, kubelet, proxy/ha, docker, containerd)"
  echo "â”œâ”€ Usage: gok start <component>"
  echo "â”œâ”€ Supported Components:"
  echo "â”‚  â”œâ”€ kubernetes  - Start complete Kubernetes cluster"
  echo "â”‚  â”œâ”€ kubelet     - Start kubelet service"
  echo "â”‚  â”œâ”€ proxy/ha    - Start HAProxy load balancer"
  echo "â”‚  â”œâ”€ docker      - Start Docker service"
  echo "â”‚  â””â”€ containerd  - Start containerd service"
  echo "â””â”€ Returns: Service startup status with detailed error reporting"
  echo ""
  
  echo "deployCmd(\$COMPONENT)"
  echo "â”œâ”€ Purpose: Deploy applications and services"
  echo "â”œâ”€ Parameters:"
  echo "â”‚  â””â”€ \$COMPONENT: Application name (currently supports: app1)"
  echo "â”œâ”€ Usage: gok deploy <component>"
  echo "â””â”€ Returns: Deployment status and application details"
  echo ""
  
  echo "patchCmd(\$RESOURCE, \$NAME, \$NAMESPACE, \$OPTIONS, \$SUBDOMAIN)"
  echo "â”œâ”€ Purpose: Patch and modify existing Kubernetes resources"
  echo "â”œâ”€ Parameters:"
  echo "â”‚  â”œâ”€ \$RESOURCE: Resource type (currently supports: ingress)"
  echo "â”‚  â”œâ”€ \$NAME: Resource name"
  echo "â”‚  â”œâ”€ \$NAMESPACE: Kubernetes namespace"
  echo "â”‚  â”œâ”€ \$OPTIONS: Patch options (letsencrypt, ldap, localtls)"
  echo "â”‚  â””â”€ \$SUBDOMAIN: Optional subdomain configuration"
  echo "â”œâ”€ Usage: gok patch <resource> <name> <namespace> <options> [subdomain]"
  echo "â””â”€ Returns: Patch operation status"
  echo ""
  
  echo "descCmd([\$TARGET])"
  echo "â”œâ”€ Purpose: Describe pods or GOK methods"
  echo "â”œâ”€ Parameters:"
  echo "â”‚  â””â”€ \$TARGET: Optional target (methods, functions, all, or empty for pod)"
  echo "â”œâ”€ Usage: gok desc [methods|functions|all]"
  echo "â””â”€ Returns: Pod description or method documentation"
  echo ""
  
  echo "logsCmd()"
  echo "â”œâ”€ Purpose: View pod logs and diagnostics"
  echo "â”œâ”€ Parameters: None (interactive pod selection)"
  echo "â”œâ”€ Usage: gok logs"
  echo "â””â”€ Returns: Pod log output"
  echo ""
  
  echo "bashCmd()"
  echo "â”œâ”€ Purpose: Open interactive terminal in pods"
  echo "â”œâ”€ Parameters: None (interactive pod selection)"
  echo "â”œâ”€ Usage: gok bash"
  echo "â””â”€ Returns: Interactive shell session"
  echo ""
  
  echo "statusCmd()"
  echo "â”œâ”€ Purpose: Check Helm release status"
  echo "â”œâ”€ Parameters: Uses global \$release variable"
  echo "â”œâ”€ Usage: gok status"
  echo "â””â”€ Returns: Helm release status information"
  echo ""
  
  echo "taintNodeCmd()"
  echo "â”œâ”€ Purpose: Configure node taints and scheduling"
  echo "â”œâ”€ Parameters: None (interactive configuration)"
  echo "â”œâ”€ Usage: gok taint-node"
  echo "â””â”€ Returns: Node taint configuration status"
  echo ""
  
  echo ""
  echo "INTERNAL UTILITY FUNCTIONS:"
  echo "==========================="
  echo ""
  
  echo "updateSys()"
  echo "â”œâ”€ Purpose: Update system packages and dependencies"
  echo "â”œâ”€ Called by: installCmd()"
  echo "â””â”€ Returns: System update status"
  echo ""
  
  echo "installDeps()"
  echo "â”œâ”€ Purpose: Install required system dependencies"
  echo "â”œâ”€ Called by: installCmd()"
  echo "â””â”€ Returns: Dependency installation status"
  echo ""
  
  echo "getpod()"
  echo "â”œâ”€ Purpose: Interactive pod selection utility"
  echo "â”œâ”€ Called by: descCmd(), logsCmd(), bashCmd()"
  echo "â””â”€ Returns: Selected pod name"
  echo ""
  
  echo "taintNode()"
  echo "â”œâ”€ Purpose: Remove NoSchedule taint from master node for pod scheduling"
  echo "â”œâ”€ Called by: taintNodeCmd(), k8sInst() (integrated)"
  echo "â””â”€ Returns: Node taint removal status"
  echo ""
  
  echo ""
  echo "COMPONENT-SPECIFIC INSTALLATION FUNCTIONS:"
  echo "=========================================="
  echo ""
  
  echo "dockrInst() - Docker installation and configuration"
  echo "k8sInst(\$TYPE) - Kubernetes cluster setup (\$TYPE: kubernetes|kubernetes-worker)"
  echo "k8sSummary() - Comprehensive Kubernetes installation status and summary"
  echo "ingressSummary() - Comprehensive NGINX Ingress Controller status and summary"
  echo "ingressReset() - Comprehensive NGINX Ingress Controller reset and cleanup"
  echo "haInst() - High availability proxy installation"
  echo "helmInst() - Helm package manager installation"
  echo "calicoInst() - Calico network plugin installation"
  echo "certmanagerInst() - Certificate manager installation"
  echo "setupCertiIssuers() - Certificate issuers configuration"
  echo "ingressInst() - NGINX ingress controller installation"
  echo "installPrometheusGrafanaWithCertMgr() - Monitoring stack installation"
  echo "installKeycloakWithCertMgr() - Identity management installation"
  echo "installRegistryWithCertMgr() - Container registry installation"
  echo "installDashboardwithCertManager() - Kubernetes dashboard installation"
  echo "jupyterHubInst() - JupyterHub installation"
  echo "argocdInst() - ArgoCD GitOps installation"
  echo "jenkinsInst() - Jenkins CI/CD installation"
  echo "spinnakerInst() - Spinnaker deployment platform installation"
  echo "fluentdInst() - Fluentd logging installation"
  echo "opensearchInst() - OpenSearch analytics installation"
  echo "opensearchDashInst() - OpenSearch dashboard installation"
  echo "vaultInstall() - HashiCorp Vault installation"
  echo "oauth2ProxyInst() - OAuth2 proxy installation"
  echo "istioInst() - Istio service mesh installation"
  echo "kyvernoInst() - Kyverno policy engine installation"
  echo "rabbitmqInst() - RabbitMQ message broker installation"
  echo "eclipseCheInst() - Eclipse Che IDE installation"
  echo "ttydInst() - Terminal over HTTP installation"
  echo "cloudshellInst() - Cloud shell installation"
  echo "consoleInst() - Web console installation"
  echo "createDevWorkspace() - Legacy developer workspace creation"
  echo "createDevWorkspaceV2() - Enhanced developer workspace creation"
  echo "gokAgentInstall() - GOK agent component installation"
  echo "gokControllerInstall() - GOK controller component installation"
  echo "gokLoginInst() - GOK authentication service installation"
  echo "chartInst() - Helm chart repository setup"
  echo "baseInst() - Base system components installation"
  echo "installBaseServices() - Complete base services stack installation"
  echo "installLdap() - LDAP directory service installation"
  echo ""
  
  echo ""
  echo "COMPONENT-SPECIFIC RESET FUNCTIONS:"
  echo "===================================="
  echo ""
  
  echo "prometheusGrafanaResetv2() - Monitoring stack reset"
  echo "dashboardReset() - Kubernetes dashboard reset"
  echo "keycloakReset() - Keycloak identity management reset"
  echo "vaultReset() - Vault secrets management reset"
  echo "ingressReset() - NGINX Ingress Controller comprehensive reset"
  echo "resetChart() - Helm chart repository reset"
  echo "ldapReset() - LDAP service reset"
  echo "gokAgentReset() - GOK agent reset"
  echo "gokControllerReset() - GOK controller reset"
  echo "argocdReset() - ArgoCD reset"
  echo "deleteDevWorkspace() - Legacy workspace deletion"
  echo "deleteDevWorkspaceV2() - Enhanced workspace deletion"
  echo "resetDockerRegistry() - Container registry reset"
  echo "fluentdReset() - Fluentd logging reset"
  echo "jupyterHubReset() - JupyterHub reset"
  echo "rabbitmqReset() - RabbitMQ reset"
  echo "resetEclipseChe() - Eclipse Che reset"
  echo "ttydReset() - Terminal service reset"
  echo "cloudshellReset() - Cloud shell reset"
  echo "consoleReset() - Web console reset"
  echo "opensearchReset() - OpenSearch reset"
  echo "opensearchDashReset() - OpenSearch dashboard reset"
  echo "jenkinsReset() - Jenkins reset"
  echo "spinnakerReset() - Spinnaker reset"
  echo "oauth2ProxyReset() - OAuth2 proxy reset"
  echo "certManagerReset() - Certificate manager reset"
  echo "istioReset() - Istio service mesh reset"
  echo "kyvernoReset() - Kyverno policy engine reset"
  echo "resetBaseServices() - Complete base services reset"
  echo ""
  
  echo ""
  echo "CONFIGURATION AND UTILITY FUNCTIONS:"
  echo "====================================="
  echo ""
  
  echo "getOAuth0Config() - Auth0 OAuth configuration"
  echo "getKeycloakConfig() - Keycloak OAuth configuration"
  echo "waitForServiceAvailable(\$NAMESPACE) - Service availability checker"
  echo "dnsUtils() - DNS utilities installation"
  echo "customDns() - Custom DNS configuration"
  echo "kcurl() - Kubernetes curl utility installation"
  echo "oauthAdmin() - OAuth admin configuration"
  echo "disableSwap() - System swap disabling"
  echo "startHa() - High availability service startup"
  echo "startKubelet() - Kubelet service startup"
  echo "createApp1() - Sample application deployment"
  echo "createLocalStorageClassAndPV() - Local storage configuration"
  echo "createExampleJenkinsPipeline() - Jenkins pipeline examples"
  echo "createJenkinsPipeline() - Jenkins pipeline creation"
  echo "createCertificate() - Certificate creation utilities"
  echo "createClientCertificate() - Client certificate creation"
  echo "createKubeConfig(\$NAME) - Kubeconfig file creation"
  echo "certificateRequestForNs(\$NAME, \$ADDITIONAL) - Namespace certificate requests"
  echo "hostSecret() - Application host secret creation"
  echo "createVaultSecretStore() - Vault secret store configuration"
  echo "create_sub_scope() - OAuth scope creation"
  echo "createUserGroup() - LDAP user and group creation"
  echo "emptyLocalFsStorage() - Local filesystem storage cleanup"
  echo "collectUserInputs() - Interactive user input collection"
  echo "promptUserInput() - User input prompting utility"
  echo "promptSecret() - Secure password input utility"
  echo ""
  
  echo "GLOBAL VARIABLES:"
  echo "================="
  echo ""
  echo "WORKING_DIR - Base working directory (\$MOUNT_PATH/kubernetes/install_k8s)"
  echo "release - Helm release name for status operations"
  echo "CMD - Current command being executed (\$1)"
  echo "MOUNT_PATH - Base mount path for GOK installation"
  echo "GOK_ROOT_DOMAIN - Root domain for GOK services"
  echo ""
  
  echo "CONFIGURATION FILES:"
  echo "===================="
  echo ""
  echo "config - Main GOK configuration file"
  echo "root_config - Root-level system configuration"
  echo "sample_service_config.yaml - Service generation template configuration"
  echo "/etc/bash.bashrc - System bash configuration (modified by GOK)"
  echo "/etc/rc.local - System startup script (created by GOK)"
  echo ""
  
  echo "For detailed usage of any command, use: gok <command> help"
  echo "For interactive pod operations, GOK provides automatic pod selection."
  echo "Most installation functions support both standalone and integrated deployment modes."
}

function logsCmd() {
  pod=$(getpod)
  echo "Viewing logs of pod $pod"
  kubectl logs "$pod"
}

# Service status check functions
check_service_status() {
  local service_name="$1"
  local namespace="$2"
  local helm_release="$3"
  local check_type="${4:-helm}"  # helm, kubectl, or both
  
  # Quick connectivity check - if cluster is not accessible, return âŒ for kubectl-based checks
  if [[ "$check_type" == "kubectl" ]] || [[ "$check_type" == "both" ]]; then
    if ! check_cluster_connectivity; then
      echo "âŒ"
      return
    fi
  fi
  
  case "$check_type" in
    "helm")
      if helm list -n "$namespace" 2>/dev/null | grep -q "^$helm_release"; then
        local status=$(helm list -n "$namespace" 2>/dev/null | grep "^$helm_release" | awk '{print $8}')
        if [[ "$status" == "deployed" ]]; then
          echo "âœ…"
        else
          echo "âš ï¸"
        fi
      else
        echo "âŒ"
      fi
      ;;
    "kubectl")
      if kubectl get deployment "$service_name" -n "$namespace" &>/dev/null; then
        local ready=$(kubectl get deployment "$service_name" -n "$namespace" -o jsonpath='{.status.readyReplicas}' 2>/dev/null)
        local desired=$(kubectl get deployment "$service_name" -n "$namespace" -o jsonpath='{.spec.replicas}' 2>/dev/null)
        if [[ "$ready" == "$desired" ]] && [[ "$ready" -gt 0 ]]; then
          echo "âœ…"
        else
          echo "âš ï¸"
        fi
      else
        echo "âŒ"
      fi
      ;;
    "both")
      local helm_status=$(check_service_status "$service_name" "$namespace" "$helm_release" "helm")
      local kubectl_status=$(check_service_status "$service_name" "$namespace" "$helm_release" "kubectl")
      if [[ "$helm_status" == "âœ…" ]] && [[ "$kubectl_status" == "âœ…" ]]; then
        echo "âœ…"
      elif [[ "$helm_status" == "âŒ" ]] && [[ "$kubectl_status" == "âŒ" ]]; then
        echo "âŒ"
      else
        echo "âš ï¸"
      fi
      ;;
  esac
}

check_ingress_status() {
  check_service_status "ingress-nginx-controller" "ingress-nginx" "ingress-nginx" "both"
}

check_certmanager_status() {
  check_service_status "cert-manager" "cert-manager" "cert-manager" "both"
}

check_registry_status() {
  # Check for docker registry deployment
  if kubectl get deployment registry-docker-registry -n registry &>/dev/null; then
    check_service_status "registry-docker-registry" "registry" "" "kubectl"
  else
    echo "âŒ"
  fi
}

check_kubernetes_status() {
  # Use kubectl cluster-info to check Kubernetes cluster status
  if kubectl cluster-info &>/dev/null; then
    # Check if core components are running
    local cluster_info_output=$(kubectl cluster-info 2>/dev/null)
    local api_server=$(echo "$cluster_info_output" | grep -i "kubernetes control plane\|kubernetes master" | wc -l)
    
    if [[ "$api_server" -ge 1 ]]; then
      # Additional checks for cluster health
      local ready_nodes=$(kubectl get nodes --no-headers 2>/dev/null | grep " Ready " | wc -l)
      local not_ready_nodes=$(kubectl get nodes --no-headers 2>/dev/null | grep " NotReady " | wc -l)
      
      if [[ "$ready_nodes" -gt 0 ]] && [[ "$not_ready_nodes" -eq 0 ]]; then
        # All nodes are ready
        echo "âœ…"
      elif [[ "$ready_nodes" -gt 0 ]] && [[ "$not_ready_nodes" -gt 0 ]]; then
        # Some nodes ready, some not ready
        echo "âš ï¸"
      elif [[ "$ready_nodes" -eq 0 ]]; then
        # No ready nodes
        echo "âŒ"
      else
        echo "âœ…"
      fi
    else
      echo "âŒ"
    fi
  else
    # kubectl cluster-info failed - cluster not accessible
    echo "âŒ"
  fi
}

check_cluster_connectivity() {
  # Quick check if kubectl can connect to cluster
  kubectl cluster-info &>/dev/null
}

check_base_status() {
  # Base is typically a custom image/deployment, check for common base components
  if kubectl get deployment base-services -n default &>/dev/null || \
     kubectl get pods -l app=base &>/dev/null | grep -q Running; then
    echo "âœ…"
  else
    echo "âŒ"
  fi
}

check_kyverno_status() {
  check_service_status "kyverno-admission-controller" "kyverno" "kyverno" "both"
}

check_ldap_status() {
  check_service_status "ldap" "ldap" "" "kubectl"
}

check_keycloak_status() {
  # Keycloak uses StatefulSet instead of Deployment
  if ! check_cluster_connectivity; then
    echo "âŒ"
    return
  fi
  
  # Check Helm release first
  local helm_status="âŒ"
  if helm list -n keycloak 2>/dev/null | grep -q "^keycloak"; then
    local status=$(helm list -n keycloak 2>/dev/null | grep "^keycloak" | awk '{print $8}')
    if [[ "$status" == "deployed" ]]; then
      helm_status="âœ…"
    else
      helm_status="âš ï¸"
    fi
  fi
  
  # Check StatefulSet status
  local kubectl_status="âŒ"
  if kubectl get statefulset keycloak -n keycloak &>/dev/null; then
    local ready=$(kubectl get statefulset keycloak -n keycloak -o jsonpath='{.status.readyReplicas}' 2>/dev/null)
    local desired=$(kubectl get statefulset keycloak -n keycloak -o jsonpath='{.spec.replicas}' 2>/dev/null)
    if [[ "$ready" == "$desired" ]] && [[ "$ready" -gt 0 ]]; then
      kubectl_status="âœ…"
    else
      kubectl_status="âš ï¸"
    fi
  fi
  
  # Combine statuses
  if [[ "$helm_status" == "âœ…" ]] && [[ "$kubectl_status" == "âœ…" ]]; then
    echo "âœ…"
  elif [[ "$helm_status" == "âŒ" ]] && [[ "$kubectl_status" == "âŒ" ]]; then
    echo "âŒ"
  else
    echo "âš ï¸"
  fi
}

check_oauth2_status() {
  check_service_status "oauth2proxy-oauth2-proxy" "oauth2" "oauth2proxy" "both"
}

check_service_status_statefulset() {
  local service_name="$1"
  local namespace="$2" 
  local helm_release="$3"
  local resource_type="${4:-statefulset}"  # statefulset or deployment
  
  if ! check_cluster_connectivity; then
    echo "âŒ"
    return
  fi
  
  # Check Helm release first
  local helm_status="âŒ"
  if helm list -n "$namespace" 2>/dev/null | grep -q "^$helm_release"; then
    local status=$(helm list -n "$namespace" 2>/dev/null | grep "^$helm_release" | awk '{print $8}')
    if [[ "$status" == "deployed" ]]; then
      helm_status="âœ…"
    else
      helm_status="âš ï¸"
    fi
  fi
  
  # Check Kubernetes resource status
  local kubectl_status="âŒ"
  if kubectl get "$resource_type" "$service_name" -n "$namespace" &>/dev/null; then
    local ready=$(kubectl get "$resource_type" "$service_name" -n "$namespace" -o jsonpath='{.status.readyReplicas}' 2>/dev/null)
    local desired=$(kubectl get "$resource_type" "$service_name" -n "$namespace" -o jsonpath='{.spec.replicas}' 2>/dev/null)
    if [[ "$ready" == "$desired" ]] && [[ "$ready" -gt 0 ]]; then
      kubectl_status="âœ…"
    else
      kubectl_status="âš ï¸"
    fi
  fi
  
  # Combine statuses
  if [[ "$helm_status" == "âœ…" ]] && [[ "$kubectl_status" == "âœ…" ]]; then
    echo "âœ…"
  elif [[ "$helm_status" == "âŒ" ]] && [[ "$kubectl_status" == "âŒ" ]]; then
    echo "âŒ"
  else
    echo "âš ï¸"
  fi
}

check_rabbitmq_status() {
  # RabbitMQ typically uses StatefulSet for clustering
  check_service_status_statefulset "rabbitmq" "rabbitmq" "rabbitmq" "statefulset"
}

check_vault_status() {
  # Vault uses StatefulSet in HA mode
  check_service_status_statefulset "vault" "vault" "vault" "statefulset"
}

check_goklogin_status() {
  check_service_status "gok-login" "gok-login" "gok-login" "both"
}

function statusCmd() {
  if [[ -n "$release" ]]; then
    # If a specific release is provided, show helm status for that release
    helm status "$release"
    return
  fi

  echo "ðŸš€ GOK Platform Status Overview"
  echo "=============================="
  echo ""
  
  # Core Infrastructure Services (Priority Order)
  echo "ðŸ“‹ Core Infrastructure Services:"
  echo ""
  
  local index=1
  
  # Priority services in specified order
  printf "  %2d. %-15s %s  %s\n" $index "ingress" "$(check_ingress_status)" "NGINX Ingress Controller"
  ((index++))
  
  printf "  %2d. %-15s %s  %s\n" $index "cert-manager" "$(check_certmanager_status)" "TLS Certificate Management"
  ((index++))
  
  printf "  %2d. %-15s %s  %s\n" $index "registry" "$(check_registry_status)" "Docker Registry"
  ((index++))
  
  printf "  %2d. %-15s %s  %s\n" $index "base" "$(check_base_status)" "Base System Components"
  ((index++))
  
  printf "  %2d. %-15s %s  %s\n" $index "kyverno" "$(check_kyverno_status)" "Policy Engine"
  ((index++))
  
  printf "  %2d. %-15s %s  %s\n" $index "ldap" "$(check_ldap_status)" "LDAP Directory Service"
  ((index++))
  
  printf "  %2d. %-15s %s  %s\n" $index "keycloak" "$(check_keycloak_status)" "Identity & Access Management"
  ((index++))
  
  printf "  %2d. %-15s %s  %s\n" $index "oauth2" "$(check_oauth2_status)" "OAuth2 Proxy"
  ((index++))
  
  printf "  %2d. %-15s %s  %s\n" $index "rabbitmq" "$(check_rabbitmq_status)" "Message Broker"
  ((index++))
  
  printf "  %2d. %-15s %s  %s\n" $index "vault" "$(check_vault_status)" "Secrets Management"
  ((index++))
  
  printf "  %2d. %-15s %s  %s\n" $index "gok-login" "$(check_goklogin_status)" "GOK Authentication Service"
  ((index++))
  
  echo ""
  echo "ðŸ“Š Additional Services:"
  echo ""
  
  # Monitoring & Observability Services
  echo "ðŸ” Monitoring & Observability:"
  local monitoring_services=(
    "monitoring:prometheus-operator:monitoring:Prometheus/Grafana Monitoring:deployment"
    "opensearch:opensearch-cluster-master:opensearch:OpenSearch Logging:statefulset"
    "fluentd:fluentd:fluentd:Log Collection:deployment"
  )
  
  for service_info in "${monitoring_services[@]}"; do
    IFS=':' read -r service_name deployment_name namespace description resource_type <<< "$service_info"
    if [[ "$resource_type" == "statefulset" ]]; then
      status=$(check_service_status_statefulset "$deployment_name" "$namespace" "$service_name" "statefulset")
    else
      status=$(check_service_status "$deployment_name" "$namespace" "$service_name" "kubectl")
    fi
    printf "  %2d. %-15s %s  %s\n" $index "$service_name" "$status" "$description"
    ((index++))
  done
  
  echo ""
  echo "ðŸš€ Development & CI/CD:"
  local devops_services=(
    "jenkins:jenkins:jenkins:CI/CD Pipeline"
    "jupyter:jupyterhub:jupyterhub:JupyterHub Development"
    "argocd:argocd-server:argocd:GitOps Deployment"
    "spinnaker:spin-deck:spinnaker:Multi-cloud Deployment Platform"
  )
  
  for service_info in "${devops_services[@]}"; do
    IFS=':' read -r service_name deployment_name namespace description <<< "$service_info"
    printf "  %2d. %-15s %s  %s\n" $index "$service_name" "$(check_service_status "$deployment_name" "$namespace" "$service_name" "kubectl")" "$description"
    ((index++))
  done
  
  echo ""
  echo "ðŸ’» Developer Tools & IDEs:"
  local dev_tools=(
    "che:che:eclipse-che:Eclipse Che IDE"
    "devworkspace:devworkspace:devworkspace:Developer Workspace (Legacy)"
    "workspace:workspace-v2:workspace:Enhanced Developer Workspace"
    "ttyd:ttyd:ttyd:Terminal over HTTP"
    "cloudshell:cloudshell:cloudshell:Cloud-based Terminal"
    "console:console:console:Web-based Console"
  )
  
  for service_info in "${dev_tools[@]}"; do
    IFS=':' read -r service_name deployment_name namespace description <<< "$service_info"
    printf "  %2d. %-15s %s  %s\n" $index "$service_name" "$(check_service_status "$deployment_name" "$namespace" "$service_name" "kubectl")" "$description"
    ((index++))
  done
  
  echo ""
  echo "ðŸŒ Service Mesh & Management:"
  local mesh_services=(
    "istio:istiod:istio-system:Service Mesh"
    "dashboard:kubernetes-dashboard:kubernetes-dashboard:K8s Dashboard"
    "chart:chartmuseum:chartmuseum:Helm Chart Repository"
  )
  
  for service_info in "${mesh_services[@]}"; do
    IFS=':' read -r service_name deployment_name namespace description <<< "$service_info"
    printf "  %2d. %-15s %s  %s\n" $index "$service_name" "$(check_service_status "$deployment_name" "$namespace" "$service_name" "kubectl")" "$description"
    ((index++))
  done
  
  echo ""
  echo "ðŸ—ï¸ Platform & Infrastructure:"
  local platform_services=(
    "kubernetes:kube-apiserver:kube-system:Kubernetes Control Plane"
    "docker:::Docker Container Runtime"
    "gok-agent:gok-agent:gok-agent:GOK Distributed System Agent"
    "gok-controller:gok-controller:gok-controller:GOK Controller"
    "base-services:::Complete Base Services Stack"
  )
  
  for service_info in "${platform_services[@]}"; do
    IFS=':' read -r service_name deployment_name namespace description <<< "$service_info"
    if [[ "$service_name" == "kubernetes" ]]; then
      # Use kubectl cluster-info for Kubernetes cluster status
      status="$(check_kubernetes_status)"
    elif [[ "$service_name" == "docker" ]]; then
      # Special check for Docker
      if systemctl is-active --quiet docker; then
        status="âœ…"
      else
        status="âŒ"
      fi
    elif [[ "$service_name" == "base-services" ]]; then
      # Special check for base-services (combination check)
      if [[ "$(check_base_status)" == "âœ…" ]] && [[ "$(check_ingress_status)" == "âœ…" ]] && [[ "$(check_certmanager_status)" == "âœ…" ]]; then
        status="âœ…"
      elif [[ "$(check_base_status)" == "âŒ" ]] && [[ "$(check_ingress_status)" == "âŒ" ]] && [[ "$(check_certmanager_status)" == "âŒ" ]]; then
        status="âŒ"
      else
        status="âš ï¸"
      fi
    else
      status="$(check_service_status "$deployment_name" "$namespace" "$service_name" "kubectl")"
    fi
    printf "  %2d. %-15s %s  %s\n" $index "$service_name" "$status" "$description"
    ((index++))
  done
  
  echo ""
  echo "ðŸ“ Status Legend:"
  echo "   âœ… Installed and Running"
  echo "   âš ï¸  Installed but Issues Detected"
  echo "   âŒ Not Installed"
  echo ""
  echo "ðŸ’¡ Usage:"
  echo "   gok status                    # Show all services status"
  echo "   gok status <helm-release>     # Show specific Helm release status"
  echo ""
}

function taintNodeCmd() {
  taintNode
}

collectUserInputs(){
  INPUT_FILE="/root/base_services_inputs"
  
  echo "# Base Services Installation Inputs" > "$INPUT_FILE"

  # Collect Docker credentials
  echo "=== Docker Registry Credentials ==="
  DOCKER_USERNAME=$(promptUserInput "Please enter docker user id: ")
  DOCKER_PASSWORD=$(promptSecret "Please enter your docker password: ")
  {
    echo "export DOCKER_USERNAME='$DOCKER_USERNAME'"
    echo "export DOCKER_PASSWORD='$DOCKER_PASSWORD'"
  } >> "$INPUT_FILE"
  
  # Collect inputs for LDAP
  echo "=== LDAP Configuration ==="
  LDAP_PASSWORD=$(promptSecret "Please enter LDAP password for admin: ")
  KERBEROS_PASSWORD=$(promptSecret "Please enter Kerberos password: ")
  KERBEROS_KDC_PASSWORD=$(promptSecret "Please enter Kerberos kdc password: ")
  KERBEROS_ADM_PASSWORD=$(promptSecret "Please enter Kerberos adm password: ")

  {
    echo "export LDAP_PASSWORD='$LDAP_PASSWORD'"
    echo "export KERBEROS_PASSWORD='$KERBEROS_PASSWORD'"
    echo "export KERBEROS_KDC_PASSWORD='$KERBEROS_KDC_PASSWORD'"
    echo "export KERBEROS_ADM_PASSWORD='$KERBEROS_ADM_PASSWORD'"
  } >> "$INPUT_FILE"

  # Collect inputs for Keycloak
  echo "=== Keycloak Configuration ==="
  KEYCLOAK_ADMIN_USERNAME=$(promptUserInput "Please enter keycloak admin username (admin): " "admin")
  KEYCLOAK_ADMIN_PASSWORD=$(promptSecret "Please enter keycloak admin password: ")
  echo "export KEYCLOAK_ADMIN_USERNAME='$KEYCLOAK_ADMIN_USERNAME'" >> "$INPUT_FILE"
  echo "export KEYCLOAK_ADMIN_PASSWORD='$KEYCLOAK_ADMIN_PASSWORD'" >> "$INPUT_FILE"

  # Collect inputs for PostgreSQL
  POSTGRESQL_USERNAME=$(promptUserInput "Please enter postgresql username (postgres): " "postgres")
  POSTGRESQL_PASSWORD=$(promptSecret "Please enter postgresql password: ")
  echo "export POSTGRESQL_USERNAME='$POSTGRESQL_USERNAME'" >> "$INPUT_FILE"
  echo "export POSTGRESQL_PASSWORD='$POSTGRESQL_PASSWORD'" >> "$INPUT_FILE"

  # Collect inputs for OIDC
  OIDC_CLIENT_ID=$(promptUserInput "Please enter OIDC client id (${OIDC_CLIENT_ID}): " "${OIDC_CLIENT_ID}")
  REALM=$(promptUserInput "Please enter realm name (${REALM}): " "${REALM}")
  echo "export OIDC_CLIENT_ID='$OIDC_CLIENT_ID'" >> "$INPUT_FILE"
  echo "export REALM='$REALM'" >> "$INPUT_FILE"
  
  SAMPLE_USER_PASSWORD=$(promptSecret "Please enter sample user password (for user: skmaji1): ")
  echo "export SAMPLE_USER_PASSWORD='$SAMPLE_USER_PASSWORD'" >> "$INPUT_FILE"

  # Collect OAuth2 client secret
  ACTIVE_PROFILE=$(promptUserInput "Enter Active Profile (keycloak): " "keycloak")
  OIDC_ISSUE_URL=$(promptUserInput "Enter OIDC Issue URL (https://keycloak.gokcloud.com/realms/$REALM): " "https://keycloak.gokcloud.com/realms/$REALM")
  OIDC_USERNAME_CLAIM=$(promptUserInput "Enter OIDC Username Claim (${OIDC_USERNAME_CLAIM}): " "${OIDC_USERNAME_CLAIM}")
  OIDC_GROUPS_CLAIM=$(promptUserInput "Enter OIDC Groups Claim (${OIDC_GROUPS_CLAIM}): " "${OIDC_GROUPS_CLAIM}")
  AUTH0_DOMAIN=$(promptUserInput "Enter Auth0 Domain (${AUTH0_DOMAIN}): " "${AUTH0_DOMAIN}")
  APP_HOST=$(promptUserInput "Enter App Host (${APP_HOST}): " "${APP_HOST}")
  JWKS_URL=$(promptUserInput "Enter JWKS URL (${JWKS_URL}): " "${JWKS_URL}")
  {
    echo "export ACTIVE_PROFILE='$ACTIVE_PROFILE'"
    echo "export OIDC_ISSUE_URL='$OIDC_ISSUE_URL'"
    echo "export OIDC_USERNAME_CLAIM='$OIDC_USERNAME_CLAIM'"
    echo "export OIDC_GROUPS_CLAIM='$OIDC_GROUPS_CLAIM'"
    echo "export AUTH0_DOMAIN='$AUTH0_DOMAIN'"
    echo "export APP_HOST='$APP_HOST'"
    echo "JWKS_URL='$JWKS_URL'"
  } >> "$INPUT_FILE"
  
  # Collect Vault inputs if needed
  # Add other inputs as needed...
  
  echo "All inputs collected and saved to $INPUT_FILE"
}

function installBaseServices(){
  STATE_FILE="/root/base_services_install_state"
  INPUT_FILE="/root/base_services_inputs"
  
  # Check current state
  if [ -f "$STATE_FILE" ]; then
    STATE=$(cat "$STATE_FILE")
    case "$STATE" in
      "post_reboot")
        echo "Continuing base services installation after reboot..."
        postRebootBaseServices
        return
        ;;
      "completed")
        echo "Base services installation already completed"
        return
        ;;
    esac
  fi
  
  # Pre-collect all user inputs
  echo "Collecting required inputs for base services installation..."
  collectUserInputs
  
  # Pre-reboot services
  echo "Installing pre-reboot base services..."
  echo "starting_install" > "$STATE_FILE"
  
  gok install ingress
  gok install cert-manager
  
  # Check if reboot is needed
  if [[ "$CERTMANAGER_CHALANGE_TYPE" == "selfsigned" ]]; then
    echo "post_reboot" > "$STATE_FILE"
    
    # Create a separate script file instead of inline bash
    cat > /root/post-reboot-script.sh <<'SCRIPT_EOF'
#!/bin/bash
# Wait for Kubernetes API to be available before proceeding
export MOUNT_PATH=/root
source $MOUNT_PATH/kubernetes/install_k8s/util
source $MOUNT_PATH/kubernetes/install_k8s/gok
export KUBECONFIG=/root/.kube/config

for i in {1..30}; do
  if kubectl get nodes &>/dev/null; then
    echo "Kubernetes is up and running."
    break
  else
    kubectl get nodes
    echo "Waiting for Kubernetes API to be available... ($i/30)"
    sleep 10
  fi
done

if ! kubectl get nodes &>/dev/null; then
  echo "Kubernetes API is not available after waiting. Exiting."
  exit 1
fi

installBaseServices
SCRIPT_EOF
    
    chmod +x /root/post-reboot-script.sh
    
    # Create systemd service for post-reboot continuation
    cat > /etc/systemd/system/base-services-post-reboot-1.service <<EOF
[Unit]
Description=Base Services Post Reboot Setup
After=multi-user.target

[Service]
Type=simple
ExecStart=/root/post-reboot-script.sh
Restart=no
StandardOutput=journal
StandardError=journal

[Install]
WantedBy=multi-user.target
EOF

    systemctl enable base-services-post-reboot-1.service
    
    echoWarning "Self-signed certificate added to root ca, rebooting system for it to take effect"
    sudo reboot
  else
    postRebootBaseServices
  fi
}

baseServicesInstallLogs(){
  journalctl -u base-services-post-reboot-1.service --no-pager -f
}

postRebootBaseServices(){
  echo "Installing post-reboot base services..."
  
  # Load saved inputs
  INPUT_FILE="/root/base_services_inputs"
  if [ -f "$INPUT_FILE" ]; then
    source "$INPUT_FILE"
    echo "Loaded saved user inputs"
  else
    echoFailed "Input file not found! Cannot continue without user inputs."
    return 1
  fi
  
  # Clean up systemd service and script
  if [ -f "/etc/systemd/system/base-services-post-reboot-1.service" ]; then
    systemctl disable base-services-post-reboot-1.service
    rm -f /etc/systemd/system/base-services-post-reboot-1.service
    rm -f /root/post-reboot-script.sh
    systemctl daemon-reload
  fi
  
  # Install services using saved inputs
  gok install kyverno
  gok install registry
  gok install base
  gok install ldap
  gok install keycloak
  gok install oauth2
  gok install rabibtmq
  gok install vault
  
  # Clean up input file
  rm -f "$INPUT_FILE"
  
  echo "completed" > "/root/base_services_install_state"
  echoSuccess "Base services installation completed."
}

function resetBaseServices(){
  echo "Resetting base services..."
  echo "This will reset the following components: vault, rabbitmq, oauth2, keycloak, ldap, registry, kyverno, cert-manager, ingress."
  sleep 5
  gok reset vault
  echoSuccess "Vault reset completed."
  gok reset rabbitmq
  echoSuccess "RabbitMQ reset completed."
  gok reset oauth2
  echoSuccess "OAuth2 reset completed."
  gok reset keycloak
  echoSuccess "Keycloak reset completed."
  gok reset ldap
  echoSuccess "LDAP reset completed."
  gok reset registry
  echoSuccess "Registry reset completed."
  gok reset kyverno
  echoSuccess "Kyverno reset completed."
  gok reset cert-manager
  echoSuccess "Cert-Manager reset completed."
  gok reset ingress
  echoSuccess "Ingress reset completed."
}


function installCmd() {
  COMPONENT=$1
  
  if [ -z "$COMPONENT" ] || [ "$COMPONENT" == "help" ] || [ "$COMPONENT" == "--help" ]; then
    echo "gok install - Install and configure Kubernetes components and services"
    echo ""
    echo "Usage: gok install <component> [--verbose|-v] [--force-update] [--skip-update] [--force-deps] [--skip-deps]"
    echo ""
    echo "Options:"
    echo "  --verbose, -v      Show detailed installation output (default: progress bars)"
    echo "  --force-update     Force system update regardless of cache"
    echo "  --skip-update      Skip system update completely"
    echo "  --force-deps       Force dependency installation regardless of cache"
    echo "  --skip-deps        Skip dependency installation completely"
    echo ""
    echo "Environment Variables:"
    echo "  GOK_VERBOSE=true         Enable verbose mode globally"
    echo "  GOK_UPDATE_CACHE_HOURS=6 Hours to cache system updates (default: 6)"
    echo "  GOK_DEPS_CACHE_HOURS=6   Hours to cache dependency installations (default: same as updates)"
    echo "  GOK_CACHE_DIR=/tmp/gok-cache  Custom cache directory location"
    echo ""
    echo "Smart Caching Examples:"
    echo "  gok install docker                     # Uses smart cache (default)"
    echo "  gok install base --skip-update         # Skip system update completely"
    echo "  gok install base --skip-deps           # Skip dependency installation"
    echo "  gok install kubernetes --force-update  # Force fresh system update"
    echo "  gok install kubernetes --force-deps    # Force fresh dependency installation"
    echo "  GOK_UPDATE_CACHE_HOURS=12 gok install helm  # Cache updates for 12 hours"
    echo "  GOK_DEPS_CACHE_HOURS=24 gok install vault   # Cache dependencies for 24 hours"
    echo ""
    echo "Cache Management:"
    echo "  gok cache status        # Check cache age and validity"
    echo "  gok cache clear         # Clear cache, force next update"
    echo ""
    echo "Core Infrastructure:"
    echo "  docker            Docker container runtime"
    echo "  helm              Helm package manager for Kubernetes"
    echo "  haproxy           HA proxy container for load balancing (aliases: ha-proxy, ha)"
    echo "  kubernetes        Complete Kubernetes cluster with HA"
    echo "  kubernetes-worker Kubernetes worker node"
    echo "  cert-manager      Certificate management and TLS automation"
    echo "  ingress           NGINX ingress controller"
    echo "  dashboard         Kubernetes web dashboard"
    echo ""
    echo "Monitoring & Logging:"
    echo "  monitoring        Prometheus and Grafana stack"
    echo "  fluentd           Log collection and forwarding"
    echo "  opensearch        Search and analytics engine with dashboard"
    echo ""
    echo "Security & Identity:"
    echo "  keycloak          Identity and access management"
    echo "  oauth2            OAuth2 proxy for authentication"
    echo "  vault             Secrets management"
    echo "  ldap              LDAP directory service"
    echo ""
    echo "Development Tools:"
    echo "  jupyter           JupyterHub for data science"
    echo "  devworkspace      Developer workspace (legacy)"
    echo "  workspace         Enhanced developer workspace"
    echo "  che               Eclipse Che IDE"
    echo "  ttyd              Terminal over HTTP"
    echo "  cloudshell        Cloud-based terminal"
    echo "  console           Web-based console"
    echo ""
    echo "CI/CD & DevOps:"
    echo "  argocd            GitOps continuous delivery"
    echo "  jenkins           CI/CD automation server"
    echo "  spinnaker         Multi-cloud deployment platform"
    echo "  registry          Container image registry"
    echo ""
    echo "Service Mesh & Networking:"
    echo "  istio             Service mesh for microservices"
    echo "  rabbitmq          Message broker"
    echo ""
    echo "Governance & Policy:"
    echo "  kyverno           Kubernetes policy engine"
    echo ""
    echo "GOK Platform:"
    echo "  gok-agent         GOK distributed system agent"
    echo "  gok-controller    GOK distributed system controller"
    echo "  controller        Install both gok-agent and gok-controller"
    echo "  gok-login         GOK authentication service"
    echo "  chart             Helm chart repository"
    echo ""
    echo "Complete Solutions:"
    echo "  base              Base system components"
    echo "  base-services     Complete base services stack"
    echo ""
    echo "Examples:"
    echo "  gok install kubernetes        # Install complete K8s cluster"
    echo "  gok install cert-manager      # Install certificate management"
    echo "  gok install monitoring        # Install Prometheus & Grafana"
    echo "  gok install base-services     # Install complete base stack"
    echo ""
    echo "Installation Features:"
    echo "  âœ… Automated dependency resolution"
    echo "  âœ… High availability configuration"
    echo "  âœ… TLS/SSL certificate automation"
    echo "  âœ… RBAC and security hardening"
    echo "  âœ… Production-ready configurations"
    echo "  âœ… Integrated monitoring and logging"
    echo "  âœ… Service mesh ready"
    echo ""
    echo "Prerequisites:"
    echo "  - Ubuntu/Debian-based system"
    echo "  - Root or sudo access"
    echo "  - Internet connectivity"
    echo "  - Minimum 4GB RAM, 2 CPU cores"
    return 0
  fi
  
  # Check for interactive mode
  if [ "$COMPONENT" == "interactive" ] || [ "$COMPONENT" == "wizard" ]; then
    interactive_installation
    return 0
  fi
  
  # Initialize component tracking
  init_component_tracking "$COMPONENT" "Installing $COMPONENT component"
  
  # Parse all flags
  local verbose_flag=""
  local update_flags=""
  local deps_flags=""
  for arg in "$@"; do
    case "$arg" in
      --verbose|-v)
        verbose_flag="--verbose"
        export GOK_VERBOSE="true"
        log_info "Verbose logging enabled"
        ;;
      --force-update)
        update_flags="$update_flags --force-update"
        ;;
      --skip-update)
        update_flags="$update_flags --skip-update"
        ;;
      --force-deps)
        deps_flags="$deps_flags --force-deps"
        ;;
      --skip-deps)
        deps_flags="$deps_flags --skip-deps"
        ;;
    esac
  done
  
  # Also check environment variable
  if [[ "$GOK_VERBOSE" == "true" ]] && [[ "$verbose_flag" == "" ]]; then
    verbose_flag="--verbose"
    log_info "Verbose logging enabled via GOK_VERBOSE"
  fi
  
  # Start component installation with enhanced logging
  start_component "$COMPONENT" "Installing $COMPONENT component"
  
  # Run smart system updates with caching
  if ! updateSys $verbose_flag $update_flags; then
    fail_component "$COMPONENT" "System update failed"
    return 1
  fi
  
  if ! installDeps $verbose_flag $deps_flags; then
    fail_component "$COMPONENT" "Dependency installation failed"
    return 1
  fi
  
  # Enhanced installation with validation
  if [ "$COMPONENT" == "docker" ]; then
    if dockrInst; then
      if validate_component_installation "docker" 120; then
        complete_component "docker" "Docker installation completed and validated"
        show_component_next_steps "docker"
      else
        complete_component "docker" "Docker installed but validation had warnings"
      fi
    else
      fail_component "docker" "Docker installation failed"
      return 1
    fi
  elif [ "$COMPONENT" == "helm" ]; then
    if helmInst; then
      if validate_component_installation "helm" 60; then
        complete_component "helm" "Helm installation completed and validated"
        show_component_next_steps "helm"
      else
        complete_component "helm" "Helm installed but validation had warnings"
      fi
    else
      fail_component "helm" "Helm installation failed"
      return 1
    fi
  elif [ "$COMPONENT" == "cert-manager" ]; then
    if certmanagerInst && setupCertiIssuers; then
      if validate_component_installation "cert-manager" 300; then
        complete_component "cert-manager" "Cert-manager installation completed and validated"
        suggest_and_install_next_module "cert-manager"
      else
        complete_component "cert-manager" "Cert-manager installed but validation had warnings"
        suggest_and_install_next_module "cert-manager"
      fi
    else
      fail_component "cert-manager" "Cert-manager installation failed"
      return 1
    fi
  elif [ "$COMPONENT" == "monitoring" ]; then
    if installPrometheusGrafanaWithCertMgr; then
      if validate_component_installation "monitoring" 600; then
        complete_component "monitoring" "Monitoring stack installation completed and validated"
        show_component_next_steps "monitoring"
      else
        complete_component "monitoring" "Monitoring stack installed but validation had warnings"
      fi
    else
      fail_component "monitoring" "Monitoring stack installation failed"
      return 1
    fi
  elif [ "$COMPONENT" == "ldap" ]; then
    installLdap
  elif [ "$COMPONENT" == "dashboard" ]; then
    installDashboardwithCertManager
  elif [ "$COMPONENT" == "jupyter" ]; then
    jupyterHubInst
  elif [ "$COMPONENT" == "devworkspace" ]; then
    createDevWorkspace
  elif [ "$COMPONENT" == "workspace" ]; then
    createDevWorkspaceV2
  elif [ "$COMPONENT" == "che" ]; then
    eclipseCheInst
  elif [ "$COMPONENT" == "ttyd" ]; then
    ttydInst
  elif [ "$COMPONENT" == "cloudshell" ]; then
    cloudshellInst
  elif [ "$COMPONENT" == "console" ]; then
    consoleInst
  elif [ "$COMPONENT" == "argocd" ]; then
    if argocdInst; then
      if validate_component_installation "argocd" 300; then
        complete_component "argocd" "ArgoCD installation completed and validated"
        show_component_next_steps "argocd"
      else
        complete_component "argocd" "ArgoCD installed but validation had warnings"
      fi
    else
      fail_component "argocd" "ArgoCD installation failed"
      return 1
    fi
  elif [ "$COMPONENT" == "gok-agent" ]; then
    if gokAgentInstall; then
      if validate_component_installation "gok-agent" 180; then
        complete_component "gok-agent" "GOK Agent installation completed and validated"
        show_component_next_steps "gok-agent"
      else
        complete_component "gok-agent" "GOK Agent installed but validation had warnings"
      fi
    else
      fail_component "gok-agent" "GOK Agent installation failed"
      return 1
    fi
  elif [ "$COMPONENT" == "gok-controller" ]; then
    if gokControllerInstall; then
      if validate_component_installation "gok-controller" 180; then
        complete_component "gok-controller" "GOK Controller installation completed and validated"
        show_component_next_steps "gok-controller"
      else
        complete_component "gok-controller" "GOK Controller installed but validation had warnings"
      fi
    else
      fail_component "gok-controller" "GOK Controller installation failed"
      return 1
    fi
  elif [ "$COMPONENT" == "controller" ]; then
    log_info "Installing GOK Platform (Agent + Controller)"
    if gok install gok-agent && gok install gok-controller; then
      complete_component "controller" "GOK Platform installation completed"
      show_component_next_steps "controller"
    else
      fail_component "controller" "GOK Platform installation failed"
      return 1
    fi
  elif [ "$COMPONENT" == "chart" ]; then
    chartInst
  elif [ "$COMPONENT" == "rabbitmq" ]; then
    rabbitmqInst
  elif [ "$COMPONENT" == "vault" ]; then
    if vaultInstall; then
      if validate_component_installation "vault" 600; then
        complete_component "vault" "Vault installation completed and validated"
        show_component_next_steps "vault"
      else
        complete_component "vault" "Vault installed but requires manual unsealing"
      fi
    else
      fail_component "vault" "Vault installation failed"
      return 1
    fi
  elif [ "$COMPONENT" == "gok-login" ]; then
    if gokLoginInst; then
      complete_component "gok-login" "GOK Login service installation completed"
      show_component_next_steps "gok-login"
    else
      fail_component "gok-login" "GOK Login service installation failed"
      return 1
    fi
  elif [ "$COMPONENT" == "keycloak" ]; then
    if installKeycloakWithCertMgr; then
      if validate_component_installation "keycloak" 300; then
        complete_component "keycloak" "Keycloak installation completed and validated"
        show_component_next_steps "keycloak"
      else
        complete_component "keycloak" "Keycloak installed but validation had warnings"
      fi
    else
      fail_component "keycloak" "Keycloak installation failed"
      return 1
    fi
  elif [ "$COMPONENT" == "ingress" ]; then
    if ingressInst; then
      if validate_component_installation "ingress" 300; then
        complete_component "ingress" "Ingress controller installation completed and validated"
        # Note: suggest_and_install_next_module is already called by ingressInst function
      else
        complete_component "ingress" "Ingress controller installed but validation had warnings"
        # Note: suggest_and_install_next_module is already called by ingressInst function
      fi
    else
      fail_component "ingress" "Ingress controller installation failed"
      return 1
    fi
  elif [ "$COMPONENT" == "registry" ]; then
    if installRegistryWithCertMgr; then
      if validate_component_installation "registry" 300; then
        complete_component "registry" "Container registry installation completed and validated"
        suggest_and_install_next_module "registry"
      else
        complete_component "registry" "Container registry installed but validation had warnings"
        suggest_and_install_next_module "registry"
      fi
    else
      fail_component "registry" "Container registry installation failed"
      return 1
    fi
  elif [ "$COMPONENT" == "fluentd" ]; then
    fluentdInst
  elif [ "$COMPONENT" == "opensearch" ]; then
    opensearchInst
    opensearchDashInst
  elif [ "$COMPONENT" == "jenkins" ]; then
    jenkinsInst
  elif [ "$COMPONENT" == "spinnaker" ]; then
    spinnakerInst
  elif [ "$COMPONENT" == "oauth2" ]; then
    oauth2ProxyInst
  elif [ "$COMPONENT" == "istio" ]; then
    istioInst
  elif [ "$COMPONENT" == "kyverno" ]; then
    kyvernoInst
  elif [ "$COMPONENT" == "base" ]; then
    if baseInst; then
      if validate_component_installation "base" 60; then
        complete_component "base" "Base platform services installation completed and validated"
        suggest_and_install_next_module "base"
      else
        complete_component "base" "Base platform services installed but validation had warnings"
        suggest_and_install_next_module "base"
      fi
    else
      fail_component "base" "Base platform services installation failed"
      return 1
    fi
  elif [ "$COMPONENT" == "kubernetes-worker" ]; then
    dockrInst $verbose_flag
    k8sInst "kubernetes-worker" "$verbose_flag"
  elif [ "$COMPONENT" == "base-services" ]; then
    installBaseServices
  elif [ "$COMPONENT" == "haproxy" ] || [ "$COMPONENT" == "ha-proxy" ] || [ "$COMPONENT" == "ha" ]; then
    if haInst; then
      if validate_haproxy_installation; then
        complete_component "haproxy" "HA proxy installation completed and validated"
        show_haproxy_next_steps
      else
        complete_component "haproxy" "HA proxy installed but validation had warnings"
      fi
    else
      fail_component "haproxy" "HA proxy installation failed"
      return 1
    fi
  elif [ "$COMPONENT" == "kubernetes" ]; then
    # Enhanced Kubernetes installation with comprehensive logging and validation
    
    # Step 1: Install Docker first (prerequisite)
    log_info "Ensuring Docker is installed and running..."
    if ! dockrInst; then
      log_error "Docker installation failed - required for Kubernetes"
      fail_component "kubernetes" "Docker prerequisite installation failed"
      return 1
    fi
    log_success "Docker prerequisite satisfied"
    
    # Step 2: Handle HA proxy setup automatically if needed
    log_info "Checking High Availability requirements for Kubernetes installation"
    
    # Check if HA is required
    local ha_required=false
    local ha_reason=""
    
    # Method 1: Check API_SERVERS configuration
    if [[ -n "$API_SERVERS" ]] && [[ "$API_SERVERS" == *","* ]]; then
        ha_required=true
        ha_reason="Multiple API servers configured in API_SERVERS: $API_SERVERS"
    fi
    
    # Method 2: Check if HA_PROXY_PORT is configured and not default
    if [[ -n "$HA_PROXY_PORT" ]] && [[ "$HA_PROXY_PORT" != "6443" ]]; then
        ha_required=true
        ha_reason="Custom HA proxy port configured: $HA_PROXY_PORT"
    fi
    
    # Check if HA is already installed and working
    local ha_already_installed=false
    if validate_ha_proxy_installation "$verbose_flag" >/dev/null 2>&1; then
        ha_already_installed=true
        log_success "HA proxy is already installed and running"
    fi
    
    if [[ "$ha_required" == "true" ]] || [[ "$ha_already_installed" == "false" ]]; then
        if [[ "$ha_required" == "true" ]]; then
            log_info "HA setup required: $ha_reason"
        else
            log_info "HA proxy not detected - installing for Kubernetes cluster stability"
        fi
        
        # If HA is not already installed, install it
        if [[ "$ha_already_installed" == "false" ]]; then
            # Check if we have remote configuration for HA installation
            if load_default_remote_config; then
                log_info "Remote configuration found - installing HA proxy on remote host ($DEFAULT_REMOTE_USER@$DEFAULT_REMOTE_HOST)"
                
                # Try to install HA proxy on remote host
                if simple_remote_exec "cd /home/$DEFAULT_REMOTE_USER/Documents/repository/kubernetes/install_k8s && sudo ./gok install haproxy"; then
                    log_success "HA proxy installed successfully on remote host for multi-master setup"
                else
                    log_error "HA proxy installation failed on remote host - cannot proceed with Kubernetes installation"
                    echo -e ""
                    echo -e "${COLOR_BRIGHT_RED}${COLOR_BOLD}âŒ KUBERNETES INSTALLATION BLOCKED${COLOR_RESET}"
                    echo -e "${COLOR_RED}High Availability setup is required but remote installation failed.${COLOR_RESET}"
                    echo -e ""
                    echo -e "${COLOR_BRIGHT_YELLOW}${COLOR_BOLD}ðŸ”§ RESOLUTION STEPS:${COLOR_RESET}"
                    echo -e "${COLOR_YELLOW}1. Check remote Docker status: ${COLOR_BOLD}ssh $DEFAULT_REMOTE_USER@$DEFAULT_REMOTE_HOST 'systemctl status docker'${COLOR_RESET}"
                    echo -e "${COLOR_YELLOW}2. Verify remote API_SERVERS: ${COLOR_BOLD}ssh $DEFAULT_REMOTE_USER@$DEFAULT_REMOTE_HOST 'echo \$API_SERVERS'${COLOR_RESET}"
                    echo -e "${COLOR_YELLOW}3. Manual remote HA install: ${COLOR_BOLD}ssh $DEFAULT_REMOTE_USER@$DEFAULT_REMOTE_HOST 'cd /home/$DEFAULT_REMOTE_USER/Documents/repository/kubernetes/install_k8s && sudo ./gok install haproxy'${COLOR_RESET}"
                    echo -e "${COLOR_YELLOW}4. Check remote HA logs: ${COLOR_BOLD}ssh $DEFAULT_REMOTE_USER@$DEFAULT_REMOTE_HOST 'docker logs master-proxy'${COLOR_RESET}"
                    echo -e "${COLOR_YELLOW}5. Retry Kubernetes installation after fixing remote issues${COLOR_RESET}"
                    echo -e ""
                    fail_component "kubernetes" "HA proxy remote installation failed - required for multi-master setup"
                    return 1
                fi
                
                # Validate the remote HA proxy installation
                log_info "Validating remote HA proxy installation..."
                if ! simple_remote_exec "cd /home/$DEFAULT_REMOTE_USER/Documents/repository/kubernetes/install_k8s && source gok && validate_ha_dependency_for_kubernetes --verbose"; then
                    log_error "Remote HA proxy validation failed after installation"
                    echo -e ""
                    echo -e "${COLOR_BRIGHT_RED}${COLOR_BOLD}âŒ KUBERNETES INSTALLATION BLOCKED${COLOR_RESET}"
                    echo -e "${COLOR_RED}HA proxy was installed on remote host but validation failed.${COLOR_RESET}"
                    echo -e ""
                    echo -e "${COLOR_BRIGHT_YELLOW}${COLOR_BOLD}ðŸ”§ TROUBLESHOOTING:${COLOR_RESET}"
                    echo -e "${COLOR_YELLOW}1. Check remote container status: ${COLOR_BOLD}ssh $DEFAULT_REMOTE_USER@$DEFAULT_REMOTE_HOST 'docker ps | grep master-proxy'${COLOR_RESET}"
                    echo -e "${COLOR_YELLOW}2. Check remote container logs: ${COLOR_BOLD}ssh $DEFAULT_REMOTE_USER@$DEFAULT_REMOTE_HOST 'docker logs master-proxy'${COLOR_RESET}"
                    echo -e "${COLOR_YELLOW}3. Verify remote port binding: ${COLOR_BOLD}ssh $DEFAULT_REMOTE_USER@$DEFAULT_REMOTE_HOST 'netstat -tlnp | grep :${HA_PROXY_PORT:-6643}'${COLOR_RESET}"
                    echo -e "${COLOR_YELLOW}4. Test remote connectivity: ${COLOR_BOLD}ssh $DEFAULT_REMOTE_USER@$DEFAULT_REMOTE_HOST 'nc -z localhost ${HA_PROXY_PORT:-6643}'${COLOR_RESET}"
                    echo -e ""
                    fail_component "kubernetes" "Remote HA proxy validation failed after installation"
                    return 1
                fi
                
                log_success "Remote HA proxy validation passed - proceeding with Kubernetes installation"
            else
                # No remote configuration - attempt local HA installation
                log_warning "No remote configuration found - attempting local HA proxy installation"
                log_info "Installing HA proxy locally for Kubernetes cluster"
                
                if haInst; then
                    log_success "HA proxy installed successfully locally"
                    
                    # Validate local HA installation
                    if validate_ha_proxy_installation "$verbose_flag"; then
                        log_success "Local HA proxy validation passed - proceeding with Kubernetes installation"
                    else
                        log_error "Local HA proxy validation failed after installation"
                        fail_component "kubernetes" "Local HA proxy validation failed after installation"
                        return 1
                    fi
                else
                    log_error "Local HA proxy installation failed"
                    echo -e ""
                    echo -e "${COLOR_BRIGHT_RED}${COLOR_BOLD}âŒ KUBERNETES INSTALLATION BLOCKED${COLOR_RESET}"
                    echo -e "${COLOR_RED}HA proxy installation failed and is required for Kubernetes.${COLOR_RESET}"
                    echo -e ""
                    echo -e "${COLOR_BRIGHT_YELLOW}${COLOR_BOLD}ðŸ”§ RESOLUTION STEPS:${COLOR_RESET}"
                    echo -e "${COLOR_YELLOW}1. Check Docker status: ${COLOR_BOLD}systemctl status docker${COLOR_RESET}"
                    echo -e "${COLOR_YELLOW}2. Verify configuration: ${COLOR_BOLD}echo \$API_SERVERS${COLOR_RESET}"
                    echo -e "${COLOR_YELLOW}3. Manual HA install: ${COLOR_BOLD}gok install haproxy${COLOR_RESET}"
                    echo -e "${COLOR_YELLOW}4. Set up remote host: ${COLOR_BOLD}gok remote setup <host> <user>${COLOR_RESET}"
                    echo -e "${COLOR_YELLOW}5. Retry Kubernetes installation${COLOR_RESET}"
                    echo -e ""
                    fail_component "kubernetes" "HA proxy installation required but local installation failed"
                    return 1
                fi
            fi
        fi
    else
        log_info "HA proxy already running - proceeding with Kubernetes installation"
    fi
    
    # Step 3: Install Kubernetes cluster
    if k8sInst "kubernetes" "$verbose_flag"; then
      log_success "Kubernetes master node installation completed"
      
      # Validate Kubernetes installation before proceeding
      if validate_component_installation "kubernetes" 300; then
        log_success "Kubernetes cluster validation passed"
      else
        log_warning "Kubernetes cluster validation had issues"
      fi
    else
      log_error "Kubernetes installation failed"
      fail_component "kubernetes" "Kubernetes cluster installation failed"
      return 1
    fi
    
    # Step 4: Install Helm package manager
    log_info "Installing Helm package manager..."
    if helmInst; then
      log_success "Helm package manager installed"
    else
      log_warning "Helm installation failed - some deployments may not work"
    fi
    
    # Step 5: Network plugin (Calico) is now installed within k8sInst
    log_info "Network plugin (Calico) was installed during cluster setup"
    
    # Step 6: Wait for system services to be ready
    log_info "Waiting for Kubernetes system services..."
    if waitForServiceAvailable kube-system; then
      log_success "Kubernetes system services are ready"
    else
      log_warning "Some system services may not be ready yet"
    fi
    
    # Step 7: Install additional utilities
    log_info "Installing cluster utilities..."
    if dnsUtils >/dev/null 2>&1; then
      log_success "DNS utilities installed"
      echo -e "    ${COLOR_DIM}â€¢ Pod: dnsutils (jessie-dnsutils:1.3) in default namespace${COLOR_RESET}"
      echo -e "    ${COLOR_DIM}â€¢ Usage: gok checkDns <domain> for DNS resolution testing${COLOR_RESET}"
    else
      log_warning "DNS utilities installation failed"
    fi
    if customDns >/dev/null 2>&1; then
      log_success "Custom DNS configured"
      echo -e "    ${COLOR_DIM}â€¢ CoreDNS custom zones: cloud.com, gokcloud.com â†’ ${MASTER_HOST_IP:-<master-ip>}${COLOR_RESET}"
      echo -e "    ${COLOR_DIM}â€¢ Kubernetes internal DNS: cloud.uat domain with 30s TTL${COLOR_RESET}"
      echo -e "    ${COLOR_DIM}â€¢ Fallback DNS: /etc/resolv.conf (max 1000 concurrent)${COLOR_RESET}"
    else
      log_warning "Custom DNS configuration failed"
    fi
    if kcurl >/dev/null 2>&1; then
      log_success "kubectl curl utility installed"
      echo -e "    ${COLOR_DIM}â€¢ Pod: curl (curlimages/curl) in default namespace${COLOR_RESET}"
      echo -e "    ${COLOR_DIM}â€¢ Usage: gok checkCurl <url> for HTTP testing within cluster${COLOR_RESET}"
    else
      log_warning "kubectl curl utility failed"
    fi
    if oauthAdmin >/dev/null 2>&1; then
      log_success "OAuth admin configured"
      echo -e "    ${COLOR_DIM}â€¢ ClusterRoleBinding: oauth-cluster-admin${COLOR_RESET}"
      echo -e "    ${COLOR_DIM}â€¢ Role: cluster-admin (full cluster access)${COLOR_RESET}"
      echo -e "    ${COLOR_DIM}â€¢ Subject: Group 'administrators' (OAuth group mapping)${COLOR_RESET}"
    else
      log_warning "OAuth admin configuration failed"
    fi
    
    # Step 8: Setup system integration
    log_info "Setting up system integration..."
    {
      grep -qxF "source $MOUNT_PATH/kubernetes/install_k8s/util" /etc/bash.bashrc || echo "source $MOUNT_PATH/kubernetes/install_k8s/util" >> /etc/bash.bashrc
      grep -qxF "source $MOUNT_PATH/kubernetes/install_k8s/gok" /etc/bash.bashrc || echo "source $MOUNT_PATH/kubernetes/install_k8s/gok" >> /etc/bash.bashrc
      ln -sf $MOUNT_PATH/kubernetes/install_k8s/gok /bin/gok
      
      cat <<EOF > /etc/rc.local
#!/bin/bash
/bin/gok start kubernetes
exit 0
EOF
      chmod +x /etc/rc.local
      systemctl enable rc-local >/dev/null 2>&1
      source /etc/bash.bashrc
    } >/dev/null 2>&1
    
    log_success "System integration configured"
    
    # Final validation and next steps
    if validate_component_installation "kubernetes" 180; then
      complete_component "kubernetes" "Kubernetes cluster installation completed and validated"
      suggest_and_install_next_module "kubernetes"
    else
      complete_component "kubernetes" "Kubernetes cluster installed but validation had warnings"
      suggest_and_install_next_module "kubernetes"
    fi
  else
    echo "Error: Unsupported component: $COMPONENT"
    echo ""
    echo "Supported components:"
    echo "Core: docker, kubernetes, kubernetes-worker, cert-manager, ingress, dashboard"
    echo "Monitoring: monitoring, fluentd, opensearch"
    echo "Security: keycloak, oauth2, vault, ldap"
    echo "Development: jupyter, devworkspace, workspace, che, ttyd, cloudshell, console"
    echo "CI/CD: argocd, jenkins, spinnaker, registry"
    echo "Networking: istio, rabbitmq"
    echo "Policy: kyverno"
    echo "GOK: gok-agent, gok-controller, controller, gok-login, chart"
    echo "Solutions: base, base-services"
    echo ""
    echo "Run 'gok install help' for detailed information"
    return 1
  fi
}

function fixCmd() {
  COMPONENT=$1
  
  if [ -z "$COMPONENT" ] || [ "$COMPONENT" == "help" ] || [ "$COMPONENT" == "--help" ]; then
    echo "gok fix - Fix common repository and installation issues"
    echo ""
    echo "Usage: gok fix <issue-type>"
    echo ""
    echo "Available Fix Options:"
    echo "  helm-repository    Fix Helm repository 404 errors and conflicts"
    echo "  repositories       Fix all package repository issues"
    echo "  package-conflicts  Resolve package manager conflicts"
    echo ""
    echo "Examples:"
    echo "  gok fix helm-repository    # Fix Helm 404 errors from baltocdn"
    echo "  gok fix repositories       # Fix all repository issues"
    echo ""
    echo "Common Issues Fixed:"
    echo "  âœ… Helm repository 404 errors (baltocdn)"
    echo "  âœ… Deprecated APT key warnings"
    echo "  âœ… Mixed installation method conflicts"
    echo "  âœ… Broken package repository configurations"
    echo "  âœ… GPG key verification issues"
    echo ""
    echo "Prevention Tips:"
    echo "  â€¢ Use snap for Helm: sudo snap install helm --classic"
    echo "  â€¢ Avoid mixing installation methods"
    echo "  â€¢ Regular repository cleanup"
    return 0
  fi
  
  case "$COMPONENT" in
    "helm-repository"|"helm-repo"|"helm")
      fix_helm_repository_errors
      ;;
    "repositories"|"repos"|"repository")  
      fix_helm_repository_errors
      # Can be extended for other repository fixes
      ;;
    "package-conflicts"|"packages")
      log_info "Package conflict resolution not yet implemented"
      echo "Available: helm-repository, repositories"
      return 1
      ;;
    *)
      echo "Error: Unknown fix type: $COMPONENT"
      echo ""
      echo "Available fix options:"
      echo "  helm-repository    Fix Helm repository 404 errors"
      echo "  repositories       Fix all repository issues"
      echo ""
      echo "Run 'gok fix help' for detailed information"
      return 1
      ;;
  esac
}

function startCmd() {
  COMPONENT=$1
  
  # Handle help flags
  if [[ "$COMPONENT" == "--help" || "$COMPONENT" == "-h" || "$COMPONENT" == "help" ]]; then
    echo
    echo "ðŸš€ GOK Start Command Help"
    echo "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€"
    echo
    echo "Purpose: Start system services and components"
    echo
    echo "Usage: gok start <component>"
    echo
    echo "Available Components:"
    echo "  kubernetes    - Start complete Kubernetes cluster (kubelet + haproxy)"
    echo "  kubelet       - Start kubelet service"
    echo "  proxy         - Start HAProxy load balancer"
    echo "  ha            - Start HAProxy load balancer (alias for proxy)"
    echo "  docker        - Start Docker service"
    echo "  containerd    - Start containerd service"
    echo
    echo "Examples:"
    echo "  gok start kubernetes    # Start full Kubernetes cluster"
    echo "  gok start kubelet       # Start only kubelet service"
    echo "  gok start docker        # Start Docker service"
    echo "  gok start --help        # Show this help message"
    echo
    return 0
  fi
  
  # Check if component is provided
  if [ -z "$COMPONENT" ]; then
    log_error "No component specified for start command"
    echo
    echo "Usage: gok start <component>"
    echo
    echo "Available components:"
    echo "  kubernetes    - Start Kubernetes cluster (kubelet + haproxy)"
    echo "  kubelet       - Start kubelet service"
    echo "  proxy         - Start HAProxy load balancer"
    echo "  ha            - Start HAProxy load balancer (alias for proxy)"
    echo "  docker        - Start Docker service"
    echo "  containerd    - Start containerd service"
    echo
    return 1
  fi
  
  log_component_start "$COMPONENT" "Starting service"
  
  case "$COMPONENT" in
    "kubernetes")
      log_step "1" "Disabling swap"
      if disableSwap; then
        log_success "Swap disabled"
      else
        log_warning "Failed to disable swap"
      fi
      
      log_step "2" "Starting HAProxy"
      if startHa; then
        log_success "HAProxy started successfully"
      else
        log_error "Failed to start HAProxy"
        return 1
      fi
      
      log_step "3" "Starting kubelet"
      if startKubelet; then
        log_success "Kubelet started successfully"
      else
        log_error "Failed to start kubelet"
        return 1
      fi
      
      log_component_success "$COMPONENT" "Kubernetes cluster services started"
      ;;
      
    "proxy"|"ha")
      log_step "1" "Starting HAProxy"
      if startHa; then
        log_component_success "$COMPONENT" "HAProxy started successfully"
      else
        log_error "Failed to start HAProxy"
        return 1
      fi
      ;;
      
    "kubelet")
      log_step "1" "Starting kubelet service"
      if startKubelet; then
        log_component_success "$COMPONENT" "Kubelet started successfully"
      else
        log_error "Failed to start kubelet"
        return 1
      fi
      ;;
      
    "docker")
      log_step "1" "Starting Docker service"
      if execute_with_suppression systemctl start docker; then
        if execute_with_suppression systemctl enable docker; then
          log_component_success "$COMPONENT" "Docker service started and enabled"
        else
          log_component_success "$COMPONENT" "Docker service started (enable failed)"
        fi
      else
        log_error "Failed to start Docker service"
        return 1
      fi
      ;;
      
    "containerd")
      log_step "1" "Starting containerd service"
      if execute_with_suppression systemctl start containerd; then
        if execute_with_suppression systemctl enable containerd; then
          log_component_success "$COMPONENT" "Containerd service started and enabled"
        else
          log_component_success "$COMPONENT" "Containerd service started (enable failed)"
        fi
      else
        log_error "Failed to start containerd service"
        return 1
      fi
      ;;
      
    *)
      log_error "Unknown component: $COMPONENT"
      echo
      echo "Available components:"
      echo "  kubernetes    - Start Kubernetes cluster (kubelet + haproxy)"
      echo "  kubelet       - Start kubelet service"
      echo "  proxy         - Start HAProxy load balancer"
      echo "  ha            - Start HAProxy load balancer (alias for proxy)"
      echo "  docker        - Start Docker service"
      echo "  containerd    - Start containerd service"
      echo
      return 1
      ;;
  esac
}

# Enhanced user confirmation with detailed information
confirm_reset() {
    local component="$1"
    local description="$2"
    local warning="$3"
    
    echo
    log_section "ðŸ”„ Reset Confirmation" "${EMOJI_WARNING}"
    
    echo -e "${COLOR_CYAN}Component:${COLOR_RESET} ${COLOR_BOLD}$component${COLOR_RESET}"
    echo -e "${COLOR_CYAN}Description:${COLOR_RESET} $description"
    
    if [ -n "$warning" ]; then
        echo
        log_warning "$warning"
    fi
    
    echo
    echo -e "${COLOR_RED}${COLOR_BOLD}âš ï¸  WARNING: This operation is destructive and irreversible!${COLOR_RESET}"
    echo -e "${COLOR_RED}   â€¢ All data and configurations will be permanently lost${COLOR_RESET}"
    echo -e "${COLOR_RED}   â€¢ Backup your important data before proceeding${COLOR_RESET}"
    echo -e "${COLOR_RED}   â€¢ Some operations require cluster admin privileges${COLOR_RESET}"
    echo
    
    while true; do
        echo -n -e "${COLOR_YELLOW}Do you want to continue? [y/N]: ${COLOR_RESET}"
        read -r response
        case $response in
            [Yy]|[Yy][Ee][Ss]) 
                log_info "User confirmed reset operation"
                return 0
                ;;
            [Nn]|[Nn][Oo]|"") 
                log_info "Reset operation cancelled by user"
                echo -e "${COLOR_GREEN}Reset cancelled. No changes made.${COLOR_RESET}"
                return 1
                ;;
            *) 
                echo -e "${COLOR_RED}Please answer yes or no.${COLOR_RESET}"
                ;;
        esac
    done
}

# Enhanced cleanup function for Kubernetes files and directories
cleanup_kubernetes_files() {
    local verbose_mode="$1"
    local is_verbose=false
    
    if [[ "$verbose_mode" == "--verbose" ]] || [[ "$GOK_VERBOSE" == "true" ]]; then
        is_verbose=true
    fi
    
    log_step 1 "Cleaning up Kubernetes configuration files"
    
    # Kubernetes config directories
    local k8s_dirs=(
        "/etc/kubernetes"
        "/var/lib/kubelet"
        "/var/lib/kube-proxy"
        "/var/lib/kube-scheduler" 
        "/var/lib/kube-controller-manager"
        "/var/lib/etcd"
        "/opt/cni/bin"
        "/etc/cni/net.d"
        "/var/lib/cni"
        "/var/run/kubernetes"
        "/etc/systemd/system/kubelet.service.d"
    )
    
    # User kubeconfig files
    local user_configs=(
        "$HOME/.kube"
        "/root/.kube"
    )
    
    # Container runtime directories
    local container_dirs=(
        "/var/lib/docker/containers"
        "/var/lib/containerd"
        "/run/containerd"
        "/var/lib/dockershim"
    )
    
    # Network configuration
    local network_files=(
        "/etc/cni/net.d/*"
        "/opt/cni/bin/*"
        "/var/lib/calico"
        "/var/lib/canal"
        "/var/lib/weave"
    )
    
    log_info "Stopping Kubernetes services..."
    
    # Stop services gracefully
    local services=("kubelet" "kube-proxy" "docker" "containerd")
    for service in "${services[@]}"; do
        if systemctl is-active --quiet "$service" 2>/dev/null; then
            if [[ "$is_verbose" == "true" ]]; then
                log_substep "Stopping $service service"
            fi
            systemctl stop "$service" 2>/dev/null || log_warning "Failed to stop $service"
        fi
    done
    
    # Remove Kubernetes directories
    log_info "Removing Kubernetes directories..."
    for dir in "${k8s_dirs[@]}"; do
        if [ -d "$dir" ]; then
            if [[ "$is_verbose" == "true" ]]; then
                log_substep "Removing directory: $dir"
            fi
            rm -rf "$dir" 2>/dev/null || log_warning "Failed to remove $dir"
        fi
    done
    
    # Handle user kubeconfig files
    log_info "Cleaning up kubeconfig files..."
    for config in "${user_configs[@]}"; do
        if [ -d "$config" ]; then
            if [[ "$is_verbose" == "true" ]]; then
                log_substep "Backing up and removing: $config"
            fi
            # Create backup before removing
            if [ -f "$config/config" ]; then
                cp "$config/config" "$config/config.backup.$(date +%Y%m%d_%H%M%S)" 2>/dev/null
                if [[ "$is_verbose" == "true" ]]; then
                    log_substep "Created backup: $config/config.backup.$(date +%Y%m%d_%H%M%S)"
                fi
            fi
            rm -rf "$config" 2>/dev/null || log_warning "Failed to remove $config"
        fi
    done
    
    # Clean network configurations
    log_info "Cleaning up network configurations..."
    for net_path in "${network_files[@]}"; do
        if ls $net_path 1> /dev/null 2>&1; then
            if [[ "$is_verbose" == "true" ]]; then
                log_substep "Removing network files: $net_path"
            fi
            rm -rf $net_path 2>/dev/null || log_warning "Failed to remove $net_path"
        fi
    done
    
    # Clean container runtime (always do full cleanup for kubernetes reset)
    log_info "Performing container runtime cleanup..."
        
        # Stop and remove all containers
        if command -v docker &> /dev/null; then
            if [[ "$is_verbose" == "true" ]]; then
                log_substep "Stopping and removing Docker containers"
            fi
            docker stop $(docker ps -aq) 2>/dev/null || true
            docker rm $(docker ps -aq) 2>/dev/null || true
            docker system prune -af 2>/dev/null || true
        fi
        
        # Clean containerd
        if command -v ctr &> /dev/null; then
            if [[ "$is_verbose" == "true" ]]; then
                log_substep "Cleaning containerd containers and images"
            fi
            ctr -n k8s.io containers rm $(ctr -n k8s.io containers list -q) 2>/dev/null || true
            ctr -n k8s.io images rm $(ctr -n k8s.io images list -q) 2>/dev/null || true
        fi
        
        # Clean container directories
        for dir in "${container_dirs[@]}"; do
            if [ -d "$dir" ]; then
                if [[ "$is_verbose" == "true" ]]; then
                    log_substep "Cleaning container directory: $dir"
                fi
                find "$dir" -type f -name "*.pid" -delete 2>/dev/null || true
                find "$dir" -type f -name "*.lock" -delete 2>/dev/null || true
            fi
        done
    
    # Clean systemd files
    log_info "Cleaning up systemd service files..."
    local systemd_files=(
        "/etc/systemd/system/kubelet.service"
        "/etc/systemd/system/kubelet.service.d"
        "/etc/systemd/system/kube-proxy.service"
        "/lib/systemd/system/kubelet.service"
    )
    
    for file in "${systemd_files[@]}"; do
        if [ -e "$file" ]; then
            if [[ "$is_verbose" == "true" ]]; then
                log_substep "Removing systemd file: $file"
            fi
            rm -rf "$file" 2>/dev/null || log_warning "Failed to remove $file"
        fi
    done
    
    # Reload systemd daemon
    if [[ "$is_verbose" == "true" ]]; then
        log_substep "Reloading systemd daemon"
    fi
    systemctl daemon-reload 2>/dev/null || log_warning "Failed to reload systemd daemon"
    
    # Clean up iptables rules (optional)
    log_info "Cleaning up iptables rules..."
    iptables -F 2>/dev/null || log_warning "Failed to flush iptables rules"
    iptables -t nat -F 2>/dev/null || log_warning "Failed to flush NAT rules"
    iptables -t mangle -F 2>/dev/null || log_warning "Failed to flush mangle rules"
    
    # Clean up network interfaces
    log_info "Cleaning up network interfaces..."
    local interfaces
    interfaces=$(ip link show 2>/dev/null | grep -E "(cni|flannel|calico|weave)" | awk -F: '{print $2}' | tr -d ' ' | grep -v '^$')
    
    if [[ -n "$interfaces" ]]; then
        while IFS= read -r iface; do
            if [[ -n "$iface" && "$iface" != "ee" ]]; then  # Skip invalid interface names
                if [[ "$is_verbose" == "true" ]]; then
                    log_substep "Removing network interface: $iface"
                fi
                ip link delete "$iface" 2>/dev/null || log_warning "Failed to remove interface $iface"
            fi
        done <<< "$interfaces"
    else
        if [[ "$is_verbose" == "true" ]]; then
            log_substep "No network interfaces to clean up"
        fi
    fi
    
    log_success "Kubernetes file cleanup completed"
}

# Enhanced progress tracking for reset operations
track_reset_progress() {
    local message="$1"
    local timestamp=$(date '+%Y-%m-%d %H:%M:%S')
    
    echo -e "${COLOR_BRIGHT_BLUE}[${timestamp}]${COLOR_RESET} ${COLOR_CYAN}${message}${COLOR_RESET}"
}

function resetCmd() {
  COMPONENT=$1
  
  if [ -z "$COMPONENT" ] || [ "$COMPONENT" == "help" ] || [ "$COMPONENT" == "--help" ]; then
    echo "gok reset - Reset and uninstall Kubernetes components and services"
    echo ""
    echo "Usage: gok reset <component> [--verbose|-v]"
    echo ""
    echo "Options:"
    echo "  --verbose, -v     Show detailed reset output (default: progress indicators)"
    echo "  GOK_VERBOSE=true  Environment variable to enable verbose mode globally"
    echo ""
    echo "Core Infrastructure:"
    echo "  kubernetes        Complete Kubernetes cluster reset"
    echo "  kubernetes-worker Kubernetes worker node reset"
    echo "  cert-manager      Certificate management system"
    echo "  ingress           NGINX ingress controller"
    echo ""
    echo "Monitoring & Logging:"
    echo "  monitoring        Prometheus and Grafana stack"
    echo "  fluentd           Log collection and forwarding"
    echo "  opensearch        Search and analytics engine with dashboard"
    echo ""
    echo "Security & Identity:"
    echo "  keycloak          Identity and access management"
    echo "  oauth2            OAuth2 proxy for authentication"
    echo "  vault             Secrets management"
    echo "  ldap              LDAP directory service"
    echo ""
    echo "Development Tools:"
    echo "  jupyter           JupyterHub for data science"
    echo "  devworkspace      Developer workspace (legacy)"
    echo "  workspace         Enhanced developer workspace"
    echo "  che               Eclipse Che IDE"
    echo "  ttyd              Terminal over HTTP"
    echo "  cloudshell        Cloud-based terminal"
    echo "  console           Web-based console"
    echo "  dashboard         Kubernetes web dashboard"
    echo ""
    echo "CI/CD & DevOps:"
    echo "  argocd            GitOps continuous delivery"
    echo "  jenkins           CI/CD automation server"
    echo "  spinnaker         Multi-cloud deployment platform"
    echo "  registry          Container image registry"
    echo ""
    echo "Service Mesh & Networking:"
    echo "  istio             Service mesh for microservices"
    echo "  rabbitmq          Message broker"
    echo ""
    echo "Governance & Policy:"
    echo "  kyverno           Kubernetes policy engine"
    echo ""
    echo "GOK Platform:"
    echo "  gok-agent         GOK distributed system agent"
    echo "  gok-controller    GOK distributed system controller"
    echo "  controller        Reset both gok-agent and gok-controller"
    echo "  gok-login         GOK authentication service"
    echo "  chart             Helm chart repository"
    echo ""
    echo "Complete Solutions:"
    echo "  base-services     Complete base services stack"
    echo ""
    echo "Examples:"
    echo "  gok reset monitoring          # Remove Prometheus & Grafana"
    echo "  gok reset keycloak            # Remove identity management"
    echo "  gok reset kubernetes          # Reset entire cluster"
    echo "  gok reset base-services       # Remove complete base stack"
    echo ""
    echo "Reset Operations:"
    echo "  âœ… Clean uninstallation of Helm releases"
    echo "  âœ… Removal of Kubernetes namespaces and resources"
    echo "  âœ… Cleanup of persistent volumes and storage"
    echo "  âœ… Reset of system configurations"
    echo "  âœ… Removal of certificates and secrets"
    echo "  âœ… Network policy and ingress cleanup"
    echo "  âœ… Service mesh configuration reset"
    echo ""
    echo "âš ï¸  Warning:"
    echo "  - Reset operations are destructive and irreversible"
    echo "  - All data and configurations will be permanently lost"
    echo "  - Ensure you have backups before proceeding"
    echo "  - Some operations require cluster admin privileges"
    return 0
  fi
  
  # Check for verbose flag in all arguments
  local verbose_flag=""
  for arg in "$@"; do
    if [[ "$arg" == "--verbose" ]] || [[ "$arg" == "-v" ]]; then
      verbose_flag="--verbose"
      export GOK_VERBOSE="true"
      log_info "Verbose logging enabled for reset operation"
      break
    fi
  done
  
  # Also check environment variable
  if [[ "$GOK_VERBOSE" == "true" ]] && [[ "$verbose_flag" == "" ]]; then
    verbose_flag="--verbose"
    log_info "Verbose logging enabled for reset operation via GOK_VERBOSE"
  fi
  
  if [ "$COMPONENT" == "kubernetes" ]; then
    track_reset_progress "Starting Kubernetes cluster reset..."
    
    # Check if any Kubernetes components are installed
    local kubeadm_installed=false
    local kubectl_installed=false
    local kubelet_installed=false
    
    if command -v kubeadm >/dev/null 2>&1; then
      kubeadm_installed=true
    fi
    
    if command -v kubectl >/dev/null 2>&1; then
      kubectl_installed=true
    fi
    
    if command -v kubelet >/dev/null 2>&1; then
      kubelet_installed=true
    fi
    
    # If no Kubernetes components are found, exit gracefully
    if [[ "$kubeadm_installed" == "false" && "$kubectl_installed" == "false" && "$kubelet_installed" == "false" ]]; then
      log_info "No Kubernetes components found to reset."
      log_info "Kubernetes appears to be not installed - nothing to reset."
      return 0
    fi
    
    # Show what components were found
    if [[ "$verbose_flag" == "--verbose" ]]; then
      log_info "Found Kubernetes components:"
      [[ "$kubeadm_installed" == "true" ]] && echo -e "  ${COLOR_GREEN}âœ“ kubeadm${COLOR_RESET}" || echo -e "  ${COLOR_DIM}- kubeadm (not found)${COLOR_RESET}"
      [[ "$kubectl_installed" == "true" ]] && echo -e "  ${COLOR_GREEN}âœ“ kubectl${COLOR_RESET}" || echo -e "  ${COLOR_DIM}- kubectl (not found)${COLOR_RESET}"
      [[ "$kubelet_installed" == "true" ]] && echo -e "  ${COLOR_GREEN}âœ“ kubelet${COLOR_RESET}" || echo -e "  ${COLOR_DIM}- kubelet (not found)${COLOR_RESET}"
    fi
    
    # Get user confirmation
    if ! confirm_reset "Kubernetes cluster" "This will completely remove the Kubernetes cluster and all data"; then
      echo "Kubernetes reset cancelled by user."
      return 0
    fi
    
    # Only run kubeadm reset if kubeadm is installed
    if [[ "$kubeadm_installed" == "true" ]]; then
      track_reset_progress "Performing kubeadm reset..."
      if [[ "$verbose_flag" == "--verbose" ]]; then
        kubeadm reset <<EOF
y
EOF
      else
        kubeadm reset <<EOF 2>/dev/null || { log_warning "kubeadm reset failed, continuing with cleanup"; }
y
EOF
      fi
    else
      track_reset_progress "Skipping kubeadm reset (kubeadm not found)..."
    fi
    
    track_reset_progress "Removing Kubernetes packages..."
    remove_kubernetes_packages "$verbose_flag"
    
    track_reset_progress "Cleaning up Kubernetes files and configurations..."
    cleanup_kubernetes_files "$verbose_flag"
    
    track_reset_progress "Kubernetes reset completed successfully."
  elif [ "$COMPONENT" == "monitoring" ]; then
    prometheusGrafanaResetv2

    emptyLocalFsStorage "Monitoring" "prometheus-pv" "prometheus-storage" "/data/volumes/pv1"
    emptyLocalFsStorage "Monitoring" "alertmanager-pv" "alertmanager-storage" "/data/volumes/pv2"
  elif [ "$COMPONENT" == "dashboard" ]; then
    dashboardReset
  elif [ "$COMPONENT" == "keycloak" ]; then
    keycloakReset
  elif [ "$COMPONENT" == "vault" ]; then
    vaultReset
  elif [ "$COMPONENT" == "ingress" ]; then
    ingressReset
  elif [ "$COMPONENT" == "chart" ]; then
    resetChart
  elif [ "$COMPONENT" == "gok-login" ]; then
    helm uninstall gok-login -n gok-login
    kubectl delete ns gok-login
  elif [ "$COMPONENT" == "ldap" ]; then
    ldapReset
  elif [ "$COMPONENT" == "gok-agent" ]; then
    gokAgentReset
  elif [ "$COMPONENT" == "gok-controller" ]; then
    gokControllerReset
  elif [ "$COMPONENT" == "controller" ]; then
    gok reset gok-agent
    gok reset gok-controller
  elif [ "$COMPONENT" == "argocd" ]; then
    argocdReset
  elif [ "$COMPONENT" == "devworkspace" ]; then
    deleteDevWorkspace
  elif [ "$COMPONENT" == "workspace" ]; then
    deleteDevWorkspaceV2
  elif [ "$COMPONENT" == "registry" ]; then
    resetDockerRegistry
  elif [ "$COMPONENT" == "fluentd" ]; then
    fluentdReset
  elif [ "$COMPONENT" == "jupyter" ]; then
    jupyterHubReset
  elif [ "$COMPONENT" == "rabbitmq" ]; then
    rabbitmqReset
  elif [ "$COMPONENT" == "che" ]; then
    resetEclipseChe
  elif [ "$COMPONENT" == "ttyd" ]; then
    ttydReset
  elif [ "$COMPONENT" == "cloudshell" ]; then
    cloudshellReset
  elif [ "$COMPONENT" == "console" ]; then
    consoleReset
  elif [ "$COMPONENT" == "opensearch" ]; then
    opensearchDashReset
    opensearchReset
  elif [ "$COMPONENT" == "jenkins" ]; then
    jenkinsReset
  elif [ "$COMPONENT" == "spinnaker" ]; then
    spinnakerReset
  elif [ "$COMPONENT" == "oauth2" ]; then
    oauth2ProxyReset
  elif [ "$COMPONENT" == "cert-manager" ]; then
    certManagerReset
  elif [ "$COMPONENT" == "istio" ]; then
    istioReset
  elif [ "$COMPONENT" == "kyverno" ]; then
    kyvernoReset
  elif [ "$COMPONENT" == "base-services" ]; then
    resetBaseServices
  elif [ "$COMPONENT" == "kubernetes-worker" ]; then
    kubeadm reset <<EOF
y
EOF
  else
    echo "Error: Unsupported component: $COMPONENT"
    echo ""
    echo "Supported components:"
    echo "Core: kubernetes, kubernetes-worker, cert-manager, ingress, dashboard"
    echo "Monitoring: monitoring, fluentd, opensearch"
    echo "Security: keycloak, oauth2, vault, ldap"
    echo "Development: jupyter, devworkspace, workspace, che, ttyd, cloudshell, console"
    echo "CI/CD: argocd, jenkins, spinnaker, registry"
    echo "Networking: istio, rabbitmq"
    echo "Policy: kyverno"
    echo "GOK: gok-agent, gok-controller, controller, gok-login, chart"
    echo "Solutions: base-services"
    echo ""
    echo "Run 'gok reset help' for detailed information"
    return 1
  fi
}

function deployCmd() {
  COMPONENT=$2
  if [ "$COMPONENT" == "app1" ]; then
    createApp1
  fi
}

function patchCmd() {
  RESOURCE=$1
  NAME=$2
  NS=$3
  OPTIONS=$4
  SUBDOMAIN=$5
  if [ "$RESOURCE" == "ingress" ]; then
    if [ "$OPTIONS" == "letsencrypt" ]; then
      patchLetsEncrypt "$NAME" "$NS" "$SUBDOMAIN"
    elif [ "$OPTIONS" == "ldap" ]; then
      patchLdapSecure "$NAME" "$NS"
    elif [ "$OPTIONS" == "localtls" ]; then
      patchLocalTls "$NAME" "$NS"
    fi
  fi
}

function createCmd() {
  RESOURCE=$1
  NAME=$2
  ADDITIONAL=$3
  
  if [ -z "$RESOURCE" ] || [ "$RESOURCE" == "help" ] || [ "$RESOURCE" == "--help" ]; then
    echo "gok create - Create Kubernetes resources and configurations"
    echo ""
    echo "Usage: gok create <resource> <name> [additional]"
    echo ""
    echo "Resource Types:"
    echo "  secret            Create Kubernetes secrets"
    echo "  certificate       Create TLS certificates for namespaces"
    echo "  kubeconfig        Create kubeconfig files for users"
    echo ""
    echo "Parameters:"
    echo "  <resource>        Resource type (required)"
    echo "  <name>            Resource name (required)"
    echo "  [additional]      Additional parameters (depends on resource type)"
    echo ""
    echo "Secret Resources:"
    echo "  apphost           Create application host secret"
    echo ""
    echo "Certificate Resources:"
    echo "  <namespace>       Create certificate request for specified namespace"
    echo "  [domain]          Custom domain name (optional)"
    echo ""
    echo "Kubeconfig Resources:"
    echo "  <username>        Create kubeconfig for specified user"
    echo ""
    echo "Examples:"
    echo "  gok create secret apphost"
    echo "  gok create certificate production"
    echo "  gok create certificate staging custom.domain.com"
    echo "  gok create kubeconfig developer-user"
    echo ""
    echo "Generated Resources:"
    echo "  âœ… Kubernetes secrets with proper encoding"
    echo "  âœ… TLS certificates with proper CA signing"
    echo "  âœ… Kubeconfig with cluster access configuration"
    echo "  âœ… Namespace-scoped certificate requests"
    echo "  âœ… Proper RBAC and security configurations"
    return 0
  fi
  
  if [ -z "$NAME" ]; then
    echo "Error: Resource name is required"
    echo "Usage: gok create <resource> <name> [additional]"
    echo "Run 'gok create help' for detailed usage information"
    return 1
  fi
  
  if [ "$RESOURCE" == "secret" ]; then
    if [ "$NAME" == "apphost" ]; then
      hostSecret
    else
      echo "Error: Unsupported secret type: $NAME"
      echo "Supported secret types: apphost"
      echo "Run 'gok create help' for detailed usage information"
      return 1
    fi
  elif [ "$RESOURCE" == "certificate" ]; then
    certificateRequestForNs "$NAME" "$ADDITIONAL"
  elif [ "$RESOURCE" == "kubeconfig" ]; then
    createKubeConfig "$NAME"
  else
    echo "Error: Unsupported resource type: $RESOURCE"
    echo "Supported resource types: secret, certificate, kubeconfig"
    echo "Run 'gok create help' for detailed usage information"
    return 1
  fi
}

function generateCmd() {
  SERVICE_TYPE=$1
  SERVICE_NAME=$2
  SERVICE_DESC=$3
  SERVICE_NS=$4
  SERVICE_HOST=$5
  
  if [ -z "$SERVICE_TYPE" ] || [ "$SERVICE_TYPE" == "help" ] || [ "$SERVICE_TYPE" == "--help" ]; then
    echo "gok generate - Generate microservices from templates"
    echo ""
    echo "Usage: gok generate <type> <name> [description] [namespace] [hostname]"
    echo ""
    echo "Service Types:"
    echo "  python-api        Generate Python Flask REST API (backend-only)"
    echo "  python-reactjs    Generate Python Flask + React.js (full-stack)"
    echo ""
    echo "Parameters:"
    echo "  <type>            Service type (required)"
    echo "  <name>            Service name (required)"
    echo "  [description]     Service description (optional)"
    echo "  [namespace]       Kubernetes namespace (optional, default: service name)"
    echo "  [hostname]        Ingress hostname (optional, default: <name>.gokcloud.com)"
    echo ""
    echo "Examples:"
    echo "  gok generate python-api user-service"
    echo "  gok generate python-api payment-api 'Payment processing API'"
    echo "  gok generate python-reactjs webapp 'Customer portal' production portal.company.com"
    echo ""
    echo "Generated Features:"
    echo "  âœ… Production-ready Docker containers"
    echo "  âœ… Kubernetes Helm charts with RBAC"
    echo "  âœ… OAuth2/OIDC authentication"
    echo "  âœ… HTTPS/TLS encryption"
    echo "  âœ… Health check endpoints"
    echo "  âœ… API documentation (Swagger UI)"
    echo "  âœ… Local development setup"
    return 0
  fi
  
  if [ -z "$SERVICE_NAME" ]; then
    echo "Error: Service name is required"
    echo "Usage: gok generate <type> <name> [description] [namespace] [hostname]"
    echo "Run 'gok generate help' for detailed usage information"
    return 1
  fi
  
  # Navigate to service generator directory
  SERVICE_GENERATOR_DIR="$WORKING_DIR/service-generator"
  
  # Fallback to current directory structure if WORKING_DIR not set
  if [ ! -d "$SERVICE_GENERATOR_DIR" ]; then
    # Try relative path from gok script location
    SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
    SERVICE_GENERATOR_DIR="$SCRIPT_DIR/service-generator"
    
    # If still not found, try current working directory
    if [ ! -d "$SERVICE_GENERATOR_DIR" ]; then
      SERVICE_GENERATOR_DIR="$(pwd)/service-generator"
    fi
    
    # Final check - see if we're already in service-generator directory
    if [ ! -d "$SERVICE_GENERATOR_DIR" ] && [ -f "generate_service.py" ]; then
      SERVICE_GENERATOR_DIR="$(pwd)"
    fi
  fi
  
  if [ ! -d "$SERVICE_GENERATOR_DIR" ] && [ ! -f "$SERVICE_GENERATOR_DIR/generate_service.py" ]; then
    echo "Error: Service generator not found"
    echo "Tried locations:"
    echo "  - $WORKING_DIR/service-generator"
    echo "  - $SCRIPT_DIR/service-generator" 
    echo "  - $(pwd)/service-generator"
    echo "  - $(pwd)"
    return 1
  fi
  
  cd "$SERVICE_GENERATOR_DIR"
  
  # Build command arguments
  GENERATE_ARGS=""
  
  # Add description if provided
  if [ ! -z "$SERVICE_DESC" ]; then
    GENERATE_ARGS="$GENERATE_ARGS --description \"$SERVICE_DESC\""
  fi
  
  # Add namespace if provided
  if [ ! -z "$SERVICE_NS" ]; then
    GENERATE_ARGS="$GENERATE_ARGS --namespace $SERVICE_NS"
  fi
  
  # Add ingress hostname if provided
  if [ ! -z "$SERVICE_HOST" ]; then
    GENERATE_ARGS="$GENERATE_ARGS --ingress-host $SERVICE_HOST"
  fi
  
  echo "ðŸš€ Generating $SERVICE_TYPE service: $SERVICE_NAME"
  echo "ðŸ“ Working directory: $SERVICE_GENERATOR_DIR"
  
  if [ "$SERVICE_TYPE" == "python-api" ]; then
    echo "ðŸ Creating Python Flask API service..."
    eval "python3 generate_service.py --python-api $SERVICE_NAME $GENERATE_ARGS"
    
  elif [ "$SERVICE_TYPE" == "python-reactjs" ]; then
    echo "ðŸâš›ï¸  Creating Python Flask + React.js full-stack service..."
    eval "python3 generate_service.py --python-reactjs $SERVICE_NAME $GENERATE_ARGS"
    
  else
    echo "Error: Unsupported service type: $SERVICE_TYPE"
    echo "Supported types: python-api, python-reactjs"
    return 1
  fi
  
  if [ $? -eq 0 ]; then
    echo ""
    echo "âœ… Service generation completed successfully!"
    echo "ðŸ“ Location: $SERVICE_GENERATOR_DIR/generated_services/$SERVICE_NAME"
    echo ""
    echo "ðŸš€ Next steps:"
    echo "1. cd $SERVICE_GENERATOR_DIR/generated_services/$SERVICE_NAME"
    echo "2. Review and customize the generated files"
    echo "3. Build container: ./build.sh"
    echo "4. Push to registry: ./tag_push.sh"
    echo "5. Deploy to Kubernetes: helm install $SERVICE_NAME ./chart"
    
    if [ ! -z "$SERVICE_HOST" ]; then
      echo "6. Access your service at: https://$SERVICE_HOST"
    else
      echo "6. Access your service at: https://$SERVICE_NAME.gokcloud.com"
    fi
  else
    echo "âŒ Service generation failed!"
    return 1
  fi
}

# Bash completion function for GOK
_gok_completion() {
  local cur prev opts base
  COMPREPLY=()
  cur="${COMP_WORDS[COMP_CWORD]}"
  prev="${COMP_WORDS[COMP_CWORD-1]}"
  
  # Main commands
  local commands="install fix reset start deploy patch create generate bash desc logs status taint-node k8sSummary ingressSummary certManagerSummary kyvernoSummary registrySummary ldapSummary keycloakSummary oauth2ProxySummary rabbitmqSummary vaultSummary ingressReset certManagerReset prometheusGrafanaReset dashboardReset vaultReset cloudshellReset consoleReset jupyterHubReset ttydReset resetChart jenkinsReset oauth2ProxyReset kyvernoReset k8sInst k8sReset calicoInst dnsUtils kcurl customDns oauthAdmin checkDns checkCurl completion help"
  
  # Install components
  local install_components="docker helm kubernetes kubernetes-worker cert-manager ingress dashboard monitoring fluentd opensearch keycloak oauth2 vault ldap jupyter devworkspace workspace che ttyd cloudshell console argocd jenkins spinnaker registry istio rabbitmq kyverno gok-agent gok-controller controller gok-login chart base base-services"
  
  # Reset components  
  local reset_components="kubernetes kubernetes-worker cert-manager ingress dashboard monitoring fluentd opensearch keycloak oauth2 vault ldap jupyter devworkspace workspace che ttyd cloudshell console argocd jenkins spinnaker registry istio rabbitmq kyverno gok-agent gok-controller controller gok-login chart base-services"
  
  # Start components
  local start_components="kubernetes proxy kubelet"
  
  # Deploy components
  local deploy_components="app1"
  
  # Fix options
  local fix_options="helm-repository repositories package-conflicts"
  
  # Create resources
  local create_resources="secret certificate kubeconfig"
  local create_secret_types="apphost"
  
  # Generate service types
  local generate_types="python-api python-reactjs"
  
  # Patch resources
  local patch_resources="ingress"
  local patch_options="letsencrypt ldap localtls"
  
  case ${COMP_CWORD} in
    1)
      # First argument - complete main commands
      COMPREPLY=($(compgen -W "${commands}" -- ${cur}))
      return 0
      ;;
    2)
      # Second argument - complete based on first command
      case ${prev} in
        install)
          COMPREPLY=($(compgen -W "${install_components} --verbose -v" -- ${cur}))
          return 0
          ;;
        reset)
          COMPREPLY=($(compgen -W "${reset_components} --verbose -v" -- ${cur}))
          return 0
          ;;
        fix)
          COMPREPLY=($(compgen -W "${fix_options}" -- ${cur}))
          return 0
          ;;
        start)
          COMPREPLY=($(compgen -W "${start_components}" -- ${cur}))
          return 0
          ;;
        deploy)
          COMPREPLY=($(compgen -W "${deploy_components}" -- ${cur}))
          return 0
          ;;
        create)
          COMPREPLY=($(compgen -W "${create_resources}" -- ${cur}))
          return 0
          ;;
        generate)
          COMPREPLY=($(compgen -W "${generate_types}" -- ${cur}))
          return 0
          ;;
        patch)
          COMPREPLY=($(compgen -W "${patch_resources}" -- ${cur}))
          return 0
          ;;
        desc)
          COMPREPLY=($(compgen -W "methods" -- ${cur}))
          return 0
          ;;
        k8sSummary|ingressSummary|certManagerSummary|kyvernoSummary|registrySummary|ldapSummary|keycloakSummary|oauth2ProxySummary|rabbitmqSummary|vaultSummary|ingressReset|k8sInst|calicoInst)
          COMPREPLY=($(compgen -W "--verbose -v" -- ${cur}))
          return 0
          ;;
        checkDns)
          COMPREPLY=($(compgen -W "kubernetes.default.svc.cloud.uat google.com example.com" -- ${cur}))
          return 0
          ;;
        checkCurl)
          COMPREPLY=($(compgen -W "https://kubernetes.default.svc http://google.com https://example.com" -- ${cur}))
          return 0
          ;;
        completion)
          COMPREPLY=($(compgen -W "enable setup" -- ${cur}))
          return 0
          ;;
      esac
      ;;
    3)
      # Third argument - complete based on first two commands
      local prev2="${COMP_WORDS[COMP_CWORD-2]}"
      case ${prev2} in
        install|reset)
          # If previous word is a component, suggest verbose flags
          if [[ "${install_components}" =~ ${prev} ]]; then
            COMPREPLY=($(compgen -W "--verbose -v" -- ${cur}))
            return 0
          fi
          ;;
        create)
          case ${prev} in
            secret)
              COMPREPLY=($(compgen -W "${create_secret_types}" -- ${cur}))
              return 0
              ;;
            certificate|kubeconfig)
              # For certificate and kubeconfig, suggest common names
              COMPREPLY=($(compgen -W "production staging development test" -- ${cur}))
              return 0
              ;;
          esac
          ;;
        generate)
          case ${prev} in
            python-api|python-reactjs)
              # Suggest common service names
              COMPREPLY=($(compgen -W "user-service auth-service api-gateway webapp frontend backend microservice" -- ${cur}))
              return 0
              ;;
          esac
          ;;
      esac
      ;;
    4)
      # Fourth argument - complete based on context
      local prev3="${COMP_WORDS[COMP_CWORD-3]}"
      local prev2="${COMP_WORDS[COMP_CWORD-2]}"
      case ${prev3} in
        patch)
          case ${prev2} in
            ingress)
              # Complete ingress names - could be dynamic from kubectl
              COMPREPLY=($(compgen -W "default production staging" -- ${cur}))
              return 0
              ;;
          esac
          ;;
      esac
      ;;
    5)
      # Fifth argument - namespaces for patch command
      local prev4="${COMP_WORDS[COMP_CWORD-4]}"
      local prev3="${COMP_WORDS[COMP_CWORD-3]}"
      case ${prev4} in
        patch)
          case ${prev3} in
            ingress)
              # Complete namespaces - could be dynamic from kubectl  
              COMPREPLY=($(compgen -W "default kube-system ingress-nginx monitoring" -- ${cur}))
              return 0
              ;;
          esac
          ;;
      esac
      ;;
    6)
      # Sixth argument - patch options
      local prev5="${COMP_WORDS[COMP_CWORD-5]}"
      case ${prev5} in
        patch)
          COMPREPLY=($(compgen -W "${patch_options}" -- ${cur}))
          return 0
          ;;
      esac
      ;;
  esac
  
  return 0
}

# Function to enable completion
enable_gok_completion() {
  complete -F _gok_completion gok
  echo "âœ… GOK tab completion enabled!"
  echo "ðŸ’¡ You can now use tab completion with gok commands:"
  echo "   gok install <TAB>     # Shows all installable components"
  echo "   gok reset <TAB>       # Shows all resettable components"  
  echo "   gok create <TAB>      # Shows resource types"
  echo "   gok generate <TAB>    # Shows service types"
  echo ""
  echo "ðŸ”§ Testing completion..."
  echo "   Try: gok ingr<TAB> (should complete to ingressSummary/ingressReset)"
  echo "   Try: gok install cert<TAB> (should complete to cert-manager)"
}

# Simple completion setup for current session only
quick_gok_completion() {
  # Enable completion immediately without sourcing the whole file
  complete -W "install fix reset start deploy patch create generate bash desc logs status taint-node k8sSummary ingressSummary certManagerSummary kyvernoSummary registrySummary ldapSummary keycloakSummary oauth2ProxySummary rabbitmqSummary vaultSummary ingressReset certManagerReset prometheusGrafanaReset dashboardReset vaultReset cloudshellReset consoleReset jupyterHubReset ttydReset resetChart jenkinsReset oauth2ProxyReset kyvernoReset k8sInst k8sReset calicoInst dnsUtils kcurl customDns oauthAdmin checkDns checkCurl completion help" gok
  echo "ðŸš€ Quick GOK completion enabled for this session!"
  echo "ðŸ’¡ Try: gok ingr<TAB>"
}

# Function to setup completion permanently
setup_gok_completion() {
  local completion_dir="/etc/bash_completion.d"
  local completion_file="$completion_dir/gok"
  local user_bashrc="$HOME/.bashrc"
  
  echo "Setting up GOK tab completion..."
  
  # Extract just the completion function to a separate file
  local temp_completion=$(mktemp)
  cat > "$temp_completion" << 'COMPLETION_EOF'
# GOK Bash Completion
_gok_completion() {
  local cur prev opts base
  COMPREPLY=()
  cur="${COMP_WORDS[COMP_CWORD]}"
  prev="${COMP_WORDS[COMP_CWORD-1]}"
  
  # Main commands
  local commands="install fix reset start deploy patch create generate bash desc logs status taint-node k8sSummary ingressSummary certManagerSummary kyvernoSummary registrySummary ldapSummary keycloakSummary oauth2ProxySummary rabbitmqSummary vaultSummary ingressReset certManagerReset prometheusGrafanaReset dashboardReset vaultReset cloudshellReset consoleReset jupyterHubReset ttydReset resetChart jenkinsReset oauth2ProxyReset kyvernoReset k8sInst k8sReset calicoInst dnsUtils kcurl customDns oauthAdmin checkDns checkCurl completion help"
  
  # Install components
  local install_components="docker helm kubernetes kubernetes-worker cert-manager ingress dashboard monitoring fluentd opensearch keycloak oauth2 vault ldap jupyter devworkspace workspace che ttyd cloudshell console argocd jenkins spinnaker registry istio rabbitmq kyverno gok-agent gok-controller controller gok-login chart base base-services"
  
  # Reset components  
  local reset_components="kubernetes kubernetes-worker cert-manager ingress dashboard monitoring fluentd opensearch keycloak oauth2 vault ldap jupyter devworkspace workspace che ttyd cloudshell console argocd jenkins spinnaker registry istio rabbitmq kyverno gok-agent gok-controller controller gok-login chart base-services"
  local start_components="kubernetes proxy kubelet"
  local deploy_components="app1"
  local fix_options="helm-repository repositories package-conflicts"
  local create_resources="secret certificate kubeconfig"
  local create_secret_types="apphost"
  local generate_types="python-api python-reactjs"
  local patch_resources="ingress"
  local patch_options="letsencrypt ldap localtls"
  
  case ${COMP_CWORD} in
    1) COMPREPLY=($(compgen -W "${commands}" -- ${cur})) ;;
    2) case ${prev} in
         install) COMPREPLY=($(compgen -W "${install_components}" -- ${cur})) ;;
         fix) COMPREPLY=($(compgen -W "${fix_options}" -- ${cur})) ;;
         reset) COMPREPLY=($(compgen -W "${reset_components}" -- ${cur})) ;;
         start) COMPREPLY=($(compgen -W "${start_components}" -- ${cur})) ;;
         deploy) COMPREPLY=($(compgen -W "${deploy_components}" -- ${cur})) ;;
         create) COMPREPLY=($(compgen -W "${create_resources}" -- ${cur})) ;;
         generate) COMPREPLY=($(compgen -W "${generate_types}" -- ${cur})) ;;
         patch) COMPREPLY=($(compgen -W "${patch_resources}" -- ${cur})) ;;
         desc) COMPREPLY=($(compgen -W "methods" -- ${cur})) ;;
         remote) COMPREPLY=($(compgen -W "setup setup-ssh setup-keys copy-key setup-sudo passwordless-sudo add list show test-connection exec status copy install-gok" -- ${cur})) ;;
         completion) COMPREPLY=($(compgen -W "enable setup" -- ${cur})) ;;
       esac ;;
    3) local prev2="${COMP_WORDS[COMP_CWORD-2]}"
       case ${prev2} in
         create) case ${prev} in
                   secret) COMPREPLY=($(compgen -W "${create_secret_types}" -- ${cur})) ;;
                   certificate|kubeconfig) COMPREPLY=($(compgen -W "production staging development test" -- ${cur})) ;;
                 esac ;;
         generate) case ${prev} in
                     python-api|python-reactjs) COMPREPLY=($(compgen -W "user-service auth-service api-gateway webapp frontend backend microservice" -- ${cur})) ;;
                   esac ;;
       esac ;;
  esac
  return 0
}

complete -F _gok_completion gok
COMPLETION_EOF

  # Try to install system-wide first (requires sudo)
  if [[ -w "$completion_dir" ]] || sudo -n true 2>/dev/null; then
    if sudo cp "$temp_completion" "$completion_file" 2>/dev/null; then
      echo "âœ… System-wide completion installed at $completion_file"
    else
      echo "âš ï¸  Could not install system-wide completion"
    fi
  fi
  
  # Install user-specific completion if system-wide failed
  if [[ ! -f "$completion_file" ]]; then
    local user_completion="$HOME/.gok_completion"
    if ! grep -q "source.*gok_completion\|_gok_completion" "$user_bashrc" 2>/dev/null; then
      cp "$temp_completion" "$user_completion"
      echo "" >> "$user_bashrc"
      echo "# GOK tab completion" >> "$user_bashrc"
      echo "source \"$user_completion\" 2>/dev/null || true" >> "$user_bashrc"
      echo "âœ… User completion added to $user_bashrc ($user_completion)"
    else
      echo "â„¹ï¸  Completion already configured in $user_bashrc"
    fi
  fi
  
  # Enable for current session
  source "$temp_completion"
  echo "âœ… Tab completion enabled for current session!"
  echo ""
  echo "ðŸ’¡ Usage examples:"
  echo "   gok install <TAB><TAB>     # Shows all installable components"
  echo "   gok install ku<TAB>        # Completes to 'kubernetes'"
  echo "   gok create cert<TAB>       # Completes to 'certificate'"
  echo "   gok generate py<TAB>       # Shows python-api, python-reactjs"
  echo ""
  echo "ðŸ”„ Restart your terminal or run 'source ~/.bashrc' to enable permanently"
  
  rm -f "$temp_completion"
}

# =============================================================================
# ðŸŽ¯ UNIFIED COMMAND DISPATCHER  
# =============================================================================

# =============================================================================
# ðŸŽ¯ UNIFIED COMMAND DISPATCHER (REPLACES OLD/NEW STYLE DISPATCHERS)  
# =============================================================================

# =============================================================================
# OLD-STYLE DISPATCHER SECTION COMPLETELY REMOVED  
# =============================================================================
# This entire section (from here to line ~12423) contained the old-style
# dispatcher code that was causing double execution. It has been removed.
#
# The script now uses ONLY the unified dispatcher below.
# =============================================================================

if false; then  # Hide all old dispatcher code
echo "Old dispatcher code disabled"
  case "$2" in
    setup)
      if [[ -z "$3" || -z "$4" ]]; then
        echo "Usage: gok remote setup <host> <user> [key_file]"
        echo ""
        echo "Set up default remote host for simple execution."
        echo ""
        echo "Examples:"
        echo "  gok remote setup 10.0.0.244 sumit"
        echo "  gok remote setup 192.168.1.100 ubuntu ~/.ssh/custom_key"
        echo ""
        echo "After setup, you can simply use:"
        echo "  gok remote exec \"kubectl get pods\""
        exit 1
      fi
      
      local host="$3"
      local user="$4"
      local key_file="${5:-$HOME/.ssh/id_rsa}"
      
      log_info "Setting up default remote host: $user@$host"
      
      # Setup SSH keys if needed
      setup_ssh_keys "$key_file"
      
      # Copy SSH key if needed
      log_info "Ensuring SSH access to $user@$host..."
      ssh -i "$key_file" -o StrictHostKeyChecking=no -o ConnectTimeout=5 \
          "$user@$host" "echo 'test'" >/dev/null 2>&1
      
      if [ $? -ne 0 ]; then
        log_info "Setting up SSH key access..."
        copy_ssh_key "$host" "$user" "$key_file"
        if [ $? -ne 0 ]; then
          log_error "Failed to setup SSH access"
          exit 1
        fi
      else
        log_info "SSH access already working"
      fi
      
      # Setup passwordless sudo
      log_info "Setting up passwordless sudo for root commands..."
      if ! setup_passwordless_sudo "$host" "$user" "$key_file"; then
        log_warning "Passwordless sudo setup failed, you'll need to use passwords for root commands"
        echo ""
        echo "You can retry passwordless sudo setup later with:"
        echo "  gok remote exec \"echo 'your_password' | sudo -S bash -c 'echo \\\"$user ALL=(ALL) NOPASSWD: ALL\\\" > /etc/sudoers.d/$user && chmod 440 /etc/sudoers.d/$user'\""
      fi
      
      # Save as default
      save_default_remote_config "$host" "$user" "$key_file"
      
      # Test the configuration
      log_info "Testing default remote configuration..."
      if simple_remote_exec "echo 'Remote setup successful! Host:' \$(hostname)"; then
        echo ""
        log_success "âœ¨ Default remote setup complete!"
        echo ""
        echo "You can now use simple commands like:"
        echo "  gok remote exec \"kubectl get pods\""
        echo "  gok remote exec \"docker ps\""
        echo "  gok remote exec \"systemctl status nginx\""
      else
        log_error "Setup test failed"
        exit 1
      fi
      ;;
    add)
      if [[ -z "$3" || -z "$4" ]]; then
        echo "Usage: gok remote add <alias> <host> [user] [key_file]"
        echo ""
        echo "Examples:"
        echo "  gok remote add master 192.168.1.100 ubuntu"
        echo "  gok remote add node1 192.168.1.101 root ~/.ssh/id_rsa"
        exit 1
      fi
      configure_remote_host "$3" "$4" "${5:-root}" "${6:-$HOME/.ssh/id_rsa}" "auto" "true"
      ;;
    list|show)
      show_remote_hosts
      ;;
    exec)
      if [[ -z "$3" ]]; then
        echo "Usage: gok remote exec [target] <command>"
        echo ""
        echo "Simple usage (after 'gok remote setup'):"
        echo "  gok remote exec \"kubectl get pods\"                         # Use default remote host"
        echo "  gok remote exec \"docker ps\"                                # Use default remote host"
        echo ""
        echo "Advanced usage with target:"
        echo "  gok remote exec master 'kubectl get nodes'                    # Use configured alias"
        echo "  gok remote exec 10.0.0.244:sumit 'kubectl get pods'          # Auto-setup sumit@10.0.0.244"
        echo "  gok remote exec 192.168.1.100 'docker ps'                    # Auto-setup root@192.168.1.100"
        echo "  gok remote exec all 'systemctl status docker'                # Execute on all hosts"
        echo ""
        echo "First-time setup:"
        echo "  gok remote setup <host> <user>                               # One-time configuration"
        echo ""
        exit 1
      fi
      
      # Check if we have a simple command (no target specified)
      # If $3 doesn't contain spaces and $4 exists, then $3 is likely a target
      # If $3 contains spaces or $4 is empty, then $3 is likely the command
      if [[ "$3" != *" "* ]] && [[ -n "$4" ]] && [[ "$3" != *":"* ]] && [[ "$3" =~ ^[a-zA-Z0-9._-]+$ ]]; then
        # Advanced usage: gok remote exec <target> <command>
        local target="$3"
        shift 3
        smart_remote_exec "$target" "$*"
      else
        # Simple usage: gok remote exec <command>
        shift 2
        simple_remote_exec "$*"
      fi
      ;;
    status)
      remote_status "${3:-all}"
      ;;
    install-gok)
      remote_install_gok "${3:-all}"
      ;;
    copy)
      if [[ -z "$3" || -z "$4" || -z "$5" ]]; then
        echo "Usage: gok remote copy <alias> <local_file> <remote_path>"
        echo ""
        echo "Examples:"
        echo "  gok remote copy master ./script.sh /tmp/script.sh"
        echo "  gok remote copy node1 ./config.yaml /etc/app/config.yaml"
        exit 1
      fi
      remote_copy "$3" "$4" "$5"
      ;;
    setup-ssh|setup-keys)
      local key_file="${3:-$HOME/.ssh/id_rsa}"
      setup_ssh_keys "$key_file"
      ;;
    copy-key)
      if [[ -z "$3" || -z "$4" ]]; then
        echo "Usage: gok remote copy-key <host> <user> [key_file]"
        echo ""
        echo "Examples:"
        echo "  gok remote copy-key 10.0.0.244 sumit"
        echo "  gok remote copy-key 192.168.1.100 ubuntu ~/.ssh/id_rsa"
        exit 1
      fi
      copy_ssh_key "$3" "$4" "${5:-$HOME/.ssh/id_rsa}"
      ;;
    setup-sudo|passwordless-sudo)
      if [[ -z "$3" || -z "$4" ]]; then
        echo "Usage: gok remote setup-sudo <host> <user> [key_file] [password]"
        echo ""
        echo "Configure passwordless sudo for root commands."
        echo ""
        echo "Examples:"
        echo "  gok remote setup-sudo 10.0.0.244 sumit"
        echo "  gok remote setup-sudo 192.168.1.100 ubuntu ~/.ssh/id_rsa mypassword"
        echo ""
        echo "Note: If password is not provided, you'll be prompted for it."
        exit 1
      fi
      setup_passwordless_sudo "$3" "$4" "${5:-$HOME/.ssh/id_rsa}" "$6"
      ;;
    test-connection)
      if [[ -z "$3" ]]; then
        echo "Usage: gok remote test-connection <alias>"
        echo ""
        echo "Examples:"
        echo "  gok remote test-connection master"
        echo "  gok remote test-connection node1"
        exit 1
      fi
      if [[ -z "${REMOTE_HOSTS[$3]}" ]]; then
        log_error "Remote host '$3' not configured"
        log_info "Available hosts: ${!REMOTE_HOSTS[*]}"
        exit 1
      fi
      local host="${REMOTE_HOSTS[$3]}"
      local user="${REMOTE_USERS[$3]}"
      local key_file="${REMOTE_KEYS[$3]}"
      
      log_info "Testing connection to $3 ($user@$host)..."
      ssh -i "$key_file" -o StrictHostKeyChecking=no -o ConnectTimeout=10 \
          "$user@$host" "echo 'SSH connection successful'; hostname; date"
      if [ $? -eq 0 ]; then
        log_success "Connection test successful for $3"
      else
        log_error "Connection test failed for $3"
      fi
      ;;
    *)
      echo "gok remote - Remote execution management"
      echo ""
      echo "ðŸŽ¯ SIMPLE 2-STEP PROCESS:"
      echo ""
      echo "Step 1 - One-time setup:"
      echo "  gok remote setup <host> <user> [key_file]         # Configure default remote"
      echo ""
      echo "Step 2 - Execute commands:"
      echo "  gok remote exec \"<command>\"                       # Run on default remote"
      echo ""
      echo "âœ¨ Example workflow:"
      echo "  gok remote setup 10.0.0.244 sumit                # Setup once"
      echo "  gok remote exec \"kubectl get pods\"               # Use anytime"
      echo "  gok remote exec \"docker ps\"                      # Use anytime"
      echo "  gok remote exec \"systemctl status nginx\"        # Use anytime"
      echo ""
      echo "ðŸ“‹ All Commands:"
      echo "  gok remote setup <host> <user> [key_file]         # Setup default remote (includes passwordless sudo)"
      echo "  gok remote exec \"<command>\"                       # Execute on default remote"
      echo "  gok remote setup-ssh [key_file]                   # Generate SSH keys"
      echo "  gok remote copy-key <host> <user> [key_file]      # Copy SSH key to host"
      echo "  gok remote setup-sudo <host> <user> [key_file]    # Setup passwordless sudo"
      echo "  gok remote add <alias> <host> [user] [key_file]   # Add remote host alias"
      echo "  gok remote list                                   # Show configured hosts"
      echo "  gok remote test-connection <alias>               # Test SSH connection"
      echo "  gok remote exec <alias> \"<command>\"              # Execute on specific alias"  
      echo "  gok remote status [alias]                        # Show system status"
      echo "  gok remote copy <alias> <local_file> <remote_path> # Copy file"
      echo "  gok remote install-gok [alias]                   # Install GOK remotely"
      echo ""
      echo "ðŸ“‹ Manual Setup Examples:"
      echo "  gok remote setup-ssh                             # Generate SSH keys"
      echo "  gok remote copy-key 10.0.0.244 sumit            # Copy key to host"
      echo "  gok remote add debug 10.0.0.244 sumit           # Add debug host"
      echo "  gok remote test-connection debug                 # Test connection"
      echo ""
      echo "ðŸ”§ Advanced Examples:"
      echo "  gok remote add master 192.168.1.100 ubuntu       # Add master node"
      echo "  gok remote add node1 192.168.1.101 ubuntu        # Add worker node"
      echo "  gok remote list                                   # Show all hosts"
      echo "  gok remote exec master 'kubectl get nodes'       # Run kubectl on master"
      echo "  gok remote exec all 'docker ps'                  # Run docker on all hosts"
      echo "  gok remote status                                 # Check all host status"
      echo "  gok remote copy master ./script.sh /tmp/         # Copy script to master"
      echo ""
      echo "ðŸ’¡ Auto-Setup Features:"
      echo "  â€¢ SSH keys generated automatically if missing"
      echo "  â€¢ SSH keys copied to hosts automatically"
      echo "  â€¢ Passwordless sudo configured automatically"
      echo "  â€¢ Remote hosts configured automatically"
      echo "  â€¢ No manual setup steps required!"
      echo "  â€¢ Execute any root command without passwords!"
      echo ""
      echo "ðŸ” Sudo Configuration:"
      echo "  export GOK_REMOTE_SUDO=auto     # Auto-detect sudo needs (default)"
      echo "  export GOK_REMOTE_SUDO=always   # Always use sudo for all commands"
      echo "  export GOK_REMOTE_SUDO=never    # Never use sudo automatically"
      echo ""
      echo "  Examples:"
      echo "  gok remote exec \"systemctl status docker\"      # Auto-adds sudo"
      echo "  gok remote exec \"kubectl get nodes\"            # Auto-adds sudo"
      echo "  gok remote exec \"ls -la\"                       # No sudo added"
      ;;
  esac
elif [ "$CMD" == "completion" ]; then
  case "$2" in
    enable)
      enable_gok_completion
      ;;
    quick)
      quick_gok_completion
      ;;
    setup)
      setup_gok_completion
      ;;
    standalone)
      echo "ðŸ”§ Standalone completion script available at: $(dirname "$0")/gok-completion.sh"
      echo "ðŸ’¡ Usage: source gok-completion.sh"
      echo "   This provides basic tab completion without sourcing the full GOK file"
      echo ""
      if [ -f "$(dirname "$0")/gok-completion.sh" ]; then
        echo "âœ… File exists and is ready to use"
      else
        echo "âŒ File not found - please ensure gok-completion.sh is in the same directory"
      fi
      ;;
    *)
      echo "ðŸŽ¯ GOK Tab Completion Management"
      echo ""
      echo "ðŸ’¡ If tab completion isn't working, try these solutions:"
      echo ""
      echo "Quick Fixes:"
      echo "  source gok-completion.sh         # Use standalone script (recommended)"
      echo "  gok completion quick             # Simple completion for current session"
      echo ""
      echo "Advanced Options:"
      echo "  gok completion enable            # Full completion for current session"
      echo "  gok completion setup             # Install completion permanently"
      echo "  gok completion standalone        # Info about standalone script"
      echo ""
      echo "ðŸš€ Recommended: Use the standalone script for most reliable completion:"
      echo "   source gok-completion.sh && gok ingr<TAB>"
      echo ""
      echo "Examples:"
      echo "  gok completion enable    # Quick enable for testing"
      echo "  gok completion setup     # Permanent installation"
      ;;
  esac
fi  # End of disabled old-style dispatcher block

# Remote command function (extracted from disabled dispatcher)
remote() {
  case "$1" in
    setup)
      if [[ -z "$2" || -z "$3" ]]; then
        echo "Usage: gok remote setup <host> <user> [key_file] [sudo_mode]"
        echo ""
        echo "Set up default remote host with automatic SSH keys and sudo configuration."
        echo ""
        echo "Parameters:"
        echo "  host       - Remote host IP/hostname"
        echo "  user       - Username for SSH connection"
        echo "  key_file   - SSH key file path (optional, default: ~/.ssh/id_rsa)"
        echo "  sudo_mode  - Sudo behavior: 'always', 'auto', 'never' (optional, will prompt if not provided)"
        echo ""
        echo "Examples:"
        echo "  gok remote setup 10.0.0.244 sumit                    # Interactive sudo setup"
        echo "  gok remote setup 192.168.1.100 ubuntu ~/.ssh/id_rsa always"
        echo "  gok remote setup 10.0.0.244 sumit ~/.ssh/id_rsa never"
        echo ""
        echo "After setup, you can simply use:"
        echo "  gok remote exec \"kubectl get pods\"                   # Uses configured settings"
        echo "  gok remote exec \"docker ps\"                         # Automatically adds sudo if needed"
        exit 1
      fi
      
      local host="$2"
      local user="$3"
      local key_file="${4:-$HOME/.ssh/id_rsa}"
      local sudo_mode="$5"
      
      # If sudo_mode not provided, ask user interactively
      if [[ -z "$sudo_mode" ]]; then
        echo ""
        echo "ðŸ” Sudo Configuration for $user@$host"
        echo ""
        echo "How should commands be executed on the remote host?"
        echo "  1) always  - Always prepend 'sudo' to all commands (for non-root users)"
        echo "  2) auto    - Automatically detect when sudo is needed (recommended)"
        echo "  3) never   - Never use sudo (for root user or when not needed)"
        echo ""
        while true; do
          read -p "Choose sudo mode [1-3] (default: auto): " choice
          case $choice in
            1|always) sudo_mode="always"; break;;
            2|auto|"") sudo_mode="auto"; break;;
            3|never) sudo_mode="never"; break;;
            *) echo "Please choose 1, 2, or 3";;
          esac
        done
      fi
      
      log_info "Setting up default remote host: $user@$host (sudo: $sudo_mode)"
      
      # Setup SSH keys and access
      setup_ssh_keys "$key_file"
      copy_ssh_key "$host" "$user" "$key_file"
      if [ $? -ne 0 ]; then
        log_error "Failed to setup SSH access"
        exit 1
      fi
      
      # Setup passwordless sudo if needed
      if [[ "$sudo_mode" == "always" ]] || [[ "$sudo_mode" == "auto" ]]; then
        log_info "Setting up passwordless sudo for root commands..."
        if ! setup_passwordless_sudo "$host" "$user" "$key_file"; then
          log_warning "Passwordless sudo setup failed"
          echo ""
          echo "You can retry passwordless sudo setup later with:"
          echo "  gok remote setup-sudo $host $user"
        fi
      fi
      
      # Save configuration as default with sudo preference
      save_default_remote_config "$host" "$user" "$key_file" "$sudo_mode"
      
      # Test the configuration
      log_info "Testing remote configuration..."
      if simple_remote_exec "echo 'Remote setup successful! Host:' \$(hostname) '| User:' \$(whoami)"; then
        log_success "Remote setup completed successfully!"
        echo ""
        echo "âœ… You can now use:"
        echo "  gok remote exec \"any-command\"           # Executes with configured sudo settings"
        echo "  gok remote exec \"docker ps\"             # Will use sudo if mode is 'always' or 'auto'"
        echo "  gok remote exec \"kubectl get pods\"      # Will use sudo if mode is 'always' or 'auto'"
      else
        log_error "Setup test failed"
        exit 1
      fi
      ;;
    add)
      if [[ -z "$2" || -z "$3" ]]; then
        echo "Usage: gok remote add <alias> <host> [user] [key_file] [sudo_mode]"
        echo ""
        echo "Examples:"
        echo "  gok remote add master 192.168.1.100 ubuntu"
        echo "  gok remote add node1 192.168.1.101 root ~/.ssh/id_rsa never"
        echo "  gok remote add debug 10.0.0.244 sumit ~/.ssh/id_rsa always"
        exit 1
      fi
      local sudo_mode="${6:-auto}"  # Default to auto for aliases
      configure_remote_host "$2" "$3" "${4:-root}" "${5:-$HOME/.ssh/id_rsa}" "$sudo_mode" "true"
      ;;
    list|show)
      show_remote_hosts
      ;;
    exec)
      if [[ -z "$2" ]]; then
        echo "Usage: gok remote exec [target] <command>"
        echo ""
        echo "Simple usage (after configuring default):"
        echo "  gok remote exec \"kubectl get pods\"                         # Use default remote host"
        echo "  gok remote exec \"docker ps\"                                # Use default remote host"
        echo ""
        echo "Advanced usage with target:"
        echo "  gok remote exec master 'kubectl get nodes'                    # Use configured alias"
        echo "  gok remote exec 10.0.0.244:sumit 'kubectl get pods'          # Auto-setup sumit@10.0.0.244"
        echo "  gok remote exec 192.168.1.100 'docker ps'                    # Auto-setup root@192.168.1.100"
        echo "  gok remote exec all 'systemctl status docker'                # Execute on all hosts"
        echo ""
        echo "First-time setup:"
        echo "  gok remote add default <host> <user>                         # One-time configuration"
        echo ""
        exit 1
      fi
      
      # Check if we have a simple command (no target specified)
      # If $2 doesn't contain spaces and $3 exists, then $2 is likely a target
      # If $2 contains spaces or $3 is empty, then $2 is likely the command
      if [[ "$2" != *" "* ]] && [[ -n "$3" ]] && [[ "$2" != *":"* ]] && [[ "$2" =~ ^[a-zA-Z0-9._-]+$ ]]; then
        # Advanced usage: gok remote exec <target> <command>
        local target="$2"
        shift 2
        smart_remote_exec "$target" "$*"
      else
        # Simple usage: gok remote exec <command>
        shift 1
        simple_remote_exec "$*"
      fi
      ;;
    status)
      remote_status "${2:-all}"
      ;;
    install-gok)
      remote_install_gok "${2:-all}"
      ;;
    copy)
      # Support both alias-based copy and default remote copy
      if [[ -n "$4" ]]; then
        # Full syntax: gok remote copy <alias> <local_file> <remote_path>
        remote_copy "$2" "$3" "$4"
      elif [[ -n "$2" ]]; then
        # Default remote syntax: gok remote copy <local_file> [remote_path]
        if ! load_default_remote_config; then
          log_error "No default remote configuration found!"
          echo ""
          echo "Please set up your default remote host first:"
          echo "  gok remote setup <host> <user> [key_file]"
          exit 1
        fi
        
        local local_file="$2"
        local remote_path="${3:-~/$(basename "$local_file")}"
        
        log_info "Copying $local_file to default remote ($DEFAULT_REMOTE_USER@$DEFAULT_REMOTE_HOST):$remote_path"
        
        scp -i "$DEFAULT_REMOTE_KEY" -o StrictHostKeyChecking=no \
            "$local_file" "$DEFAULT_REMOTE_USER@$DEFAULT_REMOTE_HOST:$remote_path"
      else
        echo "Usage: gok remote copy <local_file> [remote_path]"
        echo "   or: gok remote copy <alias> <local_file> <remote_path>"
        echo ""
        echo "Examples:"
        echo "  gok remote copy ./script.sh                                   # Copy to default remote home"
        echo "  gok remote copy ./script.sh /tmp/script.sh                    # Copy to specific path"
        echo "  gok remote copy master ./config.yaml /etc/app/config.yaml    # Copy to specific alias"
        exit 1
      fi
      ;;
    setup-ssh|setup-keys)
      local key_file="${2:-$HOME/.ssh/id_rsa}"
      setup_ssh_keys "$key_file"
      ;;
    copy-key)
      if [[ -z "$2" || -z "$3" ]]; then
        echo "Usage: gok remote copy-key <host> <user> [key_file]"
        echo ""
        echo "Examples:"
        echo "  gok remote copy-key 10.0.0.244 sumit"
        echo "  gok remote copy-key 192.168.1.100 ubuntu ~/.ssh/id_rsa"
        exit 1
      fi
      copy_ssh_key "$2" "$3" "${4:-$HOME/.ssh/id_rsa}"
      ;;
    setup-sudo|passwordless-sudo)
      if [[ -z "$2" || -z "$3" ]]; then
        echo "Usage: gok remote setup-sudo <host> <user> [key_file] [password]"
        echo ""
        echo "Configure passwordless sudo for root commands."
        echo ""
        echo "Examples:"
        echo "  gok remote setup-sudo 10.0.0.244 sumit"
        echo "  gok remote setup-sudo 192.168.1.100 ubuntu ~/.ssh/id_rsa mypassword"
        echo ""
        echo "Note: If password is not provided, you'll be prompted for it."
        exit 1
      fi
      setup_passwordless_sudo "$2" "$3" "${4:-$HOME/.ssh/id_rsa}" "$5"
      ;;
    test-connection)
      if [[ -z "$2" ]]; then
        echo "Usage: gok remote test-connection <alias>"
        echo ""
        echo "Examples:"
        echo "  gok remote test-connection master"
        echo "  gok remote test-connection node1"
        exit 1
      fi
      if [[ -z "${REMOTE_HOSTS[$2]}" ]]; then
        log_error "Remote host '$2' not configured"
        log_info "Available hosts: ${!REMOTE_HOSTS[*]}"
        exit 1
      fi
      local host="${REMOTE_HOSTS[$2]}"
      local user="${REMOTE_USERS[$2]}"
      local key_file="${REMOTE_KEYS[$2]}"
      
      log_info "Testing connection to $2 ($user@$host)..."
      ssh -i "$key_file" -o StrictHostKeyChecking=no -o ConnectTimeout=10 \
          "$user@$host" "echo 'SSH connection successful'; hostname; date"
      if [ $? -eq 0 ]; then
        log_success "Connection test successful for $2"
      else
        log_error "Connection test failed for $2"
      fi
      ;;
    *)
      echo "gok remote - Remote execution management"
      echo ""
      echo "ðŸŽ¯ QUICK START (Recommended):"
      echo ""
      echo "Step 1 - Setup default remote host with full configuration:"
      echo "  gok remote setup <host> <user> [key_file] [sudo_mode]"
      echo ""
      echo "Step 2 - Execute commands seamlessly:"
      echo "  gok remote exec \"<command>\"                       # Runs with configured sudo settings"
      echo ""
      echo "âœ¨ Complete Setup Example:"
      echo "  gok remote setup 10.0.0.244 sumit                # Interactive sudo setup"
      echo "  gok remote setup 192.168.1.100 ubuntu ~/.ssh/id_rsa always  # Always use sudo"
      echo "  gok remote exec \"docker ps\"                      # Automatically uses sudo"
      echo "  gok remote exec \"kubectl get pods\"               # Automatically uses sudo"
      echo "  gok remote exec \"ls -la\"                         # No sudo needed"
      echo ""
      echo "ðŸ”§ SETUP COMMAND:"
      echo ""
      echo "  gok remote setup <host> <user> [key_file] [sudo_mode]"
      echo ""
      echo "    Parameters:"
      echo "      host       - Remote host IP address or hostname"
      echo "      user       - Username for SSH connection"
      echo "      key_file   - SSH private key file (default: ~/.ssh/id_rsa)"
      echo "      sudo_mode  - How to handle root commands:"
      echo "                   â€¢ always - Always prepend 'sudo' to all commands"
      echo "                   â€¢ auto   - Auto-detect when sudo is needed (default)"
      echo "                   â€¢ never  - Never use sudo (for root user)"
      echo ""
      echo "    What setup does:"
      echo "      âœ… Generates SSH keys if missing"
      echo "      âœ… Copies SSH key to remote host"
      echo "      âœ… Configures passwordless sudo (if needed)"
      echo "      âœ… Tests the connection"
      echo "      âœ… Saves configuration as default"
      echo ""
      echo "    Examples:"
      echo "      gok remote setup 10.0.0.244 sumit            # Interactive sudo setup"
      echo "      gok remote setup 192.168.1.100 root never    # Root user, no sudo"
      echo "      gok remote setup server.com ubuntu always    # Always use sudo"
      echo ""
      echo "ðŸš€ EXEC COMMAND:"
      echo ""
      echo "  gok remote exec \"<command>\"                       # Execute on default host"
      echo "  gok remote exec <target> \"<command>\"              # Execute on specific target"
      echo ""
      echo "    Simple Usage (after setup):"
      echo "      gok remote exec \"docker version\"             # Uses configured sudo settings"
      echo "      gok remote exec \"systemctl status nginx\"     # Auto-adds sudo if needed"
      echo "      gok remote exec \"kubectl get nodes\"          # Auto-adds sudo if needed"
      echo "      gok remote exec \"whoami\"                     # No sudo needed"
      echo ""
      echo "    Advanced Usage with targets:"
      echo "      gok remote exec master \"kubectl get nodes\"   # Run on 'master' alias"
      echo "      gok remote exec 192.168.1.100 \"docker ps\"    # Auto-setup root@192.168.1.100"
      echo "      gok remote exec 10.0.0.244:sumit \"ls\"        # Auto-setup sumit@10.0.0.244"
      echo "      gok remote exec all \"systemctl status docker\" # Run on all configured hosts"
      echo ""
      echo "ï¿½ COPY COMMAND:"
      echo ""
      echo "  gok remote copy <filename> [remote_path]         # Copy to default remote"
      echo "  gok remote copy <alias> <file> <remote_path>     # Copy to specific alias"
      echo ""
      echo "    Simple Usage (after setup):"
      echo "      gok remote copy script.sh                    # Copy to default remote home"
      echo "      gok remote copy config.yaml /etc/           # Copy to /etc/ on default remote"
      echo "      gok remote copy ./local/file.txt            # Copy local file to remote home"
      echo ""
      echo "    Advanced Usage with aliases:"
      echo "      gok remote copy master script.sh /tmp/      # Copy to /tmp/ on master alias"
      echo "      gok remote copy node1 config.yaml /etc/app/ # Copy to specific path on node1"
      echo ""
      echo "ï¿½ðŸ“‹ All Commands:"
      echo "  gok remote add <alias> <host> [user] [key_file]   # Add remote host alias"
      echo "  gok remote list                                   # Show configured hosts"
      echo "  gok remote exec \"<command>\"                       # Execute on default remote"
      echo "  gok remote exec <alias> \"<command>\"              # Execute on specific alias"  
      echo "  gok remote test-connection <alias>               # Test SSH connection"
      echo "  gok remote status [alias]                        # Show system status"
      echo "  gok remote copy <filename> [remote_path]         # Copy file to default remote"
      echo "  gok remote copy <alias> <local_file> <remote_path> # Copy file to specific alias"
      echo "  gok remote install-gok [alias]                   # Install GOK remotely"
      echo ""
      echo "ï¿½ SSH Management:"
      echo "  gok remote setup-ssh [key_file]                  # Generate SSH keys"
      echo "  gok remote copy-key <host> <user> [key_file]     # Copy SSH key to host"
      echo "  gok remote setup-sudo <host> <user> [key_file]   # Setup passwordless sudo"
      echo ""
      echo "ðŸ”§ Advanced Examples:"
      echo "  gok remote add master 192.168.1.100 ubuntu       # Add master node"
      echo "  gok remote add node1 192.168.1.101 ubuntu        # Add worker node"
      echo "  gok remote list                                   # Show all hosts"
      echo "  gok remote exec master 'kubectl get nodes'       # Run kubectl on master"
      echo "  gok remote exec all 'docker ps'                  # Run docker on all hosts"
      echo "  gok remote status                                 # Check all host status"
      echo "  gok remote copy ./script.sh /tmp/                # Copy to default remote"
      echo "  gok remote copy master ./script.sh /tmp/         # Copy script to master"
      echo ""
      echo "ðŸ’¡ Auto-Setup Features:"
      echo "  â€¢ SSH keys generated automatically if missing"
      echo "  â€¢ SSH keys copied to hosts automatically"
      echo "  â€¢ Passwordless sudo configured automatically"
      echo "  â€¢ Remote hosts configured automatically"
      echo "  â€¢ No manual setup steps required!"
      echo "  â€¢ Execute any root command without passwords!"
      echo ""
      echo "ðŸ” SUDO MODES:"
      echo ""
      echo "    always  - Every command gets 'sudo' prefix"
      echo "              Good for: Non-root users who need admin access"
      echo "              Example: 'docker ps' becomes 'sudo docker ps'"
      echo ""
      echo "    auto    - Smart detection of commands that need root"
      echo "              Good for: Mixed workloads (default, recommended)"
      echo "              Example: 'ls' stays 'ls', 'systemctl' becomes 'sudo systemctl'"
      echo ""
      echo "    never   - Never add sudo automatically"
      echo "              Good for: Root users or non-privileged operations"
      echo "              Example: All commands run as-is"
      echo ""
      echo "ðŸŒ EXAMPLE WORKFLOWS:"
      echo ""
      echo "  1. Quick Docker debugging:"
      echo "     gok remote setup 10.0.0.244 sumit always"
      echo "     gok remote exec \"docker ps\""
      echo "     gok remote exec \"docker logs container_name\""
      echo ""
      echo "  2. Kubernetes cluster management:"
      echo "     gok remote setup 192.168.1.100 ubuntu auto"
      echo "     gok remote exec \"kubectl get nodes\""
      echo "     gok remote exec \"kubectl get pods --all-namespaces\""
      echo ""
      echo "  3. Multi-host operations:"
      echo "     gok remote add master 192.168.1.100 ubuntu auto"
      echo "     gok remote add node1 192.168.1.101 ubuntu auto"
      echo "     gok remote exec all \"systemctl status docker\""
      ;;
  esac
}

# Simple unified command dispatcher that always works
# Handle both sourced and executed scenarios the same way
if [[ $# -gt 0 ]]; then
  # Check for global verbose flag
  if [[ "$1" == "--verbose" || "$1" == "-v" ]]; then
    export GOK_VERBOSE=true
    shift
  fi
  
  # Get the command (first argument)
  command="$1"
  shift
  
  # Check for verbose flag after command
  if [[ "$1" == "--verbose" || "$1" == "-v" ]]; then
    export GOK_VERBOSE=true
    shift
  fi
  
  # Handle command aliases that map to different function names
  case "$command" in
      install)
        installCmd "$@"
        ;;
      reset)
        resetCmd "$@"
        ;;
      start)
        startCmd "$@"
        ;;
      deploy)
        deployCmd "$@"
        ;;
      patch)
        patchCmd "$@"
        ;;
      create)
        createCmd "$@"
        ;;
      generate)
        generateCmd "$@"
        ;;
      help)
        helpCmd "$@"
        ;;
      cache)
        case "${1:-}" in
          'clear' | 'clean')
            if [[ -d "$GOK_CACHE_DIR" ]]; then
              rm -rf "$GOK_CACHE_DIR"
              echo "âœ… All caches cleared (system updates & dependencies)"
            else
              echo "â„¹ï¸ Cache directory is already empty"
            fi
            ;;
          'status' | 'info')
            # System Update Cache Status
            cache_file="$GOK_CACHE_DIR/last_update"
            if [[ -f "$cache_file" ]]; then
              cache_time=$(cat "$cache_file" 2>/dev/null || echo "0")
              current_time=$(date +%s)
              cache_age_hours=$(( (current_time - cache_time) / 3600 ))
              cache_age_mins=$(( ((current_time - cache_time) % 3600) / 60 ))
              cache_hours="${GOK_UPDATE_CACHE_HOURS:-6}"
              
              echo "ðŸ“¦ System Update Cache Status:"
              echo "   Last update: $(date -d "@$cache_time" 2>/dev/null || echo "Unknown")"
              echo "   Cache age: ${cache_age_hours}h ${cache_age_mins}m"
              echo "   Cache limit: ${cache_hours}h"
              echo "   Status: $(if [[ $cache_age_hours -lt $cache_hours ]]; then echo "âœ… Valid"; else echo "âš ï¸ Expired"; fi)"
            else
              echo "ðŸ“¦ System Update Cache Status: âŒ No cache found"
            fi
            
            echo ""
            
            # Dependencies Cache Status
            deps_cache_file="$GOK_CACHE_DIR/last_deps_install"
            if [[ -f "$deps_cache_file" ]]; then
              deps_cache_time=$(cat "$deps_cache_file" 2>/dev/null || echo "0")
              deps_cache_age_hours=$(( (current_time - deps_cache_time) / 3600 ))
              deps_cache_age_mins=$(( ((current_time - deps_cache_time) % 3600) / 60 ))
              deps_cache_hours="${GOK_DEPS_CACHE_HOURS:-${GOK_UPDATE_CACHE_HOURS:-6}}"
              
              echo "ðŸ”§ Dependencies Cache Status:"
              echo "   Last install: $(date -d "@$deps_cache_time" 2>/dev/null || echo "Unknown")"
              echo "   Cache age: ${deps_cache_age_hours}h ${deps_cache_age_mins}m"
              echo "   Cache limit: ${deps_cache_hours}h"
              echo "   Status: $(if [[ $deps_cache_age_hours -lt $deps_cache_hours ]]; then echo "âœ… Valid"; else echo "âš ï¸ Expired"; fi)"
            else
              echo "ðŸ”§ Dependencies Cache Status: âŒ No cache found"
            fi
            ;;
          *)
            echo "Usage: gok cache <command>"
            echo ""
            echo "Commands:"
            echo "  status    Show cache status and age for updates & dependencies"
            echo "  clear     Clear all caches (force next update & dependency install)"
            echo ""
            echo "Environment Variables:"
            echo "  GOK_UPDATE_CACHE_HOURS=6  Hours to cache system updates (default: 6)"
            echo "  GOK_DEPS_CACHE_HOURS=6    Hours to cache dependency installs (default: same as updates)"
            ;;
        esac
        ;;
      *)
        # Check if the command is a valid function
        if declare -f "$command" >/dev/null 2>&1; then
          # Execute the function with remaining arguments
          "$command" "$@"
        else
          echo "Error: Unknown command '$command'"
          echo "Run 'gok help' for available commands"
          exit 1
        fi
        ;;
    esac
  else
    # No arguments provided, show help
    helpCmd
  fi
# End of unified dispatcher