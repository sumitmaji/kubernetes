#!/bin/bash

: ${WORKING_DIR:=$MOUNT_PATH/kubernetes/install_k8s}
source "$WORKING_DIR"/config

release=$2
CMD=$1

getOAuth0Config(){
 IFS='' read -r -d '' OAUTH <<"EOF"
export OIDC_ISSUE_URL=https://skmaji.auth0.com/
export OIDC_CLIENT_ID=C3UHISO3z60iF1JLG8L7VPUSWOASrJfO
export OIDC_USERNAME_CLAIM=sub
export OIDC_GROUPS_CLAIM=http://localhost:8080/claims/groups
export AUTH0_DOMAIN=skmaji.auth0.com
export APP_HOST=kube.gokcloud.com
export JWKS_URL=$OIDC_ISSUE_URL/.well-known/jwks.json
EOF
echo "$OAUTH"
}

getKeycloakConfig(){
  IFS='' read -r -d '' OAUTH <<"EOF"
export OIDC_ISSUE_URL=https://keycloak.gokcloud.com/realms/GokDevelopers
export OIDC_CLIENT_ID=gok-developers-client
export OIDC_USERNAME_CLAIM=sub
export OIDC_GROUPS_CLAIM=groups
export REALM=GokDevelopers
export AUTH0_DOMAIN=keycloak.gokcloud.com
export APP_HOST=kube.gokcloud.com
export JWKS_URL=$OIDC_ISSUE_URL/protocol/openid-connect/certs
EOF
echo "$OAUTH"
}

cat <<EOF  > ${MOUNT_PATH}/root_config
export LETS_ENCRYPT_PROD_URL=https://acme-v02.api.letsencrypt.org/directory
export LETS_ENCRYPT_STAGING_URL=https://acme-staging-v02.api.letsencrypt.org/directory
#dns, http, selfsigned
export CERTMANAGER_CHALANGE_TYPE=selfsigned
#staging, prod
export LETS_ENCRYPT_ENV=staging
export REGISTRY=registry
export KEYCLOAK=keycloak
export SPINNAKER=spinnaker
export VAULT=vault
export JUPYTERHUB=jupyterhub
export ARGOCD=argocd
export DEFAULT_SUBDOMAIN=kube
export GROUP_NAME=$GOK_ROOT_DOMAIN
#ldap, oidc
export AUTHENTICATION_METHOD=oidc
export IDENTITY_PROVIDER=${IDENTITY_PROVIDER}
`
case ${IDENTITY_PROVIDER} in
  "oauth0")
    echo "$(getOAuth0Config)"
    ;;
  "keycloak")
    echo "$(getKeycloakConfig)"
    ;;
  *)
    echo "Unsupported identity provider: ${IDENTITY_PROVIDER}"
    ;;
esac
`
EOF

source ${MOUNT_PATH}/root_config

rootDomain(){
  echo "$GOK_ROOT_DOMAIN"
}

sedRootDomain(){
  rootDomain | sed 's/\./-/g'
}

registrySubdomain(){
  echo "$REGISTRY"
}

defaultSubdomain(){
  echo "$DEFAULT_SUBDOMAIN"
}

keycloakSubdomain(){
  echo "$KEYCLOAK"
}

argocdSubdomain(){
  echo "$ARGOCD"
}

jupyterHubSubdomain(){
  echo "$JUPYTERHUB"
}

fullDefaultUrl(){
  echo "${DEFAULT_SUBDOMAIN}.${GOK_ROOT_DOMAIN}"
}

fullRegistryUrl(){
  echo "${REGISTRY}.${GOK_ROOT_DOMAIN}"
}

fullKeycloakUrl(){
  echo "${KEYCLOAK}.${GOK_ROOT_DOMAIN}"
}

fullVaultUrl(){
  echo "${VAULT}.${GOK_ROOT_DOMAIN}"
}

fullSpinnakerUrl(){
  echo "${SPINNAKER}.${GOK_ROOT_DOMAIN}"
}

# =============================================================================
# 🚀 GOK ENHANCED LOGGING SYSTEM
# =============================================================================

# Color constants
readonly COLOR_RESET='\033[0m'
readonly COLOR_BOLD='\033[1m'
readonly COLOR_DIM='\033[2m'
readonly COLOR_RED='\033[31m'
readonly COLOR_GREEN='\033[32m'
readonly COLOR_YELLOW='\033[33m'
readonly COLOR_BLUE='\033[34m'
readonly COLOR_MAGENTA='\033[35m'
readonly COLOR_CYAN='\033[36m'
readonly COLOR_WHITE='\033[37m'
readonly COLOR_BRIGHT_GREEN='\033[92m'
readonly COLOR_BRIGHT_YELLOW='\033[93m'
readonly COLOR_BRIGHT_BLUE='\033[94m'
readonly COLOR_BRIGHT_MAGENTA='\033[95m'
readonly COLOR_BRIGHT_CYAN='\033[96m'

# Emoji constants
readonly EMOJI_SUCCESS="✅"
readonly EMOJI_ERROR="❌"
readonly EMOJI_WARNING="⚠️"
readonly EMOJI_INFO="ℹ️"
readonly EMOJI_ROCKET="🚀"
readonly EMOJI_GEAR="⚙️"
readonly EMOJI_CLOCK="⏱️"
readonly EMOJI_CHECKMARK="✓"
readonly EMOJI_CROSS="✗"
readonly EMOJI_ARROW="➤"
readonly EMOJI_STAR="⭐"
readonly EMOJI_PACKAGE="📦"
readonly EMOJI_NETWORK="🌐"
readonly EMOJI_SHIELD="🛡️"
readonly EMOJI_TOOLS="🔧"
readonly EMOJI_CHART="📊"
readonly EMOJI_KEY="🔑"
readonly EMOJI_LINK="🔗"
readonly EMOJI_BOOK="📚"
readonly EMOJI_LIGHTBULB="💡"
readonly EMOJI_FIRE="🔥"

# Get current timestamp
get_timestamp() {
    date '+%Y-%m-%d %H:%M:%S'
}

# Get elapsed time since start
get_elapsed_time() {
    local start_time=${1:-$GOK_START_TIME}
    if [[ -n "$start_time" ]]; then
        local current_time=$(date +%s)
        local elapsed=$((current_time - start_time))
        local minutes=$((elapsed / 60))
        local seconds=$((elapsed % 60))
        if [[ $minutes -gt 0 ]]; then
            echo "${minutes}m ${seconds}s"
        else
            echo "${seconds}s"
        fi
    else
        echo "0s"
    fi
}

# Initialize timing
GOK_START_TIME=$(date +%s)

# Enhanced logging functions
log_header() {
    local title="$1"
    local subtitle="${2:-}"
    echo
    echo -e "${COLOR_BRIGHT_CYAN}${COLOR_BOLD}=================================================================${COLOR_RESET}"
    echo -e "${COLOR_BRIGHT_CYAN}${COLOR_BOLD}  ${EMOJI_ROCKET} GOK Platform - $title${COLOR_RESET}"
    if [[ -n "$subtitle" ]]; then
        echo -e "${COLOR_CYAN}  $subtitle${COLOR_RESET}"
    fi
    echo -e "${COLOR_BRIGHT_CYAN}${COLOR_BOLD}=================================================================${COLOR_RESET}"
    echo
}

log_section() {
    local title="$1"
    local emoji="${2:-$EMOJI_GEAR}"
    echo
    echo -e "${COLOR_BRIGHT_BLUE}${COLOR_BOLD}--- $emoji $title ---${COLOR_RESET}"
}

log_success() {
    local message="$1"
    local timestamp=$(get_timestamp)
    echo -e "${COLOR_BRIGHT_GREEN}${COLOR_BOLD}[$timestamp] ${EMOJI_SUCCESS} $message${COLOR_RESET}"
}

log_error() {
    local message="$1"
    local timestamp=$(get_timestamp)
    echo -e "${COLOR_RED}${COLOR_BOLD}[$timestamp] ${EMOJI_ERROR} $message${COLOR_RESET}" >&2
}

log_warning() {
    local message="$1"
    local timestamp=$(get_timestamp)
    echo -e "${COLOR_BRIGHT_YELLOW}${COLOR_BOLD}[$timestamp] ${EMOJI_WARNING} $message${COLOR_RESET}"
}

log_info() {
    local message="$1"
    local timestamp=$(get_timestamp)
    echo -e "${COLOR_BRIGHT_CYAN}[$timestamp] ${EMOJI_INFO} $message${COLOR_RESET}"
}

log_step() {
    local step="$1"
    local message="$2"
    local timestamp=$(get_timestamp)
    echo -e "${COLOR_BLUE}[$timestamp] ${COLOR_BOLD}Step $step:${COLOR_RESET} ${COLOR_CYAN}$message${COLOR_RESET}"
}

log_substep() {
    local message="$1"
    echo -e "  ${COLOR_DIM}${EMOJI_ARROW} $message${COLOR_RESET}"
}

log_progress() {
    local current="$1"
    local total="$2"
    local message="${3:-Processing}"
    local percentage=$((current * 100 / total))
    local filled=$((percentage / 2))
    local empty=$((50 - filled))
    
    local bar=""
    for ((i=0; i<filled; i++)); do bar+="█"; done
    for ((i=0; i<empty; i++)); do bar+="░"; done
    
    echo -ne "\r${COLOR_BRIGHT_BLUE}$message [${COLOR_BRIGHT_GREEN}$bar${COLOR_BRIGHT_BLUE}] $percentage% ($current/$total)${COLOR_RESET}"
    if [[ $current -eq $total ]]; then
        echo
    fi
}

log_component_start() {
    local component="$1"
    local description="${2:-Installing component}"
    echo
    echo -e "${COLOR_BRIGHT_MAGENTA}${COLOR_BOLD}┌─────────────────────────────────────────────────────────────────┐${COLOR_RESET}"
    echo -e "${COLOR_BRIGHT_MAGENTA}${COLOR_BOLD}│ ${EMOJI_PACKAGE} Starting: $component${COLOR_RESET}"
    echo -e "${COLOR_MAGENTA}│ $description${COLOR_RESET}"
    echo -e "${COLOR_BRIGHT_MAGENTA}${COLOR_BOLD}└─────────────────────────────────────────────────────────────────┘${COLOR_RESET}"
    export "GOK_${component^^}_START_TIME=$(date +%s)"
}

log_component_success() {
    local component="$1"
    local message="${2:-Installation completed successfully}"
    local start_var="GOK_${component^^}_START_TIME"
    local elapsed=$(get_elapsed_time "${!start_var}")
    echo
    echo -e "${COLOR_BRIGHT_GREEN}${COLOR_BOLD}┌─────────────────────────────────────────────────────────────────┐${COLOR_RESET}"
    echo -e "${COLOR_BRIGHT_GREEN}${COLOR_BOLD}│ ${EMOJI_SUCCESS} Success: $component${COLOR_RESET}"
    echo -e "${COLOR_GREEN}│ $message${COLOR_RESET}"
    echo -e "${COLOR_GREEN}│ ${EMOJI_CLOCK} Installation time: $elapsed${COLOR_RESET}"
    echo -e "${COLOR_BRIGHT_GREEN}${COLOR_BOLD}└─────────────────────────────────────────────────────────────────┘${COLOR_RESET}"
}

log_component_error() {
    local component="$1"
    local message="${2:-Installation failed}"
    local start_var="GOK_${component^^}_START_TIME"
    local elapsed=$(get_elapsed_time "${!start_var}")
    echo
    echo -e "${COLOR_RED}${COLOR_BOLD}┌─────────────────────────────────────────────────────────────────┐${COLOR_RESET}"
    echo -e "${COLOR_RED}${COLOR_BOLD}│ ${EMOJI_ERROR} Failed: $component${COLOR_RESET}"
    echo -e "${COLOR_RED}│ $message${COLOR_RESET}"
    echo -e "${COLOR_RED}│ ${EMOJI_CLOCK} Time elapsed: $elapsed${COLOR_RESET}"
    echo -e "${COLOR_RED}${COLOR_BOLD}└─────────────────────────────────────────────────────────────────┘${COLOR_RESET}"
}

log_next_steps() {
    local title="$1"
    shift
    local steps=("$@")
    
    echo
    echo -e "${COLOR_BRIGHT_YELLOW}${COLOR_BOLD}┌─────────────────────────────────────────────────────────────────┐${COLOR_RESET}"
    echo -e "${COLOR_BRIGHT_YELLOW}${COLOR_BOLD}│ ${EMOJI_LIGHTBULB} Next Steps: $title${COLOR_RESET}"
    echo -e "${COLOR_BRIGHT_YELLOW}${COLOR_BOLD}└─────────────────────────────────────────────────────────────────┘${COLOR_RESET}"
    
    local step_num=1
    for step in "${steps[@]}"; do
        echo -e "${COLOR_YELLOW}${step_num}. $step${COLOR_RESET}"
        ((step_num++))
    done
    echo
}

log_urls() {
    local title="$1"
    shift
    local urls=("$@")
    
    echo -e "${COLOR_BRIGHT_CYAN}${COLOR_BOLD}${EMOJI_LINK} $title:${COLOR_RESET}"
    for url in "${urls[@]}"; do
        echo -e "  ${COLOR_CYAN}• $url${COLOR_RESET}"
    done
    echo
}

log_credentials() {
    local service="$1"
    local username="$2"
    local password_info="$3"
    
    echo -e "${COLOR_BRIGHT_GREEN}${COLOR_BOLD}${EMOJI_KEY} $service Credentials:${COLOR_RESET}"
    echo -e "  ${COLOR_GREEN}Username: ${COLOR_BOLD}$username${COLOR_RESET}"
    echo -e "  ${COLOR_GREEN}Password: $password_info${COLOR_RESET}"
    echo
}

log_troubleshooting() {
    local component="$1"
    shift
    local tips=("$@")
    
    echo -e "${COLOR_BRIGHT_YELLOW}${COLOR_BOLD}${EMOJI_TOOLS} Troubleshooting $component:${COLOR_RESET}"
    for tip in "${tips[@]}"; do
        echo -e "  ${COLOR_YELLOW}• $tip${COLOR_RESET}"
    done
    echo
}

show_spinner() {
    local pid=$1
    local message="${2:-Processing}"
    local delay=0.1
    local spinstr='|/-\'
    
    while [ "$(ps a | awk '{print $1}' | grep $pid)" ]; do
        local temp=${spinstr#?}
        printf "\r${COLOR_BRIGHT_BLUE}%s [%c]${COLOR_RESET}" "$message" "$spinstr"
        local spinstr=$temp${spinstr%"$temp"}
        sleep $delay
    done
    printf "\r%*s\r" ${#message} ""
}

# Legacy compatibility functions
echoSuccess() {
    log_success "$1"
}

echoFailed() {
    log_error "$1"
}

echoWarning() {
    log_warning "$1"
}

# =============================================================================
# 🌐 REMOTE EXECUTION SYSTEM
# =============================================================================

# Remote execution configuration
declare -A REMOTE_HOSTS
declare -A REMOTE_USERS
declare -A REMOTE_KEYS

# Default remote host configuration
DEFAULT_REMOTE_CONFIG_FILE="$HOME/.gok_remote_config"

# Load default remote configuration
load_default_remote_config() {
    if [ -f "$DEFAULT_REMOTE_CONFIG_FILE" ]; then
        source "$DEFAULT_REMOTE_CONFIG_FILE" 2>/dev/null
        if [ -n "$DEFAULT_REMOTE_HOST" ] && [ -n "$DEFAULT_REMOTE_USER" ]; then
            return 0
        fi
    fi
    return 1
}

# Save default remote configuration  
save_default_remote_config() {
    local host="$1"
    local user="$2"
    local key_file="${3:-$HOME/.ssh/id_rsa}"
    
    cat > "$DEFAULT_REMOTE_CONFIG_FILE" << EOF
# GOK Default Remote Configuration
DEFAULT_REMOTE_HOST="$host"
DEFAULT_REMOTE_USER="$user" 
DEFAULT_REMOTE_KEY="$key_file"
EOF
    
    log_success "Default remote configuration saved: $user@$host"
}

# Simple remote exec using default configuration
simple_remote_exec() {
    local commands="$*"
    
    # Load default configuration
    if ! load_default_remote_config; then
        log_error "No default remote configuration found!"
        echo ""
        echo "Please set up your default remote host first:"
        echo "  gok remote setup <host> <user> [key_file]"
        echo ""
        echo "Example:"
        echo "  gok remote setup 10.0.0.244 sumit"
        return 1
    fi
    
    log_info "Executing on default remote ($DEFAULT_REMOTE_USER@$DEFAULT_REMOTE_HOST): $commands"
    
    # Execute command using SSH
    ssh -i "$DEFAULT_REMOTE_KEY" -o StrictHostKeyChecking=no -o ConnectTimeout=10 \
        "$DEFAULT_REMOTE_USER@$DEFAULT_REMOTE_HOST" "$commands"
}

# Setup SSH keys for passwordless authentication
setup_ssh_keys() {
    local key_file="${1:-$HOME/.ssh/id_rsa}"
    
    log_info "Setting up SSH keys for remote access..."
    
    # Generate SSH key if it doesn't exist
    if [ ! -f "$key_file" ]; then
        log_info "Generating SSH key pair..."
        ssh-keygen -t rsa -b 4096 -f "$key_file" -N ""
        if [ $? -eq 0 ]; then
            log_success "SSH key pair generated: $key_file"
        else
            log_error "Failed to generate SSH key pair"
            return 1
        fi
    else
        log_info "SSH key already exists: $key_file"
    fi
    
    # Set proper permissions
    chmod 600 "$key_file"
    chmod 644 "${key_file}.pub"
    
    log_success "SSH keys are ready for use"
}

# Copy SSH key to remote host
copy_ssh_key() {
    local host="$1"
    local user="${2:-root}"
    local key_file="${3:-$HOME/.ssh/id_rsa}"
    
    log_info "Copying SSH key to $user@$host..."
    
    # Ensure SSH key exists
    if [ ! -f "$key_file" ]; then
        log_warning "SSH key not found, generating it first..."
        setup_ssh_keys "$key_file"
    fi
    
    # Copy the key
    ssh-copy-id -i "${key_file}.pub" "$user@$host"
    if [ $? -eq 0 ]; then
        log_success "SSH key successfully copied to $user@$host"
        
        # Test the connection
        log_info "Testing SSH connection..."
        ssh -i "$key_file" -o StrictHostKeyChecking=no -o ConnectTimeout=10 \
            "$user@$host" "echo 'SSH connection successful'; hostname; date"
        if [ $? -eq 0 ]; then
            log_success "SSH connection test successful"
        else
            log_warning "SSH connection test failed, but key was copied"
        fi
    else
        log_error "Failed to copy SSH key to $user@$host"
        return 1
    fi
}

# Configure passwordless sudo for remote user
setup_passwordless_sudo() {
    local host="$1"
    local user="${2:-root}"
    local key_file="${3:-$HOME/.ssh/id_rsa}"
    local user_password="$4"
    
    log_info "Setting up passwordless sudo for $user@$host..."
    
    # Skip if user is already root
    if [ "$user" = "root" ]; then
        log_info "User is root, passwordless sudo not needed"
        return 0
    fi
    
    # Test if passwordless sudo is already working
    ssh -i "$key_file" -o StrictHostKeyChecking=no -o ConnectTimeout=10 \
        "$user@$host" "sudo -n whoami" >/dev/null 2>&1
    
    if [ $? -eq 0 ]; then
        log_info "Passwordless sudo already configured for $user"
        return 0
    fi
    
    # If no password provided, prompt for it
    if [ -z "$user_password" ]; then
        echo -n "Enter password for $user on $host: "
        read -s user_password
        echo
    fi
    
    # Configure passwordless sudo
    log_info "Configuring passwordless sudo..."
    ssh -i "$key_file" -o StrictHostKeyChecking=no -o ConnectTimeout=10 \
        "$user@$host" "echo '$user_password' | sudo -S bash -c 'echo \"$user ALL=(ALL) NOPASSWD: ALL\" > /etc/sudoers.d/$user && chmod 440 /etc/sudoers.d/$user'" 2>/dev/null
    
    if [ $? -eq 0 ]; then
        # Test passwordless sudo
        ssh -i "$key_file" -o StrictHostKeyChecking=no -o ConnectTimeout=10 \
            "$user@$host" "sudo whoami" >/dev/null 2>&1
        
        if [ $? -eq 0 ]; then
            log_success "Passwordless sudo configured successfully for $user"
            return 0
        else
            log_error "Passwordless sudo configuration failed - testing failed"
            return 1
        fi
    else
        log_error "Failed to configure passwordless sudo (wrong password?)"
        return 1
    fi
}

# Configure remote host
configure_remote_host() {
    local alias="$1"
    local host="$2"
    local user="${3:-root}"
    local key_file="${4:-$HOME/.ssh/id_rsa}"
    local auto_setup="${5:-true}"
    
    log_info "Configuring remote host: $alias ($user@$host)"
    
    # Automatically setup SSH keys if requested
    if [ "$auto_setup" = "true" ]; then
        setup_ssh_keys "$key_file"
        copy_ssh_key "$host" "$user" "$key_file"
        if [ $? -ne 0 ]; then
            log_error "Failed to setup SSH access to $user@$host"
            return 1
        fi
    fi
    
    REMOTE_HOSTS["$alias"]="$host"
    REMOTE_USERS["$alias"]="$user"
    REMOTE_KEYS["$alias"]="$key_file"
    
    log_success "Configured remote host: $alias ($user@$host)"
}

# Execute command on remote host
remote_exec() {
    local alias="$1"
    shift
    local commands="$*"
    
    if [[ -z "${REMOTE_HOSTS[$alias]}" ]]; then
        log_error "Remote host '$alias' not configured"
        log_info "Available hosts: ${!REMOTE_HOSTS[*]}"
        return 1
    fi
    
    local host="${REMOTE_HOSTS[$alias]}"
    local user="${REMOTE_USERS[$alias]}"
    local key_file="${REMOTE_KEYS[$alias]}"
    
    log_info "Executing on $alias ($user@$host): $commands"
    
    ssh -i "$key_file" -o StrictHostKeyChecking=no -o ConnectTimeout=10 \
        "$user@$host" "$commands"
}

# Execute command on multiple remote hosts
remote_exec_all() {
    local commands="$*"
    local failed_hosts=()
    
    log_info "Executing on all configured hosts: $commands"
    
    for alias in "${!REMOTE_HOSTS[@]}"; do
        echo
        log_section "Executing on $alias" "${EMOJI_NETWORK}"
        
        if remote_exec "$alias" "$commands"; then
            log_success "Command succeeded on $alias"
        else
            log_error "Command failed on $alias"
            failed_hosts+=("$alias")
        fi
    done
    
    if [[ ${#failed_hosts[@]} -gt 0 ]]; then
        echo
        log_warning "Commands failed on: ${failed_hosts[*]}"
        return 1
    else
        log_success "Commands executed successfully on all hosts"
        return 0
    fi
}

# Copy file to remote host
remote_copy() {
    local alias="$1"
    local local_file="$2"
    local remote_path="$3"
    
    if [[ -z "${REMOTE_HOSTS[$alias]}" ]]; then
        log_error "Remote host '$alias' not configured"
        return 1
    fi
    
    local host="${REMOTE_HOSTS[$alias]}"
    local user="${REMOTE_USERS[$alias]}"
    local key_file="${REMOTE_KEYS[$alias]}"
    
    log_info "Copying $local_file to $alias:$remote_path"
    
    scp -i "$key_file" -o StrictHostKeyChecking=no \
        "$local_file" "$user@$host:$remote_path"
}

# Setup default remote hosts from config
setup_default_remote_hosts() {
    # Configure master node
    if [[ -n "$MASTER_HOST_IP" ]]; then
        configure_remote_host "master" "$MASTER_HOST_IP" "root"
    fi
    
    # Configure additional nodes if defined
    if [[ -n "$NODE_HOST_IPS" ]]; then
        IFS=',' read -ra NODE_IPS <<< "$NODE_HOST_IPS"
        local node_num=1
        for ip in "${NODE_IPS[@]}"; do
            configure_remote_host "node$node_num" "$ip" "root"
            ((node_num++))
        done
    fi
}

# Show configured remote hosts
show_remote_hosts() {
    echo
    log_section "Configured Remote Hosts" "${EMOJI_NETWORK}"
    
    if [[ ${#REMOTE_HOSTS[@]} -eq 0 ]]; then
        log_warning "No remote hosts configured"
        echo
        echo -e "${COLOR_CYAN}To configure remote hosts:${COLOR_RESET}"
        echo -e "  ${COLOR_YELLOW}gok remote add master 192.168.1.100 ubuntu${COLOR_RESET}"
        echo -e "  ${COLOR_YELLOW}gok remote add node1 192.168.1.101 ubuntu${COLOR_RESET}"
        return 0
    fi
    
    for alias in "${!REMOTE_HOSTS[@]}"; do
        local host="${REMOTE_HOSTS[$alias]}"
        local user="${REMOTE_USERS[$alias]}"
        local key_file="${REMOTE_KEYS[$alias]}"
        
        echo -e "  ${COLOR_GREEN}${alias}${COLOR_RESET}: ${COLOR_CYAN}${user}@${host}${COLOR_RESET}"
        echo -e "    ${COLOR_DIM}Key: ${key_file}${COLOR_RESET}"
        
        # Test connectivity
        if ssh -i "$key_file" -o StrictHostKeyChecking=no -o ConnectTimeout=5 \
               "$user@$host" "echo 'Connection test'" >/dev/null 2>&1; then
            echo -e "    ${COLOR_GREEN}${EMOJI_SUCCESS} Connected${COLOR_RESET}"
        else
            echo -e "    ${COLOR_RED}${EMOJI_ERROR} Connection failed${COLOR_RESET}"
        fi
        echo
    done
}

# Auto-setup SSH and remote host configuration
auto_setup_remote_host() {
    local host="$1"
    local user="${2:-root}"
    local alias="${3:-${host//\./_}}"  # Replace dots with underscores for alias
    local key_file="${4:-$HOME/.ssh/id_rsa}"
    
    log_info "Auto-setting up remote access to $user@$host..."
    
    # Step 1: Setup SSH keys if needed
    if [ ! -f "$key_file" ]; then
        log_info "SSH keys not found, generating them..."
        setup_ssh_keys "$key_file"
        if [ $? -ne 0 ]; then
            log_error "Failed to generate SSH keys"
            return 1
        fi
    fi
    
    # Step 2: Copy SSH key if not already copied  
    log_info "Ensuring SSH key is copied to $user@$host..."
    ssh -i "$key_file" -o StrictHostKeyChecking=no -o ConnectTimeout=5 \
        "$user@$host" "echo 'test'" >/dev/null 2>&1
    
    if [ $? -ne 0 ]; then
        log_info "SSH key access not working, copying key..."
        copy_ssh_key "$host" "$user" "$key_file"
        if [ $? -ne 0 ]; then
            log_error "Failed to setup SSH access to $user@$host"
            return 1
        fi
    else
        log_info "SSH key access already working"
    fi
    
    # Step 3: Configure remote host if not already configured
    if [[ -z "${REMOTE_HOSTS[$alias]}" ]]; then
        log_info "Configuring remote host alias: $alias"
        configure_remote_host "$alias" "$host" "$user" "$key_file" "false"  # Don't auto-setup again
    else
        log_info "Remote host '$alias' already configured"
    fi
    
    log_success "Auto-setup complete for $alias ($user@$host)"
    return 0
}

# Smart remote exec that auto-configures hosts
smart_remote_exec() {
    local target="$1"
    shift
    local commands="$*"
    
    # If target looks like host:user format, auto-configure it
    if [[ "$target" == *":"* ]]; then
        local host="${target%:*}"
        local user="${target#*:}"
        local alias="${host//\./_}_${user}"
        
        log_info "Auto-configuring new remote host: $user@$host as '$alias'"
        auto_setup_remote_host "$host" "$user" "$alias"
        if [ $? -eq 0 ]; then
            target="$alias"
        else
            log_error "Failed to auto-configure $user@$host"
            return 1
        fi
    fi
    
    # Check if target is configured, if not try to auto-configure as root@target
    if [[ -z "${REMOTE_HOSTS[$target]}" ]] && [[ "$target" != "all" ]]; then
        # If target looks like an IP or hostname, try to auto-configure it
        if [[ "$target" =~ ^[0-9]{1,3}\.[0-9]{1,3}\.[0-9]{1,3}\.[0-9]{1,3}$ ]] || [[ "$target" =~ ^[a-zA-Z0-9.-]+$ ]]; then
            log_info "Auto-configuring new remote host: root@$target"
            auto_setup_remote_host "$target" "root" "$target"
            if [ $? -ne 0 ]; then
                log_error "Failed to auto-configure root@$target"
                return 1
            fi
        fi
    fi
    
    # Now execute the command using the standard remote_exec
    if [[ "$target" == "all" ]]; then
        remote_exec_all "$commands"
    else
        remote_exec "$target" "$commands"
    fi
}

# Remote command shortcuts for common operations
remote_status() {
    local alias="${1:-all}"
    
    local commands="
        echo '=== System Info ==='
        hostname && date
        echo '=== Kubernetes Status ==='
        kubectl get nodes 2>/dev/null || echo 'kubectl not available'
        echo '=== Docker Status ==='
        systemctl is-active docker 2>/dev/null || echo 'Docker not running'
        echo '=== Load Average ==='
        uptime
        echo '=== Disk Usage ==='
        df -h / | tail -1
    "
    
    if [[ "$alias" == "all" ]]; then
        remote_exec_all "$commands"
    else
        remote_exec "$alias" "$commands"
    fi
}

# Install GOK on remote hosts
remote_install_gok() {
    local alias="${1:-all}"
    
    local commands="
        # Download GOK script
        curl -fsSL https://raw.githubusercontent.com/sumitmaji/kubernetes/main/install_k8s/gok -o /tmp/gok
        chmod +x /tmp/gok
        sudo mv /tmp/gok /usr/local/bin/gok
        
        # Verify installation
        gok --version || echo 'GOK installation completed'
    "
    
    if [[ "$alias" == "all" ]]; then
        remote_exec_all "$commands"
    else
        remote_exec "$alias" "$commands"
    fi
}

# =============================================================================
# 📊 COMPONENT STATUS TRACKING SYSTEM
# =============================================================================

# Component status tracking
declare -A GOK_COMPONENT_STATUS
declare -A GOK_COMPONENT_START_TIME
declare -A GOK_COMPONENT_END_TIME
declare -A GOK_COMPONENT_DESCRIPTION

# Status constants
readonly STATUS_IDLE="idle"
readonly STATUS_STARTING="starting"
readonly STATUS_IN_PROGRESS="in_progress"
readonly STATUS_SUCCESS="success"
readonly STATUS_FAILED="failed"
readonly STATUS_SKIPPED="skipped"

# Initialize component tracking
init_component_tracking() {
    local component="$1"
    local description="$2"
    
    GOK_COMPONENT_STATUS["$component"]="$STATUS_IDLE"
    GOK_COMPONENT_DESCRIPTION["$component"]="$description"
}

# Start component installation
start_component() {
    local component="$1"
    local description="${2:-Installing $component}"
    
    GOK_COMPONENT_STATUS["$component"]="$STATUS_STARTING"
    GOK_COMPONENT_START_TIME["$component"]=$(date +%s)
    
    log_component_start "$component" "$description"
    GOK_COMPONENT_STATUS["$component"]="$STATUS_IN_PROGRESS"
}

# Complete component installation successfully
complete_component() {
    local component="$1"
    local message="${2:-Installation completed successfully}"
    
    GOK_COMPONENT_STATUS["$component"]="$STATUS_SUCCESS"
    GOK_COMPONENT_END_TIME["$component"]=$(date +%s)
    
    log_component_success "$component" "$message"
}

# Mark component installation as failed
fail_component() {
    local component="$1"
    local message="${2:-Installation failed}"
    local error_details="${3:-}"
    
    GOK_COMPONENT_STATUS["$component"]="$STATUS_FAILED"
    GOK_COMPONENT_END_TIME["$component"]=$(date +%s)
    
    log_component_error "$component" "$message"
    
    if [[ -n "$error_details" ]]; then
        echo -e "${COLOR_RED}Error details: $error_details${COLOR_RESET}"
    fi
    
    show_component_troubleshooting "$component"
}

# Skip component installation
skip_component() {
    local component="$1"
    local reason="${2:-Component already installed or not required}"
    
    GOK_COMPONENT_STATUS["$component"]="$STATUS_SKIPPED"
    GOK_COMPONENT_END_TIME["$component"]=$(date +%s)
    
    log_warning "Skipped $component: $reason"
}

# Get component status
get_component_status() {
    local component="$1"
    echo "${GOK_COMPONENT_STATUS[$component]:-idle}"
}

# Check if component is installed/successful
is_component_successful() {
    local component="$1"
    [[ "${GOK_COMPONENT_STATUS[$component]}" == "$STATUS_SUCCESS" ]]
}

# Get component installation time
get_component_time() {
    local component="$1"
    local start_time="${GOK_COMPONENT_START_TIME[$component]}"
    local end_time="${GOK_COMPONENT_END_TIME[$component]}"
    
    if [[ -n "$start_time" && -n "$end_time" ]]; then
        local elapsed=$((end_time - start_time))
        local minutes=$((elapsed / 60))
        local seconds=$((elapsed % 60))
        if [[ $minutes -gt 0 ]]; then
            echo "${minutes}m ${seconds}s"
        else
            echo "${seconds}s"
        fi
    else
        echo "Unknown"
    fi
}

# Show installation summary
show_installation_summary() {
    local total_components=0
    local successful_components=0
    local failed_components=0
    local skipped_components=0
    
    echo
    log_header "Installation Summary" "$(get_timestamp)"
    
    echo -e "${COLOR_BRIGHT_CYAN}${COLOR_BOLD}Component Status Overview:${COLOR_RESET}"
    echo
    
    for component in "${!GOK_COMPONENT_STATUS[@]}"; do
        local status="${GOK_COMPONENT_STATUS[$component]}"
        local description="${GOK_COMPONENT_DESCRIPTION[$component]}"
        local time=$(get_component_time "$component")
        
        ((total_components++))
        
        case "$status" in
            "$STATUS_SUCCESS")
                ((successful_components++))
                echo -e "  ${EMOJI_SUCCESS} ${COLOR_GREEN}${COLOR_BOLD}$component${COLOR_RESET} ${COLOR_GREEN}($time)${COLOR_RESET}"
                echo -e "    ${COLOR_DIM}$description${COLOR_RESET}"
                ;;
            "$STATUS_FAILED")
                ((failed_components++))
                echo -e "  ${EMOJI_ERROR} ${COLOR_RED}${COLOR_BOLD}$component${COLOR_RESET} ${COLOR_RED}($time)${COLOR_RESET}"
                echo -e "    ${COLOR_DIM}$description${COLOR_RESET}"
                ;;
            "$STATUS_SKIPPED")
                ((skipped_components++))
                echo -e "  ${EMOJI_WARNING} ${COLOR_YELLOW}${COLOR_BOLD}$component${COLOR_RESET} ${COLOR_YELLOW}(skipped)${COLOR_RESET}"
                echo -e "    ${COLOR_DIM}$description${COLOR_RESET}"
                ;;
            *)
                echo -e "  ${EMOJI_INFO} ${COLOR_BLUE}${COLOR_BOLD}$component${COLOR_RESET} ${COLOR_BLUE}($status)${COLOR_RESET}"
                echo -e "    ${COLOR_DIM}$description${COLOR_RESET}"
                ;;
        esac
        echo
    done
    
    # Summary statistics
    echo -e "${COLOR_BRIGHT_CYAN}${COLOR_BOLD}Summary Statistics:${COLOR_RESET}"
    echo -e "  ${COLOR_GREEN}${EMOJI_SUCCESS} Successful: $successful_components${COLOR_RESET}"
    echo -e "  ${COLOR_RED}${EMOJI_ERROR} Failed: $failed_components${COLOR_RESET}"
    echo -e "  ${COLOR_YELLOW}${EMOJI_WARNING} Skipped: $skipped_components${COLOR_RESET}"
    echo -e "  ${COLOR_BLUE}${EMOJI_PACKAGE} Total: $total_components${COLOR_RESET}"
    
    local total_time=$(get_elapsed_time "$GOK_START_TIME")
    echo -e "  ${COLOR_CYAN}${EMOJI_CLOCK} Total time: $total_time${COLOR_RESET}"
    echo
    
    # Overall result
    if [[ $failed_components -eq 0 ]]; then
        log_success "All components installed successfully!"
    else
        log_error "$failed_components component(s) failed to install"
        return 1
    fi
}

# Component-specific troubleshooting
show_component_troubleshooting() {
    local component="$1"
    
    case "$component" in
        "kubernetes")
            log_troubleshooting "Kubernetes" \
                "Check if all nodes are ready: kubectl get nodes" \
                "Verify cluster status: kubectl cluster-info" \
                "Check system pods: kubectl get pods -n kube-system" \
                "Review kubelet logs: journalctl -u kubelet -f"
            ;;
        "cert-manager")
            log_troubleshooting "Cert-Manager" \
                "Check cert-manager pods: kubectl get pods -n cert-manager" \
                "Verify webhook: kubectl get validatingwebhookconfiguration" \
                "Test certificate creation: kubectl describe certificate" \
                "Check DNS resolution for ACME challenge"
            ;;
        "vault")
            log_troubleshooting "Vault" \
                "Check vault status: kubectl get pods -n vault" \
                "Verify vault seal status: kubectl exec -n vault vault-0 -- vault status" \
                "Check vault logs: kubectl logs -n vault vault-0" \
                "Ensure proper RBAC permissions are set"
            ;;
        "monitoring")
            log_troubleshooting "Monitoring" \
                "Check Prometheus: kubectl get pods -n monitoring" \
                "Verify Grafana access: kubectl get svc -n monitoring" \
                "Check metrics endpoints: kubectl get servicemonitor" \
                "Review storage availability for persistent volumes"
            ;;
        *)
            log_troubleshooting "$component" \
                "Check pod status: kubectl get pods -A | grep $component" \
                "Review logs: kubectl logs -l app=$component" \
                "Verify service endpoints: kubectl get svc | grep $component" \
                "Check ingress configuration: kubectl get ingress"
            ;;
    esac
}

# =============================================================================
# 🎯 NEXT-STEPS GUIDANCE SYSTEM
# =============================================================================

# Show comprehensive next steps after component installation
show_component_next_steps() {
    local component="$1"
    
    case "$component" in
        "kubernetes")
            log_next_steps "Kubernetes Cluster" \
                "Verify cluster: kubectl get nodes" \
                "Check system pods: kubectl get pods -n kube-system" \
                "Install networking: gok install cert-manager" \
                "Install ingress: gok install ingress" \
                "Set up monitoring: gok install monitoring"
            
            log_urls "Cluster Information" \
                "Dashboard: https://$(fullDefaultUrl)/dashboard" \
                "API Server: https://$(kubectl config view --minify -o jsonpath='{.clusters[0].cluster.server}')"
            
            log_info "Next recommended components: cert-manager → ingress → monitoring"
            ;;
            
        "cert-manager")
            log_next_steps "Certificate Manager" \
                "Verify installation: kubectl get pods -n cert-manager" \
                "Test webhook: gok checkCMWebhook (after installing curl pod)" \
                "Install ingress: gok install ingress" \
                "Create certificates: gok create certificate production"
            
            log_info "Cert-manager is now managing TLS certificates automatically"
            log_warning "Let's Encrypt rate limits apply - use staging for testing"
            ;;
            
        "ingress")
            log_next_steps "NGINX Ingress Controller" \
                "Check ingress: kubectl get pods -n ingress-nginx" \
                "Verify external IP: kubectl get svc -n ingress-nginx" \
                "Test with sample app: gok deploy app1" \
                "Install dashboard: gok install dashboard"
            
            log_urls "Ingress Access" \
                "Default host: https://$(fullDefaultUrl)" \
                "Dashboard: https://$(fullDefaultUrl)/dashboard"
            
            log_info "All applications will now be accessible through ingress"
            ;;
            
        "monitoring")
            log_next_steps "Prometheus & Grafana Monitoring" \
                "Check monitoring pods: kubectl get pods -n monitoring" \
                "Access Grafana dashboard" \
                "Import custom dashboards" \
                "Configure alerting rules"
            
            log_urls "Monitoring Dashboards" \
                "Grafana: https://grafana.$(rootDomain)" \
                "Prometheus: https://prometheus.$(rootDomain)" \
                "AlertManager: https://alertmanager.$(rootDomain)"
            
            log_credentials "Grafana" "admin" \
                "Run: kubectl get secret -n monitoring grafana-admin-password -o jsonpath='{.data.password}' | base64 -d"
            
            log_info "Pre-configured dashboards: Kubernetes cluster, node metrics, pod metrics"
            ;;
            
        "vault")
            log_next_steps "HashiCorp Vault" \
                "Check vault status: kubectl get pods -n vault" \
                "Initialize vault: kubectl exec -n vault vault-0 -- vault operator init" \
                "Unseal vault with generated keys" \
                "Configure authentication methods" \
                "Set up secret engines and policies"
            
            log_urls "Vault Access" \
                "Vault UI: https://$(fullVaultUrl)" \
                "Vault API: https://$(fullVaultUrl)/v1/"
            
            log_warning "IMPORTANT: Save vault unseal keys and root token securely!"
            log_info "Vault is running in HA mode with auto-unseal configured"
            ;;
            
        "keycloak")
            log_next_steps "Keycloak Identity Management" \
                "Access Keycloak admin console" \
                "Create realms and clients" \
                "Configure user federation (LDAP if needed)" \
                "Set up OAuth2 integration: gok install oauth2"
            
            log_urls "Keycloak Access" \
                "Admin Console: https://$(fullKeycloakUrl)/admin" \
                "Account Console: https://$(fullKeycloakUrl)/realms/master/account"
            
            log_credentials "Keycloak Admin" "admin" \
                "Run: kubectl get secret -n keycloak keycloak-admin-password -o jsonpath='{.data.password}' | base64 -d"
            
            log_info "Default realm 'master' created. Create application-specific realms as needed."
            ;;
            
        "argocd")
            log_next_steps "ArgoCD GitOps" \
                "Access ArgoCD UI" \
                "Connect Git repositories" \
                "Create applications and sync policies" \
                "Set up RBAC and SSO integration"
            
            log_urls "ArgoCD Access" \
                "ArgoCD UI: https://argocd.$(rootDomain)" \
                "ArgoCD CLI: argocd login argocd.$(rootDomain)"
            
            log_credentials "ArgoCD Admin" "admin" \
                "Run: kubectl get secret -n argocd argocd-initial-admin-secret -o jsonpath='{.data.password}' | base64 -d"
            
            log_info "ArgoCD is configured for GitOps deployment workflows"
            ;;
            
        "jupyter")
            log_next_steps "JupyterHub Development Environment" \
                "Access JupyterHub" \
                "Configure user authentication" \
                "Install additional Python packages" \
                "Create shared notebooks and datasets"
            
            log_urls "JupyterHub Access" \
                "JupyterHub: https://jupyter.$(rootDomain)" \
                "Admin Panel: https://jupyter.$(rootDomain)/hub/admin"
            
            log_info "Default spawner configured with data science libraries pre-installed"
            ;;
            
        "registry")
            log_next_steps "Container Registry" \
                "Configure Docker daemon to use registry" \
                "Push first image: docker push $(fullRegistryUrl)/my-app:latest" \
                "Set up image scanning and policies" \
                "Configure registry webhooks"
            
            log_urls "Registry Access" \
                "Registry UI: https://$(fullRegistryUrl)" \
                "Registry API: https://$(fullRegistryUrl)/v2/"
            
            log_info "Registry is configured with TLS and basic authentication"
            ;;
            
        "gok-controller"|"controller")
            log_next_steps "GOK Platform Services" \
                "Verify controller: kubectl get pods -n gok-controller" \
                "Check agent: kubectl get pods -n gok-agent" \
                "Test command execution through web interface" \
                "Configure authentication and authorization"
            
            log_urls "GOK Platform Access" \
                "GOK Controller: https://gok-controller.$(rootDomain)" \
                "GOK Agent: Internal service for command execution"
            
            log_info "GOK distributed command execution platform is ready"
            ;;
            
        "istio")
            log_next_steps "Istio Service Mesh" \
                "Enable automatic sidecar injection for namespaces" \
                "Deploy sample applications with Istio" \
                "Configure traffic management and security policies" \
                "Set up observability with Kiali and Jaeger"
            
            log_info "Istio service mesh is installed and ready for microservices"
            ;;
            
        *)
            log_next_steps "$component Installation Complete" \
                "Check component status: kubectl get pods -A | grep $component" \
                "Review component logs: kubectl logs -l app=$component" \
                "Verify service endpoints: kubectl get svc | grep $component" \
                "Check ingress configuration: kubectl get ingress"
            
            log_info "Component $component has been installed successfully"
            ;;
    esac
}

# Show platform overview after major installations
show_platform_overview() {
    echo
    log_header "GOK Platform Overview" "Current Installation Status"
    
    echo -e "${COLOR_BRIGHT_CYAN}${COLOR_BOLD}🏗️  PLATFORM COMPONENTS STATUS${COLOR_RESET}"
    echo
    
    # Core Infrastructure
    echo -e "${COLOR_YELLOW}${COLOR_BOLD}Core Infrastructure:${COLOR_RESET}"
    check_and_display_component "kubernetes" "Kubernetes Cluster"
    check_and_display_component "cert-manager" "Certificate Manager"
    check_and_display_component "ingress" "NGINX Ingress"
    check_and_display_component "monitoring" "Prometheus & Grafana"
    echo
    
    # Security & Identity
    echo -e "${COLOR_YELLOW}${COLOR_BOLD}Security & Identity:${COLOR_RESET}"
    check_and_display_component "vault" "HashiCorp Vault"
    check_and_display_component "keycloak" "Keycloak"
    check_and_display_component "oauth2" "OAuth2 Proxy"
    echo
    
    # Development & DevOps
    echo -e "${COLOR_YELLOW}${COLOR_BOLD}Development & DevOps:${COLOR_RESET}"
    check_and_display_component "argocd" "ArgoCD"
    check_and_display_component "jenkins" "Jenkins"
    check_and_display_component "jupyter" "JupyterHub"
    check_and_display_component "registry" "Container Registry"
    echo
    
    # GOK Platform
    echo -e "${COLOR_YELLOW}${COLOR_BOLD}GOK Platform:${COLOR_RESET}"
    check_and_display_component "gok-controller" "GOK Controller"
    check_and_display_component "gok-agent" "GOK Agent"
    echo
    
    # Provide recommendations
    suggest_next_installations
}

# Check if component is installed and display status
check_and_display_component() {
    local component="$1"
    local display_name="$2"
    
    # Simple check - you can enhance this with actual status checking
    if kubectl get deployment "$component" >/dev/null 2>&1 || kubectl get statefulset "$component" >/dev/null 2>&1; then
        echo -e "  ${EMOJI_SUCCESS} ${COLOR_GREEN}$display_name${COLOR_RESET}"
    else
        echo -e "  ${EMOJI_CROSS} ${COLOR_DIM}$display_name${COLOR_RESET}"
    fi
}

# Suggest next installations based on current state
suggest_next_installations() {
    echo -e "${COLOR_BRIGHT_YELLOW}${COLOR_BOLD}💡 SUGGESTED NEXT STEPS${COLOR_RESET}"
    echo
    
    # Check if basic infrastructure is ready
    if ! kubectl get deployment cert-manager >/dev/null 2>&1; then
        echo -e "${COLOR_CYAN}• Install certificate management: ${COLOR_BOLD}gok install cert-manager${COLOR_RESET}"
    fi
    
    if ! kubectl get deployment ingress-nginx-controller >/dev/null 2>&1; then
        echo -e "${COLOR_CYAN}• Install ingress controller: ${COLOR_BOLD}gok install ingress${COLOR_RESET}"
    fi
    
    if ! kubectl get deployment -n monitoring prometheus-operator >/dev/null 2>&1; then
        echo -e "${COLOR_CYAN}• Install monitoring stack: ${COLOR_BOLD}gok install monitoring${COLOR_RESET}"
    fi
    
    if ! kubectl get statefulset -n vault vault >/dev/null 2>&1; then
        echo -e "${COLOR_CYAN}• Install secrets management: ${COLOR_BOLD}gok install vault${COLOR_RESET}"
    fi
    
    if ! kubectl get deployment -n keycloak keycloak >/dev/null 2>&1; then
        echo -e "${COLOR_CYAN}• Install identity management: ${COLOR_BOLD}gok install keycloak${COLOR_RESET}"
    fi
    
    echo -e "${COLOR_CYAN}• Generate your first microservice: ${COLOR_BOLD}gok generate python-api my-service${COLOR_RESET}"
    echo -e "${COLOR_CYAN}• Check complete platform status: ${COLOR_BOLD}gok status${COLOR_RESET}"
    echo
}

# =============================================================================
# � REPOSITORY FIX UTILITIES
# =============================================================================

# Fix Helm repository 404 errors and other package repository issues
fix_helm_repository_errors() {
    log_header "Repository Fix Utility" "Resolving Helm & Package Repository Issues"
    
    echo -e "${COLOR_BRIGHT_CYAN}${COLOR_BOLD}🔧 FIXING HELM REPOSITORY ERRORS${COLOR_RESET}"
    echo
    
    local fix_success=true
    
    # Step 1: Clean up old Helm repositories
    log_step "1" "Removing old/problematic Helm repositories"
    if sudo find /etc/apt/sources.list.d -name "*helm*" -type f -delete 2>/dev/null; then
        log_success "Old Helm repository files removed"
    else
        log_info "No old Helm repository files found"
    fi
    
    # Remove deprecated apt-key entries
    if sudo apt-key list 2>/dev/null | grep -q "Helm"; then
        log_info "Removing deprecated Helm apt-key entries"
        sudo apt-key del $(sudo apt-key list 2>/dev/null | grep -A1 "Helm" | grep pub | cut -d'/' -f2 | cut -d' ' -f1) >/dev/null 2>&1 || true
        log_success "Deprecated key entries cleaned"
    fi
    
    # Step 2: Update package lists after cleanup
    log_step "2" "Updating package lists after cleanup"
    if sudo apt-get update >/dev/null 2>&1; then
        log_success "Package lists updated successfully"
    else
        log_warning "Package update had some warnings (continuing...)"
    fi
    
    # Step 3: Verify Helm installation method
    log_step "3" "Checking current Helm installation"
    if command -v helm >/dev/null 2>&1; then
        local helm_path=$(which helm)
        local helm_version=$(helm version --short --client 2>/dev/null | grep -oE 'v[0-9]+\.[0-9]+\.[0-9]+' || echo "unknown")
        
        if [[ "$helm_path" == *"/snap/"* ]]; then
            log_success "Helm is properly installed via snap (version: $helm_version)"
            echo -e "  ${COLOR_GREEN}✓ Path: $helm_path${COLOR_RESET}"
            echo -e "  ${COLOR_GREEN}✓ Installation method: Snap package manager${COLOR_RESET}"
            echo -e "  ${COLOR_GREEN}✓ No APT repository conflicts${COLOR_RESET}"
        else
            log_info "Helm installed via: $helm_path (version: $helm_version)"
        fi
    else
        log_warning "Helm not found - would you like to install it?"
        echo -e "  ${COLOR_YELLOW}Run: ${COLOR_BOLD}sudo ./gok install helm${COLOR_RESET}"
    fi
    
    # Step 4: Setup modern Helm repository (if using APT method)
    log_step "4" "Setting up modern Helm repository (if needed)"
    if [[ "$helm_path" != *"/snap/"* ]] && command -v helm >/dev/null 2>&1; then
        log_info "Setting up modern Helm APT repository..."
        if install_helm_via_apt_fix; then
            log_success "Modern Helm repository configured successfully"
        else
            log_warning "Modern APT setup failed - recommend using snap: sudo snap install helm --classic"
            fix_success=false
        fi
    else
        log_info "Using snap installation - no APT repository needed"
    fi
    
    # Step 5: Test repository access
    log_step "5" "Testing package repository access"
    if sudo apt-get update >/dev/null 2>&1; then
        log_success "All package repositories accessible"
        
        # Check for any remaining 404 errors
        local error_check=$(sudo apt-get update 2>&1 | grep -i "404\|not found\|failed\|err:" || true)
        if [[ -z "$error_check" ]]; then
            log_success "No 404 or repository errors detected"
        else
            log_warning "Some repository warnings detected:"
            echo "$error_check" | head -3
        fi
    else
        log_error "Package repository access issues detected"
        fix_success=false
    fi
    
    # Summary and recommendations
    echo
    log_header "Fix Summary" "Repository Status"
    
    if [[ "$fix_success" == "true" ]]; then
        echo -e "${COLOR_BRIGHT_GREEN}${COLOR_BOLD}✅ REPOSITORY ISSUES RESOLVED${COLOR_RESET}"
        echo
        echo -e "${COLOR_GREEN}✓${COLOR_RESET} Old Helm repositories cleaned up"
        echo -e "${COLOR_GREEN}✓${COLOR_RESET} Package lists updated successfully"
        echo -e "${COLOR_GREEN}✓${COLOR_RESET} No 404 errors detected"
        echo -e "${COLOR_GREEN}✓${COLOR_RESET} Helm installation verified"
    else
        echo -e "${COLOR_BRIGHT_YELLOW}${COLOR_BOLD}⚠️  PARTIAL RESOLUTION${COLOR_RESET}"
        echo
        echo -e "${COLOR_YELLOW}!${COLOR_RESET} Some issues may require manual intervention"
    fi
    
    echo
    echo -e "${COLOR_BRIGHT_CYAN}${COLOR_BOLD}📋 PREVENTION TIPS:${COLOR_RESET}"
    echo -e "${COLOR_CYAN}• Use snap for Helm: ${COLOR_BOLD}sudo snap install helm --classic${COLOR_RESET}"
    echo -e "${COLOR_CYAN}• Regular updates: ${COLOR_BOLD}sudo snap refresh helm${COLOR_RESET}"
    echo -e "${COLOR_CYAN}• Avoid mixing installation methods (APT + Snap + Script)${COLOR_RESET}"
    echo -e "${COLOR_CYAN}• Run this fix utility: ${COLOR_BOLD}./gok fix helm-repository${COLOR_RESET}"
    echo
    
    echo -e "${COLOR_BRIGHT_CYAN}${COLOR_BOLD}🛠️  TROUBLESHOOTING:${COLOR_RESET}"
    echo -e "${COLOR_CYAN}• If issues persist: ${COLOR_BOLD}sudo apt-get clean && sudo apt-get update${COLOR_RESET}"
    echo -e "${COLOR_CYAN}• Check logs: ${COLOR_BOLD}sudo apt-get update -o Debug::Acquire::http=true${COLOR_RESET}"
    echo -e "${COLOR_CYAN}• Fresh Helm install: ${COLOR_BOLD}sudo snap remove helm && sudo snap install helm --classic${COLOR_RESET}"
    echo
    
    return $([[ "$fix_success" == "true" ]] && echo 0 || echo 1)
}

# Helper function for modern Helm APT setup
install_helm_via_apt_fix() {
    # Clean up any old repositories first
    sudo rm -f /etc/apt/sources.list.d/helm*.list 2>/dev/null || true
    
    # Add official Helm APT repository with proper key management
    if curl -fsSL https://baltocdn.com/helm/signing.asc | gpg --dearmor | sudo tee /usr/share/keyrings/helm.gpg > /dev/null 2>&1; then
        echo "deb [arch=$(dpkg --print-architecture) signed-by=/usr/share/keyrings/helm.gpg] https://baltocdn.com/helm/stable/debian/ all main" | sudo tee /etc/apt/sources.list.d/helm-stable-debian.list >/dev/null
        
        if sudo apt-get update >/dev/null 2>&1; then
            return 0
        fi
    fi
    return 1
}

# =============================================================================
# �🔍 INSTALLATION VALIDATION SYSTEM
# =============================================================================

# Validate component installation with comprehensive health checks
validate_component_installation() {
    local component="$1"
    local timeout="${2:-300}" # Default 5 minutes timeout
    
    log_info "Validating $component installation..."
    
    case "$component" in
        "kubernetes")
            validate_kubernetes_cluster "$timeout"
            ;;
        "docker")
            validate_docker_installation "$timeout"
            ;;
        "helm")
            validate_helm_installation "$timeout"
            ;;
        "cert-manager")
            validate_cert_manager "$timeout"
            ;;
        "ingress")
            validate_ingress_controller "$timeout"
            ;;
        "monitoring")
            validate_monitoring_stack "$timeout"
            ;;
        "vault")
            validate_vault_installation "$timeout"
            ;;
        "keycloak")
            validate_keycloak_installation "$timeout"
            ;;
        "argocd")
            validate_argocd_installation "$timeout"
            ;;
        "jupyter")
            validate_jupyter_installation "$timeout"
            ;;
        "registry")
            validate_registry_installation "$timeout"
            ;;
        "gok-controller"|"controller")
            validate_gok_controller_installation "$timeout"
            ;;
        *)
            validate_generic_component "$component" "$timeout"
            ;;
    esac
}

# Generic component validation
validate_generic_component() {
    local component="$1"
    local timeout="$2"
    local validation_passed=true
    
    log_step "1" "Checking $component pods"
    if ! wait_for_pods_ready "$component" "$timeout"; then
        log_error "Pods not ready for $component"
        validation_passed=false
    else
        log_success "All $component pods are ready"
    fi
    
    log_step "2" "Checking $component services"
    if kubectl get svc -A | grep -q "$component"; then
        log_success "$component services are available"
    else
        log_warning "No services found for $component"
    fi
    
    return $([[ "$validation_passed" == "true" ]] && echo 0 || echo 1)
}

# Docker installation validation
validate_docker_installation() {
    local timeout="$1"
    local validation_passed=true
    
    log_step "1" "Checking Docker daemon status"
    if systemctl is-active --quiet docker; then
        log_success "Docker daemon is running"
    else
        log_error "Docker daemon is not running"
        validation_passed=false
    fi
    
    log_step "2" "Checking Docker version"
    if docker --version >/dev/null 2>&1; then
        local version=$(docker --version | grep -oE '[0-9]+\.[0-9]+\.[0-9]+')
        log_success "Docker version: $version"
    else
        log_error "Cannot get Docker version"
        validation_passed=false
    fi
    
    log_step "3" "Testing Docker functionality"
    if docker run --rm hello-world >/dev/null 2>&1; then
        log_success "Docker can run containers successfully"
    else
        log_warning "Docker container test failed (may require proxy configuration)"
    fi
    
    log_step "4" "Checking Docker group membership"
    if groups $USER | grep -q docker; then
        log_success "User is in docker group"
    else
        log_warning "User not in docker group - may need 'sudo' for docker commands"
    fi
    
    return $([[ "$validation_passed" == "true" ]] && echo 0 || echo 1)
}

# Helm installation validation
validate_helm_installation() {
    local timeout="$1"
    local validation_passed=true
    
    log_step "1" "Checking Helm binary"
    if command -v helm >/dev/null 2>&1; then
        local version=$(helm version --short --client 2>/dev/null | grep -oE 'v[0-9]+\.[0-9]+\.[0-9]+' || echo "unknown")
        log_success "Helm is installed (version: $version)"
    else
        log_error "Helm binary not found in PATH"
        validation_passed=false
        return 1
    fi
    
    log_step "2" "Testing Helm functionality"
    if helm version --short >/dev/null 2>&1; then
        log_success "Helm version command works"
    else
        log_error "Helm version command failed"
        validation_passed=false
    fi
    
    log_step "3" "Checking Helm repositories"
    local repo_count=$(helm repo list 2>/dev/null | tail -n +2 | wc -l)
    if [[ $repo_count -gt 0 ]]; then
        log_success "Helm has $repo_count repositories configured"
    else
        log_info "No Helm repositories configured yet (this is normal for new installations)"
    fi
    
    log_step "4" "Testing repository access"
    if helm repo add bitnami https://charts.bitnami.com/bitnami >/dev/null 2>&1 && \
       helm repo update >/dev/null 2>&1; then
        log_success "Helm can access remote repositories"
        # Clean up test repository
        helm repo remove bitnami >/dev/null 2>&1 || true
    else
        log_warning "Cannot access remote repositories (may require proxy configuration)"
    fi
    
    return $([[ "$validation_passed" == "true" ]] && echo 0 || echo 1)
}

# Kubernetes cluster validation
validate_kubernetes_cluster() {
    local timeout="$1"
    local validation_passed=true
    
    log_step "1" "Checking cluster API server connectivity"
    if kubectl cluster-info >/dev/null 2>&1; then
        log_success "Kubernetes API server is accessible"
    else
        log_error "Cannot connect to Kubernetes API server"
        validation_passed=false
    fi
    
    log_step "2" "Checking node status"
    local ready_nodes=$(kubectl get nodes --no-headers | grep -c " Ready ")
    local total_nodes=$(kubectl get nodes --no-headers | wc -l)
    
    if [[ $ready_nodes -eq $total_nodes && $ready_nodes -gt 0 ]]; then
        log_success "All $total_nodes nodes are ready"
    else
        log_error "Only $ready_nodes out of $total_nodes nodes are ready"
        validation_passed=false
    fi
    
    log_step "3" "Checking system components"
    if wait_for_pods_ready "kube-system" "$timeout"; then
        log_success "All system components are ready"
    else
        log_error "Some system components are not ready"
        validation_passed=false
    fi
    
    return $([[ "$validation_passed" == "true" ]] && echo 0 || echo 1)
}

# Cert-manager validation
validate_cert_manager() {
    local timeout="$1"
    local validation_passed=true
    
    log_step "1" "Checking cert-manager pods"
    if ! wait_for_pods_ready "cert-manager" "$timeout"; then
        log_error "Cert-manager pods not ready"
        validation_passed=false
    else
        log_success "Cert-manager pods are ready"
    fi
    
    log_step "2" "Checking cert-manager webhook"
    if kubectl get validatingwebhookconfiguration cert-manager-webhook >/dev/null 2>&1; then
        log_success "Cert-manager webhook is configured"
    else
        log_error "Cert-manager webhook not found"
        validation_passed=false
    fi
    
    log_step "3" "Testing cert-manager functionality"
    # Create a test certificate
    cat <<EOF | kubectl apply -f - >/dev/null 2>&1
apiVersion: cert-manager.io/v1
kind: Certificate
metadata:
  name: test-certificate
  namespace: default
spec:
  secretName: test-certificate-secret
  issuerRef:
    name: selfsigned-issuer
    kind: ClusterIssuer
  dnsNames:
  - test.example.com
EOF
    
    # Wait a moment and check if certificate was processed
    sleep 10
    if kubectl get certificate test-certificate -o jsonpath='{.status.conditions[0].type}' 2>/dev/null | grep -q "Ready"; then
        log_success "Cert-manager is functioning correctly"
        kubectl delete certificate test-certificate >/dev/null 2>&1
        kubectl delete secret test-certificate-secret >/dev/null 2>&1
    else
        log_warning "Cert-manager test certificate creation needs verification"
    fi
    
    return $([[ "$validation_passed" == "true" ]] && echo 0 || echo 1)
}

# Ingress controller validation
validate_ingress_controller() {
    local timeout="$1"
    local validation_passed=true
    
    log_step "1" "Checking ingress controller pods"
    if ! wait_for_pods_ready "ingress-nginx" "$timeout"; then
        log_error "Ingress controller pods not ready"
        validation_passed=false
    else
        log_success "Ingress controller pods are ready"
    fi
    
    log_step "2" "Checking ingress controller service"
    if kubectl get svc -n ingress-nginx ingress-nginx-controller >/dev/null 2>&1; then
        local external_ip=$(kubectl get svc -n ingress-nginx ingress-nginx-controller -o jsonpath='{.status.loadBalancer.ingress[0].ip}')
        if [[ -n "$external_ip" && "$external_ip" != "null" ]]; then
            log_success "Ingress controller has external IP: $external_ip"
        else
            log_warning "Ingress controller service exists but no external IP yet"
        fi
    else
        log_error "Ingress controller service not found"
        validation_passed=false
    fi
    
    return $([[ "$validation_passed" == "true" ]] && echo 0 || echo 1)
}

# Monitoring stack validation
validate_monitoring_stack() {
    local timeout="$1"
    local validation_passed=true
    
    log_step "1" "Checking monitoring namespace pods"
    if ! wait_for_pods_ready "monitoring" "$timeout"; then
        log_error "Monitoring pods not ready"
        validation_passed=false
    else
        log_success "Monitoring pods are ready"
    fi
    
    log_step "2" "Checking Prometheus"
    if kubectl get statefulset -n monitoring prometheus-prometheus >/dev/null 2>&1; then
        log_success "Prometheus is deployed"
    else
        log_error "Prometheus not found"
        validation_passed=false
    fi
    
    log_step "3" "Checking Grafana"
    if kubectl get deployment -n monitoring grafana >/dev/null 2>&1; then
        log_success "Grafana is deployed"
    else
        log_error "Grafana not found"
        validation_passed=false
    fi
    
    return $([[ "$validation_passed" == "true" ]] && echo 0 || echo 1)
}

# Vault installation validation
validate_vault_installation() {
    local timeout="$1"
    local validation_passed=true
    
    log_step "1" "Checking Vault pods"
    if ! wait_for_pods_ready "vault" "$timeout"; then
        log_error "Vault pods not ready"
        validation_passed=false
    else
        log_success "Vault pods are ready"
    fi
    
    log_step "2" "Checking Vault status"
    local vault_status=$(kubectl exec -n vault vault-0 -- vault status -format=json 2>/dev/null | jq -r '.sealed // "unknown"')
    case "$vault_status" in
        "false")
            log_success "Vault is unsealed and ready"
            ;;
        "true")
            log_warning "Vault is sealed - requires manual unsealing"
            ;;
        *)
            log_warning "Could not determine Vault status"
            ;;
    esac
    
    return $([[ "$validation_passed" == "true" ]] && echo 0 || echo 1)
}

# GOK Controller validation
validate_gok_controller_installation() {
    local timeout="$1"
    local validation_passed=true
    
    log_step "1" "Checking GOK Controller pods"
    if ! wait_for_pods_ready "gok-controller" "$timeout"; then
        log_error "GOK Controller pods not ready"
        validation_passed=false
    else
        log_success "GOK Controller pods are ready"
    fi
    
    log_step "2" "Checking GOK Agent pods"
    if ! wait_for_pods_ready "gok-agent" "$timeout"; then
        log_error "GOK Agent pods not ready"
        validation_passed=false
    else
        log_success "GOK Agent pods are ready"
    fi
    
    return $([[ "$validation_passed" == "true" ]] && echo 0 || echo 1)
}

# Utility function to wait for pods to be ready
wait_for_pods_ready() {
    local namespace="$1"
    local timeout="$2"
    local start_time=$(date +%s)
    
    while true; do
        local current_time=$(date +%s)
        local elapsed=$((current_time - start_time))
        
        if [[ $elapsed -gt $timeout ]]; then
            return 1
        fi
        
        # Check if all pods in namespace are ready
        local not_ready_pods=$(kubectl get pods -n "$namespace" --no-headers 2>/dev/null | grep -v "Running\|Completed" | wc -l)
        
        if [[ $not_ready_pods -eq 0 ]]; then
            return 0
        fi
        
        sleep 5
    done
}

# Keycloak validation
validate_keycloak_installation() {
    local timeout="$1"
    local validation_passed=true
    
    log_step "1" "Checking Keycloak pods"
    if ! wait_for_pods_ready "keycloak" "$timeout"; then
        log_error "Keycloak pods not ready"
        validation_passed=false
    else
        log_success "Keycloak pods are ready"
    fi
    
    return $([[ "$validation_passed" == "true" ]] && echo 0 || echo 1)
}

# ArgoCD validation
validate_argocd_installation() {
    local timeout="$1"
    local validation_passed=true
    
    log_step "1" "Checking ArgoCD pods"
    if ! wait_for_pods_ready "argocd" "$timeout"; then
        log_error "ArgoCD pods not ready"
        validation_passed=false
    else
        log_success "ArgoCD pods are ready"
    fi
    
    return $([[ "$validation_passed" == "true" ]] && echo 0 || echo 1)
}

# Jupyter validation
validate_jupyter_installation() {
    local timeout="$1"
    local validation_passed=true
    
    log_step "1" "Checking JupyterHub pods"
    if ! wait_for_pods_ready "jupyterhub" "$timeout"; then
        log_error "JupyterHub pods not ready"
        validation_passed=false
    else
        log_success "JupyterHub pods are ready"
    fi
    
    return $([[ "$validation_passed" == "true" ]] && echo 0 || echo 1)
}

# Registry validation
validate_registry_installation() {
    local timeout="$1"
    local validation_passed=true
    
    log_step "1" "Checking Registry pods"
    if ! wait_for_pods_ready "registry" "$timeout"; then
        log_error "Registry pods not ready"
        validation_passed=false
    else
        log_success "Registry pods are ready"
    fi
    
    return $([[ "$validation_passed" == "true" ]] && echo 0 || echo 1)
}

# =============================================================================
# 🎯 INTERACTIVE INSTALLATION MODE
# =============================================================================

# Interactive installation wizard
interactive_installation() {
    log_header "GOK Interactive Installation Wizard" "Guided Platform Setup"
    
    echo -e "${COLOR_BRIGHT_CYAN}Welcome to the GOK Platform Interactive Installation!${COLOR_RESET}"
    echo -e "${COLOR_CYAN}This wizard will guide you through setting up your Kubernetes platform.${COLOR_RESET}"
    echo
    
    # Check prerequisites
    if ! check_prerequisites; then
        log_error "Prerequisites check failed. Please resolve the issues and try again."
        return 1
    fi
    
    # Show installation options
    show_installation_categories
    
    # Get user selection
    local selection
    while true; do
        echo
        echo -e "${COLOR_BRIGHT_YELLOW}Select installation type:${COLOR_RESET}"
        echo -e "${COLOR_CYAN}1) ${COLOR_BOLD}Quick Start${COLOR_RESET} - Essential components for a basic cluster"
        echo -e "${COLOR_CYAN}2) ${COLOR_BOLD}Full Platform${COLOR_RESET} - Complete GOK platform with all features"
        echo -e "${COLOR_CYAN}3) ${COLOR_BOLD}Custom Selection${COLOR_RESET} - Choose specific components"
        echo -e "${COLOR_CYAN}4) ${COLOR_BOLD}Development Setup${COLOR_RESET} - Optimized for development workflows"
        echo -e "${COLOR_CYAN}5) ${COLOR_BOLD}Production Setup${COLOR_RESET} - Production-ready configuration"
        echo -e "${COLOR_CYAN}6) ${COLOR_BOLD}Exit${COLOR_RESET} - Exit the installer"
        echo
        
        read -p "$(echo -e "${COLOR_BRIGHT_WHITE}Enter your choice (1-6): ${COLOR_RESET}")" selection
        
        case "$selection" in
            1) install_quick_start; break ;;
            2) install_full_platform; break ;;
            3) install_custom_selection; break ;;
            4) install_development_setup; break ;;
            5) install_production_setup; break ;;
            6) log_info "Installation cancelled by user"; return 0 ;;
            *) log_warning "Invalid selection. Please choose 1-6." ;;
        esac
    done
}

# Check system prerequisites
check_prerequisites() {
    log_section "Prerequisites Check" "${EMOJI_GEAR}"
    local prereq_passed=true
    
    # Check if running as root or with sudo
    if [[ $EUID -ne 0 ]]; then
        log_error "This script must be run as root or with sudo privileges"
        prereq_passed=false
    else
        log_success "Running with appropriate privileges"
    fi
    
    # Check system resources
    local mem_gb=$(free -g | awk '/^Mem:/{print $2}')
    local cpu_cores=$(nproc)
    
    if [[ $mem_gb -lt 4 ]]; then
        log_warning "System has ${mem_gb}GB RAM. Minimum 4GB recommended for basic installation."
    else
        log_success "Memory: ${mem_gb}GB (sufficient)"
    fi
    
    if [[ $cpu_cores -lt 2 ]]; then
        log_warning "System has ${cpu_cores} CPU cores. Minimum 2 cores recommended."
    else
        log_success "CPU: ${cpu_cores} cores (sufficient)"
    fi
    
    # Check disk space
    local disk_gb=$(df / | awk 'NR==2{printf "%.0f", $4/1024/1024}')
    if [[ $disk_gb -lt 20 ]]; then
        log_warning "Available disk space: ${disk_gb}GB. Minimum 20GB recommended."
    else
        log_success "Disk space: ${disk_gb}GB available (sufficient)"
    fi
    
    # Check network connectivity
    if ping -c 1 google.com >/dev/null 2>&1; then
        log_success "Internet connectivity: Available"
    else
        log_error "Internet connectivity: Not available (required for installation)"
        prereq_passed=false
    fi
    
    # Check if Docker is installed
    if command -v docker >/dev/null 2>&1; then
        log_success "Docker: Installed"
    else
        log_info "Docker: Not installed (will be installed automatically)"
    fi
    
    # Check if kubectl is installed
    if command -v kubectl >/dev/null 2>&1; then
        log_success "kubectl: Installed"
    else
        log_info "kubectl: Not installed (will be installed with Kubernetes)"
    fi
    
    return $([[ "$prereq_passed" == "true" ]] && echo 0 || echo 1)
}

# Show available installation categories
show_installation_categories() {
    echo
    log_section "Available Components" "${EMOJI_PACKAGE}"
    
    echo -e "${COLOR_BRIGHT_BLUE}${COLOR_BOLD}📦 Core Infrastructure (Essential):${COLOR_RESET}"
    echo -e "  ${COLOR_CYAN}• docker            - Container runtime${COLOR_RESET}"
    echo -e "  ${COLOR_CYAN}• kubernetes        - Container orchestration platform${COLOR_RESET}"
    echo -e "  ${COLOR_CYAN}• cert-manager      - Automated TLS certificate management${COLOR_RESET}"
    echo -e "  ${COLOR_CYAN}• ingress           - NGINX ingress controller${COLOR_RESET}"
    echo
    
    echo -e "${COLOR_BRIGHT_GREEN}${COLOR_BOLD}📊 Monitoring & Observability:${COLOR_RESET}"
    echo -e "  ${COLOR_CYAN}• monitoring        - Prometheus + Grafana stack${COLOR_RESET}"
    echo -e "  ${COLOR_CYAN}• fluentd           - Log collection and forwarding${COLOR_RESET}"
    echo -e "  ${COLOR_CYAN}• opensearch        - Search and analytics engine${COLOR_RESET}"
    echo
    
    echo -e "${COLOR_BRIGHT_YELLOW}${COLOR_BOLD}🔐 Security & Identity:${COLOR_RESET}"
    echo -e "  ${COLOR_CYAN}• vault             - HashiCorp Vault secrets management${COLOR_RESET}"
    echo -e "  ${COLOR_CYAN}• keycloak          - Identity and access management${COLOR_RESET}"
    echo -e "  ${COLOR_CYAN}• oauth2            - OAuth2 proxy for authentication${COLOR_RESET}"
    echo -e "  ${COLOR_CYAN}• ldap              - LDAP directory service${COLOR_RESET}"
    echo
    
    echo -e "${COLOR_BRIGHT_MAGENTA}${COLOR_BOLD}🚀 Development & DevOps:${COLOR_RESET}"
    echo -e "  ${COLOR_CYAN}• argocd            - GitOps continuous delivery${COLOR_RESET}"
    echo -e "  ${COLOR_CYAN}• jenkins           - CI/CD automation server${COLOR_RESET}"
    echo -e "  ${COLOR_CYAN}• jupyter           - JupyterHub for data science${COLOR_RESET}"
    echo -e "  ${COLOR_CYAN}• registry          - Container image registry${COLOR_RESET}"
    echo -e "  ${COLOR_CYAN}• spinnaker         - Multi-cloud deployment platform${COLOR_RESET}"
    echo
    
    echo -e "${COLOR_BRIGHT_RED}${COLOR_BOLD}🌐 Service Mesh & Messaging:${COLOR_RESET}"
    echo -e "  ${COLOR_CYAN}• istio             - Service mesh for microservices${COLOR_RESET}"
    echo -e "  ${COLOR_CYAN}• rabbitmq          - Message broker${COLOR_RESET}"
    echo
    
    echo -e "${COLOR_BRIGHT_CYAN}${COLOR_BOLD}🏗️ GOK Platform:${COLOR_RESET}"
    echo -e "  ${COLOR_CYAN}• gok-controller    - GOK distributed system controller${COLOR_RESET}"
    echo -e "  ${COLOR_CYAN}• gok-agent         - GOK distributed system agent${COLOR_RESET}"
    echo -e "  ${COLOR_CYAN}• controller        - Install both gok-controller and gok-agent${COLOR_RESET}"
}

# Quick start installation
install_quick_start() {
    log_header "Quick Start Installation" "Essential components for a basic cluster"
    
    local components=("docker" "kubernetes" "cert-manager" "ingress" "monitoring")
    
    echo -e "${COLOR_BRIGHT_CYAN}Quick Start includes:${COLOR_RESET}"
    for component in "${components[@]}"; do
        echo -e "  ${COLOR_GREEN}• $component${COLOR_RESET}"
    done
    echo
    
    if confirm_installation "Quick Start"; then
        install_component_list "${components[@]}"
        show_platform_overview
    fi
}

# Full platform installation
install_full_platform() {
    log_header "Full Platform Installation" "Complete GOK platform with all features"
    
    local components=(
        "docker" "kubernetes" "cert-manager" "ingress" "monitoring"
        "vault" "keycloak" "oauth2" "argocd" "jenkins" "jupyter"
        "registry" "gok-controller" "rabbitmq"
    )
    
    echo -e "${COLOR_BRIGHT_CYAN}Full Platform includes:${COLOR_RESET}"
    for component in "${components[@]}"; do
        echo -e "  ${COLOR_GREEN}• $component${COLOR_RESET}"
    done
    echo
    
    local estimated_time="45-60 minutes"
    echo -e "${COLOR_YELLOW}Estimated installation time: ${COLOR_BOLD}$estimated_time${COLOR_RESET}"
    echo
    
    if confirm_installation "Full Platform"; then
        install_component_list "${components[@]}"
        show_platform_overview
    fi
}

# Development setup installation
install_development_setup() {
    log_header "Development Setup" "Optimized for development workflows"
    
    local components=(
        "docker" "kubernetes" "cert-manager" "ingress"
        "monitoring" "argocd" "jupyter" "registry" "gok-controller"
    )
    
    echo -e "${COLOR_BRIGHT_CYAN}Development Setup includes:${COLOR_RESET}"
    for component in "${components[@]}"; do
        echo -e "  ${COLOR_GREEN}• $component${COLOR_RESET}"
    done
    echo
    
    if confirm_installation "Development Setup"; then
        install_component_list "${components[@]}"
        show_development_next_steps
    fi
}

# Production setup installation
install_production_setup() {
    log_header "Production Setup" "Production-ready configuration with security"
    
    local components=(
        "docker" "kubernetes" "cert-manager" "ingress" "monitoring"
        "vault" "keycloak" "oauth2" "argocd" "fluentd" "opensearch"
        "kyverno" "gok-controller" "istio"
    )
    
    echo -e "${COLOR_BRIGHT_CYAN}Production Setup includes:${COLOR_RESET}"
    for component in "${components[@]}"; do
        echo -e "  ${COLOR_GREEN}• $component${COLOR_RESET}"
    done
    echo
    
    local estimated_time="60-90 minutes"
    echo -e "${COLOR_YELLOW}Estimated installation time: ${COLOR_BOLD}$estimated_time${COLOR_RESET}"
    echo
    
    if confirm_installation "Production Setup"; then
        install_component_list "${components[@]}"
        show_production_next_steps
    fi
}

# Custom component selection
install_custom_selection() {
    log_header "Custom Component Selection" "Choose specific components to install"
    
    local all_components=(
        "docker:Container runtime"
        "kubernetes:Container orchestration"
        "cert-manager:Certificate management"
        "ingress:NGINX ingress controller"
        "monitoring:Prometheus + Grafana"
        "vault:HashiCorp Vault"
        "keycloak:Identity management"
        "oauth2:OAuth2 proxy"
        "ldap:LDAP directory"
        "argocd:GitOps deployment"
        "jenkins:CI/CD server"
        "jupyter:JupyterHub"
        "registry:Container registry"
        "spinnaker:Multi-cloud deployment"
        "istio:Service mesh"
        "rabbitmq:Message broker"
        "fluentd:Log collection"
        "opensearch:Search engine"
        "kyverno:Policy engine"
        "gok-controller:GOK platform controller"
        "gok-agent:GOK platform agent"
    )
    
    local selected_components=()
    
    echo -e "${COLOR_BRIGHT_CYAN}Select components to install (y/N for each):${COLOR_RESET}"
    echo
    
    for component_desc in "${all_components[@]}"; do
        local component="${component_desc%%:*}"
        local description="${component_desc##*:}"
        
        echo -e "${COLOR_CYAN}Install ${COLOR_BOLD}$component${COLOR_RESET} ${COLOR_DIM}($description)${COLOR_RESET}?"
        read -p "$(echo -e "${COLOR_WHITE}[y/N]: ${COLOR_RESET}")" choice
        
        case "$choice" in
            [Yy]|[Yy][Ee][Ss])
                selected_components+=("$component")
                echo -e "  ${COLOR_GREEN}✓ Added $component${COLOR_RESET}"
                ;;
            *)
                echo -e "  ${COLOR_DIM}- Skipped $component${COLOR_RESET}"
                ;;
        esac
        echo
    done
    
    if [[ ${#selected_components[@]} -eq 0 ]]; then
        log_warning "No components selected. Exiting."
        return 0
    fi
    
    echo -e "${COLOR_BRIGHT_CYAN}Selected components:${COLOR_RESET}"
    for component in "${selected_components[@]}"; do
        echo -e "  ${COLOR_GREEN}• $component${COLOR_RESET}"
    done
    echo
    
    if confirm_installation "Custom Selection"; then
        # Check dependencies
        check_component_dependencies "${selected_components[@]}"
        install_component_list "${selected_components[@]}"
    fi
}

# Check component dependencies and suggest additions
check_component_dependencies() {
    local components=("$@")
    local missing_deps=()
    
    # Basic dependency checking
    local has_kubernetes=false
    local has_cert_manager=false
    local has_ingress=false
    
    for component in "${components[@]}"; do
        case "$component" in
            "kubernetes") has_kubernetes=true ;;
            "cert-manager") has_cert_manager=true ;;
            "ingress") has_ingress=true ;;
        esac
    done
    
    # Check if Kubernetes is selected for components that need it
    for component in "${components[@]}"; do
        case "$component" in
            "cert-manager"|"ingress"|"monitoring"|"vault"|"keycloak"|"argocd"|"jenkins"|"jupyter"|"registry"|"gok-controller"|"gok-agent")
                if [[ "$has_kubernetes" != "true" ]]; then
                    missing_deps+=("kubernetes")
                    break
                fi
                ;;
        esac
    done
    
    # Check if cert-manager is needed
    for component in "${components[@]}"; do
        case "$component" in
            "ingress"|"monitoring"|"vault"|"keycloak"|"argocd")
                if [[ "$has_cert_manager" != "true" ]]; then
                    missing_deps+=("cert-manager")
                    break
                fi
                ;;
        esac
    done
    
    if [[ ${#missing_deps[@]} -gt 0 ]]; then
        echo
        log_warning "Missing required dependencies detected!"
        echo -e "${COLOR_YELLOW}The following components are recommended:${COLOR_RESET}"
        for dep in "${missing_deps[@]}"; do
            echo -e "  ${COLOR_YELLOW}• $dep${COLOR_RESET}"
        done
        echo
        
        read -p "$(echo -e "${COLOR_BRIGHT_WHITE}Add missing dependencies? [Y/n]: ${COLOR_RESET}")" add_deps
        case "$add_deps" in
            [Nn]|[Nn][Oo])
                log_info "Proceeding without dependencies (may cause installation issues)"
                ;;
            *)
                components+=("${missing_deps[@]}")
                log_success "Added dependencies to installation list"
                ;;
        esac
    fi
}

# Confirm installation with user
confirm_installation() {
    local installation_type="$1"
    
    echo -e "${COLOR_BRIGHT_WHITE}Proceed with ${COLOR_BOLD}$installation_type${COLOR_RESET} ${COLOR_BRIGHT_WHITE}installation?${COLOR_RESET}"
    read -p "$(echo -e "${COLOR_BRIGHT_WHITE}[Y/n]: ${COLOR_RESET}")" confirm
    
    case "$confirm" in
        [Nn]|[Nn][Oo])
            log_info "Installation cancelled by user"
            return 1
            ;;
        *)
            return 0
            ;;
    esac
}

# Install list of components with progress tracking
install_component_list() {
    local components=("$@")
    local total_components=${#components[@]}
    local current_component=0
    
    log_header "Installation Progress" "Installing $total_components components"
    
    for component in "${components[@]}"; do
        ((current_component++))
        
        log_progress "$current_component" "$total_components" "Installing $component"
        
        start_component "$component" "Installing $component ($current_component/$total_components)"
        
        # Call the actual installation function
        if installCmd "$component" >/dev/null 2>&1; then
            # Validate installation
            if validate_component_installation "$component" 180; then
                complete_component "$component" "Successfully installed and validated"
                show_component_next_steps "$component"
            else
                complete_component "$component" "Installed but validation warnings detected"
            fi
        else
            fail_component "$component" "Installation failed"
        fi
        
        echo
    done
    
    show_installation_summary
}

# Show development-specific next steps
show_development_next_steps() {
    log_next_steps "Development Environment Ready" \
        "Generate your first microservice: gok generate python-api my-service" \
        "Access JupyterHub for data science: https://jupyter.$(rootDomain)" \
        "Set up Git repositories in ArgoCD" \
        "Use container registry for your images: $(fullRegistryUrl)" \
        "Monitor your applications via Grafana dashboards"
    
    log_info "Your development environment is ready for microservice development!"
}

# Show production-specific next steps
show_production_next_steps() {
    log_next_steps "Production Environment Ready" \
        "Configure backup and disaster recovery procedures" \
        "Set up monitoring alerts and notification channels" \
        "Configure authentication and RBAC policies" \
        "Review security policies and compliance settings" \
        "Set up automated certificate renewal monitoring" \
        "Configure log retention and archival policies"
    
    log_warning "Remember to secure your Vault unseal keys and root tokens!"
    log_info "Your production environment is ready for enterprise workloads!"
}

replaceEnvVariable(){
  wget -O- $1 | envsubst
}

promptUserInput(){
 MSG=$1
 DEFAULT=$2
id=$(python3 -c "
import sys
sys.stderr.write('${MSG}')
id=input()
print(id)
")
output=${id:-$DEFAULT}
echo $output
}

promptSecret(){
  MSG=$1
  secret=$(python3 -c "
import getpass
secret = getpass.getpass('${MSG}')
print(secret)
")
  echo $secret
}

dataFromSecret(){
  NAME=$1
  NS=$2
  KEY=$3
  kubectl get secret $NAME -n $NS -o jsonpath="{['data']['$KEY']}" | base64 --decode
}

createApp1() {
  cat <<EOF | kubectl apply -f -
apiVersion: apps/v1
kind: Deployment
metadata:
  name: app1
  namespace: default
spec:
  replicas: 1
  selector:
    matchLabels:
      app: app1
  template:
    metadata:
      labels:
        app: app1
    spec:
      containers:
      - name: app1
        image: dockersamples/static-site
        env:
        - name: AUTHOR
          value: app1
        ports:
        - containerPort: 80
---
apiVersion: v1
kind: Service
metadata:
  name: appsvc1
  namespace: default
spec:
  ports:
  - port: 80
    protocol: TCP
    targetPort: 80
  selector:
    app: app1
---
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  annotations:
    nginx.ingress.kubernetes.io/rewrite-target: /
    kubernetes.io/ingress.class: "nginx"
  name: app-ingress
  namespace: default
spec:
  rules:
  - host: $(fullDefaultUrl)
    http:
      paths:
      - backend:
          service:
            name: appsvc1
            port:
              number: 80
        path: /app1
        pathType: Prefix
EOF
}

#This deploys a pod that has curl installed
kcurl(){
  cat <<EOF | kubectl apply -f -
apiVersion: v1
kind: Pod
metadata:
  name: curl
  namespace: default
  labels:
    app: curl
spec:
  containers:
  - name: main
    image: curlimages/curl
    command: ["sleep", "9999999"]
EOF
  echo "Commands"
  echo "checkCurl https://kubernetes"
}

checkCurl(){
  kubectl exec -i -t curl -n default -- curl -kv "$@"
}

checkCMWebhook(){
  kubectl exec -i -t curl -n default -- curl -kv \
      --cacert <(kubectl -n cert-manager get secret cert-manager-webhook-ca -ojsonpath='{.data.ca\.crt}' | base64 -d) \
      https://cert-manager-webhook.cert-manager.svc:443/validate 2>&1 -d@- <<'EOF' | sed '/^* /d; /bytes data]$/d; s/> //; s/< //'
{"kind":"AdmissionReview","apiVersion":"admission.k8s.io/v1","request":{"requestKind":{"group":"cert-manager.io","version":"v1","kind":"Certificate"},"requestResource":{"group":"cert-manager.io","version":"v1","resource":"certificates"},"name":"foo","namespace":"default","operation":"CREATE","object":{"apiVersion":"cert-manager.io/v1","kind":"Certificate","spec":{"dnsNames":["foo"],"issuerRef":{"group":"cert-manager.io","kind":"Issuer","name":"letsencrypt"},"secretName":"foo","usages":["digital signature"]}}}}
EOF
}

#This gives token to join a new node to kubernetes cluster
join(){
  kubeadm token create --print-join-command
}

dnsUtils(){
  cat <<EOF | kubectl apply -f -
apiVersion: v1
kind: Pod
metadata:
  name: dnsutils
  namespace: default
spec:
  containers:
  - name: dnsutils
    image: k8s.gcr.io/e2e-test-images/jessie-dnsutils:1.3
    command:
      - sleep
      - "3600"
    imagePullPolicy: IfNotPresent
  restartPolicy: Always
EOF
  echo "Commands"
  echo "checkDns kubernetes.default.svc.cloud.uat"

}

istioInst(){
  helm repo add istio https://istio-release.storage.googleapis.com/charts
  helm repo update
  helm install istio-base istio/base -n istio-system \
    --create-namespace \
    --set defaultRevision=default

  helm install istiod istio/istiod -n istio-system --wait
  helm ls -n istio-system

}

enableIstio(){
  NAMESPACE=$1
  kubectl label namespace $NAMESPACE istio-injection=enabled
}

istioReset(){
  helm -n istio-system delete istiod
  helm -n istio-system delete istio-base
  kubectl delete ns istio-system
}

checkDns(){
  kubectl exec -i -t dnsutils -n default -- nslookup "$@"
}

getpod() {
  pod=$(kubectl get po -l app.kubernetes.io/name="$release" 2>/dev/null | awk "/${release}/" | awk '{print $1}' | head -n 1)
  echo "$pod"
}

# =============================================================================
# 📦 SYSTEM UPDATE & DEPENDENCY MANAGEMENT
# Enhanced with progress bars and optional verbose logging
# =============================================================================

# Global flag for verbose output (can be set via environment or command line)
: ${GOK_VERBOSE:=false}
: ${GOK_SHOW_PROGRESS:=true}

# Function to check if verbose mode is enabled
is_verbose_mode() {
    [[ "$GOK_VERBOSE" == "true" ]] || [[ "$1" == "--verbose" ]] || [[ "$1" == "-v" ]]
}

# Enhanced system update with progress bar
updateSys() {
    local verbose_flag="${1:-}"
    local start_time=$(date +%s)
    
    log_step "System Update" "Updating package repositories"
    
    if is_verbose_mode "$verbose_flag"; then
        log_info "Running in verbose mode - showing detailed output"
        if ! apt-get update; then
            log_error "System update failed"
            return 1
        fi
    else
        # Run with progress bar
        log_substep "Downloading package information"
        
        # Create a temporary file for capturing output
        local temp_log=$(mktemp)
        local pid
        
        # Start the update process in background
        {
            apt-get update > "$temp_log" 2>&1
            echo $? > "${temp_log}.exit"
        } &
        pid=$!
        
        # Show progress while update is running
        local progress=0
        local spinner_chars="|/-\\"
        local spinner_idx=0
        
        while kill -0 $pid 2>/dev/null; do
            local char=${spinner_chars:spinner_idx:1}
            printf "\r${COLOR_BLUE}  Updating repositories [%c] %d%%${COLOR_RESET}" "$char" "$progress"
            
            spinner_idx=$(( (spinner_idx + 1) % 4 ))
            progress=$(( (progress + 2) % 101 ))
            sleep 0.1
        done
        
        # Wait for process to complete and get exit code
        wait $pid
        local exit_code
        if [[ -f "${temp_log}.exit" ]]; then
            exit_code=$(cat "${temp_log}.exit")
        else
            exit_code=1
        fi
        
        printf "\r${COLOR_GREEN}  Repository update completed [✓] 100%%${COLOR_RESET}\n"
        
        # Check for errors
        if [[ $exit_code -ne 0 ]]; then
            log_error "System update failed - showing detailed output:"
            cat "$temp_log"
            rm -f "$temp_log" "${temp_log}.exit"
            return 1
        fi
        
        # Check for important warnings in output
        if grep -qi "error\|fail\|warning" "$temp_log"; then
            log_warning "Update completed with warnings - use --verbose flag for details"
            log_info "Run: GOK_VERBOSE=true gok install <component> for detailed output"
        fi
        
        # Clean up
        rm -f "$temp_log" "${temp_log}.exit"
    fi
    
    local end_time=$(date +%s)
    local duration=$((end_time - start_time))
    log_success "System update completed in ${duration}s"
    
    return 0
}

setupDockerRegistry(){
  if [[ -n $TRACE ]]; then
    set -x
  fi

  EXPORTDIR=$MOUNT_PATH

  if [ "$(hostname)" == 'master.cloud.com' ]; then
      mkdir -p "$EXPORTDIR"/certs
      mkdir -p /mnt/registry
      rm -rf /mnt/registry/config.yml
      wget https://github.com/sumitmaji/kubernetes/raw/master/install_k8s/registry/config.yml -P /mnt/registry/
      pushd "$EXPORTDIR" || exit
      USERNAME=master.cloud.com
      FILENAME=registry
      CERTIFICATE_KEY_NAME=$USERNAME
      rm /etc/docker/certs.d/master.cloud.com:5000/${CERTIFICATE_KEY_NAME}.crt
      rm /root/certs/${CERTIFICATE_KEY_NAME}.crt
      rm /root/certs/${CERTIFICATE_KEY_NAME}.key
      if [ -f /etc/docker/certs.d/master.cloud.com:5000/${CERTIFICATE_KEY_NAME}.crt ]
      then
        echo "The file is present, not creating it!!!!!"
      else
        createCertificate -i "${MASTER_HOST_IP}" -h master.cloud.com -t server -f registry
      fi

      docker stop registry
      docker rm registry

      docker run -d \
        --restart=always \
        --name registry \
        -v $EXPORTDIR/certs:/root/certs \
        -e REGISTRY_HTTP_ADDR=0.0.0.0:5000 \
        -e REGISTRY_HTTP_TLS_CERTIFICATE=/root/certs/${CERTIFICATE_KEY_NAME}.crt \
        -e REGISTRY_HTTP_TLS_KEY=/root/certs/${CERTIFICATE_KEY_NAME}.key \
        -v /mnt/registry:/var/lib/registry \
        -v /mnt/registry/config.yml:/etc/docker/registry/config.yml \
        -p 5000:5000 \
        registry:latest

      mkdir -p /etc/docker/certs.d/master.cloud.com:5000

      cp "$EXPORTDIR"/certs/${CERTIFICATE_KEY_NAME}.crt /etc/docker/certs.d/master.cloud.com:5000/${CERTIFICATE_KEY_NAME}.crt

      popd || exit
  else
      mkdir -p /etc/docker/certs.d/master.cloud.com:5000
      cp "$EXPORTDIR"/certs/${USERNAME}.crt /etc/docker/certs.d/master.cloud.com:5000/${USERNAME}.crt
  fi
}

# Enhanced dependency installation with progress tracking
installDeps() {
    local verbose_flag="${1:-}"
    local start_time=$(date +%s)
    
    log_step "Dependencies" "Installing essential system dependencies"
    
    # Define packages to install
    local packages=(
        "net-tools:Network utilities and tools"
        "jq:JSON processor for command line"
        "python3:Python 3 interpreter"
        "python3-pip:Python package installer"
        "curl:Command line tool for transferring data"
        "wget:Network downloader"
        "gnupg:GNU Privacy Guard"
        "software-properties-common:Manage software repositories"
        "apt-transport-https:HTTPS transport for APT"
        "ca-certificates:Common CA certificates"
    )
    
    local total_packages=${#packages[@]}
    local current_package=0
    local failed_packages=()
    
    for package_info in "${packages[@]}"; do
        IFS=':' read -r package_name package_desc <<< "$package_info"
        current_package=$((current_package + 1))
        
        log_substep "Installing $package_name ($current_package/$total_packages)"
        
        if is_verbose_mode "$verbose_flag"; then
            log_info "$package_desc"
            if ! apt-get install -y "$package_name"; then
                log_error "Failed to install $package_name"
                failed_packages+=("$package_name")
            fi
        else
            # Install with progress indication
            local temp_log=$(mktemp)
            local pid
            
            # Start installation in background
            {
                apt-get install -y "$package_name" > "$temp_log" 2>&1
                echo $? > "${temp_log}.exit"
            } &
            pid=$!
            
            # Show progress
            local dots=""
            while kill -0 $pid 2>/dev/null; do
                printf "\r${COLOR_CYAN}    Installing $package_name${dots}${COLOR_RESET}"
                dots="${dots}."
                if [[ ${#dots} -gt 3 ]]; then dots=""; fi
                sleep 0.3
            done
            
            wait $pid
            local exit_code
            if [[ -f "${temp_log}.exit" ]]; then
                exit_code=$(cat "${temp_log}.exit")
            else
                exit_code=1
            fi
            
            if [[ $exit_code -eq 0 ]]; then
                printf "\r${COLOR_GREEN}    ✓ $package_name installed successfully${COLOR_RESET}\n"
            else
                printf "\r${COLOR_RED}    ✗ $package_name installation failed${COLOR_RESET}\n"
                failed_packages+=("$package_name")
                
                # Show error details for failed packages
                log_error "Failed to install $package_name - error details:"
                tail -10 "$temp_log" | while read line; do
                    log_error "  $line"
                done
            fi
            
            rm -f "$temp_log" "${temp_log}.exit"
        fi
        
        # Update progress
        log_progress "$current_package" "$total_packages" "Installing dependencies"
    done
    
    echo # New line after progress
    
    # Summary
    local successful_packages=$((total_packages - ${#failed_packages[@]}))
    local end_time=$(date +%s)
    local duration=$((end_time - start_time))
    
    if [[ ${#failed_packages[@]} -eq 0 ]]; then
        log_success "All $total_packages dependencies installed successfully in ${duration}s"
    else
        log_warning "$successful_packages/$total_packages dependencies installed successfully"
        log_error "Failed packages: ${failed_packages[*]}"
        
        # Provide troubleshooting suggestions
        log_troubleshooting "Dependency Installation" \
            "Run with verbose mode: GOK_VERBOSE=true gok install <component>" \
            "Update package cache: apt-get update" \
            "Check available disk space: df -h" \
            "Check network connectivity: ping archive.ubuntu.com" \
            "Try installing manually: apt-get install ${failed_packages[*]}"
        
        if [[ ${#failed_packages[@]} -gt 3 ]]; then
            log_error "Too many failed packages - aborting installation"
            return 1
        else
            log_warning "Continuing with partial dependency installation"
        fi
    fi
    
    # Verify critical dependencies
    verify_critical_dependencies
    
    return 0
}

# Verify that critical dependencies are available
verify_critical_dependencies() {
    log_substep "Verifying critical dependencies"
    
    local critical_commands=(
        "curl:HTTP client"
        "wget:Download utility" 
        "jq:JSON processor"
        "python3:Python interpreter"
    )
    
    local missing_critical=()
    
    for cmd_info in "${critical_commands[@]}"; do
        IFS=':' read -r cmd_name cmd_desc <<< "$cmd_info"
        
        if ! command -v "$cmd_name" >/dev/null 2>&1; then
            missing_critical+=("$cmd_name")
            log_error "Critical dependency missing: $cmd_name ($cmd_desc)"
        else
            log_substep "✓ $cmd_name available"
        fi
    done
    
    if [[ ${#missing_critical[@]} -gt 0 ]]; then
        log_error "Critical dependencies missing: ${missing_critical[*]}"
        log_error "Installation cannot continue without these dependencies"
        return 1
    fi
    
    log_success "All critical dependencies verified"
    return 0
}

ingressUnInst() {
  output=$(kubectl get po -n ingress-nginx -l app.kubernetes.io/component=controller -o json | jq '.items | length')
  if [ "$output" == "1" ]; then
    helm uninstall ingress-nginx -n ingress-nginx
    kubectl delete ns ingress-nginx
  fi
}

patchNginxConfig() {
  echo "Patching nginx-configuration ConfigMap to enable debug logging..."

  # Patch the ConfigMap
  kubectl patch configmap ingress-nginx-controller -n ingress-nginx --type merge --patch "$(
    cat <<EOF
data:
  enable-vts-status: "true"
  log-format-upstream: '{"time": "\$time_iso8601", "remote_addr": "\$remote_addr", "x-forwarded-for": "\$http_x_forwarded_for", "request_id": "\$request_id", "remote_user": "\$remote_user", "bytes_sent": "\$bytes_sent", "request_time": "\$request_time", "status": "\$status", "vhost": "\$host", "request_proto": "\$server_protocol", "path": "\$uri", "request_query": "\$args", "request_length": "\$request_length", "duration": "\$request_time", "method": "\$request_method", "http_referrer": "\$http_referer", "http_user_agent": "\$http_user_agent", "upstream_addr": "\$upstream_addr", "upstream_status": "\$upstream_status", "upstream_response_length": "\$upstream_response_length", "upstream_response_time": "\$upstream_response_time", "upstream_cache_status": "\$upstream_cache_status", "authorization": "\$http_authorization", "request_body": "\$request_body"}'
EOF
  )"

  # Restart the ingress-nginx-controller deployment
  echo "Restarting ingress-nginx-controller deployment..."
  kubectl rollout restart deployment ingress-nginx-controller -n ingress-nginx

  echo "Nginx configuration patched and ingress-nginx-controller restarted successfully!"
}

ingressInst() {
  helm repo add ingress-nginx https://kubernetes.github.io/ingress-nginx
  helm repo update
  helm install \
    ingress-nginx ingress-nginx/ingress-nginx --version 4.12.1 \
    --namespace ingress-nginx \
    --create-namespace \
    --set controller.service.nodePorts.http=80 \
    --set controller.service.nodePorts.https=443 \
    --set controller.service.type=NodePort \
    --set defaultBackend.enabled=true
  #    -f charts/values.yaml

  waitForServiceAvailable ingress-nginx
}

resetChart(){
  echo "Resetting ChartMuseum installation..."
  helm uninstall chartmuseum --namespace chartmuseum
  emptyLocalFsStorage "ChartMuseum" "chart-pv" "chart-storage" "/data/volumes/chart-storage"
  kubectl delete namespace chartmuseum
  echo "ChartMuseum has been reset successfully."
}

baseInst(){
  echo "Installing base image"
  pushd "$MOUNT_PATH"/kubernetes/install_k8s/base || exit
  find . -type f -name "*.sh" -exec chmod +x {} \;
  ./build.sh
  popd || exit
  echoSuccess "Base image installed successfully!"
}


chartInst() {
  # Step 1: Create a namespace for ChartMuseum
  kubectl create namespace chartmuseum || echo "Namespace chartmuseum already exists"

  # Step 2: Install ChartMuseum using Helm
  helm repo add chartmuseum https://chartmuseum.github.io/charts
  helm repo update
  
  createLocalStorageClassAndPV "chart-storage" "chart-pv" "/data/volumes/chart-storage"

  # Install ChartMuseum with persistent storage
  helm install chartmuseum chartmuseum/chartmuseum \
    --namespace chartmuseum \
    --set persistence.enabled=true \
    --set persistence.size=10Gi \
    --set persistence.storageClass="chart-storage" \
    --set env.open.STORAGE="local" \
    --set env.open.STORAGE_LOCAL_ROOTDIR="/charts" \
    --values "$MOUNT_PATH"/kubernetes/install_k8s/chart-registry/values.yaml

  gok patch ingress chartmuseum chartmuseum letsencrypt chart

  # Step 4: Verify ChartMuseum Installation
  echo "Waiting for ChartMuseum to be ready..."
  kubectl --namespace chartmuseum wait --for=condition=Ready pods --all --timeout=120s
  if [[ $? -eq 0 ]]; then
    echo "ChartMuseum is now up and running!"
    echo "Access it at: https://chartmuseum.gokcloud.com"
  else
    echo "ChartMuseum setup timed out. Please check the logs."
    return 1
  fi

  helm plugin install https://github.com/chartmuseum/helm-push

  # Step 5: Add the ChartMuseum Repository to Helm
  helm repo add gok https://chart.gokcloud.com --username sumit --password abcdef
  helm repo update
  #helm cm-push ldap-0.1.0.tgz gok
  echo "Helm repository added successfully!"
}

dockrInst() {
  log_section "Docker Container Runtime Installation" "$EMOJI_PACKAGE"
  
  # Pre-installation validation
  log_step "1" "Validating system requirements for Docker"
  
  # Check if running as root or with sudo
  if [[ $EUID -ne 0 ]]; then
    log_error "Docker installation requires root privileges"
    return 1
  fi
  
  # Check system compatibility
  local os_info=$(lsb_release -d 2>/dev/null | cut -f2 || echo "Unknown")
  log_info "Operating System: $os_info"
  
  # Check if Docker is already installed
  if command -v docker >/dev/null 2>&1; then
    local docker_version=$(docker --version 2>/dev/null | cut -d' ' -f3 | cut -d',' -f1)
    log_warning "Docker is already installed (version: $docker_version)"
    
    # Validate existing installation
    if validate_docker_installation; then
      log_success "Existing Docker installation is working correctly"
      show_docker_next_steps
      return 0
    else
      log_warning "Existing Docker installation has issues, proceeding with reinstallation"
    fi
  fi
  
  # Step 2: Install prerequisites
  log_step "2" "Installing Docker prerequisites and dependencies"
  log_substep "Installing required packages"
  
  if ! apt-get update; then
    log_error "Failed to update package list"
    return 1
  fi
  
  if ! apt-get install -y \
    apt-transport-https \
    ca-certificates \
    curl \
    software-properties-common \
    gnupg \
    lsb-release; then
    log_error "Failed to install prerequisite packages"
    return 1
  fi
  
  log_success "Prerequisites installed successfully"
  
  # Step 3: Add Docker repository
  log_step "3" "Adding Docker official repository"
  
  log_substep "Creating keyrings directory"
  if ! sudo install -m 0755 -d /etc/apt/keyrings; then
    log_error "Failed to create keyrings directory"
    return 1
  fi
  
  log_substep "Adding Docker GPG key"
  if ! sudo curl -fsSL https://download.docker.com/linux/ubuntu/gpg -o /etc/apt/keyrings/docker.asc; then
    log_error "Failed to download Docker GPG key"
    return 1
  fi
  
  if ! sudo chmod a+r /etc/apt/keyrings/docker.asc; then
    log_error "Failed to set permissions on Docker GPG key"
    return 1
  fi
  
  log_substep "Adding Docker repository to sources"
  if ! echo \
    "deb [arch=$(dpkg --print-architecture) signed-by=/etc/apt/keyrings/docker.asc] https://download.docker.com/linux/ubuntu \
    $(. /etc/os-release && echo "$VERSION_CODENAME") stable" | \
    sudo tee /etc/apt/sources.list.d/docker.list > /dev/null; then
    log_error "Failed to add Docker repository"
    return 1
  fi
  
  log_substep "Updating package list with Docker repository"
  if ! sudo apt-get update; then
    log_error "Failed to update package list after adding Docker repository"
    return 1
  fi
  
  log_success "Docker repository added successfully"
  
  # Step 4: Install Docker Engine
  log_step "4" "Installing Docker Engine and components"
  
  local docker_packages="docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin"
  log_substep "Installing: $docker_packages"
  
  if ! apt-get install -y $docker_packages; then
    log_error "Failed to install Docker packages"
    return 1
  fi
  
  log_success "Docker Engine installed successfully"
  
  # Step 5: Configure Docker daemon
  log_step "5" "Configuring Docker daemon for Kubernetes compatibility"
  
  log_substep "Creating systemd service directory"
  if ! sudo mkdir -p /etc/systemd/system/docker.service.d; then
    log_error "Failed to create Docker systemd directory"
    return 1
  fi
  
  log_substep "Creating Docker daemon configuration"
  if ! sudo tee /etc/docker/daemon.json <<EOF
{
  "exec-opts": ["native.cgroupdriver=systemd"],
  "log-driver": "json-file",
  "log-opts": {
    "max-size": "100m",
    "max-file": "3"
  },
  "storage-driver": "overlay2",
  "storage-opts": [
    "overlay2.override_kernel_check=true"
  ],
  "live-restore": true,
  "default-address-pools": [
    {
      "base": "172.17.0.0/12",
      "size": 24
    }
  ]
}
EOF
  then
    log_error "Failed to create Docker daemon configuration"
    return 1
  fi
  
  log_success "Docker daemon configured for Kubernetes"
  
  # Step 6: Configure and start services
  log_step "6" "Starting and enabling Docker services"
  
  log_substep "Reloading systemd daemon"
  if ! sudo systemctl daemon-reload; then
    log_error "Failed to reload systemd daemon"
    return 1
  fi
  
  log_substep "Starting Docker service"
  if ! sudo systemctl start docker; then
    log_error "Failed to start Docker service"
    return 1
  fi
  
  log_substep "Enabling Docker service for auto-start"
  if ! sudo systemctl enable docker; then
    log_error "Failed to enable Docker service"
    return 1
  fi
  
  # Configure containerd
  log_substep "Configuring containerd for Kubernetes"
  
  if ! sudo systemctl enable containerd; then
    log_error "Failed to enable containerd service"
    return 1
  fi
  
  if ! sudo systemctl start containerd; then
    log_error "Failed to start containerd service"
    return 1
  fi
  
  # Remove default containerd config to use defaults
  if [[ -f /etc/containerd/config.toml ]]; then
    log_substep "Removing default containerd configuration"
    sudo rm /etc/containerd/config.toml
    sudo systemctl restart containerd
  fi
  
  log_success "Docker and containerd services started successfully"
  
  # Step 7: Post-installation validation
  log_step "7" "Validating Docker installation"
  
  if validate_docker_installation; then
    log_success "Docker installation validation passed"
  else
    log_error "Docker installation validation failed"
    return 1
  fi
  
  # Step 8: Set up user permissions (if not root)
  if [[ -n "$SUDO_USER" ]]; then
    log_step "8" "Configuring user permissions for Docker"
    log_substep "Adding user $SUDO_USER to docker group"
    
    if ! sudo usermod -aG docker "$SUDO_USER"; then
      log_warning "Failed to add user to docker group - manual setup may be required"
    else
      log_success "User $SUDO_USER added to docker group"
      log_info "User $SUDO_USER will need to log out and back in for group membership to take effect"
    fi
  fi
  
  # Show Docker information
  local docker_version=$(docker --version | cut -d' ' -f3 | cut -d',' -f1)
  local containerd_version=$(containerd --version | cut -d' ' -f3)
  
  log_success "Docker installation completed successfully!"
  log_info "Docker version: $docker_version"
  log_info "Containerd version: $containerd_version"
  
  # Show next steps
  show_docker_next_steps
  
  return 0
}

# Docker installation validation function
validate_docker_installation() {
  log_substep "Checking Docker daemon status"
  if ! sudo systemctl is-active --quiet docker; then
    log_error "Docker service is not running"
    return 1
  fi
  
  log_substep "Checking containerd status"
  if ! sudo systemctl is-active --quiet containerd; then
    log_error "Containerd service is not running"
    return 1
  fi
  
  log_substep "Testing Docker functionality"
  if ! docker info >/dev/null 2>&1; then
    log_error "Docker daemon is not responding properly"
    return 1
  fi
  
  log_substep "Testing container creation"
  if ! timeout 30 docker run --rm hello-world >/dev/null 2>&1; then
    log_warning "Docker hello-world test failed - may need internet connectivity"
  else
    log_substep "Container test passed"
  fi
  
  log_substep "Checking Docker configuration"
  local cgroup_driver=$(docker info 2>/dev/null | grep "Cgroup Driver" | cut -d: -f2 | tr -d ' ')
  if [[ "$cgroup_driver" != "systemd" ]]; then
    log_warning "Docker cgroup driver is not set to systemd (current: $cgroup_driver)"
  else
    log_substep "Cgroup driver correctly set to systemd"
  fi
  
  return 0
}

# Show Docker next steps
show_docker_next_steps() {
  log_next_steps "Docker Installation Complete" \
    "Test Docker functionality: docker run hello-world" \
    "Check Docker service status: systemctl status docker" \
    "View Docker system information: docker info" \
    "Verify container runtime: docker version" \
    "Install Kubernetes cluster: gok install kubernetes"
  
  log_urls "Docker Resources & Documentation" \
    "Docker Documentation: https://docs.docker.com/" \
    "Docker Hub Registry: https://hub.docker.com/" \
    "Kubernetes Container Runtime Guide: https://kubernetes.io/docs/setup/production-environment/container-runtimes/" \
    "Docker Best Practices: https://docs.docker.com/develop/best-practices/"
  
  log_credentials "Docker Management" "Current User" \
    "Docker group membership: Required for non-root access" \
    "Restart required: Log out and back in to apply group changes" \
    "Test access: docker ps (should work without sudo)"
  
  # Enhanced HA proxy detection and recommendation
  check_and_suggest_ha_setup
  
  log_info "Docker container runtime is now ready for Kubernetes installation"
  
  # Show Docker system status
  show_docker_system_status
}

# Enhanced HA setup detection and suggestions
check_and_suggest_ha_setup() {
  local suggest_ha=false
  local ha_reason=""
  
  # Check for multiple API servers configuration
  if [[ -n "$API_SERVERS" ]] && [[ "$API_SERVERS" == *","* ]]; then
    suggest_ha=true
    local server_count=$(echo "$API_SERVERS" | tr ',' '\n' | wc -l)
    ha_reason="Multiple API servers detected ($server_count servers) in API_SERVERS configuration"
  fi
  
  # Check for multiple network interfaces (potential multi-node setup)
  local interface_count=$(ip route | grep -E "^[0-9]" | wc -l)
  if [[ $interface_count -gt 2 ]]; then
    suggest_ha=true
    ha_reason="${ha_reason:+$ha_reason; }Multiple network interfaces detected (potential multi-node setup)"
  fi
  
  # Check available RAM (high RAM might indicate server-class machine)
  local total_ram_gb=$(free -g | awk '/^Mem:/{print $2}')
  if [[ $total_ram_gb -gt 8 ]]; then
    suggest_ha=true
    ha_reason="${ha_reason:+$ha_reason; }High RAM detected (${total_ram_gb}GB) - suitable for HA setup"
  fi
  
  if [[ "$suggest_ha" == "true" ]]; then
    echo
    log_header "High Availability Recommendation" "Multi-Master Kubernetes Setup Detected"
    
    log_warning "HA setup recommended: $ha_reason"
    
    log_next_steps "High Availability Container Setup" \
      "Set up API servers: export API_SERVERS='192.168.1.10:master1,192.168.1.11:master2,192.168.1.12:master3'" \
      "Configure HA proxy port: export HA_PROXY_PORT=8443 (optional, defaults to 8443)" \
      "Install HA proxy container: Call haInst() function or use manual setup" \
      "Verify HA proxy: docker ps | grep master-proxy" \
      "Proceed with Kubernetes: gok install kubernetes (will use HA endpoint)"
    
    log_urls "HA Container Management" \
      "HA Proxy Configuration: /opt/haproxy.cfg" \
      "Container Logs: docker logs master-proxy" \
      "HA Endpoint: https://localhost:\${HA_PROXY_PORT:-8443}"
    
    log_info "HA proxy container will provide load balancing and failover for Kubernetes API servers"
    
    # Show how to call haInst function
    echo
    echo -e "${COLOR_BRIGHT_YELLOW}${COLOR_BOLD}🚀 Ready to set up HA proxy container?${COLOR_RESET}"
    echo -e "${COLOR_CYAN}Run the following commands:${COLOR_RESET}"
    echo
    echo -e "${COLOR_GREEN}${COLOR_BOLD}# Configure your API servers${COLOR_RESET}"
    echo -e "${COLOR_CYAN}export API_SERVERS='192.168.1.10:master1,192.168.1.11:master2,192.168.1.12:master3'${COLOR_RESET}"
    echo
    echo -e "${COLOR_GREEN}${COLOR_BOLD}# Set up HA proxy container${COLOR_RESET}"
    echo -e "${COLOR_CYAN}haInst  # Call the HA installation function directly${COLOR_RESET}"
    echo
    echo -e "${COLOR_GREEN}${COLOR_BOLD}# Or configure manually and install Kubernetes${COLOR_RESET}"
    echo -e "${COLOR_CYAN}gok install kubernetes  # Will detect HA setup automatically${COLOR_RESET}"
    echo
  else
    log_info "Single-node setup detected - HA proxy not required for basic installation"
    
    if [[ -z "$API_SERVERS" ]]; then
      log_info "To enable HA setup later, configure: export API_SERVERS='ip1:node1,ip2:node2'"
    fi
  fi
}

# Show Docker system status
show_docker_system_status() {
  echo
  log_section "Docker System Status" "$EMOJI_CHECKMARK"
  
  # Docker daemon status
  if systemctl is-active --quiet docker; then
    log_success "Docker daemon: Running"
  else
    log_error "Docker daemon: Not running"
  fi
  
  # Containerd status
  if systemctl is-active --quiet containerd; then
    log_success "Containerd: Running"
  else
    log_error "Containerd: Not running"
  fi
  
  # Docker version info
  local docker_version=$(docker --version 2>/dev/null | cut -d' ' -f3 | cut -d',' -f1)
  local containerd_version=$(containerd --version 2>/dev/null | cut -d' ' -f3)
  
  log_info "Docker version: ${docker_version:-Unknown}"
  log_info "Containerd version: ${containerd_version:-Unknown}"
  
  # Docker configuration check
  local cgroup_driver=$(docker info 2>/dev/null | grep "Cgroup Driver" | cut -d: -f2 | tr -d ' ')
  if [[ "$cgroup_driver" == "systemd" ]]; then
    log_success "Cgroup driver: systemd (Kubernetes compatible)"
  else
    log_warning "Cgroup driver: $cgroup_driver (may need systemd for Kubernetes)"
  fi
  
  # Storage driver check
  local storage_driver=$(docker info 2>/dev/null | grep "Storage Driver" | cut -d: -f2 | tr -d ' ')
  log_info "Storage driver: ${storage_driver:-Unknown}"
  
  # Container count
  local running_containers=$(docker ps -q | wc -l)
  local total_containers=$(docker ps -aq | wc -l)
  log_info "Containers: $running_containers running, $total_containers total"
}

customDns() {
  echo "Going to add custom dns server"
  #Adding custom dns server
  cat <<EOF | kubectl apply -f -
apiVersion: v1
data:
  Corefile: |
    .:53 {
        errors
        health {
           lameduck 5s
        }
        ready
        kubernetes cloud.uat in-addr.arpa ip6.arpa {
           pods insecure
           fallthrough in-addr.arpa ip6.arpa
           ttl 30
        }
        prometheus :9153
        forward . /etc/resolv.conf {
           max_concurrent 1000
        }
        cache 30
        loop
        reload
        loadbalance
    }
    cloud.com:53 {
        errors
        cache 30
        forward . ${MASTER_HOST_IP}
    }
    gokcloud.com:53 {
        errors
        cache 30
        forward . ${MASTER_HOST_IP}
    }
kind: ConfigMap
metadata:
  name: coredns
  namespace: kube-system
EOF

  kubectl delete pod --namespace kube-system -l k8s-app=kube-dns
}

taintNode() {
  echo "Going to taint node for scheduling in master"

  # Define a function to get IP address
  getIP() {
    ifconfig eth1 2>/dev/null | awk '/inet / {print $2}' | sed 's/addr://'
  }

  # Get the IP address
  IP=$(getIP)

  # If IP is empty, try another network interface
  if [ -z "$IP" ]; then
    IP=$(ifconfig enp1s0.100 2>/dev/null | awk '/inet / {print $2}' | sed 's/addr://')
  fi

  # If IP is still empty, print an error message and exit
  if [ -z "$IP" ]; then
    echo "Error: Could not determine IP address"
    return 1
  fi

  # Get the node name
  JSONPATH="{.items[?(@.status.addresses[0].address == \"${IP}\")].metadata.name}"
  NODE_NAME="$(kubectl get nodes -o jsonpath="$JSONPATH")"

  # If node name is empty, print an error message and exit
  if [ -z "$NODE_NAME" ]; then
    echo "Error: Could not determine node name"
    return 1
  fi

  # Taint the node
  kubectl taint node "${NODE_NAME}" node-role.kubernetes.io/control-plane:NoSchedule-
}

k8sInst() {
  local k8s_type="${1:-kubernetes}"
  local verbose_mode="${GOK_VERBOSE:-false}"
  
  # Check for verbose flags in all arguments
  for arg in "$@"; do
    if [[ "$arg" == "--verbose" ]] || [[ "$arg" == "-v" ]]; then
      verbose_mode="true"
      break
    fi
  done
  
  # Also check if GOK_VERBOSE environment variable is set
  if [[ "${GOK_VERBOSE}" == "true" ]]; then
    verbose_mode="true"
  fi
  
  # Debug: Show verbose mode status
  if [[ "$verbose_mode" == "true" ]]; then
    echo -e "${COLOR_BRIGHT_GREEN}[DEBUG] k8sInst: Verbose mode is ENABLED${COLOR_RESET}"
  else
    echo -e "${COLOR_BRIGHT_YELLOW}[DEBUG] k8sInst: Verbose mode is DISABLED${COLOR_RESET}"
    echo -e "${COLOR_DIM}Arguments received: $@${COLOR_RESET}"
    echo -e "${COLOR_DIM}GOK_VERBOSE: ${GOK_VERBOSE}${COLOR_RESET}"
    echo -e "${COLOR_DIM}First arg: '$1', Second arg: '$2'${COLOR_RESET}"
  fi
  
  log_header "Kubernetes Cluster" "Starting ${k8s_type} Installation"
  
  echo -e "${COLOR_BRIGHT_CYAN}${COLOR_BOLD}🚀 KUBERNETES CLUSTER INSTALLATION${COLOR_RESET}"
  echo -e "${COLOR_YELLOW}Installation Type: ${COLOR_BOLD}${k8s_type}${COLOR_RESET}"
  if [[ "$verbose_mode" == "true" ]]; then
    echo -e "${COLOR_DIM}Verbose mode: Enabled${COLOR_RESET}"
  fi
  echo
  
  # Step 1: System Prerequisites and Kernel Modules
  log_step "1" "Configuring system prerequisites and kernel modules"
  
  if ! validate_system_requirements; then
    log_error "System requirements validation failed"
    return 1
  fi
  
  log_info "Loading required kernel modules..."
  local mod_output
  if [[ "$verbose_mode" == "true" ]]; then
    echo -e "${COLOR_DIM}Executing: sudo modprobe overlay br_netfilter${COLOR_RESET}"
    if mod_output=$(sudo modprobe overlay && sudo modprobe br_netfilter 2>&1); then
      log_success "Kernel modules loaded successfully"
      [[ -n "$mod_output" ]] && echo -e "${COLOR_DIM}$mod_output${COLOR_RESET}"
    else
      log_error "Failed to load kernel modules"
      echo -e "${COLOR_RED}Error details: $mod_output${COLOR_RESET}"
      return 1
    fi
  else
    if mod_output=$(sudo modprobe overlay && sudo modprobe br_netfilter 2>&1); then
      log_success "Kernel modules loaded successfully"
    else
      log_error "Failed to load kernel modules"
      echo -e "${COLOR_RED}Error details: $mod_output${COLOR_RESET}"
      return 1
    fi
  fi
  
  # Configure persistent module loading
  log_info "Configuring persistent kernel modules..."
  sudo tee /etc/modules-load.d/containerd.conf <<EOF >/dev/null
overlay
br_netfilter
EOF
  log_success "Kernel modules configured for persistence"
  
  # Step 2: Network Configuration
  log_step "2" "Configuring network settings for Kubernetes"
  
  log_info "Setting up network bridge configurations..."
  sudo tee /etc/sysctl.d/kubernetes.conf <<EOF >/dev/null
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
net.ipv4.ip_forward = 1
EOF
  
  local sysctl_output
  if [[ "$verbose_mode" == "true" ]]; then
    echo -e "${COLOR_DIM}Executing: sudo sysctl --system${COLOR_RESET}"
    if sysctl_output=$(sudo sysctl --system 2>&1); then
      log_success "Network settings applied successfully"
      echo -e "${COLOR_DIM}$sysctl_output${COLOR_RESET}"
    else
      log_error "Failed to apply network settings"
      echo -e "${COLOR_RED}Error details: $sysctl_output${COLOR_RESET}"
      return 1
    fi
  else
    if sysctl_output=$(sudo sysctl --system 2>&1); then
      log_success "Network settings applied successfully"
    else
      log_error "Failed to apply network settings"
      echo -e "${COLOR_RED}Error details: $sysctl_output${COLOR_RESET}"
      return 1
    fi
  fi
  
  # Step 3: Container Runtime Configuration
  log_step "3" "Configuring containerd container runtime"
  
  log_info "Setting up containerd configuration..."
  sudo mkdir -p /etc/containerd
  
  local containerd_config_output
  if [[ "$verbose_mode" == "true" ]]; then
    echo -e "${COLOR_DIM}Creating /etc/containerd directory and configuration...${COLOR_RESET}"
    echo -e "${COLOR_DIM}Executing: containerd config default | sudo tee /etc/containerd/config.toml${COLOR_RESET}"
    if containerd_config_output=$(containerd config default 2>&1) && echo "$containerd_config_output" | sudo tee /etc/containerd/config.toml >/dev/null; then
      log_success "Default containerd configuration created"
      echo -e "${COLOR_DIM}Configuration written to /etc/containerd/config.toml${COLOR_RESET}"
    else
      log_error "Failed to create containerd configuration"
      echo -e "${COLOR_RED}Error details: $containerd_config_output${COLOR_RESET}"
      return 1
    fi
  else
    if containerd_config_output=$(containerd config default 2>&1) && echo "$containerd_config_output" | sudo tee /etc/containerd/config.toml >/dev/null; then
      log_success "Default containerd configuration created"
    else
      log_error "Failed to create containerd configuration"
      echo -e "${COLOR_RED}Error details: $containerd_config_output${COLOR_RESET}"
      return 1
    fi
  fi
  
  # Configure systemd cgroup driver
  log_info "Configuring systemd cgroup driver..."
  local sed_output
  if [[ "$verbose_mode" == "true" ]]; then
    echo -e "${COLOR_DIM}Updating containerd config: SystemdCgroup = false -> SystemdCgroup = true${COLOR_RESET}"
    echo -e "${COLOR_DIM}Executing: sudo sed -i 's/SystemdCgroup = false/SystemdCgroup = true/g' /etc/containerd/config.toml${COLOR_RESET}"
    if sed_output=$(sudo sed -i 's/SystemdCgroup = false/SystemdCgroup = true/g' /etc/containerd/config.toml 2>&1); then
      log_success "Systemd cgroup driver configured"
      echo -e "${COLOR_DIM}Verification:${COLOR_RESET}"
      local cgroup_check=$(grep -n "SystemdCgroup" /etc/containerd/config.toml || echo "SystemdCgroup setting not found")
      echo -e "${COLOR_DIM}$cgroup_check${COLOR_RESET}"
    else
      log_warning "Cgroup driver configuration may have failed"
      echo -e "${COLOR_YELLOW}Warning details: $sed_output${COLOR_RESET}"
    fi
  else
    if sed_output=$(sudo sed -i 's/SystemdCgroup = false/SystemdCgroup = true/g' /etc/containerd/config.toml 2>&1); then
      log_success "Systemd cgroup driver configured"
    else
      log_warning "Cgroup driver configuration may have failed"
      echo -e "${COLOR_YELLOW}Warning details: $sed_output${COLOR_RESET}"
    fi
  fi
  
  # Restart and enable containerd
  log_info "Starting containerd service..."
  local containerd_service_output
  if [[ "$verbose_mode" == "true" ]]; then
    echo -e "${COLOR_DIM}Executing: sudo systemctl restart containerd && sudo systemctl enable containerd${COLOR_RESET}"
    if containerd_service_output=$(sudo systemctl restart containerd 2>&1 && sudo systemctl enable containerd 2>&1); then
      log_success "Containerd service started and enabled"
      [[ -n "$containerd_service_output" ]] && echo -e "${COLOR_DIM}$containerd_service_output${COLOR_RESET}"
    else
      log_error "Failed to start containerd service"
      echo -e "${COLOR_RED}Error details: $containerd_service_output${COLOR_RESET}"
      echo -e "${COLOR_YELLOW}Try: ${COLOR_CYAN}sudo systemctl status containerd${COLOR_RESET} for more details"
      return 1
    fi
  else
    if containerd_service_output=$(sudo systemctl restart containerd 2>&1 && sudo systemctl enable containerd 2>&1); then
      log_success "Containerd service started and enabled"
    else
      log_error "Failed to start containerd service"
      echo -e "${COLOR_RED}Error details: $containerd_service_output${COLOR_RESET}"
      echo -e "${COLOR_YELLOW}Try: ${COLOR_CYAN}sudo systemctl status containerd${COLOR_RESET} for more details"
      return 1
    fi
  fi
  
  # Step 4: Kubernetes Repository Setup
  log_step "4" "Setting up Kubernetes package repository"
  
  log_info "Installing required packages..."
  local repo_packages_output
  if [[ "$verbose_mode" == "true" ]]; then
    echo -e "${COLOR_DIM}Executing: sudo apt-get update && sudo apt-get install -y apt-transport-https ca-certificates curl${COLOR_RESET}"
    if repo_packages_output=$(sudo apt-get update 2>&1 && sudo apt-get install -y apt-transport-https ca-certificates curl 2>&1); then
      log_success "Required packages installed"
      echo -e "${COLOR_DIM}$repo_packages_output${COLOR_RESET}"
    else
      log_error "Failed to install required packages"
      echo -e "${COLOR_RED}Error details: $repo_packages_output${COLOR_RESET}"
      return 1
    fi
  else
    if repo_packages_output=$(sudo apt-get update 2>&1 && sudo apt-get install -y apt-transport-https ca-certificates curl 2>&1); then
      log_success "Required packages installed"
    else
      log_error "Failed to install required packages"
      echo -e "${COLOR_RED}Error details: $repo_packages_output${COLOR_RESET}"
      return 1
    fi
  fi
  
  # Setup Kubernetes signing key
  log_info "Adding Kubernetes signing key..."
  sudo mkdir -p -m 755 /etc/apt/keyrings
  
  if [[ "$verbose_mode" == "true" ]]; then
    echo -e "${COLOR_DIM}Downloading Kubernetes signing key from pkgs.k8s.io...${COLOR_RESET}"
    if curl -fsSL --show-error https://pkgs.k8s.io/core:/stable:/v1.32/deb/Release.key | \
       sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg; then
      sudo chmod 644 /etc/apt/keyrings/kubernetes-apt-keyring.gpg
      log_success "Kubernetes signing key added"
      echo -e "${COLOR_DIM}Key location: /etc/apt/keyrings/kubernetes-apt-keyring.gpg${COLOR_RESET}"
    else
      log_error "Failed to add Kubernetes signing key"
      return 1
    fi
  else
    if curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.32/deb/Release.key 2>/dev/null | \
       sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg 2>/dev/null; then
      sudo chmod 644 /etc/apt/keyrings/kubernetes-apt-keyring.gpg
      log_success "Kubernetes signing key added"
    else
      log_error "Failed to add Kubernetes signing key"
      return 1
    fi
  fi
  
  # Add Kubernetes repository
  log_info "Adding Kubernetes repository..."
  if [[ "$verbose_mode" == "true" ]]; then
    echo -e "${COLOR_DIM}Creating repository configuration: /etc/apt/sources.list.d/kubernetes.list${COLOR_RESET}"
    echo 'deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.32/deb/ /' | \
      sudo tee /etc/apt/sources.list.d/kubernetes.list
  else
    echo 'deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.32/deb/ /' | \
      sudo tee /etc/apt/sources.list.d/kubernetes.list >/dev/null
  fi
  sudo chmod 644 /etc/apt/sources.list.d/kubernetes.list
  log_success "Kubernetes repository added"
  
  # Step 5: Kubernetes Components Installation
  log_step "5" "Installing Kubernetes components"
  
  log_info "Updating package lists..."
  local apt_update_output
  if [[ "$verbose_mode" == "true" ]]; then
    echo -e "${COLOR_DIM}Executing: sudo apt-get update${COLOR_RESET}"
    if apt_update_output=$(sudo apt-get update 2>&1); then
      log_success "Package lists updated"
      echo -e "${COLOR_DIM}$apt_update_output${COLOR_RESET}"
    else
      log_error "Failed to update package lists"
      echo -e "${COLOR_RED}Error details: $apt_update_output${COLOR_RESET}"
      return 1
    fi
  else
    if apt_update_output=$(sudo apt-get update 2>&1); then
      log_success "Package lists updated"
    else
      log_error "Failed to update package lists"
      echo -e "${COLOR_RED}Error details: $apt_update_output${COLOR_RESET}"
      return 1
    fi
  fi
  
  log_info "Installing kubectl, kubeadm, and kubelet..."
  local apt_install_output
  if [[ "$verbose_mode" == "true" ]]; then
    echo -e "${COLOR_DIM}Executing: sudo apt-get install -y kubectl kubeadm kubelet${COLOR_RESET}"
    if apt_install_output=$(sudo apt-get install -y kubectl kubeadm kubelet 2>&1); then
      log_success "Kubernetes components installed successfully"
      echo -e "${COLOR_DIM}$apt_install_output${COLOR_RESET}"
    else
      log_error "Failed to install Kubernetes components"
      echo -e "${COLOR_RED}Error details: $apt_install_output${COLOR_RESET}"
      return 1
    fi
  else
    if apt_install_output=$(sudo apt-get install -y kubectl kubeadm kubelet 2>&1); then
      log_success "Kubernetes components installed successfully"
    else
      log_error "Failed to install Kubernetes components"
      echo -e "${COLOR_RED}Error details: $apt_install_output${COLOR_RESET}"
      return 1
    fi
  fi
  
  # Show installed versions
  local kubectl_version=$(kubectl version --client --short 2>/dev/null | grep -oE 'v[0-9]+\.[0-9]+\.[0-9]+' || echo "unknown")
  local kubeadm_version=$(kubeadm version -o short 2>/dev/null || echo "unknown")
  echo -e "  ${COLOR_GREEN}✓ kubectl: ${kubectl_version}${COLOR_RESET}"
  echo -e "  ${COLOR_GREEN}✓ kubeadm: ${kubeadm_version}${COLOR_RESET}"
  echo -e "  ${COLOR_GREEN}✓ kubelet: ${kubectl_version}${COLOR_RESET}"
  
  # Step 6: Cluster Initialization (Master) or Worker Setup
  if [ "$k8s_type" == "kubernetes" ]; then
    initialize_kubernetes_master "$verbose_mode"
  elif [ "$k8s_type" == "kubernetes-worker" ]; then
    setup_kubernetes_worker "$verbose_mode"
  else
    log_error "Unknown Kubernetes installation type: $k8s_type"
    return 1
  fi
}

# Validate system requirements for Kubernetes
validate_system_requirements() {
  log_info "Validating system requirements..."
  
  local validation_passed=true
  
  # Check available memory
  local mem_gb=$(free -g | awk '/^Mem:/{print $2}')
  if [ "$mem_gb" -lt 2 ]; then
    log_warning "System has ${mem_gb}GB RAM, minimum 2GB recommended"
  else
    log_success "Memory requirement satisfied (${mem_gb}GB available)"
  fi
  
  # Check available disk space
  local disk_gb=$(df / | awk 'NR==2 {print int($4/1024/1024)}')
  if [ "$disk_gb" -lt 10 ]; then
    log_warning "Available disk space: ${disk_gb}GB, minimum 20GB recommended"
  else
    log_success "Disk space requirement satisfied (${disk_gb}GB available)"
  fi
  
  # Check if Docker is running
  if systemctl is-active --quiet docker; then
    log_success "Docker service is running"
  else
    log_error "Docker service is not running - please install Docker first"
    validation_passed=false
  fi
  
  # Check if swap is disabled
  if [ "$(swapon --show | wc -l)" -gt 0 ]; then
    log_info "Swap is enabled - will be disabled during installation"
  else
    log_success "Swap is already disabled"
  fi
  
  return $([[ "$validation_passed" == "true" ]] && echo 0 || echo 1)
}

# Initialize Kubernetes master node
initialize_kubernetes_master() {
  local verbose_mode="${1:-false}"
  log_step "6" "Initializing Kubernetes master node"
  
  # Disable swap
  log_info "Disabling swap for Kubernetes..."
  sudo sed -i '/ swap / s/^\(.*\)$/#\1/g' /etc/fstab
  sudo swapoff -a
  log_success "Swap disabled successfully"
  
  # Enable kubelet service
  log_info "Enabling kubelet service..."
  if sudo systemctl enable kubelet >/dev/null 2>&1; then
    log_success "Kubelet service enabled"
  else
    log_warning "Kubelet service enable may have failed"
  fi
  
  # Pull container images
  log_info "Pulling required container images..."
  local pull_output
  if [[ "$verbose_mode" == "true" ]]; then
    echo -e "${COLOR_DIM}Executing: kubeadm config images pull${COLOR_RESET}"
    if pull_output=$(kubeadm config images pull 2>&1); then
      log_success "Container images pulled successfully"
      echo -e "${COLOR_DIM}$pull_output${COLOR_RESET}"
    else
      log_warning "Some container images may not have been pulled"
      echo -e "${COLOR_YELLOW}Warning details: $pull_output${COLOR_RESET}"
    fi
  else
    if pull_output=$(kubeadm config images pull 2>&1); then
      log_success "Container images pulled successfully"
    else
      log_warning "Some container images may not have been pulled"
      echo -e "${COLOR_YELLOW}Warning details: $pull_output${COLOR_RESET}"
    fi
  fi
  
  # Generate cluster configuration
  log_info "Generating cluster configuration..."
  if [ -f "$WORKING_DIR/cluster-config-master.yaml" ]; then
    envsubst <"$WORKING_DIR/cluster-config-master.yaml" >"$WORKING_DIR/config.yaml"
    log_success "Cluster configuration generated"
  else
    log_warning "Using default cluster configuration"
  fi
  
  # Initialize the cluster
  log_info "Initializing Kubernetes cluster (this may take several minutes)..."
  
  local init_cmd="kubeadm init"
  if [ -f "$WORKING_DIR/config.yaml" ]; then
    init_cmd="$init_cmd --config=$WORKING_DIR/config.yaml"
  fi
  init_cmd="$init_cmd --upload-certs"
  
  local init_output
  if [[ "$verbose_mode" == "true" ]]; then
    echo -e "${COLOR_DIM}Executing: sudo $init_cmd${COLOR_RESET}"
    echo -e "${COLOR_DIM}This will initialize the control plane and may take 5-10 minutes...${COLOR_RESET}"
    if init_output=$(sudo $init_cmd 2>&1); then
      log_success "Kubernetes cluster initialized successfully"
      echo -e "${COLOR_DIM}$init_output${COLOR_RESET}"
    else
      log_error "Kubernetes cluster initialization failed"
      echo -e "${COLOR_RED}Error details: $init_output${COLOR_RESET}"
      echo -e "${COLOR_YELLOW}Troubleshooting commands:${COLOR_RESET}"
      echo -e "  ${COLOR_CYAN}sudo journalctl -xeu kubelet${COLOR_RESET}"
      echo -e "  ${COLOR_CYAN}sudo kubeadm reset -f${COLOR_RESET} (to reset and try again)"
      return 1
    fi
  else
    if init_output=$(sudo $init_cmd 2>&1); then
      log_success "Kubernetes cluster initialized successfully"
    else
      log_error "Kubernetes cluster initialization failed"
      echo -e "${COLOR_RED}Error details: $init_output${COLOR_RESET}"
      echo -e "${COLOR_YELLOW}Troubleshooting commands:${COLOR_RESET}"
      echo -e "  ${COLOR_CYAN}sudo journalctl -xeu kubelet${COLOR_RESET}"
      echo -e "  ${COLOR_CYAN}sudo kubeadm reset -f${COLOR_RESET} (to reset and try again)"
      return 1
    fi
  fi
  
  # Setup kubectl configuration
  log_info "Setting up kubectl configuration..."
  export KUBECONFIG=/etc/kubernetes/admin.conf
  mkdir -p "$HOME/.kube"
  
  local config_output
  if [[ "$verbose_mode" == "true" ]]; then
    echo -e "${COLOR_DIM}Copying admin.conf to ~/.kube/config${COLOR_RESET}"
    echo -e "${COLOR_DIM}Setting proper ownership and permissions...${COLOR_RESET}"
    if config_output=$(sudo cp -i /etc/kubernetes/admin.conf "$HOME/.kube/config" 2>&1 && sudo chown $(id -u):$(id -g) "$HOME/.kube/config" 2>&1); then
      log_success "Kubectl configuration completed"
      echo -e "${COLOR_DIM}Config location: $HOME/.kube/config${COLOR_RESET}"
      [[ -n "$config_output" ]] && echo -e "${COLOR_DIM}$config_output${COLOR_RESET}"
    else
      log_error "Failed to setup kubectl configuration"
      echo -e "${COLOR_RED}Error details: $config_output${COLOR_RESET}"
      return 1
    fi
  else
    if config_output=$(sudo cp -i /etc/kubernetes/admin.conf "$HOME/.kube/config" 2>&1 && sudo chown $(id -u):$(id -g) "$HOME/.kube/config" 2>&1); then
      log_success "Kubectl configuration completed"
    else
      log_error "Failed to setup kubectl configuration"
      echo -e "${COLOR_RED}Error details: $config_output${COLOR_RESET}"
      return 1
    fi
  fi
  
  # Validate cluster is running
  log_info "Validating cluster status..."
  sleep 10  # Give cluster time to start
  
  if [[ "$verbose_mode" == "true" ]]; then
    echo -e "${COLOR_DIM}Checking cluster connectivity and status...${COLOR_RESET}"
    if kubectl cluster-info; then
      log_success "Kubernetes cluster is running and accessible"
      echo -e "${COLOR_DIM}Checking node status:${COLOR_RESET}"
      kubectl get nodes
      echo -e "${COLOR_DIM}Checking system pods:${COLOR_RESET}"
      kubectl get pods -n kube-system
      show_kubernetes_master_next_steps
    else
      log_error "Cluster validation failed"
      return 1
    fi
  else
    if kubectl cluster-info >/dev/null 2>&1; then
      log_success "Kubernetes cluster is running and accessible"
      show_kubernetes_master_next_steps
    else
      log_error "Cluster validation failed"
      return 1
    fi
  fi
}

# Setup Kubernetes worker node
setup_kubernetes_worker() {
  local verbose_mode="${1:-false}"
  log_step "6" "Setting up Kubernetes worker node"
  
  log_info "Setting up CA certificates..."
  if [ -f "/export/certs/issuer.crt" ]; then
    sudo cp /export/certs/issuer.crt /usr/local/share/ca-certificates/issuer.crt
    sudo update-ca-certificates >/dev/null 2>&1
    log_success "CA certificates updated"
  else
    log_warning "CA certificate not found at /export/certs/issuer.crt"
  fi
  
  # Disable swap
  log_info "Disabling swap for Kubernetes..."
  sudo sed -i '/ swap / s/^\(.*\)$/#\1/g' /etc/fstab
  sudo swapoff -a
  log_success "Swap disabled successfully"
  
  show_kubernetes_worker_next_steps
}

# Show next steps after successful Kubernetes master installation
show_kubernetes_master_next_steps() {
  echo
  log_header "Kubernetes Master Ready" "Next Steps & Information"
  
  echo -e "${COLOR_BRIGHT_GREEN}${COLOR_BOLD}🎉 KUBERNETES MASTER NODE READY${COLOR_RESET}"
  echo
  
  # Cluster information
  echo -e "${COLOR_YELLOW}${COLOR_BOLD}📊 Cluster Information:${COLOR_RESET}"
  local cluster_info=$(kubectl cluster-info 2>/dev/null | head -2)
  echo "$cluster_info" | while read line; do
    echo -e "  ${COLOR_GREEN}✓${COLOR_RESET} $line"
  done
  echo
  
  # Node status
  echo -e "${COLOR_YELLOW}${COLOR_BOLD}🖥️  Node Status:${COLOR_RESET}"
  local node_info=$(kubectl get nodes --no-headers 2>/dev/null | head -1)
  if [ -n "$node_info" ]; then
    local node_name=$(echo "$node_info" | awk '{print $1}')
    local node_status=$(echo "$node_info" | awk '{print $2}')
    local node_roles=$(echo "$node_info" | awk '{print $3}')
    echo -e "  ${COLOR_GREEN}✓${COLOR_RESET} Node: ${COLOR_BOLD}$node_name${COLOR_RESET}"
    echo -e "  ${COLOR_GREEN}✓${COLOR_RESET} Status: ${COLOR_BOLD}$node_status${COLOR_RESET}"
    echo -e "  ${COLOR_GREEN}✓${COLOR_RESET} Roles: ${COLOR_BOLD}$node_roles${COLOR_RESET}"
  fi
  echo
  
  # Get join command for worker nodes
  echo -e "${COLOR_YELLOW}${COLOR_BOLD}🔗 Worker Node Join Command:${COLOR_RESET}"
  local join_cmd=$(kubeadm token create --print-join-command 2>/dev/null)
  if [ -n "$join_cmd" ]; then
    echo -e "${COLOR_CYAN}$join_cmd${COLOR_RESET}"
    echo -e "${COLOR_DIM}(Use this command on worker nodes to join the cluster)${COLOR_RESET}"
  else
    echo -e "${COLOR_YELLOW}Run: ${COLOR_BOLD}kubeadm token create --print-join-command${COLOR_RESET}"
  fi
  echo
  
  # Critical next steps
  echo -e "${COLOR_BRIGHT_YELLOW}${COLOR_BOLD}🚀 CRITICAL NEXT STEPS:${COLOR_RESET}"
  echo
  
  echo -e "${COLOR_RED}${COLOR_BOLD}1. Install Network Plugin (REQUIRED):${COLOR_RESET}"
  echo -e "   ${COLOR_CYAN}The cluster needs a network plugin to function properly.${COLOR_RESET}"
  echo -e "   ${COLOR_GREEN}Run: ${COLOR_BOLD}calicoInst${COLOR_RESET} ${COLOR_DIM}# Install Calico networking${COLOR_RESET}"
  echo
  
  echo -e "${COLOR_YELLOW}${COLOR_BOLD}2. Install Base Services (RECOMMENDED):${COLOR_RESET}"
  echo -e "   ${COLOR_CYAN}Install essential cluster services and management tools.${COLOR_RESET}"
  echo -e "   ${COLOR_GREEN}Run: ${COLOR_BOLD}./gok install base-services${COLOR_RESET}"
  echo -e "   ${COLOR_DIM}   Includes: cert-manager, ingress-nginx, monitoring, vault${COLOR_RESET}"
  echo
  
  echo -e "${COLOR_YELLOW}${COLOR_BOLD}3. Verify Installation:${COLOR_RESET}"
  echo -e "   ${COLOR_GREEN}kubectl get nodes${COLOR_RESET}                    ${COLOR_DIM}# Check node status${COLOR_RESET}"
  echo -e "   ${COLOR_GREEN}kubectl get pods -A${COLOR_RESET}                  ${COLOR_DIM}# Check all pods${COLOR_RESET}"
  echo -e "   ${COLOR_GREEN}kubectl cluster-info${COLOR_RESET}                 ${COLOR_DIM}# Cluster information${COLOR_RESET}"
  echo
  
  echo -e "${COLOR_BRIGHT_CYAN}${COLOR_BOLD}🎯 RECOMMENDED INSTALLATION SEQUENCE:${COLOR_RESET}"
  echo -e "   ${COLOR_GREEN}1.${COLOR_RESET} ${COLOR_BOLD}calicoInst${COLOR_RESET}                        ${COLOR_DIM}# Network plugin (essential)${COLOR_RESET}"
  echo -e "   ${COLOR_GREEN}2.${COLOR_RESET} ${COLOR_BOLD}./gok install base-services${COLOR_RESET}       ${COLOR_DIM}# Core platform services${COLOR_RESET}"
  echo -e "   ${COLOR_GREEN}3.${COLOR_RESET} ${COLOR_BOLD}./gok install dashboard${COLOR_RESET}           ${COLOR_DIM}# Web dashboard${COLOR_RESET}"
  echo -e "   ${COLOR_GREEN}4.${COLOR_RESET} ${COLOR_BOLD}./gok install argocd${COLOR_RESET}              ${COLOR_DIM}# GitOps deployment${COLOR_RESET}"
  echo
  
  echo -e "${COLOR_BRIGHT_CYAN}${COLOR_BOLD}📚 USEFUL COMMANDS:${COLOR_RESET}"
  echo -e "   ${COLOR_GREEN}./gok status${COLOR_RESET}                         ${COLOR_DIM}# Check platform status${COLOR_RESET}"
  echo -e "   ${COLOR_GREEN}./gok install help${COLOR_RESET}                   ${COLOR_DIM}# See all installable components${COLOR_RESET}"
  echo -e "   ${COLOR_GREEN}kubectl get namespaces${COLOR_RESET}               ${COLOR_DIM}# List all namespaces${COLOR_RESET}"
  echo
}

# Show next steps for worker node setup
show_kubernetes_worker_next_steps() {
  echo
  log_header "Kubernetes Worker Setup" "Next Steps & Information"
  
  echo -e "${COLOR_BRIGHT_BLUE}${COLOR_BOLD}🔧 KUBERNETES WORKER NODE PREPARED${COLOR_RESET}"
  echo
  
  echo -e "${COLOR_YELLOW}${COLOR_BOLD}📋 Required Actions:${COLOR_RESET}"
  echo
  
  echo -e "${COLOR_RED}${COLOR_BOLD}1. Join the Cluster:${COLOR_RESET}"
  echo -e "   ${COLOR_CYAN}Get the join command from your master node:${COLOR_RESET}"
  echo -e "   ${COLOR_GREEN}kubeadm token create --print-join-command${COLOR_RESET}"
  echo -e "   ${COLOR_CYAN}Then run the output command on this worker node.${COLOR_RESET}"
  echo
  
  echo -e "${COLOR_YELLOW}${COLOR_BOLD}2. Label the Node (Optional):${COLOR_RESET}"
  echo -e "   ${COLOR_CYAN}After joining, label this node as a worker:${COLOR_RESET}"
  echo -e "   ${COLOR_GREEN}kubectl label node <node-name> node-role.kubernetes.io/worker=worker${COLOR_RESET}"
  echo
  
  echo -e "${COLOR_YELLOW}${COLOR_BOLD}3. Reboot System:${COLOR_RESET}"
  echo -e "   ${COLOR_CYAN}Reboot to ensure all changes take effect:${COLOR_RESET}"
  echo -e "   ${COLOR_GREEN}sudo reboot${COLOR_RESET}"
  echo
  
  echo -e "${COLOR_BRIGHT_CYAN}${COLOR_BOLD}🔍 VERIFICATION:${COLOR_RESET}"
  echo -e "   ${COLOR_CYAN}From the master node, verify this worker joined:${COLOR_RESET}"
  echo -e "   ${COLOR_GREEN}kubectl get nodes${COLOR_RESET}"
  echo
}

# Show comprehensive next steps after complete Kubernetes installation
show_kubernetes_complete_next_steps() {
  echo
  log_header "Kubernetes Platform Ready" "Complete Installation Next Steps"
  
  echo -e "${COLOR_BRIGHT_GREEN}${COLOR_BOLD}🎉 KUBERNETES PLATFORM INSTALLATION COMPLETE${COLOR_RESET}"
  echo
  
  # Cluster status overview
  echo -e "${COLOR_YELLOW}${COLOR_BOLD}📊 Cluster Status:${COLOR_RESET}"
  local cluster_status=$(kubectl cluster-info 2>/dev/null | head -1 | grep -o "is running at.*" || echo "Status check failed")
  local node_status=$(kubectl get nodes --no-headers 2>/dev/null | head -1)
  
  if [ -n "$node_status" ]; then
    local node_name=$(echo "$node_status" | awk '{print $1}')
    local node_ready=$(echo "$node_status" | awk '{print $2}')
    echo -e "  ${COLOR_GREEN}✓${COLOR_RESET} Cluster: $cluster_status"
    echo -e "  ${COLOR_GREEN}✓${COLOR_RESET} Master Node: $node_name ($node_ready)"
  fi
  
  # Installed components
  echo -e "  ${COLOR_GREEN}✓${COLOR_RESET} Container Runtime: containerd"
  echo -e "  ${COLOR_GREEN}✓${COLOR_RESET} Network Plugin: Calico"
  echo -e "  ${COLOR_GREEN}✓${COLOR_RESET} Package Manager: Helm"
  echo
  
  # Critical next steps
  echo -e "${COLOR_BRIGHT_YELLOW}${COLOR_BOLD}🚀 CRITICAL NEXT STEP:${COLOR_RESET}"
  echo
  
  echo -e "${COLOR_RED}${COLOR_BOLD}Install Base Services (HIGHLY RECOMMENDED):${COLOR_RESET}"
  echo -e "   ${COLOR_CYAN}Your cluster is ready, but needs essential services for production use.${COLOR_RESET}"
  echo
  
  echo -e "   ${COLOR_BRIGHT_GREEN}${COLOR_BOLD}Run this command to install all essential services:${COLOR_RESET}"
  echo -e "   ${COLOR_GREEN}${COLOR_BOLD}./gok install base-services${COLOR_RESET}"
  echo
  
  echo -e "   ${COLOR_YELLOW}Base services include:${COLOR_RESET}"
  echo -e "   ${COLOR_DIM}• Certificate Manager (cert-manager) - TLS/SSL automation${COLOR_RESET}"
  echo -e "   ${COLOR_DIM}• NGINX Ingress Controller - HTTP/HTTPS routing${COLOR_RESET}"
  echo -e "   ${COLOR_DIM}• Monitoring Stack - Prometheus & Grafana${COLOR_RESET}"
  echo -e "   ${COLOR_DIM}• Secrets Management - HashiCorp Vault${COLOR_RESET}"
  echo -e "   ${COLOR_DIM}• Identity Management - Keycloak${COLOR_RESET}"
  echo
  
  echo -e "${COLOR_BRIGHT_CYAN}${COLOR_BOLD}🎯 COMPLETE SETUP SEQUENCE:${COLOR_RESET}"
  echo -e "   ${COLOR_GREEN}${COLOR_BOLD}1. ./gok install base-services${COLOR_RESET}       ${COLOR_DIM}# Essential platform services${COLOR_RESET}"
  echo -e "   ${COLOR_GREEN}2. ./gok install dashboard${COLOR_RESET}           ${COLOR_DIM}# Kubernetes web dashboard${COLOR_RESET}"
  echo -e "   ${COLOR_GREEN}3. ./gok install argocd${COLOR_RESET}              ${COLOR_DIM}# GitOps continuous delivery${COLOR_RESET}"
  echo -e "   ${COLOR_GREEN}4. ./gok install jupyter${COLOR_RESET}             ${COLOR_DIM}# Data science environment${COLOR_RESET}"
  echo
  
  echo -e "${COLOR_BRIGHT_CYAN}${COLOR_BOLD}🔍 VERIFICATION COMMANDS:${COLOR_RESET}"
  echo -e "   ${COLOR_GREEN}kubectl get nodes${COLOR_RESET}                    ${COLOR_DIM}# Check node status${COLOR_RESET}"
  echo -e "   ${COLOR_GREEN}kubectl get pods -A${COLOR_RESET}                  ${COLOR_DIM}# Check all pods${COLOR_RESET}"
  echo -e "   ${COLOR_GREEN}kubectl get svc -A${COLOR_RESET}                   ${COLOR_DIM}# Check all services${COLOR_RESET}"
  echo -e "   ${COLOR_GREEN}./gok status${COLOR_RESET}                         ${COLOR_DIM}# GOK platform status${COLOR_RESET}"
  echo
  
  echo -e "${COLOR_BRIGHT_CYAN}${COLOR_BOLD}📚 USEFUL RESOURCES:${COLOR_RESET}"
  echo -e "   ${COLOR_GREEN}./gok install help${COLOR_RESET}                   ${COLOR_DIM}# See all installable components${COLOR_RESET}"
  echo -e "   ${COLOR_GREEN}./gok create help${COLOR_RESET}                    ${COLOR_DIM}# Create certificates, secrets${COLOR_RESET}"
  echo -e "   ${COLOR_GREEN}./gok generate help${COLOR_RESET}                  ${COLOR_DIM}# Generate microservices${COLOR_RESET}"
  echo
  
  echo -e "${COLOR_BRIGHT_YELLOW}${COLOR_BOLD}⚡ QUICK START:${COLOR_RESET}"
  echo -e "   ${COLOR_CYAN}To get a fully functional production cluster:${COLOR_RESET}"
  echo -e "   ${COLOR_BOLD}./gok install base-services && ./gok install dashboard${COLOR_RESET}"
  echo
  
  # Wait message
  figlet "Ready for base-services!" 2>/dev/null || echo -e "${COLOR_BRIGHT_GREEN}${COLOR_BOLD}🚀 Ready for base-services installation! 🚀${COLOR_RESET}"
}

calicoInst(){
  local verbose_mode="${GOK_VERBOSE:-false}"
  
  # Check for verbose flags in all arguments
  for arg in "$@"; do
    if [[ "$arg" == "--verbose" ]] || [[ "$arg" == "-v" ]]; then
      verbose_mode="true"
      break
    fi
  done
  
  # Also check if GOK_VERBOSE environment variable is set
  if [[ "${GOK_VERBOSE}" == "true" ]]; then
    verbose_mode="true"
  fi
  
  log_header "Calico Network Plugin" "Installing Container Networking"
  
  echo -e "${COLOR_BRIGHT_CYAN}${COLOR_BOLD}🌐 CALICO NETWORK PLUGIN INSTALLATION${COLOR_RESET}"
  if [[ "$verbose_mode" == "true" ]]; then
    echo -e "${COLOR_DIM}Verbose mode: Enabled${COLOR_RESET}"
  fi
  echo
  
  # Step 1: Validate cluster is ready
  log_step "1" "Validating Kubernetes cluster readiness"
  
  if ! kubectl cluster-info >/dev/null 2>&1; then
    log_error "Kubernetes cluster is not accessible"
    log_error "Please ensure Kubernetes master is running and kubectl is configured"
    return 1
  fi
  log_success "Kubernetes cluster is accessible"
  
  # Check if nodes are ready (they won't be without networking)
  local node_count=$(kubectl get nodes --no-headers 2>/dev/null | wc -l)
  log_info "Found $node_count node(s) in cluster"
  
  # Step 2: Check if Calico is already installed
  log_step "2" "Checking for existing network plugins"
  
  if kubectl get pods -n kube-system -l k8s-app=calico-node --no-headers 2>/dev/null | grep -q "Running"; then
    log_warning "Calico appears to be already running"
    echo -e "  ${COLOR_YELLOW}Use: ${COLOR_BOLD}kubectl get pods -n kube-system -l k8s-app=calico-node${COLOR_RESET} to verify"
    return 0
  fi
  
  # Check for other CNI plugins
  if kubectl get pods -n kube-system --no-headers 2>/dev/null | grep -qE "flannel|weave|cilium"; then
    log_warning "Another CNI plugin may already be installed"
    echo -e "  ${COLOR_YELLOW}Check with: ${COLOR_BOLD}kubectl get pods -n kube-system${COLOR_RESET}"
  fi
  
  # Step 3: Download and apply Calico manifest
  log_step "3" "Installing Calico network plugin"
  
  local calico_version="v3.25.1"
  local calico_manifest="https://raw.githubusercontent.com/projectcalico/calico/$calico_version/manifests/calico.yaml"
  
  log_info "Downloading Calico manifest (version: $calico_version)..."
  if curl -s --connect-timeout 10 "$calico_manifest" >/dev/null 2>&1; then
    log_success "Calico manifest is accessible"
  else
    log_error "Cannot access Calico manifest - check internet connectivity"
    return 1
  fi
  
  log_info "Applying Calico network plugin..."
  local calico_apply_output
  if [[ "$verbose_mode" == "true" ]]; then
    echo -e "${COLOR_DIM}Executing: kubectl create -f $calico_manifest${COLOR_RESET}"
    if calico_apply_output=$(kubectl create -f "$calico_manifest" 2>&1); then
      log_success "Calico manifest applied successfully"
      echo -e "${COLOR_DIM}$calico_apply_output${COLOR_RESET}"
    else
      log_error "Failed to apply Calico manifest"
      echo -e "${COLOR_RED}Error details: $calico_apply_output${COLOR_RESET}"
      echo -e "${COLOR_YELLOW}Troubleshooting:${COLOR_RESET}"
      echo -e "  ${COLOR_CYAN}kubectl get nodes${COLOR_RESET} - Check node status"
      echo -e "  ${COLOR_CYAN}kubectl get pods -n kube-system${COLOR_RESET} - Check system pods"
      echo -e "  ${COLOR_CYAN}kubectl describe nodes${COLOR_RESET} - Check node details"
      return 1
    fi
  else
    if calico_apply_output=$(kubectl create -f "$calico_manifest" 2>&1); then
      log_success "Calico manifest applied successfully"
    else
      log_error "Failed to apply Calico manifest"
      echo -e "${COLOR_RED}Error details: $calico_apply_output${COLOR_RESET}"
      echo -e "${COLOR_YELLOW}Troubleshooting:${COLOR_RESET}"
      echo -e "  ${COLOR_CYAN}kubectl get nodes${COLOR_RESET} - Check node status"
      echo -e "  ${COLOR_CYAN}kubectl get pods -n kube-system${COLOR_RESET} - Check system pods"
      echo -e "  ${COLOR_CYAN}kubectl describe nodes${COLOR_RESET} - Check node details"
      return 1
    fi
  fi
  
  # Step 4: Wait for Calico pods to be ready
  log_step "4" "Waiting for Calico pods to become ready"
  
  log_info "Waiting for Calico system pods (this may take 2-3 minutes)..."
  local timeout=180  # 3 minutes
  local elapsed=0
  
  while [ $elapsed -lt $timeout ]; do
    local ready_pods=$(kubectl get pods -n kube-system -l k8s-app=calico-node --no-headers 2>/dev/null | grep -c "Running" || echo "0")
    local total_pods=$(kubectl get pods -n kube-system -l k8s-app=calico-node --no-headers 2>/dev/null | wc -l || echo "0")
    
    if [ "$ready_pods" -gt 0 ] && [ "$ready_pods" -eq "$total_pods" ]; then
      log_success "All Calico pods are running ($ready_pods/$total_pods)"
      break
    fi
    
    if [ $((elapsed % 30)) -eq 0 ]; then
      log_info "Calico pods status: $ready_pods/$total_pods ready (${elapsed}s elapsed)"
    fi
    
    sleep 5
    elapsed=$((elapsed + 5))
  done
  
  if [ $elapsed -ge $timeout ]; then
    log_warning "Calico pods took longer than expected to start"
    echo "  Current status:"
    kubectl get pods -n kube-system -l k8s-app=calico-node 2>/dev/null | head -5
  fi
  
  # Step 5: Validate network plugin installation
  log_step "5" "Validating network plugin installation"
  
  # Check if nodes are now ready
  sleep 10  # Give nodes time to update status
  local ready_nodes=$(kubectl get nodes --no-headers 2>/dev/null | grep -c "Ready" || echo "0")
  local total_nodes=$(kubectl get nodes --no-headers 2>/dev/null | wc -l || echo "0")
  
  if [ "$ready_nodes" -eq "$total_nodes" ] && [ "$ready_nodes" -gt 0 ]; then
    log_success "All nodes are now Ready ($ready_nodes/$total_nodes)"
  else
    log_warning "Some nodes may not be Ready yet ($ready_nodes/$total_nodes)"
    echo "  This is normal immediately after installation"
  fi
  
  # Show Calico components status
  log_info "Calico components status:"
  kubectl get pods -n kube-system -l k8s-app=calico-node --no-headers 2>/dev/null | while read line; do
    local pod_name=$(echo "$line" | awk '{print $1}')
    local pod_status=$(echo "$line" | awk '{print $3}')
    if [ "$pod_status" = "Running" ]; then
      echo -e "  ${COLOR_GREEN}✓${COLOR_RESET} $pod_name"
    else
      echo -e "  ${COLOR_YELLOW}⏳${COLOR_RESET} $pod_name ($pod_status)"
    fi
  done
  
  show_calico_next_steps
}

# Show next steps after Calico installation
show_calico_next_steps() {
  echo
  log_header "Calico Network Ready" "Next Steps & Verification"
  
  echo -e "${COLOR_BRIGHT_GREEN}${COLOR_BOLD}🌐 CALICO NETWORK PLUGIN INSTALLED${COLOR_RESET}"
  echo
  
  echo -e "${COLOR_YELLOW}${COLOR_BOLD}🔍 Verification Commands:${COLOR_RESET}"
  echo -e "   ${COLOR_GREEN}kubectl get nodes${COLOR_RESET}                    ${COLOR_DIM}# All nodes should be 'Ready'${COLOR_RESET}"
  echo -e "   ${COLOR_GREEN}kubectl get pods -n kube-system${COLOR_RESET}      ${COLOR_DIM}# Check system pods${COLOR_RESET}"
  echo -e "   ${COLOR_GREEN}kubectl get pods -A${COLOR_RESET}                  ${COLOR_DIM}# Check all namespaces${COLOR_RESET}"
  echo
  
  echo -e "${COLOR_BRIGHT_CYAN}${COLOR_BOLD}🚀 RECOMMENDED NEXT STEPS:${COLOR_RESET}"
  echo
  
  echo -e "${COLOR_YELLOW}${COLOR_BOLD}1. Install Base Services:${COLOR_RESET}"
  echo -e "   ${COLOR_CYAN}Install essential cluster services and management tools:${COLOR_RESET}"
  echo -e "   ${COLOR_GREEN}${COLOR_BOLD}./gok install base-services${COLOR_RESET}"
  echo -e "   ${COLOR_DIM}   • Certificate Manager (TLS automation)${COLOR_RESET}"
  echo -e "   ${COLOR_DIM}   • NGINX Ingress Controller (HTTP routing)${COLOR_RESET}"
  echo -e "   ${COLOR_DIM}   • Monitoring Stack (Prometheus & Grafana)${COLOR_RESET}"
  echo -e "   ${COLOR_DIM}   • Secrets Management (Vault)${COLOR_RESET}"
  echo
  
  echo -e "${COLOR_YELLOW}${COLOR_BOLD}2. Install Web Dashboard:${COLOR_RESET}"
  echo -e "   ${COLOR_GREEN}./gok install dashboard${COLOR_RESET}              ${COLOR_DIM}# Kubernetes web UI${COLOR_RESET}"
  echo
  
  echo -e "${COLOR_YELLOW}${COLOR_BOLD}3. Setup GitOps:${COLOR_RESET}"
  echo -e "   ${COLOR_GREEN}./gok install argocd${COLOR_RESET}                 ${COLOR_DIM}# GitOps continuous delivery${COLOR_RESET}"
  echo
  
  echo -e "${COLOR_BRIGHT_YELLOW}${COLOR_BOLD}⚡ QUICK START SEQUENCE:${COLOR_RESET}"
  echo -e "   ${COLOR_GREEN}# Install everything needed for a production cluster:${COLOR_RESET}"
  echo -e "   ${COLOR_BOLD}./gok install base-services${COLOR_RESET}           ${COLOR_DIM}# Core services${COLOR_RESET}"
  echo -e "   ${COLOR_BOLD}./gok install dashboard${COLOR_RESET}               ${COLOR_DIM}# Web interface${COLOR_RESET}"
  echo -e "   ${COLOR_BOLD}./gok install argocd${COLOR_RESET}                  ${COLOR_DIM}# GitOps${COLOR_RESET}"
  echo
  
  echo -e "${COLOR_BRIGHT_CYAN}${COLOR_BOLD}📊 Platform Status:${COLOR_RESET}"
  echo -e "   ${COLOR_GREEN}./gok status${COLOR_RESET}                         ${COLOR_DIM}# Check overall platform status${COLOR_RESET}"
  echo
}

helmInst() {
  log_header "Helm Package Manager" "Starting Installation"
  
  # Check if Helm is already installed
  if command -v helm >/dev/null 2>&1; then
    local current_version=$(helm version --short --client 2>/dev/null | grep -oE 'v[0-9]+\.[0-9]+\.[0-9]+' || echo "unknown")
    log_info "Helm is already installed (version: ${current_version})"
    
    # Test if Helm works
    if helm version --short >/dev/null 2>&1; then
      log_success "Helm is working correctly"
      show_helm_next_steps
      return 0
    else
      log_warning "Helm installed but not working correctly, attempting reinstall..."
    fi
  fi
  
  log_info "Installing Helm package manager..."
  
  # Method 1: Try snap first (recommended for Ubuntu)
  if command -v snap >/dev/null 2>&1; then
    log_info "Installing Helm via snap package manager..."
    if sudo snap install helm --classic >/dev/null 2>&1; then
      log_success "Helm installed successfully via snap"
      helm version --short
      show_helm_next_steps
      return 0
    else
      log_warning "Snap installation failed, trying official script method..."
    fi
  fi
  
  # Method 2: Official Helm installation script
  log_info "Installing Helm via official installation script..."
  if curl -fsSL -o get_helm.sh https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3 2>/dev/null; then
    chmod 700 get_helm.sh
    if ./get_helm.sh >/dev/null 2>&1; then
      rm -f get_helm.sh
      log_success "Helm installed successfully via official script"
      helm version --short
      show_helm_next_steps
      return 0
    else
      rm -f get_helm.sh
      log_error "Official script installation failed, trying APT method..."
    fi
  else
    log_warning "Could not download Helm installation script"
  fi
  
  # Method 3: APT with updated repository (fallback)
  log_info "Installing Helm via APT package manager..."
  if install_helm_via_apt; then
    log_success "Helm installed successfully via APT"
    helm version --short
    show_helm_next_steps
    return 0
  fi
  
  log_error "All Helm installation methods failed"
  
  # Check for network connectivity issues
  if ! ping -c 1 8.8.8.8 >/dev/null 2>&1; then
    log_error "Network connectivity issue detected"
    echo "  Please check your internet connection and try again"
    echo "  Or install Helm manually when online:"
  else
    log_error "Installation failed despite network connectivity"
    echo "  Please install Helm manually using one of these methods:"
  fi
  
  echo "  1. Snap: sudo snap install helm --classic"
  echo "  2. Script: curl https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3 | bash"
  echo "  3. GitHub Releases: https://github.com/helm/helm/releases"
  echo "  4. Direct binary: wget https://get.helm.sh/helm-v3.14.0-linux-amd64.tar.gz"
  return 1
}

# Helper function for APT installation
install_helm_via_apt() {
  # Clean up any old repositories first
  sudo rm -f /etc/apt/sources.list.d/helm*.list 2>/dev/null
  
  # Check network connectivity first
  if ! curl -s --connect-timeout 5 https://baltocdn.com >/dev/null 2>&1; then
    log_warning "Cannot reach baltocdn.com - network connectivity issue"
    return 1
  fi
  
  # Add official Helm APT repository with proper key management
  if curl -fsSL https://baltocdn.com/helm/signing.asc 2>/dev/null | gpg --dearmor | sudo tee /usr/share/keyrings/helm.gpg > /dev/null 2>&1; then
    echo "deb [arch=$(dpkg --print-architecture) signed-by=/usr/share/keyrings/helm.gpg] https://baltocdn.com/helm/stable/debian/ all main" | sudo tee /etc/apt/sources.list.d/helm-stable-debian.list >/dev/null
    
    if sudo apt-get update >/dev/null 2>&1 && sudo apt-get install -y helm >/dev/null 2>&1; then
      return 0
    fi
  fi
  return 1
}

# Show next steps after Helm installation
show_helm_next_steps() {
  echo
  log_header "Helm Next Steps" "Package Manager Ready"
  
  echo -e "${COLOR_BRIGHT_CYAN}${COLOR_BOLD}🔧 HELM PACKAGE MANAGER READY${COLOR_RESET}"
  echo
  
  echo -e "${COLOR_YELLOW}${COLOR_BOLD}Common Helm Commands:${COLOR_RESET}"
  echo -e "  ${COLOR_GREEN}helm repo add <name> <url>${COLOR_RESET}     - Add a chart repository"
  echo -e "  ${COLOR_GREEN}helm repo update${COLOR_RESET}               - Update repository information"
  echo -e "  ${COLOR_GREEN}helm search repo <keyword>${COLOR_RESET}     - Search for charts"
  echo -e "  ${COLOR_GREEN}helm install <name> <chart>${COLOR_RESET}    - Install a chart"
  echo -e "  ${COLOR_GREEN}helm list${COLOR_RESET}                      - List installed releases"
  echo
  
  echo -e "${COLOR_YELLOW}${COLOR_BOLD}Popular Chart Repositories:${COLOR_RESET}"
  echo -e "  ${COLOR_CYAN}Bitnami:${COLOR_RESET}        helm repo add bitnami https://charts.bitnami.com/bitnami"
  echo -e "  ${COLOR_CYAN}Jetstack:${COLOR_RESET}       helm repo add jetstack https://charts.jetstack.io"
  echo -e "  ${COLOR_CYAN}Ingress-NGINX:${COLOR_RESET}  helm repo add ingress-nginx https://kubernetes.github.io/ingress-nginx"
  echo -e "  ${COLOR_CYAN}Prometheus:${COLOR_RESET}     helm repo add prometheus-community https://prometheus-community.github.io/helm-charts"
  echo
  
  echo -e "${COLOR_YELLOW}${COLOR_BOLD}GOK Integration:${COLOR_RESET}"
  echo -e "  ${COLOR_GREEN}./gok install cert-manager${COLOR_RESET}    - Install cert-manager via Helm"
  echo -e "  ${COLOR_GREEN}./gok install ingress${COLOR_RESET}         - Install NGINX Ingress via Helm"
  echo -e "  ${COLOR_GREEN}./gok install monitoring${COLOR_RESET}      - Install Prometheus & Grafana"
  echo
}

certmanagerInst() {
  helm repo add jetstack https://charts.jetstack.io
  helm repo update
  #kubectl apply -f https://github.com/cert-manager/cert-manager/releases/download/v1.14.5/cert-manager.crds.yaml

  #--set serviceAccount.automountServiceAccountToken=false \
  #--set webhook.timeoutSeconds=30
  #--set startupapicheck.timeout=10m
  # --debug
  helm install \
    cert-manager jetstack/cert-manager \
    --namespace cert-manager \
    --create-namespace \
    --set installCRDs=true \
    --set global.leaderElection.namespace=cert-manager \
    --version v1.14.5 \
    --values https://github.com/sumitmaji/kubernetes/raw/master/install_k8s/cert-manager/values.yaml
}

subDomain(){
  if [ -z $1 ]; then
    echo "$(defaultSubdomain)"
  else
    echo "$1"
  fi
}

certificateRequestForNs() {
  NS=$1
  SUBDOMAIN=$(subDomain $2)
  cat <<EOF | kubectl apply -f -
apiVersion: cert-manager.io/v1
kind: Certificate
metadata:
  name: ${SUBDOMAIN}-$(sedRootDomain)-tls
  namespace: ${NS}
spec:
  secretName: ${SUBDOMAIN}-$(sedRootDomain)
  issuerRef:
    name: $(getClusterIssuerName)
    kind: ClusterIssuer
  commonName: ${SUBDOMAIN}.$(rootDomain)
  dnsNames:
    - ${SUBDOMAIN}.$(rootDomain)
  issuerRef:
    name: $(getClusterIssuerName)
    kind: ClusterIssuer
EOF
  echo "Certificate request for NS $NS created, executing below command to know current status"
  echo "kubectl get Issuers,ClusterIssuers,Certificates,CertificateRequests,Orders,Challenges --all-namespaces"
  kubectl get Issuers,ClusterIssuers,Certificates,CertificateRequests,Orders,Challenges --all-namespaces
  kubectl --timeout=10s -n ${NS} wait --for=condition=Ready certificates.cert-manager.io "${SUBDOMAIN}"-"$(sedRootDomain)"-tls
}

getLetsEncEnv(){
  echo "${LETS_ENCRYPT_ENV}"
}

getLetsEncryptUrl(){
  [[ $(getLetsEncEnv) == 'prod' ]] && echo "https://acme-v02.api.letsencrypt.org/directory " || echo "https://acme-staging-v02.api.letsencrypt.org/directory"
}

isProd(){
  [[ $(getLetsEncEnv) == 'prod' ]] && echo "true" || echo "false"
}

getClusterIssuerName(){
  case "$CERTMANAGER_CHALANGE_TYPE" in
   'dns') echo "letsencrypt-$(getLetsEncEnv)" ;;
   'http') echo "letsencrypt-$(getLetsEncEnv)" ;;
   'selfsigned') echo "gokselfsign-ca-cluster-issuer" ;;
  esac
}

#Godday api calls are disabled, hence going to remove this call.
godaddyWebhook() {
  replaceEnvVariable  https://github.com/sumitmaji/kubernetes/raw/master/install_k8s/godaddy-cert-webhook/webhook-all.yml | kubectl create -f - --validate=false
  echo "Provide godaddy apikey and secret <API_KEY:SECRET>"
  API_KEY=$(promptSecret "Provide godaddy apikey and secret <API_KEY:SECRET>")

  cat <<EOF | kubectl apply -f -
apiVersion: v1
kind: Secret
metadata:
  name: godaddy-api-key-secret
  namespace: cert-manager
type: Opaque
stringData:
  api-key: ${API_KEY}
EOF

  cat <<EOF | kubectl apply -f -
apiVersion: cert-manager.io/v1
kind: ClusterIssuer
metadata:
  name: $(getClusterIssuerName)
spec:
  acme:
    email: majisumitkumar@gmail.com
    server: $(getLetsEncryptUrl)
    privateKeySecretRef:
      name: letsencrypt-$(getLetsEncEnv)
    solvers:
    - dns01:
        webhook:
          config:
            apiKeySecretRef:
              name: godaddy-api-key-secret
              key: api-key
            production: $(isProd)
            ttl: 600
          groupName: $(rootDomain)
          solverName: godaddy
      selector:
       dnsNames:
       - '$(defaultSubdomain).$(rootDomain)'
       - '*.$(rootDomain)'
EOF

  cat <<EOF | kubectl apply -f -
apiVersion: cert-manager.io/v1
kind: Certificate
metadata:
  name: $(sedRootDomain)-tls
  namespace: default
spec:
  secretName: $(sedRootDomain)
  issuerRef:
    name: letsencrypt-prod
    kind: ClusterIssuer
  commonName: $(defaultSubdomain).$(rootDomain)
  dnsNames:
    - $(defaultSubdomain).$(rootDomain)
  issuerRef:
    name: letsencrypt-prod
    kind: ClusterIssuer
EOF

}



addLetsEncryptStagingCertificates(){
  wget https://letsencrypt.org/certs/staging/letsencrypt-stg-root-x1.pem
  sudo cp letsencrypt-stg-root-x1.pem /usr/local/share/ca-certificates/
  sudo update-ca-certificates
  echo "Added letsencrypt staging certificates, please reboot the system for it to effect"
}


godaddyWebhookReset() {
  kubectl delete -f https://github.com/sumitmaji/kubernetes/raw/master/install_k8s/godaddy-cert-webhook/webhook-all.yml
  kubectl delete secret godaddy-api-key-secret -n cert-manager
}

setupCertiIssuers() {

if [ $CERTMANAGER_CHALANGE_TYPE == 'dns' ]; then
  cat <<EOF | kubectl apply -f -
apiVersion: cert-manager.io/v1
kind: ClusterIssuer
metadata:
  name: $(getClusterIssuerName)
spec:
  acme:
    email: majisumitkumar@gmail.com
    server: $(getLetsEncryptUrl)
    privateKeySecretRef:
      name: letsencrypt-$(getLetsEncEnv)
    solvers:
    - dns01:
        webhook:
          config:
            apiKeySecretRef:
              name: godaddy-api-key-secret
              key: api-key
            production: $(isProd)
            ttl: 600
          groupName: $(rootDomain)
          solverName: godaddy
      selector:
       dnsNames:
       - '$(defaultSubdomain).$(rootDomain)'
       - '*.$(rootDomain)'
EOF
  godaddyWebhook
  addLetsEncryptStagingCertificates
elif [ $CERTMANAGER_CHALANGE_TYPE == 'http' ]; then
    

  cat <<EOF | kubectl apply -f -
apiVersion: cert-manager.io/v1
kind: ClusterIssuer
metadata:
  name: $(getClusterIssuerName)
spec:
  acme:
    email: majisumitkumar@gmail.com
    server: $(getLetsEncryptUrl)
    #preferredChain: "(STAGING) Pretend Pear X1"
    privateKeySecretRef:
      name: letsencrypt-$(getLetsEncEnv)
    solvers:
    - http01:
        ingress:
          ingressClassName: nginx
EOF
  addLetsEncryptStagingCertificates
elif [ $CERTMANAGER_CHALANGE_TYPE == 'selfsigned' ]; then
  # https://medium.com/geekculture/a-simple-ca-setup-with-kubernetes-cert-manager-bc8ccbd9c2
  # https://gist.github.com/jakexks/c1de8238cbee247333f8c274dc0d6f0f
  # Create self signed cluster issuer:
  echo "Creating self-signed cluster-issuer..."
  until cat <<EOYAML | kubectl apply -f -
apiVersion: cert-manager.io/v1
kind: ClusterIssuer
metadata:
  name: selfsigned-cluster-issuer
spec:
  selfSigned: {}
EOYAML
  do sleep 1; done
  kubectl --timeout=10s wait --for=condition=Ready clusterissuers.cert-manager.io selfsigned-cluster-issuer

  # Create CA certificate. If you want to use it as a ClusterIssuer the secret must be in the cert-manager namespace:
  echo "Creating self-signed certificate..."
  cat <<EOYAML | kubectl apply -f -
apiVersion: cert-manager.io/v1
kind: Certificate
metadata:
  name: gokselfsign-ca
  namespace: cert-manager
spec:
  isCA: true
  commonName: gokselfsign-ca
  secretName: gokselfsign-ca
  subject:
    organizations:
      - GOK Inc.
    organizationalUnits:
      - Widgets
  privateKey:
    algorithm: ECDSA
    size: 256
  issuerRef:
    name: selfsigned-cluster-issuer
    kind: ClusterIssuer
    group: cert-manager.io
EOYAML
  kubectl --timeout=10s -n cert-manager wait --for=condition=Ready certificates.cert-manager.io gokselfsign-ca

  # Create clusterissuer
  echo "Creating CA cluster issuer..."
  cat <<EOYAML | kubectl apply -f -
apiVersion: cert-manager.io/v1
kind: ClusterIssuer
metadata:
  name: $(getClusterIssuerName)
spec:
  ca:
    secretName: gokselfsign-ca
EOYAML
  kubectl --timeout=10s wait --for=condition=Ready clusterissuers.cert-manager.io "$(getClusterIssuerName)"

  # Add the self signed issuer(ca) certificate to authorized certificates
  kubectl get secrets -n cert-manager gokselfsign-ca -o json | jq -r '.data["tls.crt"]' | base64 -d > /usr/local/share/ca-certificates/issuer.crt

  # Add the issuer.crt to export directory so that the worker nodes can add the same to their trusted certificates.
  mkdir -p /export/certs
  cp /usr/local/share/ca-certificates/issuer.crt /export/certs/issuer.crt
  update-ca-certificates
  #Need to restart the container
fi


  cat <<EOF | kubectl apply -f -
apiVersion: cert-manager.io/v1
kind: Certificate
metadata:
  name: $(sedRootDomain)-tls
  namespace: default
spec:
  secretName: $(sedRootDomain)
  issuerRef:
    name: $(getClusterIssuerName)
    kind: ClusterIssuer
  commonName: $(defaultSubdomain).$(rootDomain)
  dnsNames:
    - $(defaultSubdomain).$(rootDomain)
  issuerRef:
    name: $(getClusterIssuerName)
    kind: ClusterIssuer
EOF

echoWarning "Selfsigned certificate is added to root ca, please reboot the system for it to effect"
}

certManagerReset() {
  kubectl delete Issuers,ClusterIssuers,Certificates,CertificateRequests,Orders,Challenges --all --all-namespaces
  if [[ $CERTMANAGER_CHALANGE_TYPE == 'dns' ]]; then godaddyWebhookReset; fi
  helm --namespace cert-manager delete cert-manager
  kubectl delete namespace cert-manager
}

haInst() {
  log_section "High Availability Proxy Installation" "$EMOJI_NETWORK"
  
  # Pre-installation validation
  log_step "1" "Validating HA proxy requirements"
  
  # Check if Docker is installed and running
  if ! command -v docker >/dev/null 2>&1; then
    log_error "Docker is required for HA proxy but not installed"
    log_info "Install Docker first: gok install docker"
    return 1
  fi
  
  if ! docker info >/dev/null 2>&1; then
    log_error "Docker is installed but not running"
    log_info "Start Docker: systemctl start docker"
    return 1
  fi
  
  log_success "Docker is available and running"
  
  # Check if API_SERVERS is configured
  if [[ -z "$API_SERVERS" ]]; then
    log_error "API_SERVERS environment variable is not set"
    log_info "Set API_SERVERS with comma-separated master nodes: export API_SERVERS='192.168.1.10:master1,192.168.1.11:master2'"
    return 1
  fi
  
  # Validate API_SERVERS format and count
  local server_count=$(echo "$API_SERVERS" | tr ',' '\n' | wc -l)
  if [[ $server_count -lt 2 ]]; then
    log_warning "Only $server_count API server configured - HA proxy is recommended for 2+ servers"
    log_info "Current API_SERVERS: $API_SERVERS"
  else
    log_success "Multiple API servers detected ($server_count servers) - HA setup recommended"
    log_info "API servers: $API_SERVERS"
  fi
  
  # Set default HA proxy port if not specified
  if [[ -z "$HA_PROXY_PORT" ]]; then
    export HA_PROXY_PORT=8443
    log_info "Using default HA proxy port: $HA_PROXY_PORT"
  else
    log_info "Using configured HA proxy port: $HA_PROXY_PORT"
  fi
  
  # Step 2: Stop existing proxy if running
  log_step "2" "Cleaning up existing HA proxy container"
  
  if docker ps -a --format "table {{.Names}}" | grep -q "master-proxy"; then
    log_substep "Stopping existing master-proxy container"
    docker stop master-proxy >/dev/null 2>&1 || true
    
    log_substep "Removing existing master-proxy container"
    docker rm master-proxy >/dev/null 2>&1 || true
    
    log_success "Existing proxy container cleaned up"
  else
    log_info "No existing proxy container found"
  fi
  
  # Step 3: Generate HAProxy configuration
  log_step "3" "Generating HAProxy configuration"
  
  log_substep "Creating HAProxy configuration file at /opt/haproxy.cfg"
  
  if ! cat <<EOF >/opt/haproxy.cfg
global
        log 127.0.0.1 local0
        log 127.0.0.1 local1 notice
        maxconn 4096
        maxpipes 1024
        daemon
        user haproxy
        group haproxy

defaults
        log global
        mode tcp
        option tcplog
        option dontlognull
        option redispatch
        retries 3
        timeout connect 5000ms
        timeout client 50000ms
        timeout server 50000ms
        
frontend kubernetes-apiserver
        bind *:$HA_PROXY_PORT
        mode tcp
        option tcplog
        default_backend kubernetes-apiserver

backend kubernetes-apiserver
        mode tcp
        balance roundrobin
        option tcp-check
        option tcplog
$(
    # Generate backend servers from API_SERVERS
    IFS=','
    counter=0
    for worker in $API_SERVERS; do
      oifs=$IFS
      IFS=':'
      read -r ip node <<<"$worker"
      counter=$((counter + 1))
      echo "        server $node $ip:6443 check fall 3 rise 2"
      IFS=$oifs
    done
    unset IFS
)
EOF
  then
    log_error "Failed to create HAProxy configuration file"
    return 1
  fi
  
  log_success "HAProxy configuration generated"
  
  # Display configuration summary
  log_info "Configuration summary:"
  log_substep "Frontend port: $HA_PROXY_PORT"
  log_substep "Backend servers:"
  
  IFS=','
  counter=0
  for worker in $API_SERVERS; do
    oifs=$IFS
    IFS=':'
    read -r ip node <<<"$worker"
    counter=$((counter + 1))
    log_substep "  $counter. $node ($ip:6443)"
    IFS=$oifs
  done
  unset IFS
  
  # Step 4: Pull HAProxy image
  log_step "4" "Pulling HAProxy Docker image"
  
  if ! docker pull haproxy:latest; then
    log_error "Failed to pull HAProxy Docker image"
    return 1
  fi
  
  log_success "HAProxy image pulled successfully"
  
  # Step 5: Start HAProxy container
  log_step "5" "Starting HAProxy container"
  
  log_substep "Running HAProxy container with host networking"
  
  if ! docker run -d --name master-proxy \
    --restart=unless-stopped \
    -v /opt/haproxy.cfg:/usr/local/etc/haproxy/haproxy.cfg:ro \
    --net=host \
    haproxy:latest; then
    log_error "Failed to start HAProxy container"
    return 1
  fi
  
  log_success "HAProxy container started successfully"
  
  # Step 6: Validate installation
  log_step "6" "Validating HA proxy installation"
  
  # Wait a moment for container to start
  sleep 3
  
  if validate_haproxy_installation; then
    log_success "HA proxy installation validation passed"
  else
    log_error "HA proxy installation validation failed"
    return 1
  fi
  
  # Show next steps
  show_haproxy_next_steps
  
  return 0
}

# HAProxy installation validation
validate_haproxy_installation() {
  log_substep "Checking HAProxy container status"
  if ! docker ps --format "table {{.Names}}\t{{.Status}}" | grep -q "master-proxy.*Up"; then
    log_error "HAProxy container is not running"
    docker logs master-proxy 2>&1 | tail -5 | while read line; do
      log_error "Container log: $line"
    done
    return 1
  fi
  
  log_substep "Checking HAProxy port binding"
  if ! netstat -tlnp 2>/dev/null | grep -q ":$HA_PROXY_PORT.*LISTEN" && ! ss -tlnp 2>/dev/null | grep -q ":$HA_PROXY_PORT.*LISTEN"; then
    log_error "HAProxy is not listening on port $HA_PROXY_PORT"
    return 1
  fi
  
  log_substep "Testing HAProxy configuration"
  if ! docker exec master-proxy haproxy -c -f /usr/local/etc/haproxy/haproxy.cfg >/dev/null 2>&1; then
    log_error "HAProxy configuration validation failed"
    return 1
  fi
  
  log_substep "Checking backend server connectivity"
  local healthy_backends=0
  local total_backends=0
  
  IFS=','
  for worker in $API_SERVERS; do
    oifs=$IFS
    IFS=':'
    read -r ip node <<<"$worker"
    total_backends=$((total_backends + 1))
    
    if timeout 5 nc -z "$ip" 6443 2>/dev/null; then
      log_substep "  Backend $node ($ip:6443): ${EMOJI_SUCCESS} Reachable"
      healthy_backends=$((healthy_backends + 1))
    else
      log_substep "  Backend $node ($ip:6443): ${EMOJI_WARNING} Not reachable (may not be ready yet)"
    fi
    IFS=$oifs
  done
  unset IFS
  
  if [[ $healthy_backends -eq 0 ]]; then
    log_warning "No backend servers are currently reachable"
    log_info "This is normal if Kubernetes masters are not yet installed"
  else
    log_success "$healthy_backends out of $total_backends backend servers are reachable"
  fi
  
  return 0
}

# Show HAProxy next steps
show_haproxy_next_steps() {
  log_next_steps "HA Proxy Installation Complete" \
    "Check proxy status: docker ps | grep master-proxy" \
    "View proxy logs: docker logs master-proxy" \
    "Test connectivity: curl -k https://localhost:$HA_PROXY_PORT/healthz" \
    "Install Kubernetes masters using HA endpoint" \
    "Configure kubectl to use HA endpoint: https://localhost:$HA_PROXY_PORT"
  
  log_urls "HA Proxy Access Points" \
    "Load Balancer Endpoint: https://localhost:$HA_PROXY_PORT" \
    "HAProxy Stats: http://localhost:$HA_PROXY_PORT/stats (if enabled)" \
    "Configuration File: /opt/haproxy.cfg"
  
  log_credentials "HA Proxy Management" "docker" \
    "Container name: master-proxy" \
    "Restart: docker restart master-proxy" \
    "Stop: docker stop master-proxy"
  
  log_info "HA proxy is now load balancing across $(($(echo "$API_SERVERS" | tr ',' '\n' | wc -l))) Kubernetes API servers"
  
  log_troubleshooting "HA Proxy" \
    "Check container logs: docker logs master-proxy" \
    "Verify port availability: netstat -tlnp | grep $HA_PROXY_PORT" \
    "Test backend connectivity: nc -z <backend-ip> 6443" \
    "Validate configuration: docker exec master-proxy haproxy -c -f /usr/local/etc/haproxy/haproxy.cfg" \
    "Restart proxy: docker restart master-proxy"
  
  echo
  log_header "Kubernetes Installation with HA" "Ready for Multi-Master Setup"
  
  echo -e "${COLOR_BRIGHT_GREEN}${COLOR_BOLD}Your HA proxy is ready! Next steps:${COLOR_RESET}"
  echo
  echo -e "${COLOR_CYAN}1. ${COLOR_BOLD}Install first Kubernetes master:${COLOR_RESET}"
  echo -e "   ${COLOR_DIM}Use the HA proxy endpoint for --control-plane-endpoint${COLOR_RESET}"
  echo
  echo -e "${COLOR_CYAN}2. ${COLOR_BOLD}Join additional masters:${COLOR_RESET}"
  echo -e "   ${COLOR_DIM}Each additional master will connect through the HA proxy${COLOR_RESET}"
  echo
  echo -e "${COLOR_CYAN}3. ${COLOR_BOLD}Configure kubectl:${COLOR_RESET}"
  echo -e "   ${COLOR_DIM}Point kubectl to https://localhost:$HA_PROXY_PORT${COLOR_RESET}"
  echo
}

startKubelet() {
  systemctl stop kubelet
  systemctl start kubelet
}

startHa() {
  docker stop master-proxy
  docker rm master-proxy
  docker run -d --name master-proxy \
    -v /opt/haproxy.cfg:/usr/local/etc/haproxy/haproxy.cfg:ro \
    --net=host haproxy
}

disableSwap() {
  sudo sed -i '/ swap / s/^\(.*\)$/#\1/g' /etc/fstab
  sudo swapoff -a
}

hostSecret() {
  openssl genrsa -out ${APP_HOST}.key 4096
  openssl req -new -key ${APP_HOST}.key -out ${APP_HOST}.csr -subj "/CN=${APP_HOST}" \
    -addext "subjectAltName = DNS:${APP_HOST}"
  openssl x509 -req -in ${APP_HOST}.csr -CA /etc/kubernetes/pki/ca.crt -CAkey /etc/kubernetes/pki/ca.key -CAcreateserial -out ${APP_HOST}.crt -days 7200

  kubectl create secret tls appingress-certificate --key ${APP_HOST}.key --cert ${APP_HOST}.crt -n default
}

dashboardInst() {
  helm repo add kubernetes-dashboard https://kubernetes.github.io/dashboard/
  kubectl delete ns kubernetes-dashboard
  helm install kubernetes-dashboard \
    kubernetes-dashboard/kubernetes-dashboard \
    --namespace kubernetes-dashboard \
    --create-namespace \
    -f "${MOUNT_PATH}"/kubernetes/install_k8s/dashboard/values2.yaml
}

prometheusGrafanaReset() {
  helm -n monitoring delete monitoring
  kubectl delete ns monitoring
  helm -n db delete postgres
  kubectl delete ns db
}

emptyLocalFsStorage() {
  local service=$1
  local pvName=$2
  local scName=$3
  local volumePath=$4
  local namespace=$5

  if [[ -n $namespace ]]; then
    kubectl delete pvc --all -n $namespace
  fi

  kubectl delete pv $pvName
  kubectl delete sc $scName
  rm -rf $volumePath
}

createLocalStorageClassAndPV() {
  local storageClassName=$1
  local pvName=$2
  local volumePath=$3

  cat << EOF | kubectl apply -f -
kind: StorageClass
apiVersion: storage.k8s.io/v1
metadata:
  name: ${storageClassName}
provisioner: kubernetes.io/no-provisioner
volumeBindingMode: WaitForFirstConsumer
EOF

  mkdir -p "${volumePath}"
  chmod 777 "${volumePath}"

  cat << EOF | kubectl apply -f -
apiVersion: v1
kind: PersistentVolume
metadata:
  name: ${pvName}
spec:
  capacity:
    storage: 10Gi
  accessModes:
  - ReadWriteOnce
  persistentVolumeReclaimPolicy: Retain
  storageClassName: ${storageClassName}
  local:
    path: ${volumePath}
  nodeAffinity:
    required:
      nodeSelectorTerms:
      - matchExpressions:
        - key: kubernetes.io/hostname
          operator: In
          values:
          - master.cloud.com
EOF
}

prometheusGrafanaResetv2(){
  helm -n monitoring delete prometheus
  helm -n monitoring delete grafana
  kubectl delete ns monitoring

}

prometheusGrafanaInstv2(){
  CLIENT_SECRET=$(dataFromSecret oauth-secrets kube-system OIDC_CLIENT_SECRET)
  CLIENT_ID=$(dataFromSecret oauth-secrets kube-system OIDC_CLIENT_ID)
  URL=$(fullDefaultUrl)
  OAUTH2_HOST=$(fullKeycloakUrl)
  REALM=$(dataFromSecret oauth-secrets kube-system OIDC_REALM)

  kubectl create ns monitoring
  kubectl create secret generic kube-prometheus-stack-grafana-oauth \
    --from-literal GF_AUTH_KEYCLOAK_CLIENT_ID="${CLIENT_ID}" \
    --from-literal GF_AUTH_KEYCLOAK_CLIENT_SECRET="${CLIENT_SECRET}" \
    --from-literal=OAUTH_AUTH_URL="https://${OAUTH2_HOST}/realms/${REALM}/protocol/openid-connect/auth" \
    --from-literal=OAUTH_TOKEN_URL="https://${OAUTH2_HOST}/realms/${REALM}/protocol/openid-connect/token" \
    --from-literal=OAUTH_API_URL="https://${OAUTH2_HOST}/realms/${REALM}/protocol/openid-connect/userinfo" \
    --from-literal=GRAFANA_DOMAIN="${URL}" \
    --from-literal=GRAFANA_ROOT_URL="https://${URL}/grafana" \
    --namespace monitoring

  kubectl create configmap -n monitoring env-data \
    --from-literal=OAUTH_AUTH_URL="https://${OAUTH2_HOST}/realms/${REALM}/protocol/openid-connect/auth" \
    --from-literal=OAUTH_TOKEN_URL="https://${OAUTH2_HOST}/realms/${REALM}/protocol/openid-connect/token" \
    --from-literal=OAUTH_API_URL="https://${OAUTH2_HOST}/realms/${REALM}/protocol/openid-connect/userinfo" \
    --from-literal=GRAFANA_DOMAIN="${URL}" \
    --from-literal=GRAFANA_ROOT_URL="https://${URL}/grafana"

  kubectl get secrets -n cert-manager gokselfsign-ca -o json | jq -r '.data["tls.crt"]' | base64 -d > ~/issuer.crt
  kubectl get secrets -n keycloak keycloak-gokcloud-com -o json | jq -r '.data["tls.crt"]' | base64 -d > ~/keycloak.crt
  kubectl create configmap certs-configmap -n monitoring --from-file=/root/issuer.crt --from-file=/root/keycloak.crt
  rm /root/issuer.crt
  rm /root/keycloak.crt
  helm repo add prometheus-community https://prometheus-community.github.io/helm-charts
  helm repo add grafana https://grafana.github.io/helm-charts
  helm repo update
  helm install prometheus prometheus-community/prometheus \
    --set server.extraFlags=\{web.enable-lifecycle,web.route-prefix=/,web.external-url=prometheus\} \
    --values $MOUNT_PATH/kubernetes/install_k8s/prometheus-grafana/prometheus-values.yaml \
    --namespace monitoring \
    --create-namespace

  helm install grafana grafana/grafana \
    --set grafana.ini.server.root_url=https://$URL/grafana \
    --values $MOUNT_PATH/kubernetes/install_k8s/prometheus-grafana/grafana-values.yaml \
    --namespace monitoring

  kubectl -n kube-system get cm kube-proxy -o yaml | sed 's/metricsBindAddress: ""/metricsBindAddress: 0.0.0.0:10249/' | kubectl apply -f -

  kubectl -n kube-system patch ds kube-proxy -p \
      '{"spec":{"template":{"metadata":{"labels":{"updateTime":"'`date +'%s'`'"}}}}}'

}

prometheusGrafanaInst() {

  helm repo add prometheus-community \
    https://prometheus-community.github.io/helm-charts
  helm repo update
  helm install monitoring \
    prometheus-community/kube-prometheus-stack \
    --values https://github.com/sumitmaji/kubernetes/raw/master/install_k8s/prometheus-grafana/values.yaml \
    --version 39.6.0 \
    --namespace monitoring \
    --create-namespace

#  kubectl -n kube-system get cm kube-proxy-config -o yaml | sed \
#    's/metricsBindAddress: 127.0.0.1:10249/metricsBindAddress: 0.0.0.0:10249' \
#    kubectl apply -f -

  kubectl -n kube-system get cm kube-proxy -o yaml | sed 's/metricsBindAddress: ""/metricsBindAddress: 0.0.0.0:10249/' | kubectl apply -f -

  kubectl -n kube-system patch ds kube-proxy -p \
    '{"spec":{"template":{"metadata":{"labels":{"updateTime":"'`date +'%s'`'"}}}}}'

  helm repo add bitnami https://charts.bitnami.com/bitnami
  helm repo update
  helm install postgres \
    bitnami/postgresql \
    --values https://github.com/sumitmaji/kubernetes/raw/master/install_k8s/prometheus-grafana/postgres-values.yaml \
    --version 11.7.1 \
    --namespace db \
    --create-namespace

}

dashboardReset() {
  helm uninstall kubernetes-dashboard -n kubernetes-dashboard
  kubectl delete ns kubernetes-dashboard
}

#This creates a cluster-role-binding for admin user
adminRole() {
  WC=$(kubectl get clusterrolebinding cloud-cluster-admin 2>/dev/null | wc -l)
  if [ "$WC" == "0" ]; then

    cat <<EOF | kubectl apply -f -
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: cloud-cluster-admin
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: cluster-admin
subjects:
- apiGroup: rbac.authorization.k8s.io
  kind: Group
  name: cloud:masters
EOF
  fi

}

#Creating user role developers which would allow users authenticated with oauth and
#having developers role to connect with cluster
oauthDev(){
  cat <<EOF | kubectl create -f -
kind: Role
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  namespace: default
  name: pod-reader
rules:
- apiGroups: [""]
  resources: ["pods"]
  verbs: ["get", "watch", "list", "create", "update", "patch", "delete"]
- apiGroups: ["extensions", "apps"]
  resources: ["deployments"]
  verbs: ["get", "list", "watch", "create", "update", "patch", "delete"]
---
kind: RoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: read-pods
  namespace: default
subjects:
- kind: Group
  name: developers
  apiGroup: rbac.authorization.k8s.io
roleRef:
  kind: Role
  name: pod-reader
  apiGroup: rbac.authorization.k8s.io
EOF
}

# Run kubectl command on pod
runKubectlOnPod(){

  # The container will run will priveledges that are provided in the service account
  cat <<EOF | kubectl apply -f -
apiVersion: v1
kind: ServiceAccount
metadata:
  name: internal-deployer
  namespace: default
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: internal-deployer
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: cluster-admin
subjects:
  - kind: ServiceAccount
    name: internal-deployer
    namespace: default
EOF
  # Execute the container as job as it will execute the kubectl command and exit
  cat <<EOF | kubectl apply -f -
apiVersion: batch/v1
kind: Job
metadata:
  name: kubectl-executor
  namespace: default
spec:
  template:
    metadata:
      name: kubectl-executor
    spec:
      # The service account that will be used to run the container
      serviceAccountName: internal-deployer
      containers:
      - name: kubectl
        image: bitnami/kubectl:latest
        args: ["cluster-info"]
      restartPolicy: Never
EOF
}

#Creating user role administrators which would allow users authenticated with oauth and
#having administrators role to connect with cluster
oauthAdmin(){
  cat <<EOF | kubectl create -f -
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: oauth-cluster-admin
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: cluster-admin
subjects:
- apiGroup: rbac.authorization.k8s.io
  kind: Group
  name: administrators
EOF
}

#Create kubeconfig file for oauth user used with kube-login
oauthUser(){
  kubectl config set-cluster cloud.com --certificate-authority=/etc/kubernetes/pki/ca.crt --embed-certs=true --server="$(apiUrl)" --kubeconfig=/root/oauth.conf
  kubectl config --kubeconfig=/root/oauth.conf set-context oauthuser@cloud.com --cluster=cloud.com --user=oauthuser
  kubectl config --kubeconfig=/root/oauth.conf use-context oauthuser@cloud.com

  echoSuccess "OAuth kubeconfig file create in /root/oauth.conf"
  echoSuccess "Use below command to use oauth.conf"
  echoSuccess "kubectl --kubeconfig=/root/oauth.conf --token=__USER_TOKEN__ rest of command"
  echoSuccess "alias kctl='kubectl --kubeconfig=/root/oauth.conf --token=\${__USER_TOKEN__}'"
  echoSuccess "alias kcd='kctl config set-context \$(kctl config current-context) --namespace'"
}

#This gives api server url
apiUrl(){
  kubectl config view -o json | jq -r '.clusters[] | .cluster.server'
}

opensearchReset(){
  helm uninstall opensearch -n opensearch
  emptyLocalFsStorage "Opensearch" "opensearch-pv" "opensearch-storage" "/data/volumes/pv5" "opensearch"
  kubectl delete ns opensearch
}

jenkinsReset(){
  helm uninstall jenkins -n jenkins
  emptyLocalFsStorage "Jenkins" "jenkins-pv" "jenkins-storage" "/data/volumes/jenkins"
  kubectl delete ns jenkins
}

opensearchDashReset(){
  helm uninstall opensearch-dashboard -n opensearch
}

opensearchDashInst(){
  helm repo add openSearch https://opensearch-project.github.io/helm-charts/
  helm repo update
  helm install opensearch-dashboard \
      --namespace opensearch \
      --values https://github.com/sumitmaji/kubernetes/raw/master/install_k8s/opensearch/values-dashboard.yaml \
      openSearch/opensearch-dashboards

  gok patch ingress opensearch-dashboard-opensearch-dashboards opensearch letsencrypt opensearch
  echo "Waiting for dashboard service to be up!!!!"
    kubectl --timeout=240s wait --for=condition=Ready pods --all --namespace opensearch
    [[ $? -eq 0 ]] && echoSuccess "Opensearch dashboard service now up!\n You can access the service using https://opensearch.gokcloud.com" || echoFailed "Opensearch dashboard service timed-out, plaese check!!"
}


callJenkinsApi(){

  # Jenkins API URL
  JENKINS_URL="https://jenkins.gokcloud.com/api/xml"
  QUERY_PARAMS="tree=jobs%5Bname%2ClastBuild%5Bactions%5BfailCount%2CskipCount%2CtotalCount%2CurlName%5D%2Cduration%2Cnumber%2Ctimestamp%2Cresult%2Cbuilding%2Curl%5D%2Cjobs%5Bname%2ClastBuild%5Bactions%5BfailCount%2CskipCount%2CtotalCount%2CurlName%5D%2Cduration%2Cnumber%2Ctimestamp%2Cresult%2Cbuilding%2Curl%5D%2Cjobs%5Bname%2ClastBuild%5Bactions%5BfailCount%2CskipCount%2CtotalCount%2CurlName%5D%2Cduration%2Cnumber%2Ctimestamp%2Cresult%2Cbuilding%2Curl%5D%2Cjobs%5Bname%2ClastBuild%5Bactions%5BfailCount%2CskipCount%2CtotalCount%2CurlName%5D%2Cduration%2Cnumber%2Ctimestamp%2Cresult%2Cbuilding%2Curl%5D%2Cjobs%5Bname%2ClastBuild%5Bactions%5BfailCount%2CskipCount%2CtotalCount%2CurlName%5D%2Cduration%2Cnumber%2Ctimestamp%2Cresult%2Cbuilding%2Curl%5D%2Cjobs%5Bname%2ClastBuild%5Bactions%5BfailCount%2CskipCount%2CtotalCount%2CurlName%5D%2Cduration%2Cnumber%2Ctimestamp%2Cresult%2Cbuilding%2Curl%5D%2Cjobs%5Bname%2ClastBuild%5Bactions%5BfailCount%2CskipCount%2CtotalCount%2CurlName%5D%2Cduration%2Cnumber%2Ctimestamp%2Cresult%2Cbuilding%2Curl%5D%2Cjobs%5Bname%2ClastBuild%5Bactions%5BfailCount%2CskipCount%2CtotalCount%2CurlName%5D%2Cduration%2Cnumber%2Ctimestamp%2Cresult%2Cbuilding%2Curl%5D%2Cjobs%5Bname%2ClastBuild%5Bactions%5BfailCount%2CskipCount%2CtotalCount%2CurlName%5D%2Cduration%2Cnumber%2Ctimestamp%2Cresult%2Cbuilding%2Curl%5D%5D%5D%5D%5D%5D%5D%5D%5D%5D"
  EXCLUDE_PARAMS="exclude=%2F*%2F*%2F*%2Faction%5Bnot%28totalCount%29%5D"

  # Jenkins credentials
  JENKINS_USER=$(promptUserInput "Enter Jenkins Username (skmaji1): " "skmaji1")
  JENKINS_API_TOKEN=$(promptSecret "Enter Jenkins API Token(Check README.md for steps): ")

  # Make the API call
  response=$(curl -s -u "$JENKINS_USER:$JENKINS_API_TOKEN" "$JENKINS_URL?$QUERY_PARAMS&$EXCLUDE_PARAMS")

  # Check if the response is empty
  if [ -z "$response" ]; then
    echo "Failed to fetch data from Jenkins API. Please check your credentials or URL."
    return 1
  fi

  # Print the response
  echo "Jenkins API Response:"
  echo "$response"
}

createExampleJenkinsPipeline() {
  # Prompt for Jenkins username and API token
  JENKINS_USER=$(promptUserInput "Enter Jenkins Username (skmaji1): " "skmaji1")
  JENKINS_API_TOKEN=$(promptSecret "Enter Jenkins API Token(Check README.md for steps): ")

  # Prompt for Jenkins URL and pipeline details
  JENKINS_URL=$(promptUserInput "Enter Jenkins URL (https://jenkins.gokcloud.com): " "https://jenkins.gokcloud.com")
  PIPELINE_NAME=$(promptUserInput "Enter Pipeline Name(Kaniko-Pipeline): " "Kaniko-Pipeline")

  pushd "$MOUNT_PATH/kubernetes/install_k8s/jenkins"
  # Make the API call to create the pipeline
  RESPONSE=$(curl -s -o /dev/null -w "%{http_code}" -X POST "$JENKINS_URL/createItem?name=$PIPELINE_NAME" \
    --user "$JENKINS_USER:$JENKINS_API_TOKEN" \
    -H "Content-Type: application/xml" \
    --data-binary @pipeline-config.xml)

  popd
  # Check the response
  if [ "$RESPONSE" -eq 200 ]; then
    echoSuccess "Pipeline '$PIPELINE_NAME' created successfully!"
  else
    echoFailed "Failed to create pipeline. HTTP Status Code: $RESPONSE"
  fi
}


createJenkinsPipeline() {
  # Prompt for Jenkins username and API token
  JENKINS_USER=$(promptUserInput "Enter Jenkins Username(skmaji1): " "skmaji1")
  JENKINS_API_TOKEN=$(promptSecret "Enter Jenkins API Token: ")

  # Prompt for Jenkins URL and pipeline details
  JENKINS_URL=$(promptUserInput "Enter Jenkins URL (e.g., https://jenkins.gokcloud.com): " "https://jenkins.gokcloud.com")
  PIPELINE_NAME=$(promptUserInput "Enter Pipeline Name: " "Git-Pipeline")
  GIT_REPO_URL=$(promptUserInput "Enter Git Repository URL (e.g., https://github.com/your-username/your-repo.git): ")
  GIT_BRANCH=$(promptUserInput "Enter Git Branch (e.g., main): " "main")
  CREDENTIALS_ID=$(promptUserInput "Enter Jenkins Credentials ID: ")

  # Create the pipeline configuration XML
  cat <<EOF > pipeline-config.xml
<flow-definition plugin="workflow-job">
  <description>Pipeline created automatically to refer to Jenkinsfile in Git</description>
  <keepDependencies>false</keepDependencies>
  <definition class="org.jenkinsci.plugins.workflow.cps.CpsScmFlowDefinition" plugin="workflow-cps">
    <scm class="hudson.plugins.git.GitSCM" plugin="git">
      <configVersion>2</configVersion>
      <userRemoteConfigs>
        <hudson.plugins.git.UserRemoteConfig>
          <url>${GIT_REPO_URL}</url>
          <credentialsId>${CREDENTIALS_ID}</credentialsId>
        </hudson.plugins.git.UserRemoteConfig>
      </userRemoteConfigs>
      <branches>
        <hudson.plugins.git.BranchSpec>
          <name>*/${GIT_BRANCH}</name>
        </hudson.plugins.git.BranchSpec>
      </branches>
    </scm>
    <scriptPath>Jenkinsfile</scriptPath>
    <lightweight>true</lightweight>
  </definition>
  <triggers/>
</flow-definition>
EOF

  # Make the API call to create the pipeline
  RESPONSE=$(curl -s -o /dev/null -w "%{http_code}" -X POST "$JENKINS_URL/createItem?name=$PIPELINE_NAME" \
    --user "$JENKINS_USER:$JENKINS_API_TOKEN" \
    -H "Content-Type: application/xml" \
    --data-binary @pipeline-config.xml)

  # Check the response
  if [ "$RESPONSE" -eq 200 ]; then
    echoSuccess "Pipeline '$PIPELINE_NAME' created successfully!"
  else
    echoFailed "Failed to create pipeline. HTTP Status Code: $RESPONSE"
  fi

  # Clean up the temporary XML file
  rm -f pipeline-config.xml
}

jenkinsInst() {
  helm repo add jenkins https://charts.jenkins.io
  helm repo update
  kubectl create ns jenkins

  kubectl create configmap jenkins-logging-config \
  --from-file=${MOUNT_PATH}/kubernetes/install_k8s/jenkins/logging.properties \
  -n jenkins

  ADMIN_PASSWORD=$(promptSecret "Enter Jenkins Admin Password: ")
  kubectl create secret generic jenkins-admin-password \
    --from-literal=jenkins-admin-password="${ADMIN_PASSWORD}" -n jenkins

  CLIENT_SECRET=$(dataFromSecret oauth-secrets kube-system OIDC_CLIENT_SECRET)
  CLIENT_ID=$(dataFromSecret oauth-secrets kube-system OIDC_CLIENT_ID)
  keycloakUrl="https://$(fullKeycloakUrl)"
  REALM=$(dataFromSecret oauth-secrets kube-system OAUTH_REALM)


  kubectl create secret generic oic-auth \
  --from-literal=clientID="${CLIENT_ID}" \
  --from-literal=clientSecret="${CLIENT_SECRET}" \
  --from-literal=keycloakUrl=${keycloakUrl} \
  --from-literal=realm=${REALM} \
  --namespace jenkins
  DOCKER_BUILD_ENABLED=true
  if [ "$DOCKER_BUILD_ENABLED" == "true" ]; then
    # kubectl create secret generic kaniko-docker-config \
    #   --from-file=/root/.docker/config.json \
    #   -n jenkins

    kubectl create secret generic docker-credentials \
    --from-file=.dockerconfigjson=/root/.docker/config.json \
    --type=kubernetes.io/dockerconfigjson \
    -n jenkins

    kubectl create secret generic registry-credentials \
    --from-file=config.json=/root/.docker/config.json \
    -n jenkins
  fi

  helm install jenkins \
    --namespace jenkins \
    --set dockerBuildEnabled=$DOCKER_BUILD_ENABLED \
    --values $MOUNT_PATH/kubernetes/install_k8s/jenkins/values-mod.yaml \
    jenkins/jenkins

  createLocalStorageClassAndPV "jenkins-storage" "jenkins-pv" "/data/volumes/jenkins"
  gok patch ingress jenkins jenkins letsencrypt jenkins
  echo "Waiting for Jenkins services to be up!!!!"
  kubectl --timeout=120s wait --for=condition=Ready pods --all --namespace jenkins
  [[ $? -eq 0 ]] && echoSuccess "Jenkins services are now up!" || echoFailed "Jenkins services timed-out, please check!!"
  checkCurl curl -XGET http://jenkins.jenkins.svc:8080 -u "admin:${ADMIN_PASSWORD}" --insecure
}

opensearchInst(){
  helm repo add openSearch https://opensearch-project.github.io/helm-charts/
  helm repo update
  kubectl create ns opensearch
  ADMIN_PASSWORD=$(promptSecret "Enter Admin Password: ")
  kubectl create secret generic opensearch-password \
        --from-literal=OPENSEARCH_INITIAL_ADMIN_PASSWORD="${ADMIN_PASSWORD}" -n opensearch
  helm install opensearch \
    --namespace opensearch \
    --create-namespace \
    --values $MOUNT_PATH/kubernetes/install_k8s/opensearch/values.yaml \
    openSearch/opensearch

  createLocalStorageClassAndPV "opensearch-storage" "opensearch-pv" "/data/volumes/pv5"
  echo "Waiting for services to be up!!!!"
  kubectl --timeout=120s wait --for=condition=Ready pods --all --namespace opensearch
  [[ $? -eq 0 ]] && echoSuccess "Opensearch services are now up!" || echoFailed "Opensearch services timed-out, plaese check!!"
  checkCurl curl -XGET https://opensearch-cluster-master.opensearch.svc:9200 -u "admin:${ADMIN_PASSWORD}" --insecure
  #gok patch ingress opensearch opensearch letsencrypt opensearch
  opensearchSecret $ADMIN_PASSWORD
}

fluentdReset(){
  helm uninstall fluentd -n fluentd
  kubectl delete ns fluentd
}

fluentdInst(){
  helm repo add fluent https://fluent.github.io/helm-charts
  helm repo update
  kubectl create ns fluentd

  # Workaround for now before permanent solution is identified, Begin
  ADMIN_PASSWORD=$(kubectl get secret opensearch-secrets -n kube-system -o=jsonpath='{.data.OPENSEARCH_INITIAL_ADMIN_PASSWORD}' | base64 -d)
  cat $MOUNT_PATH/kubernetes/install_k8s/fluentd/values.yaml | \
    sed "s|__PASSWORD__|${ADMIN_PASSWORD}|g" > $MOUNT_PATH/kubernetes/install_k8s/fluentd/values_temp.yaml
  # End

  helm install fluentd \
        --namespace fluentd \
        --create-namespace \
        --values $MOUNT_PATH/kubernetes/install_k8s/fluentd/values_temp.yaml \
        fluent/fluentd
  rm $MOUNT_PATH/kubernetes/install_k8s/fluentd/values_temp.yaml
  gok patch ingress fluentd fluentd letsencrypt fluentd
}

resetDockerRegistry(){
  helm uninstall registry -n registry

  emptyLocalFsStorage "Registry" "registry-pv" "registry-storage" "/data/volumes/pv4" "registry"
  kubectl delete ns registry
}

# createDevWorkspace and deleteDevWorkspace methods to prompt for namespace, username, workspace name, and manifest file, then call apply_devworkspace.py for create/delete actions.
createDevWorkspace() {
  echo "=== Create Che DevWorkspace ==="
  NAMESPACE=$(promptUserInput "Enter namespace (che-user): " "che-user")
  USERNAME=$(promptUserInput "Enter username (user1): " "user1")
  WORKSPACE=$(promptUserInput "Enter workspace name (devworkspace1): " "devworkspace1")
  MANIFEST_FILE=$(promptUserInput "Enter devworkspace manifest file path (devworkspace.yaml): " "devworkspace.yaml")
  export CHE_USER_NAMESPACE="$NAMESPACE"
  export CHE_USER_NAME="$USERNAME"
  export CHE_WORKSPACE_NAME="$WORKSPACE"
  export DW_FILE="$MANIFEST_FILE"
  export DW_DELETE="false"
  pushd "$MOUNT_PATH/kubernetes/install_k8s/eclipseche" || exit
  if ! python3 -c "import kubernetes" &>/dev/null; then
    apt-get install -y python3-kubernetes
  else
    echo "python3-kubernetes is already installed."
  fi

  if ! python3 -c "import yaml" &>/dev/null; then
    apt-get install -y python3-yaml
  else
    echo "python3-yaml is already installed."
  fi
  python3 "$WORKING_DIR/eclipseche/apply_devworkspace.py"
  popd || exit
}

deleteDevWorkspace() {
  echo "=== Delete Che DevWorkspace ==="
  NAMESPACE=$(promptUserInput "Enter namespace: " "che-user")
  USERNAME=$(promptUserInput "Enter username: " "user1")
  WORKSPACE=$(promptUserInput "Enter workspace name: " "devworkspace1")
  MANIFEST_FILE=$(promptUserInput "Enter devworkspace manifest file path: " "devworkspace.yaml")
  export CHE_USER_NAMESPACE="$NAMESPACE"
  export CHE_USER_NAME="$USERNAME"
  export CHE_WORKSPACE_NAME="$WORKSPACE"
  export DW_FILE="$MANIFEST_FILE"
  export DW_DELETE="true"
  pushd "$MOUNT_PATH/kubernetes/install_k8s/eclipseche" || exit
  python3 "$WORKING_DIR/eclipseche/apply_devworkspace.py"
  popd || exit
}

createDevWorkspaceV2() {
  echo "=== Create Che DevWorkspace ==="
  USERNAME=$(promptUserInput "Enter username (user1): " "user1")
  
  echo "Select workspace type:"
  echo "1 => core-java"
  echo "2 => spring-web"
  echo "3 => python-web"
  echo "4 => springboot-backend"
  echo "5 => tensorflow"
  echo "6 => microservice-study"
  echo "7 => javaparser"
  echo "8 => nlp"
  echo "9 => kubeauthentication"

  WORKSPACE_TYPE_INDEX=$(promptUserInput "Enter workspace type index (1): " "1")
  case "$WORKSPACE_TYPE_INDEX" in
    1) WORKSPACE_TYPE="core-java" ;;
    2) WORKSPACE_TYPE="springboot-web" ;;
    3) WORKSPACE_TYPE="python-web" ;;
    4) WORKSPACE_TYPE="springboot-backend" ;;
    5) WORKSPACE_TYPE="tensorflow" ;;
    6) WORKSPACE_TYPE="microservice-study" ;;
    7) WORKSPACE_TYPE="javaparser" ;;
    8) WORKSPACE_TYPE="nlp" ;;
    9) WORKSPACE_TYPE="kubeauthentication";;
    *) WORKSPACE_TYPE="core-java" ;;
  esac

  case "$WORKSPACE_TYPE" in
    core-java) WORKSPACE="java" ;;
    springboot-web) WORKSPACE="spring" ;;
    python-web) WORKSPACE="python" ;;
    springboot-backend) WORKSPACE="spring" ;;
    tensorflow) WORKSPACE="tensorflow" ;;
    microservice-study) WORKSPACE="microservice-study" ;;
    javaparser) WORKSPACE="javaparser" ;;
    nlp) WORKSPACE="nlp" ;;
    kubeauthentication) WORKSPACE="kubeauthentication" ;;
    *) WORKSPACE="java" ;;
  esac

  export CHE_USER_NAMESPACE="$USERNAME"
  export CHE_USER_NAME="$USERNAME"
  export CHE_WORKSPACE_NAME="$WORKSPACE"
  export WORKSPACE_TYPE="$WORKSPACE_TYPE"
  export DW_DELETE="false"
  pushd "$MOUNT_PATH/kubernetes/install_k8s/eclipseche" || exit
  if ! python3 -c "import kubernetes" &>/dev/null; then
    apt-get install -y python3-kubernetes
  else
    echo "python3-kubernetes is already installed."
  fi

  if ! python3 -c "import yaml" &>/dev/null; then
    apt-get install -y python3-yaml
  else
    echo "python3-yaml is already installed."
  fi
  python3 "$WORKING_DIR/eclipseche/create_devworkspace.py"
  popd || exit
}

deleteDevWorkspaceV2() {
  echo "=== Delete Che DevWorkspace ==="
  USERNAME=$(promptUserInput "Enter username (user1): " "user1")
  
  echo "Select workspace type:"
  echo "1 => core-java"
  echo "2 => springboot-web"
  echo "3 => python-web"
  echo "4 => springboot-backend"
  echo "5 => tensorflow"
  echo "6 => microservice-study"
  echo "7 => javaparser"
  echo "8 => nlp"
  echo "9 => kubeauthentication"
  WORKSPACE_TYPE_INDEX=$(promptUserInput "Enter workspace type index (1): " "1")
  case "$WORKSPACE_TYPE_INDEX" in
    1) WORKSPACE_TYPE="core-java" ;;
    2) WORKSPACE_TYPE="springboot-web" ;;
    3) WORKSPACE_TYPE="python-web" ;;
    4) WORKSPACE_TYPE="springboot-backend" ;;
    5) WORKSPACE_TYPE="tensorflow" ;;
    6) WORKSPACE_TYPE="microservice-study" ;;
    7) WORKSPACE_TYPE="javaparser" ;;
    8) WORKSPACE_TYPE="nlp" ;;
    9) WORKSPACE_TYPE="kubeauthentication" ;;
    *) WORKSPACE_TYPE="core-java" ;;
  esac

  case "$WORKSPACE_TYPE" in
    core-java) WORKSPACE="java" ;;
    springboot-web) WORKSPACE="spring" ;;
    python-web) WORKSPACE="python" ;;
    springboot-backend) WORKSPACE="spring" ;;
    tensorflow) WORKSPACE="tensorflow" ;;
    nlp) WORKSPACE="nlp" ;;
    microservice-study) WORKSPACE="microservice-study" ;;
    javaparser) WORKSPACE="javaparser" ;;
    kubeauthentication) WORKSPACE="kubeauthentication" ;;
    *) WORKSPACE="java" ;;
  esac

  export CHE_USER_NAMESPACE="$USERNAME"
  export CHE_USER_NAME="$USERNAME"
  export CHE_WORKSPACE_NAME="$WORKSPACE"
  export WORKSPACE_TYPE="$WORKSPACE_TYPE"
  export DW_DELETE="true"
  pushd "$MOUNT_PATH/kubernetes/install_k8s/eclipseche" || exit
  python3 "$WORKING_DIR/eclipseche/create_devworkspace.py"
  popd || exit
}

# https://itnext.io/error-x509-certificate-signed-by-unknown-authority-error-is-returned-f0e5d436f467
# Generate User & Password
genRegistryPassword(){
  export REGISTRY_USER="$1"
  export REGISTRY_PASS="$2"
  export DESTINATION_FOLDER=./registry-creds

  # Backup credentials to local files (in case you'll forget them later on)
  mkdir -p ${DESTINATION_FOLDER}
  echo ${REGISTRY_USER} > ${DESTINATION_FOLDER}/registry-user.txt
  echo ${REGISTRY_PASS} > ${DESTINATION_FOLDER}/registry-pass.txt

  docker run --entrypoint htpasswd registry:2.7.0 \
      -Bbn ${REGISTRY_USER} ${REGISTRY_PASS} \
      > ${DESTINATION_FOLDER}/htpasswd

  unset REGISTRY_USER REGISTRY_PASS DESTINATION_FOLDER
}

imagePullSecrets(){
  # Create a secret for the registry
  : "${DOCKER_USERNAME:=$(promptUserInput "Please enter docker user id: ")}"
  : "${DOCKER_PASSWORD:=$(promptSecret "Please enter your docker password: ")}"
  DOCKER_EMAIL="skmaji@outlook.com"

  # Delete the secret if it already exists
  kubectl get secret regcred -n kube-system 2>/dev/null && kubectl delete secret regcred -n kube-system
  kubectl create secret docker-registry regcred \
    --docker-server=$(fullRegistryUrl) \
    --docker-username=$DOCKER_USERNAME \
    --docker-password=$DOCKER_PASSWORD \
    --docker-email=$DOCKER_EMAIL -n kube-system

  genRegistryPassword $DOCKER_USERNAME $DOCKER_PASSWORD
}

# https://blog.zachinachshon.com/docker-registry/
# Verify access to the registry
verifyRegistryInst(){
  export DESTINATION_FOLDER=./registry-creds
  export USER=$(cat ${DESTINATION_FOLDER}/registry-user.txt)
  export PASSWORD=$(cat ${DESTINATION_FOLDER}/registry-pass.txt)

  curl -kiv -H \
    "Authorization: Basic $(echo -n "${USER}:${PASSWORD}" | base64)" \
    https://"$(registrySubdomain).$(rootDomain)"/v2/_catalog

  wget --no-check-certificate --header \
    "Authorization: Basic $(echo -n "${USER}:${PASSWORD}" | base64)" \
    https://"$(registrySubdomain).$(rootDomain)"/v2/_catalog

  unset USER PASSWORD
}

# https://itnext.io/error-x509-certificate-signed-by-unknown-authority-error-is-returned-f0e5d436f467
dockerRegistryInst(){
  imagePullSecrets
  helm repo add twuni https://helm.twun.io
  export DESTINATION_FOLDER=./registry-creds
  helm upgrade --install registry \
      --namespace registry \
      --create-namespace \
      --set replicaCount=1 \
      --set persistence.enabled=true \
      --set persistence.size=10Gi \
      --set persistence.deleteEnabled=true \
      --set persistence.storageClass=registry-storage \
      --set secrets.htpasswd="$(cat ${DESTINATION_FOLDER}/htpasswd)" \
      --values https://github.com/sumitmaji/kubernetes/raw/master/install_k8s/registry/values.yaml \
      twuni/docker-registry

}

# It is used to generate both client/server certificate using
# Kubernetes ca certificate in /etc/kubernetes/pki/ca.crt
createCertificate(){
  if [[ -n $TRACE ]]; then
    set -x
  fi

  : ${INSTALL_PATH:=$MOUNT_PATH/kubernetes/install_k8s}

  while [[ $# -gt 0 ]]
  do
  key="$1"
  case $key in
   -i|--ip)
   NODE_IP="$2"
   shift
   shift
   ;;
   -h|--host)
   HOSTNAME="$2"
   shift
   shift
   ;;
   -f|--file)
   FILENAME="$2"
   shift
   shift
   ;;
   -t|--type)
   TYPE="$2"
   shift
   shift
   ;;
  esac
  done

  if [ -z "$NODE_IP" ]
  then
  	echo "Please provide node ip"
  	exit 0
  fi
  if [ -z "$HOSTNAME" ]
  then
          echo "Please provide node hostname"
          exit 0
  fi
  if [ -z "$FILENAME" ]
  then
          echo "Please provide node filename"
          exit 0
  fi
  if [ -z "$TYPE" ]
  then
          echo "Please provide file type"
          exit 0
  fi

  : ${COUNTRY:=IN}
  : ${STATE:=UP}
  : ${LOCALITY:=GN}
  : ${ORGANIZATION:=CloudInc}
  : ${ORGU:=IT}
  : ${EMAIL:=cloudinc.gmail.com}
  : ${COMMONNAME:=kube-system}

  mkdir -p $MOUNT_PATH/certs
  pushd $MOUNT_PATH/certs

  if [ $TYPE == 'server' ]
  then
   keyUsage='extendedKeyUsage = clientAuth,serverAuth'
   #HOSTNAME="${HOSTNAME}-${FILENAME}" # Need to see why I did that
  else
   keyUsage='extendedKeyUsage = clientAuth'
   FILENAME="${FILENAME}-client"
   #HOSTNAME="${HOSTNAME}-$FILENAME"
  fi

  cat <<EOF | sudo tee ${FILENAME}-openssl.cnf
[req]
req_extensions = v3_req
distinguished_name = req_distinguished_name
[req_distinguished_name]
[ v3_req ]
basicConstraints = CA:FALSE
keyUsage = nonRepudiation, digitalSignature, keyEncipherment
$keyUsage
`
if [ $TYPE == 'server' ]
then
echo "subjectAltName = IP:$NODE_IP, DNS:$HOSTNAME"
fi`
EOF

  #Create a private key
  openssl genrsa -out $HOSTNAME.key 2048

  #Create CSR for the node
  openssl req -new -key $HOSTNAME.key \
    -subj "/CN=$NODE_IP" \
    -subj "/C=$COUNTRY/ST=$STATE/L=$LOCALITY/O=$ORGANIZATION/OU=$ORGU/CN=$HOSTNAME/emailAddress=$EMAIL" \
    -out $HOSTNAME.csr -config ${FILENAME}-openssl.cnf

  #Create a self signed certificate
  openssl x509 -req -in $HOSTNAME.csr -CA /etc/kubernetes/pki/ca.crt -CAkey /etc/kubernetes/pki/ca.key -CAcreateserial \
    -out $HOSTNAME.crt -days 10000 -extensions v3_req -extfile ${FILENAME}-openssl.cnf

  #Copy ca.crt to crt
  cat /etc/kubernetes/pki/ca.crt >> $HOSTNAME.crt

  #Verify a Private Key Matches a Certificate
  openssl x509 -noout -text -in $HOSTNAME.crt

  popd
}

# It is used to generate client certificate
# using kubectl command
createClientCertificate(){
  USERNAME=$1
  GROUPNAME=$2
  echo + Creating private key: ${USERNAME}.key
  openssl genrsa -out ${USERNAME}.key 4096

  echo + Creating signing request: ${USERNAME}-csr
  openssl req -new -key ${USERNAME}.key -out ${USERNAME}.csr -subj "/CN=${USERNAME}/O=${GROUPNAME}" \
          -addext "subjectAltName = DNS:${USERNAME}"

  WC=$(kubectl get csr ${USERNAME}-csr 2>/dev/null | wc -l)
  if [ "$WC" != "0" ]; then
    kubectl delete csr ${USERNAME}-csr
  fi
  echo + Creating signing request in kubernetes
  cat <<EOF | kubectl create -f -
apiVersion: certificates.k8s.io/v1
kind: CertificateSigningRequest
metadata:
  name: ${USERNAME}-csr
spec:
  groups:
    - system:authenticated
    - ${GROUPNAME}
  request: $(cat ${USERNAME}.csr | base64 | tr -d '\n')
  signerName: kubernetes.io/kube-apiserver-client
  usages:
    - digital signature
    - key encipherment
    - client auth
EOF

  kubectl certificate approve ${USERNAME}-csr

  kubectl get csr ${USERNAME}-csr -o jsonpath='{.status.certificate}' | base64 -d >${USERNAME}.crt
}

# The method creates certificate for user having admin role and generate kube config file
# for login to api server
createKubeConfig() {
  USERNAME=$1
  : ${IP:=$(ifconfig eth0 2>/dev/null | awk '/inet / {print $2}' | sed 's/addr://')}
  if [ -z "$IP" ]; then
    : ${IP:=$(ifconfig enp0s3 2>/dev/null | awk '/inet / {print $2}' | sed 's/addr://')}
  fi
  adminRole
  echo + Creating private key: ${USERNAME}.key
  openssl genrsa -out ${USERNAME}.key 4096

  echo + Creating signing request: ${USERNAME}.csr
  openssl req -new -key ${USERNAME}.key -out ${USERNAME}.csr -subj "/CN=${USERNAME}/O=cloud:masters"
  WC=$(kubectl get csr ${USERNAME}-csr 2>/dev/null | wc -l)
  if [ "$WC" != "0" ]; then
    kubectl delete csr ${USERNAME}-csr
  fi
  echo + Creating signing request in kubernetes
  cat <<EOF | kubectl create -f -
apiVersion: certificates.k8s.io/v1
kind: CertificateSigningRequest
metadata:
  name: ${USERNAME}-csr
spec:
  groups:
    - system:authenticated
    - cloud:masters
  request: $(cat ${USERNAME}.csr | base64 | tr -d '\n')
  signerName: kubernetes.io/kube-apiserver-client
  usages:
    - digital signature
    - key encipherment
    - client auth
EOF

  kubectl certificate approve ${USERNAME}-csr

  kubectl get csr ${USERNAME}-csr -o jsonpath='{.status.certificate}' | base64 -d >${USERNAME}.crt

  echo "======Kubeconfig file user ${USERNAME}.conf generated"

  kubectl config set-cluster cloud.com --certificate-authority=/etc/kubernetes/pki/ca.crt --embed-certs=true --server=https://${IP}:6443 --kubeconfig=/root/${USERNAME}.conf
  kubectl config set-credentials ${USERNAME} --client-key=${USERNAME}.key --client-certificate=${USERNAME}.crt --embed-certs=true --kubeconfig=/root/${USERNAME}.conf
  kubectl config --kubeconfig=/root/${USERNAME}.conf set-context ${USERNAME}@cloud.com --cluster=cloud.com --user=${USERNAME}
  kubectl config --kubeconfig=/root/${USERNAME}.conf use-context ${USERNAME}@cloud.com

  rm ${USERNAME}.key ${USERNAME}.csr ${USERNAME}.crt

}

patchLdapSecure() {
  kubectl patch ing "$NAME" --patch "$(
    cat <<EOF
metadata:
  annotations:
    nginx.ingress.kubernetes.io/auth-signin: https://$(defaultSubdomain).$(rootDomain)/authenticate
    nginx.ingress.kubernetes.io/auth-url: https://$(defaultSubdomain).$(rootDomain)/check
EOF
  )" -n "$NS"
}

patchOauth2Secure() {
  NAME=$1
  NS=$2
  RD=$3
  kubectl patch ing "$NAME" --patch "$(
    cat <<EOF
metadata:
  annotations:
    nginx.ingress.kubernetes.io/auth-signin: https://$(defaultSubdomain).$(rootDomain)/oauth2/start?rd=${RD}
    nginx.ingress.kubernetes.io/auth-url: https://$(defaultSubdomain).$(rootDomain)/oauth2/auth
    nginx.ingress.kubernetes.io/auth-response-headers: Authorization
EOF
  )" -n "$NS"
}


patchLetsEncrypt() {
  NAME=$1
  NS=$2
  SUBDOMAIN=$(subDomain $3)

  kubectl patch ing "$NAME" --patch "$(
    cat <<EOF
metadata:
  annotations:
    #certmanager.k8s.io/cluster-issuer: $(getClusterIssuerName)
    cert-manager.io/cluster-issuer: $(getClusterIssuerName)
    nginx.ingress.kubernetes.io/ssl-redirect: "true"
spec:
  tls:
    - hosts:
        - ${SUBDOMAIN}.$(rootDomain)
      secretName: ${SUBDOMAIN}-$(sedRootDomain)
EOF
  )" -n "$NS"
  kubectl patch ing "$NAME" --type=json -p='[{"op": "replace", "path": "/spec/rules/0/host", "value":"'$SUBDOMAIN'.'$(rootDomain)'"}]' -n "$NS"

  kubectl --timeout=120s -n ${NS} wait --for=condition=Ready certificates.cert-manager.io ${SUBDOMAIN}-$(sedRootDomain)
}

# Is is used see the values that would be generated by helm
helmShowValues(){
  CHART_NAME=$1
  helm get values $CHART_NAME -a
}

# Is is used to see all the resources that would be generated by helm
helmShowAll(){
  CHART_NAME=$1
  helm get all $CHART_NAME
}

helmTemplate(){
  CHART_REPO=$1
  helm template $CHART_REPO -g
}

# Is is used see the template that would be generated by helm
helmShowTemplate(){
  CHART_REPO=$1
  helm install $CHART_REPO  -g \
    --dry-run
}

fetch_client_secret() {
  # Parameters
  local KEYCLOAK_URL=$1
  local REALM_NAME=$2
  local CLIENT_ID=$3
  local ADMIN_USERNAME=$4
  local ADMIN_PASSWORD=$5

  # Validate input
  if [[ -z "$KEYCLOAK_URL" || -z "$REALM_NAME" || -z "$CLIENT_ID" || -z "$ADMIN_USERNAME" || -z "$ADMIN_PASSWORD" ]]; then
    echo "Usage: fetch_client_secret <KEYCLOAK_URL> <REALM_NAME> <CLIENT_ID> <ADMIN_USERNAME> <ADMIN_PASSWORD>"
    return 1
  fi

  # Fetch admin access token
  ACCESS_TOKEN=$(curl -s -X POST https://"$KEYCLOAK_URL/realms/master/protocol/openid-connect/token" \
    -H "Content-Type: application/x-www-form-urlencoded" \
    -d "username=$ADMIN_USERNAME" \
    -d "password=$ADMIN_PASSWORD" \
    -d "grant_type=password" \
    -d "client_id=admin-cli" | jq -r '.access_token')

  if [ -z "$ACCESS_TOKEN" ]; then
    echo "Failed to fetch access token. Please check your credentials."
    return 1
  fi

  # Fetch client UUID from the realm
  CLIENT_UUID=$(curl -s -X GET https://"$KEYCLOAK_URL/admin/realms/$REALM_NAME/clients" \
    -H "Authorization: Bearer $ACCESS_TOKEN" \
    -H "Content-Type: application/json" | jq -r ".[] | select(.clientId==\"$CLIENT_ID\") | .id")

  if [ -z "$CLIENT_UUID" ]; then
    echo "Client '$CLIENT_ID' not found in realm '$REALM_NAME'."
    return 1
  fi

  # Fetch client secret
  CLIENT_SECRET=$(curl -s -X GET https://"$KEYCLOAK_URL/admin/realms/$REALM_NAME/clients/$CLIENT_UUID/client-secret" \
    -H "Authorization: Bearer $ACCESS_TOKEN" \
    -H "Content-Type: application/json" | jq -r '.value')

  if [ -z "$CLIENT_SECRET" ]; then
    echo "Failed to fetch client secret for client '$CLIENT_ID'."
    return 1
  fi

  # Return the client secret
  echo "$CLIENT_SECRET"
}

oauth2Secret(){
  CLIENT_ID=$(kubectl get secret keycloak-secrets -n keycloak -o jsonpath="{['data']['CLIENT_ID']}" | base64 --decode)
  REALM=$(kubectl get secret keycloak-secrets -n keycloak -o jsonpath="{['data']['OAUTH_REALM']}" | base64 --decode)
  KEYCLOAK_URL=$(fullKeycloakUrl)
  ADMIN_USERNAME=$(kubectl get secret keycloak-secrets -n keycloak -o jsonpath="{['data']['KEYCLOAK_ADMIN']}" | base64 --decode)
  ADMIN_PASSWORD=$(kubectl get secret keycloak -n keycloak -o jsonpath="{.data.admin-password}" | base64 --decode)

  client_secret=$(fetch_client_secret "$KEYCLOAK_URL" "$REALM" "$CLIENT_ID" "$ADMIN_USERNAME" "$ADMIN_PASSWORD")
  : "${ACTIVE_PROFILE:=$(promptUserInput "Enter Active Profile (keycloak): " "keycloak")}"
  : "${OIDC_ISSUE_URL:=$(promptUserInput "Enter OIDC Issue URL (https://keycloak.gokcloud.com/realms/$REALM): " "https://keycloak.gokcloud.com/realms/$REALM")}"
  : "${OIDC_USERNAME_CLAIM:=$(promptUserInput "Enter OIDC Username Claim (${OIDC_USERNAME_CLAIM}): " "${OIDC_USERNAME_CLAIM}")}"
  : "${OIDC_GROUPS_CLAIM:=$(promptUserInput "Enter OIDC Groups Claim (${OIDC_GROUPS_CLAIM}): " "${OIDC_GROUPS_CLAIM}")}"
  : "${AUTH0_DOMAIN:=$(promptUserInput "Enter Auth0 Domain (${AUTH0_DOMAIN}): " "${AUTH0_DOMAIN}")}"
  : "${APP_HOST:=$(promptUserInput "Enter App Host (${APP_HOST}): " "${APP_HOST}")}"
  : "${JWKS_URL:=$(promptUserInput "Enter JWKS URL (${JWKS_URL}): " "${JWKS_URL}")}"
  OAUTH_SERVER_URI="https://$(fullKeycloakUrl)"

  # If secret already exists then delete it
  kubectl get secret oauth-secrets -n kube-system 2>/dev/null && kubectl delete secret oauth-secrets -n kube-system
  kubectl create secret generic oauth-secrets \
    --from-literal=OAUTH_REALM="${REALM}" \
    --from-literal=ACTIVE_PROFILE="${ACTIVE_PROFILE}" \
    --from-literal=OIDC_CLIENT_ID="${CLIENT_ID}" \
    --from-literal=OIDC_ISSUE_URL="${OIDC_ISSUE_URL}" \
    --from-literal=OIDC_USERNAME_CLAIM="${OIDC_USERNAME_CLAIM}" \
    --from-literal=OIDC_GROUPS_CLAIM="${OIDC_GROUPS_CLAIM}" \
    --from-literal=AUTH0_DOMAIN="${AUTH0_DOMAIN}" \
    --from-literal=APP_HOST="${APP_HOST}" \
    --from-literal=JWKS_URL="${JWKS_URL}" \
    --from-literal=OAUTH_SERVER_URI="${OAUTH_SERVER_URI}" \
    --from-literal=OIDC_CLIENT_SECRET="${client_secret}" -n kube-system
}

opensearchSecret(){
  ADMIN_PASSWORD=$1
  # If secret already exists then delete it
  kubectl get secret opensearch-secrets -n kube-system 2>/dev/null && kubectl delete secret opensearch-secrets -n kube-system
  kubectl create secret generic opensearch-secrets \
    --from-literal=OPENSEARCH_INITIAL_ADMIN_PASSWORD="${ADMIN_PASSWORD}" -n kube-system
}

csiDriverInstall(){
  helm repo add secrets-store-csi-driver https://kubernetes-sigs.github.io/secrets-store-csi-driver/charts 
  helm repo update
  helm install csi-secrets-store secrets-store-csi-driver/secrets-store-csi-driver --namespace kube-system 

}

csiDriverUnInstall(){
  helm uninstall csi-secrets-store --namespace kube-system
}

vaultLogin(){
  ROOT_TOKEN=$(kubectl get secret vault-init-keys -n vault -o json | jq -r '.data["vault-init.json"]' | base64 -d | jq -r '.root_token')
  kubectl exec -it vault-0 -n vault -- vault login ${ROOT_TOKEN}
}

cleanExampleSecretStoreInVault(){
  kubectl delete pod vault-secret-pod -n default
  kubectl delete secretproviderclass vault-secret-provider -n default
  kubectl delete secret my-k8s-secret -n default >/dev/null 2>&1

  vaultLogin
  kubectl exec -it vault-0 -n vault -- vault kv delete secret/my-secret
  kubectl exec -it vault-0 -n vault -- vault delete auth/kubernetes/role/my-role
  kubectl exec -it vault-0 -n vault -- vault policy delete my-policy  
}

cleanDockerRegistrySecretStoreInVault(){
  kubectl delete pod docker-registry-secret-pod -n default
  kubectl delete secretproviderclass docker-registry-secret-provider -n default
  
}

verifyVault(){
  pushd $MOUNT_PATH/kubernetes/install_k8s/vault
  chmod +x *.sh
  ./verification.sh
  popd
}

verifyCertManater(){
  pushd $MOUNT_PATH/kubernetes/install_k8s/scripts
  chmod +x *.sh
  ./verify_cert_manager.sh
  popd
}

debugVault(){
  pushd $MOUNT_PATH/kubernetes/install_k8s/vault
  chmod +x *.sh
  ./debug-vault.sh
  popd
}

createVaultSecretStore() {
  # Initialize variables
  local secret_path=""
  local role_name=""
  local policy_name=""
  local secret_provider_class_name=""
  local prefix=""
  local namespace="default"
  local kv_pairs=()
  local secret_name=""

  # Parse command-line arguments
  while [[ $# -gt 0 ]]; do
    case $1 in
      -p|--secret-path)
        secret_path="$2"
        shift 2
        ;;
      -r|--role-name)
        role_name="$2"
        shift 2
        ;;
      -l|--policy-name)
        policy_name="$2"
        shift 2
        ;;
      -s|--secret-provider-class-name)
        secret_provider_class_name="$2"
        shift 2
        ;;
      -x|--prefix)
        prefix="$2"
        shift 2
        ;;
      -n|--namespace)
        namespace="$2"
        shift 2
        ;;
      -k|--key-value)
        kv_pairs+=("$2")
        shift 2
        ;;
      --secret-name)
        secret_name="$2"
        shift 2
        ;;
      *)
        echo "Unknown option: $1"
        echo "Usage: createVaultSecretStore -p <secret_path> -r <role_name> -l <policy_name> -s <secret_provider_class_name> -x <prefix> -n <namespace> -k <key=value> [--secret-name <secret_name>]"
        return 1
        ;;
    esac
  done

  # Validate required arguments
  if [[ -z "$secret_path" || -z "$role_name" || -z "$policy_name" || -z "$secret_provider_class_name" || -z "$prefix" || ${#kv_pairs[@]} -eq 0 ]]; then
    echo "Usage: createVaultSecretStore -p <secret_path> -r <role_name> -l <policy_name> -s <secret_provider_class_name> -x <prefix> -n <namespace> -k <key=value> [--secret-name <secret_name>]"
    return 1
  fi

  # Default secret name if not provided
  if [[ -z "$secret_name" ]]; then
    secret_name="${secret_provider_class_name}-secret"
  fi

  echo "SECRET_PATH: $secret_path"
  echo "ROLE_NAME: $role_name"
  echo "POLICY_NAME: $policy_name"
  echo "SECRET_PROVIDER_CLASS_NAME: $secret_provider_class_name"
  echo "PREFIX: $prefix"
  echo "NAMESPACE: $namespace"
  echo "KV_PAIRS: ${kv_pairs[*]}"
  echo "SECRET_NAME: $secret_name"

  vaultLogin

  # Create the secret in Vault with multiple key-value pairs
  if kubectl exec -it vault-0 -n vault -- vault kv get "$secret_path" >/dev/null 2>&1; then
    echo "Secret at path $secret_path already exists in Vault. Skipping creation."
  else
    local secret_command="vault kv put $secret_path"
    for kv_pair in "${kv_pairs[@]}"; do
      secret_command+=" $kv_pair"
    done
    kubectl exec -it vault-0 -n vault -- $secret_command
  fi

  # Create a policy for the secret
  if kubectl exec -it vault-0 -n vault -- vault policy read "$policy_name" >/dev/null 2>&1; then
    echo "Policy $policy_name already exists in Vault. Skipping creation."
  else
    kubectl exec -i vault-0 -n vault -- vault policy write "$policy_name" - <<EOF
path "$secret_path" {
  capabilities = ["read", "list"]
}
EOF
  fi

  # Create a role for Kubernetes authentication
  if kubectl exec -it vault-0 -n vault -- vault read auth/kubernetes/role/"$role_name" >/dev/null 2>&1; then
    echo "Role $role_name already exists in Vault. Skipping creation."
  else
    kubectl exec -it vault-0 -n vault -- vault write auth/kubernetes/role/"$role_name" \
      bound_service_account_names=* \
      bound_service_account_namespaces="$namespace" \
      policies="$policy_name" \
      ttl=24h
  fi

  # Generate the `objects` section dynamically
  local objects=""
  for kv_pair in "${kv_pairs[@]}"; do
    local key=$(echo "$kv_pair" | cut -d '=' -f 1)
    objects+="      - objectName: \"$prefix-$key\"\n"
    objects+="        objectType: \"kv\"\n"
    objects+="        secretPath: \"$secret_path\"\n"
    objects+="        secretKey: \"$key\"\n"
    objects+="        objectVersion: \"\"\n"
  done

  # Add the `secretObjects` section
  local secret_objects="  secretObjects:\n"
  secret_objects+="    - secretName: $secret_name\n"
  secret_objects+="      type: Opaque\n"
  secret_objects+="      data:\n"
  for kv_pair in "${kv_pairs[@]}"; do
    local key=$(echo "$kv_pair" | cut -d '=' -f 1)
    secret_objects+="        - objectName: \"$prefix-$key\"\n"
    secret_objects+="          key: \"$key\"\n"
  done

  # Generate the YAML for the SecretProviderClass
  local yaml=$(cat <<EOF
apiVersion: secrets-store.csi.x-k8s.io/v1
kind: SecretProviderClass
metadata:
  name: "$secret_provider_class_name"
  namespace: "$namespace"
spec:
  provider: vault
  parameters:
    vaultAddress: "https://vault.gokcloud.com"
    roleName: "$role_name"
    skipVerify: "true"
    vaultSkipTLSVerify: "true"
    objects: |
$objects
$secret_objects
EOF
)

  # Echo the generated YAML
  echo "Generated YAML:"
  printf "$yaml"

  # Apply the YAML
  printf "$yaml" | kubectl apply -f -
}

exampleSecretStoreInVaule(){
  vaultLogin
  kubectl exec -it vault-0 -n vault -- vault kv put secret/my-secret username="my-username" password="my-password"

  kubectl exec -it vault-0 -n vault -- vault write auth/kubernetes/role/my-role \
    bound_service_account_names=default \
    bound_service_account_namespaces=default \
    policies=my-policy \
    ttl=24h

  # For K/V v2 secrets engine
#   kubectl exec -i vault-0 -n vault -- vault policy write my-policy - <<EOF
# path "secret/data/my-secret" {
#   capabilities = ["read", "list"]
# }
# EOF

  # For K/V v1 secrets engine
  kubectl exec -i vault-0 -n vault -- vault policy write my-policy - <<EOF
path "secret/my-secret" {
  capabilities = ["read", "list"]
}
EOF

  cat <<EOF | kubectl apply -f -
apiVersion: secrets-store.csi.x-k8s.io/v1
kind: SecretProviderClass
metadata:
  name: vault-secret-provider
  namespace: default
spec:
  provider: vault
  parameters:
    vaultAddress: "http://vault.vault.svc.cloud.uat:8200"  
    roleName: "my-role"  
    skipVerify: "true"   
    vaultSkipTLSVerify: "true"                 
    objects: |
      - objectName: "my-username"
        objectType: "kv"
        secretPath: "secret/my-secret"
        objectVersion: ""
        secretKey: "username"
      - objectName: "my-password"
        objectType: "kv"
        secretPath: "secret/my-secret"
        objectVersion: ""
        secretKey: "password"
EOF

  cat <<EOF | kubectl apply -f -
apiVersion: v1
kind: Pod
metadata:
  name: vault-secret-pod
  namespace: default
spec:
  containers:
  - name: app
    image: busybox
    command: ["sleep", "3600"]
    volumeMounts:
    - name: secrets-store-inline
      mountPath: "/mnt/secrets-store"  # Path where the secret will be mounted
      readOnly: true
  volumes:
  - name: secrets-store-inline
    csi:
      driver: secrets-store.csi.k8s.io
      readOnly: true
      volumeAttributes:
        secretProviderClass: "vault-secret-provider"
EOF

}

dockerRegistrySecretStoreInVault(){
  vaultLogin
  kubectl exec -it vault-0 -n vault -- vault kv put secret/docker-registry \
    server="https://index.docker.io/v1/" \
    username="my-username" \
    password="my-password" \
    email="my-email@example.com"

  kubectl exec -i vault-0 -n vault -- vault write auth/kubernetes/role/docker-registry-role \
    bound_service_account_names=my-service-account \
    bound_service_account_namespaces=default \
    policies=docker-registry-policy \
    ttl=24h

  kubectl exec -i vault-0 -n vault -- vault policy write docker-registry-policy - <<EOF
path "secret/data/docker-registry" {
  capabilities = ["read"]
}
EOF


  cat <<EOF | kubectl apply -f -
apiVersion: secrets-store.csi.x-k8s.io/v1
kind: SecretProviderClass
metadata:
  name: docker-registry-secret-provider
  namespace: default
spec:
  provider: vault
  parameters:
    vaultAddress: "https://vault.vault.svc.cloud.uat"
    roleName: "docker-registry-role"
    objects: |
      - objectName: "docker-registry"
        objectType: "kv"
        objectVersion: ""
  secretObjects:          
    - secretName: docker-registry-secret
      type: kubernetes.io/dockerconfigjson
      data:
        - objectName: "docker-registry"
          key: ".dockerconfigjson"
EOF

}

vaultInstall() {
  csiDriverInstall
  # Create namespace for Vault
  kubectl create namespace vault

  # Prompt user for Vault storage configuration
  STORAGE_CLASS_NAME="vault-storage"
  PV_NAME="vault-pv"
  VOLUME_PATH="/data/volumes/vault"

  # Create local storage class and persistent volume
  createLocalStorageClassAndPV "$STORAGE_CLASS_NAME" "$PV_NAME" "$VOLUME_PATH"

  # kubectl apply -f $MOUNT_PATH/kubernetes/install_k8s/vault/vault-k8s-seal-role.yaml
  # Add HashiCorp Helm repository
  helm repo add hashicorp https://helm.releases.hashicorp.com
  helm repo update

  # Install Vault using Helm
  helm install vault hashicorp/vault \
    --namespace vault \
    --values $MOUNT_PATH/kubernetes/install_k8s/vault/values.yaml


  # Valut UI is disabled, so ingress is not created
  gok patch ingress vault vault letsencrypt vault
  if [[ $? -ne 0 ]]; then
    echoFailed "Failed to patch Vault ingress with Let's Encrypt!"
    return 1
  fi
  echoSuccess "Vault ingress patched successfully with Let's Encrypt!"
  
  echo "Vault installation started. This may take a few minutes..."

  kubectl apply -f $MOUNT_PATH/kubernetes/install_k8s/vault/vault-k8s-seal-role.yaml
  copySecret regcred default vault
  pushd $MOUNT_PATH/kubernetes/install_k8s/vault
  chmod +x *.sh
  ./ci.sh
  popd
  kubectl patch serviceaccount vault -p '{"imagePullSecrets": [{"name": "regcred"}]}' -n vault
  mkdir -p $MOUNT_PATH/vault-keys
  kubectl apply -f $MOUNT_PATH/kubernetes/install_k8s/vault/vault-init-unseal-job.yaml
  
  # Wait for the Job to complete
  if kubectl wait --for=condition=complete job.batch/vault-init-unseal -n vault --timeout=30s; then
    # Check if the Job completed successfully
    STATUS=$(kubectl get job vault-init-unseal -n vault -o jsonpath='{.status.succeeded}')
    if [[ "$STATUS" -eq 1 ]]; then
      echoSuccess "Vault unseal completed successfully."
      kubectl delete -f $MOUNT_PATH/kubernetes/install_k8s/vault/vault-init-unseal-job.yaml
    else
      echoFailed "JVault unsealob did not complete successfully."
      exit 1  # Return failure
    fi
  else
    echoSuccess "Vault unseal did not complete within the timeout."
    exit 1  # Return failure
  fi

  # Install Vault unseal service
  pushd $MOUNT_PATH/kubernetes/install_k8s/vault
  chmod +x *.sh
  ./install-vault-unseal-service.sh
  popd

  # Wait for Vault pods to be ready
  echo "Waiting for Vault services to be up!!!!"
  kubectl --timeout=120s wait --for=condition=Ready pods --all --namespace vault
  if [[ $? -eq 0 ]]; then
    echoSuccess "Vault services are now up!"
    echoSuccess "Vault installation completed!"
    echoSuccess "You can access Vault at: https://$(fullVaultUrl)"

    kubectl create clusterrolebinding vault-auth-delegator \
      --clusterrole=system:auth-delegator \
      --serviceaccount=vault:vault

    vaultLogin

    kubectl exec -it vault-0 -n vault -- vault auth enable kubernetes

    TOKEN=$(kubectl exec -it vault-0 -n vault -- cat /var/run/secrets/kubernetes.io/serviceaccount/token)

    kubectl exec -it vault-0 -n vault -- vault write auth/kubernetes/config \
      token_reviewer_jwt="$TOKEN" \
      kubernetes_host="https://11.0.0.1:6643" \
      kubernetes_ca_cert=@/var/run/secrets/kubernetes.io/serviceaccount/ca.crt
    
    kubectl exec -it vault-0 -n vault -- vault secrets enable -path=secret kv
    pushd $MOUNT_PATH/kubernetes/install_k8s/vault
    ./create_gok_vault_secrets.sh
    popd
    
  else
    echoFailed "Vault services timed-out, please check!"
  fi

}

gokAgentReset(){
  # Uninstall Gok Agent using Helm
  helm uninstall gok-agent -n gok-agent

  # Delete the Gok Agent namespace
  kubectl delete ns gok-agent
}


gokAgentInstall(){
  
  pushd ${MOUNT_PATH}/kubernetes/install_k8s/gok-cloud/agent
  ./build.sh
  ./tag_push.sh
  popd

  # Create namespace for gok-agent
  kubectl create namespace gok-agent

  # Moved the secret creation in create_gok_vault_secretes.sh
  # Moved the SecretProviderClass creation to helm chart
  # createVaultSecretStore \
  # -p "secret/gok-agent/config" \
  # -r "gok-agent" \
  # -l "gok-agent-policy" \
  # -s "gok-agent-provider" \
  # -x "gok-agent" \
  # -n "gok-agent" \
  # -k "oauth_client_id=adfasdfsasdfasdfasd" \
  # -k "static_config=asdfasdf"

  kubectl create configmap ca-cert --from-file=issuer.crt=/usr/local/share/ca-certificates/issuer.crt -n gok-agent

  # Install Vault using Helm
  helm install gok-agent ${MOUNT_PATH}/kubernetes/install_k8s/gok-cloud/agent/chart \
    --namespace gok-agent

}

# Function to delete old Docker images
# that are not tagged as "latest"
# Usage: deleteOldDockerImages <image_name>
deleteOldDockerImages() {
  IMAGE_NAME="$1"
  if [[ -z "$IMAGE_NAME" ]]; then
    echo "Usage: deleteOldDockerImages <image_name>"
    return 1
  fi
  docker images | grep "registry.gokcloud.com/${IMAGE_NAME}" | grep -v latest | awk '{print $3}' | xargs -r docker rmi
}

gokControllerReset(){
  # Uninstall Gok Controller using Helm
  helm uninstall gok-controller -n gok-controller

  # Delete the Gok Controller namespace
  kubectl delete ns gok-controller
}


gokControllerInstall(){
  pushd ${MOUNT_PATH}/kubernetes/install_k8s/gok-cloud/controller
  ./build.sh
  ./tag_push.sh
  popd

  # Create namespace for gok-controller
  kubectl create namespace gok-controller

  # Moved the secret creation in create_gok_vault_secretes.sh
  # Moved the SecretProviderClass creation to helm chart  
  # createVaultSecretStore \
  # -p "secret/gok-controller" \
  # -r "gok-controller" \
  # -l "gok-controller-policy" \
  # -s "gok-controller-provider" \
  # -x "gok-controller" \
  # -n "gok-controller" \
  # -k "api-token=adfasdfasdfasdfasd"
  
  kubectl create configmap ca-cert --from-file=issuer.crt=/usr/local/share/ca-certificates/issuer.crt -n gok-controller

  # Install Vault using Helm
  helm install gok-controller ${MOUNT_PATH}/kubernetes/install_k8s/gok-cloud/controller/chart \
    --namespace gok-controller

  # Patch the ingress with Let's Encrypt
  gok patch ingress gok-controller gok-controller letsencrypt controller
  patchControllerWithOauth
  if [[ $? -ne 0 ]]; then
    echoFailed "Failed to patch Gok Controller ingress with Let's Encrypt!"
    return 1
  fi
  echoSuccess "Gok Controller ingress patched successfully with Let's Encrypt!"
  echoSuccess "Access Gok Controller at: https://controller.$(rootDomain)"
}

vaultReset() {
  # Uninstall Vault using Helm
  helm uninstall vault -n vault

  # Delete all PersistentVolumeClaims in the Vault namespace
  kubectl delete pvc --all -n vault

  # Clean up the local persistent volume and storage class
  emptyLocalFsStorage "Vault" "vault-pv" "vault-storage" "/data/volumes/vault"

  # Delete the Vault namespace
  kubectl delete ns vault

  csiDriverUnInstall

  kubectl delete clusterrolebinding vault-auth-delegator
  systemctl stop vault-unseal.service
  systemctl disable vault-unseal.service
  rm -f /etc/systemd/system/vault-unseal.service
  systemctl daemon-reload

  # Provide feedback to the user
  echo "Vault has been reset successfully!"
}


keycloakReset(){
  helm uninstall keycloak -n keycloak
  kubectl delete pvc --all -n keycloak

  emptyLocalFsStorage "Keycloak" "keycloak-pv" "keycloak-storage" "/data/volumes/pv3"
  kubectl delete ns keycloak
  kubectl get secret oauth-secrets >/dev/null 2>&1

  # vaultLogin
  # kubectl delete secretProviderClass keycloak-admin-secret-provider -n keycloak >/dev/null 2>&1
  # kubectl exec -it vault-0 -n vault -- vault kv delete secret/keycloak/admin
  # kubectl exec -it vault-0 -n vault -- vault delete auth/kubernetes/role/keycloak-admin-role
  # kubectl exec -it vault-0 -n vault -- vault policy delete keycloak-admin-policy

  # kubectl delete secretProviderClass keycloak-postgresql-secret-provider -n keycloak >/dev/null 2>&1
  # kubectl exec -it vault-0 -n vault -- vault kv delete secret/keycloak/postgresql
  # kubectl exec -it vault-0 -n vault -- vault delete auth/kubernetes/role/keycloak-postgresql-role
  # kubectl exec -it vault-0 -n vault -- vault policy delete keycloak-postgresql-policy
  # kubectl delete secret keycloak-secrets -n keycloak >/dev/null 2>&1
  # kubectl delete secret keycloak-postgresql -n keycloak >/dev/null 2>&1
  # kubectl delete secret keycloak-secrets -n kube-system >/dev/null 2>&1
  # kubectl delete secret keycloak-postgresql -n kube-system >/dev/null 2>&1
  # kubectl delete secret keycloak-secrets -n default >/dev/null 2>&1
  # kubectl delete secret keycloak-postgresql -n default >/dev/null 2>&1

}

keycloakInst(){
  #Admin user is "user" and password is fetched from secret keycloak with key "admin-password"
  #Hence removing the below 2 lines
  #keycloak_adminid=$(promptUserInput "Please enter keycloak admin id (admin): " "admin")
  #keycloak_adminpwd=$(promptSecret "Please enter keycloak admin pwd: ")
  kubectl create ns keycloak
  : "${KEYCLOAK_ADMIN_USERNAME:=$(promptUserInput "Please enter keycloak admin username (admin): " "admin")}"
  : "${KEYCLOAK_ADMIN_PASSWORD:=$(promptSecret "Please enter keycloak admin password: ")}"
  
  # createVaultSecretStore \
  # -p "secret/keycloak/admin" \
  # -r "keycloak-admin-role" \
  # -l "keycloak-admin-policy" \
  # -s "keycloak-admin-secret-provider" \
  # -x "keycloak" \
  # -n "keycloak" \
  # -k "adminUser=${KEYCLOAK_ADMIN_USERNAME}" \
  # -k "adminPassword=${KEYCLOAK_ADMIN_PASSWORD}"
  
  : "${POSTGRESQL_USERNAME:=$(promptUserInput "Please enter postgresql username (postgres): " "postgres")}"
  : "${POSTGRESQL_PASSWORD:=$(promptSecret "Please enter postgresql password: ")}"

  # createVaultSecretStore \
  # -p "secret/keycloak/postgresql" \
  # -r "keycloak-postgresql-role" \
  # -l "keycloak-postgresql-policy" \
  # -s "keycloak-postgresql-secret-provider" \
  # -x "keycloak" \
  # -n "keycloak" \
  # -k "username=${POSTGRESQL_USERNAME}" \
  # -k "password=${POSTGRESQL_PASSWORD}"
  
  : "${OIDC_CLIENT_ID:=$(promptUserInput "Please enter OIDC client id (${OIDC_CLIENT_ID}): " "${OIDC_CLIENT_ID}")}"
  : "${REALM:=$(promptUserInput "Please enter realm name (${REALM}): " "${REALM}")}"

  kubectl create ns keycloak
  kubectl create secret generic keycloak-secrets \
    --from-literal=KEYCLOAK_LOG_LEVEL="TRACE" \
    --from-literal=KEYCLOAK_ADMIN="${KEYCLOAK_ADMIN_USERNAME}" \
    --from-literal=CLIENT_ID="${OIDC_CLIENT_ID}" \
    --from-literal=OAUTH_REALM="${REALM}"  -n keycloak \
    --from-literal=KEYCLOAK_ADMIN_PASSWORD="${KEYCLOAK_ADMIN_PASSWORD}" -n keycloak

  kubectl create secret generic keycloak-postgresql \
    --from-literal=username="${POSTGRESQL_USERNAME}" \
    --from-literal=password="${POSTGRESQL_PASSWORD}" \
    --from-literal=postgres-password="${POSTGRESQL_PASSWORD}" -n keycloak

  hostname="$(subDomain 'keycloak').$(rootDomain)"

  helm install keycloak oci://registry-1.docker.io/bitnamicharts/keycloak \
        --namespace keycloak \
        --set ingress.hostname="$hostname" \
        --set auth.adminUser="$KEYCLOAK_ADMIN_USERNAME" \
        --set auth.adminPassword="$KEYCLOAK_ADMIN_PASSWORD" \
        --values "${MOUNT_PATH}"/kubernetes/install_k8s/keycloak/values2.yaml
}



addPolicyToSyncSecrets(){
  cat <<EOF | kubectl apply -f -  
apiVersion: kyverno.io/v1
kind: ClusterPolicy
metadata:
  name: sync-regcred
spec:
  rules:
    - name: sync-regcred-secret
      match:
        any:
          - resources:
              kinds:
                - Namespace
      generate:
        apiVersion: v1
        kind: Secret
        name: regcred
        namespace: "{{request.object.metadata.name}}"
        synchronize: true
        clone:
          namespace: kube-system
          name: regcred
    - name: patch-serviceaccount
      match:
        any:
          - resources:
              kinds:
                - ServiceAccount
              namespaces:
                - "*"
      mutate:
        patchStrategicMerge:
          spec:
            imagePullSecrets:
              - name: regcred
EOF
}

installRegistryWithCertMgr(){
  createLocalStorageClassAndPV "registry-storage" "registry-pv" "/data/volumes/pv4"
  dockerRegistryInst
  # No need to generate the certificate.
  # The certificate and secret will be directory issued the ingress annotation cert-manager.io/cluster-issuer
  # gok create certificate registry registry
  gok patch ingress registry-docker-registry registry letsencrypt $(registrySubdomain)

  # Let docker trust the self-signed certificates
  # https://itnext.io/error-x509-certificate-signed-by-unknown-authority-error-is-returned-f0e5d436f467
  #BEGIN
  rm /etc/docker/certs.d/$(registrySubdomain).$(rootDomain)/ca.crt
  mkdir -p /etc/docker/certs.d/$(registrySubdomain).$(rootDomain)/
  kubectl get secret $(registrySubdomain)-$(sedRootDomain) -n registry -o jsonpath="{['data']['tls\.crt']}" | base64 --decode > /etc/docker/certs.d/$(registrySubdomain).$(rootDomain)/ca.crt
  kubectl get secret $(registrySubdomain)-$(sedRootDomain) -n registry -o jsonpath="{['data']['ca\.crt']}" | base64 --decode >> /etc/docker/certs.d/$(registrySubdomain).$(rootDomain)/ca.crt
  systemctl restart docker
  #END

  # Restarting docker stops the haproxy, need to start it again
  gok start proxy
  docker login $(registrySubdomain).$(rootDomain)
  [[ $? -eq 0 ]] && echoSuccess "OK" || echoFailed "FAILED"
  openssl s_client -connect $(registrySubdomain).$(rootDomain):443 -showcerts </dev/null | grep 'Verify return code: 0 (ok)'
  [[ $? -eq 0 ]] && echoSuccess "Registry certificate verification succeeded" || echoFailed "Registry certificate verification failed"
  addPolicyToSyncSecrets
}



create_sub_scope() {
  # Parameters
  local KEYCLOAK_URL=$1
  local REALM_NAME=$2
  local ADMIN_USERNAME=$3
  local ADMIN_PASSWORD=$4
  local CLIENT_ID=$5

  # Validate input
  if [[ -z "$KEYCLOAK_URL" || -z "$REALM_NAME" || -z "$ADMIN_USERNAME" || -z "$ADMIN_PASSWORD" || -z "$CLIENT_ID" ]]; then
    echo "Usage: create_sub_scope <KEYCLOAK_URL> <REALM_NAME> <ADMIN_USERNAME> <ADMIN_PASSWORD> <CLIENT_ID>"
    return 1
  fi

  # Fetch admin access token
  ACCESS_TOKEN=$(curl -s -X POST "$KEYCLOAK_URL/realms/master/protocol/openid-connect/token" \
    -H "Content-Type: application/x-www-form-urlencoded" \
    -d "username=$ADMIN_USERNAME" \
    -d "password=$ADMIN_PASSWORD" \
    -d "grant_type=password" \
    -d "client_id=admin-cli" | jq -r '.access_token')

  if [ -z "$ACCESS_TOKEN" ]; then
    echo "Failed to fetch access token. Please check your credentials."
    return 1
  fi

  # Create the `sub` client scope
  CLIENT_SCOPE_ID=$(curl -s -X POST "$KEYCLOAK_URL/admin/realms/$REALM_NAME/client-scopes" \
    -H "Authorization: Bearer $ACCESS_TOKEN" \
    -H "Content-Type: application/json" \
    -d '{
          "name": "sub",
          "description": "Custom scope for the sub claim",
          "protocol": "openid-connect"
        }' | jq -r '.id')

  if [ -z "$CLIENT_SCOPE_ID" ]; then
    echo "Failed to create client scope 'sub'. It may already exist."
    CLIENT_SCOPE_ID=$(curl -s -X GET "$KEYCLOAK_URL/admin/realms/$REALM_NAME/client-scopes" \
      -H "Authorization: Bearer $ACCESS_TOKEN" \
      -H "Content-Type: application/json" | jq -r '.[] | select(.name=="sub") | .id')
  fi

  echo "Client Scope ID for 'sub': $CLIENT_SCOPE_ID"

  # Add a protocol mapper for the `sub` claim
  curl -s -X POST "$KEYCLOAK_URL/admin/realms/$REALM_NAME/client-scopes/$CLIENT_SCOPE_ID/protocol-mappers/models" \
    -H "Authorization: Bearer $ACCESS_TOKEN" \
    -H "Content-Type: application/json" \
    -d '{
          "name": "sub-mapper",
          "protocol": "openid-connect",
          "protocolMapper": "oidc-usermodel-property-mapper",
          "consentRequired": false,
          "config": {
            "user.attribute": "sub",
            "claim.name": "sub",
            "jsonType.label": "String",
            "id.token.claim": "true",
            "access.token.claim": "true",
            "userinfo.token.claim": "true"
          }
        }'

  echo "Protocol mapper for 'sub' added successfully."

  # Fetch client UUID
  CLIENT_UUID=$(curl -s -X GET "$KEYCLOAK_URL/admin/realms/$REALM_NAME/clients" \
    -H "Authorization: Bearer $ACCESS_TOKEN" \
    -H "Content-Type: application/json" | jq -r ".[] | select(.clientId==\"$CLIENT_ID\") | .id")

  if [ -z "$CLIENT_UUID" ]; then
    echo "Client '$CLIENT_ID' not found in realm '$REALM_NAME'."
    return 1
  fi

  # Assign the `sub` scope to the client
  curl -s -X PUT "$KEYCLOAK_URL/admin/realms/$REALM_NAME/clients/$CLIENT_UUID/optional-client-scopes/$CLIENT_SCOPE_ID" \
    -H "Authorization: Bearer $ACCESS_TOKEN" \
    -H "Content-Type: application/json"

  echo "Client scope 'sub' assigned to client '$CLIENT_ID'."

  # Verify the configuration
  echo "Verifying the configuration..."
  curl -s -X GET "$KEYCLOAK_URL/admin/realms/$REALM_NAME/clients/$CLIENT_UUID" \
    -H "Authorization: Bearer $ACCESS_TOKEN" \
    -H "Content-Type: application/json" | jq

  echo "Automation for 'sub' scope creation completed successfully."
}

debugScope(){
  # This function is used to debug the scope of the keycloak client
  # It will fetch the client secret and print it
  KEYCLOAK_URL=$(fullKeycloakUrl)
  REALM_NAME=$(kubectl get secret keycloak-secrets -n keycloak -o jsonpath="{['data']['OAUTH_REALM']}" | base64 --decode)
  CLIENT_ID=$(kubectl get secret keycloak-secrets -n keycloak -o jsonpath="{['data']['CLIENT_ID']}" | base64 --decode)
  ADMIN_USERNAME=$(kubectl get secret keycloak-secrets -n keycloak -o jsonpath="{['data']['KEYCLOAK_ADMIN']}" | base64 --decode)
  ADMIN_PASSWORD=$(kubectl get secret keycloak -n keycloak -o jsonpath="{.data.admin-password}" | base64 --decode)

  CLIENT_SECRET=$(fetch_client_secret "$KEYCLOAK_URL" "$REALM_NAME" "$CLIENT_ID" "$ADMIN_USERNAME" "$ADMIN_PASSWORD")
  echo "Client Secret: $CLIENT_SECRET"
  if [[ -z "$CLIENT_SECRET" ]]; then
    echoFailed "Failed to fetch client secret, please check the logs!"
    return 1
  fi
  echoSuccess "Client Secret fetched successfully!"
  generateAccessToken "$KEYCLOAK_URL" "$REALM_NAME" "$CLIENT_ID" "$CLIENT_SECRET" "openid profile email"
  if [[ $? -ne 0 ]]; then
    echoFailed "Failed to generate access token, please check the logs!"
    return 1
  fi  
}

generateAccessToken() {
    local KEYCLOAK_URL=$1
    local REALM_NAME=$2
    local CLIENT_ID=$3
    local CLIENT_SECRET=$4
    local SCOPE=$5

    if [[ -z "$KEYCLOAK_URL" || -z "$REALM_NAME" || -z "$CLIENT_ID" || -z "$CLIENT_SECRET" || -z "$SCOPE" ]]; then
      echo "Usage: generateAccessToken <KEYCLOAK_URL> <REALM_NAME> <CLIENT_ID> <CLIENT_SECRET> <SCOPE>"
      return 1
    fi

    curl -s -X POST https://"$KEYCLOAK_URL/realms/$REALM_NAME/protocol/openid-connect/token" \
      -H "Content-Type: application/x-www-form-urlencoded" \
      -d "client_id=$CLIENT_ID" \
      -d "client_secret=$CLIENT_SECRET" \
      -d "grant_type=client_credentials" \
      -d "scope=$SCOPE" | jq -r '.access_token'
  }

installKeycloakWithCertMgr(){
  keycloakInst

  createLocalStorageClassAndPV "keycloak-storage" "keycloak-pv" "/data/volumes/pv3"
  # No need to generate the certificate.
  # The certificate and secret will be directory issued the ingress annotation cert-manager.io/cluster-issuer
  #gok create certificate keycloak keycloak
  gok patch ingress keycloak keycloak letsencrypt $(keycloakSubdomain)

  echo "Waiting for services to be up!!!!"
  kubectl --timeout=240s wait --for=condition=Ready pods --all --namespace keycloak
  [[ $? -eq 0 ]] && echoSuccess "Keycloak services are now up!\nAccess it using https://$(keycloakSubdomain).$(rootDomain)/" || echoFailed "Keycloak services timed-out, plaese check!!"
  echo "Wait for 10 seconds"
  sleep 10
  apt install python3-dotenv python3-requests python3-jose -y
  ADMIN_ID=$(kubectl get secret keycloak-secrets -n keycloak -o jsonpath="{['data']['KEYCLOAK_ADMIN']}" | base64 --decode)
  ADMIN_PWD=$(kubectl get secret keycloak -n keycloak -o jsonpath="{.data.admin-password}" | base64 --decode)
  CLIENT_ID=$(kubectl get secret keycloak-secrets -n keycloak -o jsonpath="{['data']['CLIENT_ID']}" | base64 --decode)
  REALM=$(kubectl get secret keycloak-secrets -n keycloak -o jsonpath="{['data']['OAUTH_REALM']}" | base64 --decode)

  python3 $MOUNT_PATH/kubernetes/install_k8s/keycloak/keycloak-client.py all $ADMIN_ID $ADMIN_PWD $CLIENT_ID $REALM

  KEYCLOAK_URL=$(fullKeycloakUrl)
  # Example usage
  create_sub_scope "https://${KEYCLOAK_URL}" "${REALM}" "${ADMIN_ID}" "${ADMIN_PWD}" "${CLIENT_ID}"
  
  oauth2Secret
  # Check if LDAP service is running
  echo "Checking if LDAP service is running..."
  LDAP_STATUS=$(kubectl get svc ldap -n ldap 2>/dev/null | grep ldap | wc -l)

  if [ "$LDAP_STATUS" -eq 0 ]; then
    echo "LDAP service is not running. Please ensure the LDAP service is up and running before proceeding."
    exit 0
  else
    echo "LDAP service is running. Proceeding with user federation creation."
  fi

  # Create the audience scope
  pushd $MOUNT_PATH/kubernetes/install_k8s/keycloak
  echo "Creating audience scope"
  chmod +x setup_kubernetes_audience.sh
  ./setup_kubernetes_audience.sh $ADMIN_ID $ADMIN_PWD
  if [[ $? -ne 0 ]]; then
    echoFailed "Audience scope creation failed, please check the logs!"
    popd
    exit 1
  fi
  echoSuccess "Audience scope created successfully!"
  popd

  # Create User Federation and Groups
  pushd $MOUNT_PATH/kubernetes/install_k8s/keycloak
  echo "Creating User Federation"
  chmod +x setup_user_federation.sh setup_group_mappers.sh
  : "${LDAP_PASSWORD:=$(promptSecret "Please enter LDAP password for admin: ")}"
  
  ./setup_user_federation.sh $ADMIN_ID $ADMIN_PWD $LDAP_PASSWORD
  if [[ $? -ne 0 ]]; then
    echoFailed "User Federation creation failed, please check the logs!"
    popd
    exit 1
  fi
  echoSuccess "User Federation created successfully!"
  echo "Creating Keycloak Groups"
  ./setup_group_mappers.sh $ADMIN_ID $ADMIN_PWD
  if [[ $? -ne 0 ]]; then
    echoFailed "Keycloak Groups creation failed, please check the logs!"
    popd
    exit 1
  fi
  echoSuccess "Keycloak Groups created successfully!"
  popd
}

installLdap(){
  pushd $MOUNT_PATH/kubernetes/install_k8s/ldap

  : "${LDAP_PASSWORD:=${1:-$(promptSecret "Please enter LDAP password for admin: ")} }"
  : "${KERBEROS_PASSWORD:=${1:-$(promptSecret "Please enter Kerberos password: ")} }"
  : "${KERBEROS_KDC_PASSWORD:=${1:-$(promptSecret "Please enter Kerberos kdc password: ")} }"
  : "${KERBEROS_ADM_PASSWORD:=${1:-$(promptSecret "Please enter Kerberos adm password: ")} }"
  
  ./run_ldap.sh $LDAP_PASSWORD $KERBEROS_PASSWORD $KERBEROS_KDC_PASSWORD $KERBEROS_ADM_PASSWORD
  if [[ $? -ne 0 ]]; then
    echoFailed "LDAP installation failed, please check the logs!"
    popd
    return 1
  fi
  gok patch ingress ldap ldap letsencrypt $(defaultSubdomain)
  popd
}

updateLdapConfig(){

  # Usage: ./update_configmap.sh <LDAP_HOSTNAME> <LDAP_ADMIN_DN> <BASE_DN>
  LDAP_HOSTNAME=$1
  BASE_DN=$2

  if [[ -z "$LDAP_HOSTNAME" || -z "$BASE_DN" ]]; then
    echo "Usage: $0 <LDAP_HOSTNAME> <BASE_DN>"
    exit 1
  fi

  # Create or update the ConfigMap
  kubectl create configmap ldap-env-config \
    --from-literal=LDAP_HOSTNAME="$LDAP_HOSTNAME" \
    --from-literal=BASE_DN="$BASE_DN" \
    --namespace=default \
    --dry-run=client -o yaml | kubectl apply -f -
}

updateUserData(){

  # Usage: ./update_user_data.sh <USERNAME> <PASSWORD> <EMAIL> <FIRST_NAME> <LAST_NAME> <GROUP_NAME>
  USERNAME=$1
  PASSWORD=$2
  EMAIL=$3
  FIRST_NAME=$4
  LAST_NAME=$5
  GROUP_NAME=$6

  # Prepare the `kubectl create configmap` command
  CONFIGMAP_CMD="kubectl create configmap ldap-user-data --namespace=default"

  # Add non-empty parameters to the ConfigMap
  [[ -n "$USERNAME" ]] && CONFIGMAP_CMD+=" --from-literal=USERNAME=\"$USERNAME\""
  [[ -n "$PASSWORD" ]] && CONFIGMAP_CMD+=" --from-literal=USER_PASSWORD=\"$PASSWORD\""
  [[ -n "$EMAIL" ]] && CONFIGMAP_CMD+=" --from-literal=EMAIL=\"$EMAIL\""
  [[ -n "$FIRST_NAME" ]] && CONFIGMAP_CMD+=" --from-literal=FIRST_NAME=\"$FIRST_NAME\""
  [[ -n "$LAST_NAME" ]] && CONFIGMAP_CMD+=" --from-literal=LAST_NAME=\"$LAST_NAME\""
  [[ -n "$GROUP_NAME" ]] && CONFIGMAP_CMD+=" --from-literal=GROUP_NAME=\"$GROUP_NAME\""

  # Add dry-run and apply options
  CONFIGMAP_CMD+=" --dry-run=client -o yaml | kubectl apply -f -"

  # Execute the command
  eval "$CONFIGMAP_CMD"
}

copySecret(){
  # Usage: ./copy_secret.sh <secret-name> <source-namespace> <target-namespace>
  SECRET_NAME=$1
  SOURCE_NAMESPACE=$2
  TARGET_NAMESPACE=$3

  if [[ -z "$SECRET_NAME" || -z "$SOURCE_NAMESPACE" || -z "$TARGET_NAMESPACE" ]]; then
    echo "Usage: $0 <secret-name> <source-namespace> <target-namespace>"
    exit 1
  fi

  # Export the secret from the source namespace
  kubectl get secret "$SECRET_NAME" -n "$SOURCE_NAMESPACE" -o yaml | \
  # Update the namespace in the YAML and apply it to the target namespace
  sed "s/namespace: $SOURCE_NAMESPACE/namespace: $TARGET_NAMESPACE/" | \
  kubectl apply -n "$TARGET_NAMESPACE" -f -
}

# createUserGroup -u skmaji -p sumit -e skm@outlook.com -f Sumit -l Maji -g hadoop -s create_ldap_user.sh
# createUserGroup -g sqa -s create_ldap_group.sh
# Function to create an LDAP user and group
createUserGroup(){

  local script_name=""
  while [[ $# -gt 0 ]]; do
    case $1 in
      -u|--username)
        local username="$2"
        shift 2
        ;;
      -p|--password)
        local password="$2"
        shift 2
        ;;
      -e|--email)
        local email="$2"
        shift 2
        ;;
      -f|--first-name)
        local first_name="$2"
        shift 2
        ;;
      -l|--last-name)
        local last_name="$2"
        shift 2
        ;;
      -g|--group-name)
        local group_name="$2"
        shift 2
        ;;
      -s|--script-name)
        script_name="$2"
        shift 2
        ;;
      *)
        echo "Unknown option: $1"
        exit 1
        ;;
    esac
  done

  if [[ -z "$script_name" ]]; then
    echo "Error: Script name is required. Use -s or --script-name to specify it."
    exit 1
  fi
  
  source $MOUNT_PATH/kubernetes/install_k8s/ldap/config/config
  
  BASE_DN=${DC}
  updateUserData "$username" "$password" "$email" "$first_name" "$last_name" "$group_name"
  copySecret ldapsecret ldap default
  updateLdapConfig "ldap://ldap.ldap.svc.cloud.uat" "$BASE_DN"

  cp ${MOUNT_PATH}/kubernetes/install_k8s/ldap/${script_name} /tmp/user_script.sh

  kubectl create configmap user-script --from-file=/tmp/user_script.sh -n default
  
  # Apply the Job
  kubectl apply -f ${MOUNT_PATH}/kubernetes/install_k8s/gokutil/job.yaml

  # Wait for the Job to complete
  kubectl wait --for=condition=complete job/gokclient-runtime-job --timeout=300s

  # Get the logs from the Job
  kubectl logs job/gokclient-runtime-job -n default


  { 
    kubectl delete job gokclient-runtime-job -n default;
    kubectl delete configmap ldap-user-data -n default; 
    kubectl delete configmap user-script -n default; 
    kubectl delete configmap ldap-env-config -n default; 
    kubectl delete secret ldapsecret -n default;
  } >> /dev/null 2>&1
}

ldapReset(){
  helm uninstall ldap -n ldap
  kubectl delete ns ldap
}

kcAdminPwd(){
  kubectl get secret keycloak -n keycloak -o jsonpath="{.data.admin-password}" | base64 --decode
}

argocdAdminPwd(){
  kubectl get secret argocd-secret -n argocd -o jsonpath='{.data.admin\.password}' | base64 -d
}

installPrometheusGrafanaWithCertMgr(){

  createLocalStorageClassAndPV "prometheus-storage" "prometheus-pv" "/data/volumes/pv1"
  createLocalStorageClassAndPV "alertmanager-storage" "alertmanager-pv" "/data/volumes/pv2"
  prometheusGrafanaInstv2

  # No need to generate the certificate.
  # The certificate and secret will be directory issued the ingress annotation cert-manager.io/cluster-issuer
  #gok create certificate monitoring kube
  #No need for prometheus to be externalized, removing the ingress as well.
  gok patch ingress prometheus-server monitoring letsencrypt $(defaultSubdomain)
  gok patch ingress grafana monitoring letsencrypt $(defaultSubdomain)

  echo "Waiting for services to be up!!!!"
  kubectl --timeout=120s wait --for=condition=Ready pods --all --namespace monitoring
  [[ $? -eq 0 ]] && echoSuccess "Prometheus and Grafana services are now up!\nAccess it using \nprometheus: https://$(defaultSubdomain).$(rootDomain)/prometheus\ngrafana: https://$(defaultSubdomain).$(rootDomain)/grafana" || echoFailed "Keycloak services timed-out, plaese check!!"
}

patchDashboardWithOauth(){
  patchOauth2Secure kubernetes-dashboard kubernetes-dashboard https://kube.gokcloud.com/dashboard/
}

patchJupyterWithOauth() {
  patchOauth2Secure jupyterhub jupyterhub https://$(defaultSubdomain).$(rootDomain)/hub/
}

patchTtydWithOauth() {
  patchOauth2Secure ttyd ttyd https://ttyd.$(rootDomain)
}

patchCloudshellWithOauth() {
  patchOauth2Secure cloudshell-cloudshell cloudshell https://$(defaultSubdomain).$(rootDomain)
}

patchControllerWithOauth() {
  patchOauth2Secure gok-controller gok-controller https://controller.$(rootDomain)
}

checkAndPatchDashboard() {
  # Check if the oauth2-proxy service is running
  echo "Checking if oauth2-proxy service is running..."
  if kubectl get pods -n oauth2 -l app.kubernetes.io/name=oauth2-proxy 2>/dev/null | grep -q "Running"; then
    echo "oauth2-proxy service is running. Proceeding to patch the dashboard..."
    patchDashboardWithOauth
  else
    echoFailed "oauth2-proxy service is not running. Please ensure it is running before patching the dashboard."
  fi
}

checkAndPatchHub() {
  # Check if the oauth2-proxy service is running
  echo "Checking if oauth2-proxy service is running..."
  if kubectl get pods -n oauth2 -l app.kubernetes.io/name=oauth2-proxy 2>/dev/null | grep -q "Running"; then
    echo "oauth2-proxy service is running. Proceeding to patch JupyterHub..."
    patchJupyterWithOauth
  else
    echo "oauth2-proxy service is not running. Please ensure it is running before patching JupyterHub."
  fi
}

cloudshellReset() {
  helm uninstall cloudshell -n cloudshell
  kubectl delete ns cloudshell
  echo "cloudshell has been uninstalled and the namespace deleted."
}


gokLoginInst() {
  pushd ${MOUNT_PATH}/kubernetes/install_k8s/gok-login
  chmod +x build.sh tag_push.sh
  ./build.sh
  ./tag_push.sh
  popd

  # Create namespace for gok-login
  kubectl create namespace gok-login || echo "Namespace gok-login already exists"

  # # Copy image pull secret if needed
  # copySecret regcred kube-system gok-login

  kubectl create configmap ca-cert --from-file=issuer.crt=/usr/local/share/ca-certificates/issuer.crt -n gok-login

  # Install gok-login using Helm chart (adjust path if needed)
  helm install gok-login ${MOUNT_PATH}/kubernetes/install_k8s/gok-login/chart -n gok-login \
  --set oidc.clientSecret="$(dataFromSecret oauth-secrets kube-system OIDC_CLIENT_SECRET)"

  # Patch ingress with letsencrypt and oauth if needed
  gok patch ingress gok-login-gok-login gok-login letsencrypt gok-login
  # No need for oauth integrtion with ingress as user will provide username and password
  # patchOauth2Secure gok-login-gok-login gok-login https://gok-login.$(rootDomain)

  echo "Waiting for gok-login to be ready..."
  kubectl --namespace gok-login wait --for=condition=Ready pods --all --timeout=120s
  if [[ $? -eq 0 ]]; then
    echoSuccess "gok-login is now up! Access it at: https://gok-login.$(rootDomain)/"
  else
    echoFailed "gok-login setup timed out. Please check the logs."
    return 1
  fi
}

cloudshellInst() {
  pushd ${MOUNT_PATH}/kubernetes/install_k8s/cloud-shell/gok
  ./build.sh
  ./tag_push.sh
  popd

  # Create namespace for cloudshell
  kubectl create namespace cloudshell || echo "Namespace cloudshell already exists"

  copySecret regcred kube-system cloudshell

  kubectl create configmap ca-cert --from-file=issuer.crt=/usr/local/share/ca-certificates/issuer.crt -n cloudshell

  helm install cloudshell ${MOUNT_PATH}/kubernetes/install_k8s/cloud-shell/gok/chart -n cloudshell

  # Patch ingress with letsencrypt and oauth if needed
  gok patch ingress cloudshell-cloudshell cloudshell letsencrypt $(defaultSubdomain)
  patchCloudshellWithOauth

  echo "Waiting for cloudshell to be ready..."
  kubectl --namespace cloudshell wait --for=condition=Ready pods --all --timeout=120s
  if [[ $? -eq 0 ]]; then
    echoSuccess "cloudshell is now up! Access it at: https://$(defaultSubdomain).$(rootDomain)/cloudshell/home"
  else
    echoFailed "cloudshell setup timed out. Please check the logs."
    return 1
  fi
}

consoleReset() {
  helm uninstall console -n console
  kubectl delete ns console
  echo "console has been uninstalled and the namespace deleted."
}

patchConsoleWithOauth() {
  patchOauth2Secure console-console console https://console.$(rootDomain)/
}

consoleInst() {
  pushd ${MOUNT_PATH}/kubernetes/install_k8s/console/app
  ./build.sh
  ./tag_push.sh
  popd


  # Create namespace for console
  kubectl create namespace console || echo "Namespace console already exists"

  # Copy image pull secret if needed
  copySecret regcred kube-system console

  kubectl create configmap ca-cert --from-file=issuer.crt=/usr/local/share/ca-certificates/issuer.crt -n console

  # Install console using Helm chart (adjust path if needed)
  helm install console ${MOUNT_PATH}/kubernetes/install_k8s/console/app/chart -n console

  # Patch ingress with letsencrypt and oauth if needed
  gok patch ingress console-console console letsencrypt console
  # Optionally patch with oauth if you use oauth2-proxy or similar
  patchConsoleWithOauth

  echo "Waiting for console to be ready..."
  kubectl --namespace console wait --for=condition=Ready pods --all --timeout=120s
  if [[ $? -eq 0 ]]; then
    echoSuccess "console is now up! Access it at: https://console.$(rootDomain)/"
  else
    echoFailed "console setup timed out. Please check the logs."
    return 1
  fi
}

installDashboardwithCertManager(){
  dashboardInst
  # No need to generate the certificate.
  # The certificate and secret will be directory issued the ingress annotation cert-manager.io/cluster-issuer
  #gok create certificate kubernetes-dashboard kube
  gok patch ingress kubernetes-dashboard kubernetes-dashboard letsencrypt $(defaultSubdomain)
  checkAndPatchDashboard
  echo "Waiting for services to be up!!!!"
  kubectl --timeout=120s wait --for=condition=Ready pods --all --namespace kubernetes-dashboard
  [[ $? -eq 0 ]] && echoSuccess "Dashboard services are now up!\nAccess it using https://$(defaultSubdomain).$(rootDomain)/dashboard/" || echoFailed "Keycloak services timed-out, plaese check!!"
}

# add_audience_mapper_to_groups_scope \
#   "https://keycloak.gokcloud.com" \
#   "GokDevelopers" \
#   "admin" \
#   "your-admin-password" \
#   "gok-developers-client"

add_audience_mapper_to_groups_scope() {
  # Required inputs
  local KEYCLOAK_URL=$1         # e.g. https://keycloak.gokcloud.com
  local REALM=$2                # e.g. GokDevelopers
  local ADMIN_USER=$3           # e.g. admin
  local ADMIN_PASS=$4           # e.g. <admin-password>
  local CLIENT_ID=$5            # e.g. gok-developers-client

  # Get admin access token
  local ADMIN_TOKEN=$(curl -s -X POST "$KEYCLOAK_URL/realms/master/protocol/openid-connect/token" \
    -d "username=$ADMIN_USER" \
    -d "password=$ADMIN_PASS" \
    -d "grant_type=password" \
    -d "client_id=admin-cli" \
    | jq -r .access_token)

  if [[ -z "$ADMIN_TOKEN" || "$ADMIN_TOKEN" == "null" ]]; then
    echo "Failed to get admin token"
    return 1
  fi

  # Get client UUID
  local CLIENT_UUID=$(curl -s -X GET "$KEYCLOAK_URL/admin/realms/$REALM/clients" \
    -H "Authorization: Bearer $ADMIN_TOKEN" \
    | jq -r ".[] | select(.clientId==\"$CLIENT_ID\") | .id")

  if [[ -z "$CLIENT_UUID" ]]; then
    echo "Client $CLIENT_ID not found"
    return 1
  fi

  # Add Audience protocol mapper to client
  curl -s -X POST "$KEYCLOAK_URL/admin/realms/$REALM/clients/$CLIENT_UUID/protocol-mappers/models" \
    -H "Authorization: Bearer $ADMIN_TOKEN" \
    -H "Content-Type: application/json" \
    -d "{
      \"name\": \"audience-groups\",
      \"protocol\": \"openid-connect\",
      \"protocolMapper\": \"oidc-audience-mapper\",
      \"consentRequired\": false,
      \"config\": {
        \"included.client.audience\": \"$CLIENT_ID\",
        \"id.token.claim\": \"true\",
        \"access.token.claim\": \"true\"
      }
    }"

  echo "Audience protocol mapper added to client $CLIENT_ID."

  # (Optional) Assign groups scope to client if not already assigned
  # Get groups client scope ID
  local GROUPS_SCOPE_ID=$(curl -s -X GET "$KEYCLOAK_URL/admin/realms/$REALM/client-scopes" \
    -H "Authorization: Bearer $ADMIN_TOKEN" \
    | jq -r '.[] | select(.name=="groups") | .id')

  if [[ -n "$GROUPS_SCOPE_ID" ]]; then
    # Assign as default client scope if not already
    curl -s -X PUT "$KEYCLOAK_URL/admin/realms/$REALM/clients/$CLIENT_UUID/default-client-scopes/$GROUPS_SCOPE_ID" \
      -H "Authorization: Bearer $ADMIN_TOKEN"
    echo "Assigned 'groups' scope to client $CLIENT_ID."
  else
    echo "No 'groups' client scope found, skipping scope assignment."
  fi
}

generate_kubeconfig() {
  read -p "Enter your Kubernetes API server URL (e.g., https://kubernetes.default.svc): " KUBE_API
  read -p "Enter your Kubernetes namespace: " KUBE_NAMESPACE
  read -p "Enter your Bearer token: " KUBE_TOKEN

  # Optional: set a context name
  CONTEXT_NAME="user-ttyd"

  cat > kubeconfig <<EOF
apiVersion: v1
kind: Config
clusters:
- cluster:
    server: ${KUBE_API}
    insecure-skip-tls-verify: true
  name: cluster
users:
- name: user
  user:
    token: ${KUBE_TOKEN}
contexts:
- context:
    cluster: cluster
    user: user
    namespace: ${KUBE_NAMESPACE}
  name: ${CONTEXT_NAME}
current-context: ${CONTEXT_NAME}
EOF

  echo "Kubeconfig written to ./kubeconfig"
  echo "To use it, run:"
  echo "  export KUBECONFIG=$(pwd)/kubeconfig"
}

get_keycloak_token() {
  read -p "Keycloak URL (default: https://keycloak.gokcloud.com/realms/GokDevelopers/protocol/openid-connect/token): " KEYCLOAK_URL
  KEYCLOAK_URL=${KEYCLOAK_URL:-https://keycloak.gokcloud.com/realms/GokDevelopers/protocol/openid-connect/token}

  read -p "Client ID (default: gok-developers-client): " CLIENT_ID
  CLIENT_ID=${CLIENT_ID:-gok-developers-client}
  read -p "Client Secret: " CLIENT_SECRET
  read -p "Username: " USERNAME
  read -s -p "Password: " PASSWORD
  echo

  TOKEN_RESPONSE=$(curl -s -X POST "$KEYCLOAK_URL" \
    -d "client_id=$CLIENT_ID" \
    -d "client_secret=$CLIENT_SECRET" \
    -d "grant_type=password" \
    -d "username=$USERNAME" \
    -d "password=$PASSWORD")

  ACCESS_TOKEN=$(echo "$TOKEN_RESPONSE" | grep -o '"access_token":"[^"]*' | grep -o '[^"]*$')
  if [ -n "$ACCESS_TOKEN" ]; then
    echo "Your access token:"
    echo "$ACCESS_TOKEN"
  else
    echo "Failed to get token. Response:"
    echo "$TOKEN_RESPONSE"
  fi
}

oauth2ProxyReset(){
  helm uninstall oauth2proxy -n oauth2

  emptyLocalFsStorage "OAuth2" "oauth-pv" "oauth-storage" "/data/volumes/pv6" "oauth2"
  kubectl delete ns oauth2
}

#export NODE_TLS_REJECT_UNAUTHORIZED=0
jupyterHubInst() {
  # Add the Helm repository for JupyterHub
  helm repo add jupyterhub https://jupyterhub.github.io/helm-chart/
  helm repo update

  # Create the storage class for JupyterHub using the generic method
  createLocalStorageClassAndPV "jupyter-storage" "jupyter-pv" "/data/volumes/jupyter"
  createLocalStorageClassAndPV "jupyter-user-storage" "jupyter-user-pv" "/data/volumes/jupyter-user"
  # Create a namespace for JupyterHub
  kubectl create namespace jupyterhub || echo "Namespace jupyterhub already exists"

  # Generate a random secret token for JupyterHub
  JUPYTERHUB_SECRET=$(openssl rand -hex 32)

  # Create a Kubernetes secret for the JupyterHub proxy
  kubectl create secret generic hub-secret \
    --from-literal=hub-secret-key="${JUPYTERHUB_SECRET}" \
    --namespace jupyterhub || echo "Secret hub-secret already exists"

  CLIENT_SECRET=$(dataFromSecret oauth-secrets kube-system OIDC_CLIENT_SECRET)
  CLIENT_ID=$(dataFromSecret oauth-secrets kube-system OIDC_CLIENT_ID)
  keycloakUrl="https://$(fullKeycloakUrl)"
  REALM=$(dataFromSecret oauth-secrets kube-system OAUTH_REALM)
  OAUTH2_HOST="$(fullKeycloakUrl)"
  ENABLE_OAUTH2="true"

  # Install JupyterHub using Helm with the provided values.yaml
  helm install jupyterhub jupyterhub/jupyterhub \
    --namespace jupyterhub \
    --set proxy.secretToken="${JUPYTERHUB_SECRET}" \
    --values "${MOUNT_PATH}/kubernetes/install_k8s/jupyter/values.yaml" \
    $(if [[ "$ENABLE_OAUTH2" == "true" ]]; then
      echo "--set hub.config.GenericOAuthenticator.client_id=${CLIENT_ID} \
      --set hub.config.GenericOAuthenticator.client_secret=${CLIENT_SECRET} \
      --set hub.config.GenericOAuthenticator.oauth_callback_url=https://$(jupyterHubSubdomain).$(rootDomain)/oauth_callback \
      --set hub.config.GenericOAuthenticator.authorize_url=https://${OAUTH2_HOST}/realms/${REALM}/protocol/openid-connect/auth \
      --set hub.config.GenericOAuthenticator.token_url=https://${OAUTH2_HOST}/realms/${REALM}/protocol/openid-connect/token \
      --set hub.config.GenericOAuthenticator.userdata_url=https://${OAUTH2_HOST}/realms/${REALM}/protocol/openid-connect/userinfo \
      --set hub.config.GenericOAuthenticator.login_service=keycloak \
      --set hub.config.GenericOAuthenticator.username_claim=preferred_username \
      --set hub.config.GenericOAuthenticator.userdata_params.state=state \
      --set hub.config.GenericOAuthenticator.allow_all=true \
      --set hub.config.GenericOAuthenticator.tls_verify=false \
      --set hub.config.GenericOAuthenticator.admin_users[0]=admin \
      --set hub.config.GenericOAuthenticator.admin_users[1]=skmaji1 \
      --set hub.config.JupyterHub.authenticator_class=generic-oauth"
    fi)

  # Wait for the JupyterHub pods to be ready
  echo "Waiting for JupyterHub services to be up..."
  kubectl --namespace jupyterhub wait --for=condition=Ready pods --all --timeout=300s

  gok patch ingress jupyterhub jupyterhub letsencrypt $(jupyterHubSubdomain)
  # checkAndPatchHub

  if [[ $? -eq 0 ]]; then
    echoSuccess "JupyterHub services are now up!"
    echoSuccess "Access it using https://master.cloud.com"
  else
    echoFailed "JupyterHub services timed out. Please check the logs!"
  fi
}

jupyterHubReset() {
  helm uninstall jupyterhub -n jupyterhub
  kubectl delete secret hub-secret -n jupyterhub
  kubectl delete pod --all -n jupyterhub
  kubectl delete pvc --all -n jupyterhub
  emptyLocalFsStorage "JupyterHub" "jupyter-pv" "jupyter-storage" "/data/volumes/jupyter"
  emptyLocalFsStorage "JupyterHub User" "jupyter-user-pv" "jupyter-user-storage" "/data/volumes/jupyter-user"
  kubectl delete ns jupyterhub
}

installKubectlClient(){
  # Install kubectl client
  if ! command -v kubectl &> /dev/null; then
    echo "kubectl not found, installing..."
    curl -LO "https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl"
    chmod +x kubectl
    sudo mv kubectl /usr/local/bin/
    echoSuccess "kubectl installed successfully!"
  else
    echo "kubectl is already installed."
  fi
}   

installDocker(){
  # Install Docker
  if ! command -v docker &> /dev/null; then
    echo "Docker not found, installing..."
    sudo apt-get update
    sudo apt-get install -y docker.io
    sudo systemctl start docker
    sudo systemctl enable docker
    echoSuccess "Docker installed successfully!"
  else
    echo "Docker is already installed."
  fi

  # Add current user to the docker group
  sudo usermod -aG docker $USER
  echoSuccess "Current user added to the docker group. Please log out and log back in for changes to take effect."
}

# Method to remove claimRef from a specific Persistent Volume
# removeClaimRefFromPV "eclipse-che-pv"
removeClaimRefFromPV() {
  local pv_name=$1

  if [ -z "$pv_name" ]; then
    echo "Error: No PV name provided."
    return 1
  fi

  # Check if the PV exists and is in the "Released" state
  pv_state=$(kubectl get pv "$pv_name" --no-headers | awk '{print $5}')
  if [ "$pv_state" != "Released" ]; then
    echo "Error: PV '$pv_name' is not in the 'Released' state. Current state: $pv_state"
    return 1
  fi

  echo "Processing PV: $pv_name"

  # Patch the PV to remove the claimRef
  kubectl patch pv "$pv_name" --type=json -p='[{"op": "remove", "path": "/spec/claimRef"}]'

  if [ $? -eq 0 ]; then
    echo "Successfully removed claimRef from PV: $pv_name"
    return 0
  else
    echo "Failed to remove claimRef from PV: $pv_name"
    return 1
  fi
}


eclipseCheInst(){

  # Fetch OIDC client secret and other required parameters from the secret
  CLIENT_SECRET=$(dataFromSecret oauth-secrets kube-system OIDC_CLIENT_SECRET)
  CLIENT_ID=$(dataFromSecret oauth-secrets kube-system OIDC_CLIENT_ID)
  REALM_NAME=$(dataFromSecret oauth-secrets kube-system OAUTH_REALM)

  createLocalStorageClassAndPV "eclipse-che-storage" "eclipse-che-pv" "/data/volumes/eclipse-che"
  
  # Install Chectl
  wget https://che-incubator.github.io/chectl/install.sh
  chmod +x install.sh
  ./install.sh
  [[ $? -ne 0 ]] && echoFailed "Chectl installation failed, please check the logs!" && return 1
  echoSuccess "Chectl installed successfully!"

  # Create a patch file for CheCluster
  cat > che-patch.yaml << EOF
kind: CheCluster
apiVersion: org.eclipse.che/v2
spec:
  devEnvironments:
    secondsOfInactivityBeforeIdling: -1
    storage:
      perUserStrategyPvcConfig:
        claimSize: 2Gi 
        storageClass: eclipse-che-storage
      pvcStrategy: per-user 
    
  networking:
    auth:
      oAuthClientName: ${CLIENT_ID}
      oAuthSecret: ${CLIENT_SECRET}
      identityProviderURL: "https://$(fullKeycloakUrl)/realms/${REALM_NAME}"
      gateway:
        oAuthProxy:
          cookieExpireSeconds: 300
  components:
    cheServer:
      extraVolumes:
        - name: keycloak-ca
          configMap:
            name: keycloak-certs
      extraVolumeMounts:
        - name: keycloak-ca
          mountPath: /etc/ssl/certs/keycloak-ca.crt
          subPath: keycloak-ca.crt
          readOnly: true
      extraProperties:
        CHE_OIDC_USERNAME__CLAIM: email
        CHE_OIDC_SKIP_CERTIFICATE_VERIFICATION: "true"
        REQUESTS_CA_BUNDLE: /etc/ssl/certs/keycloak-ca.crt
EOF

  kubectl create ns eclipse-che || echo "Namespace eclipse-che already exists"

  # Create a secret for Keycloak CA certificate
  kubectl get secret keycloak-gokcloud-com -n keycloak -o jsonpath="{.data['ca\.crt']}" | base64 -d > keycloak-ca.crt
  kubectl create configmap keycloak-certs --from-file=keycloak-ca.crt=keycloak-ca.crt -n eclipse-che
  kubectl label configmap keycloak-certs app.kubernetes.io/part-of=che.eclipse.org app.kubernetes.io/component=ca-bundle -n eclipse-che

  # Deploy Eclipse Che using chectl
  chectl server:deploy --platform k8s --domain che.gokcloud.com --che-operator-cr-patch-yaml che-patch.yaml --skip-cert-manager
  [[ $? -eq 0 ]] && echoSuccess "Eclipse Che is now installed and running!" || echoFailed "Eclipse Che installation failed, please check the logs!"
  rm -f keycloak-ca.crt che-patch.yaml install.sh
}

resetEclipseChe(){
  # Uninstall Eclipse Che
  kubectl delete deployment --all -n eclipse-che
  chectl server:delete --delete-namespace --delete-all
  [[ $? -ne 0 ]] && echoFailed "Eclipse Che uninstallation failed, please check the logs!" && return 1
  kubectl delete configmap keycloak-certs -n eclipse-che || echo "ConfigMap keycloak-certs already deleted or does not exist"
  emptyLocalFsStorage "Eclipse Che" "eclipse-che-pv" "eclipse-che-storage" "/data/volumes/eclipse-che"
  echoSuccess "Eclipse Che has been uninstalled successfully!"
}

oauth2ProxyInst(){

  createLocalStorageClassAndPV "oauth-storage" "oauth-pv" "/data/volumes/pv6"
  kubectl create ns oauth2

  OAUTH2_PROXY_CLIENT_ID=$(dataFromSecret oauth-secrets kube-system OIDC_CLIENT_ID)
  REALM=$(dataFromSecret oauth-secrets kube-system OAUTH_REALM)
  OAUTH2_HOST="$(fullKeycloakUrl)"
  OAUTH2_OIDC_ISSUE_URL="$(dataFromSecret oauth-secrets kube-system OIDC_ISSUE_URL)"

  kubectl create configmap oauth2-proxy-config \
      --from-literal=clientID="${OAUTH2_PROXY_CLIENT_ID}" \
      --from-literal=oidcIssuerUrl="${OAUTH2_OIDC_ISSUE_URL}" \
      --from-literal=redirectUrl=oauth2/callback \
      -n oauth2

  kubectl create configmap oauth2-ca-cert --from-file=issuer.crt=/usr/local/share/ca-certificates/issuer.crt -n oauth2

  helm install oauth2proxy oci://registry-1.docker.io/bitnamicharts/oauth2-proxy \
    --namespace oauth2 \
    --set "extraArgs[0]=--provider=oidc" \
    --set "extraArgs[1]=--login-url=https://${OAUTH2_HOST}/realms/${REALM}/protocol/openid-connect/auth" \
    --set "extraArgs[2]=--redeem-url=https://${OAUTH2_HOST}/realms/${REALM}/protocol/openid-connect/token" \
    --set "extraArgs[3]=--profile-url=https://${OAUTH2_HOST}/realms/${REALM}/protocol/openid-connect/userinfo" \
    --set "extraArgs[4]=--validate-url=https://${OAUTH2_HOST}/realms/${REALM}/protocol/openid-connect/userinfo" \
    --set "extraArgs[5]=--keycloak-group=administrators" \
    --set "extraArgs[6]=--keycloak-group=developers" \
    --set "extraArgs[7]=--allowed-group=administrators" \
    --set "extraArgs[8]=--allowed-group=developers" \
    --set "extraArgs[9]=--scope=openid email profile groups sub offline_access" \
    --set "extraArgs[10]=--ssl-insecure-skip-verify=false" \
    --set "extraArgs[11]=--set-authorization-header=true" \
    --set "extraArgs[12]=--whitelist-domain=.gokcloud.com" \
    --set "extraArgs[13]=--oidc-groups-claim=groups" \
    --set "extraArgs[14]=--user-id-claim=sub" \
    --set "extraArgs[15]=--cookie-domain=.gokcloud.com" \
    --set "extraArgs[16]=--cookie-secure=true" \
    --set "extraArgs[17]=--pass-access-token=true" \
    --set "extraArgs[18]=--pass-authorization-header=true" \
    --set "extraArgs[19]=--oidc-issuer-url=${OAUTH2_OIDC_ISSUE_URL}" \
    --set "extraArgs[20]=--standard-logging=true" \
    --set "extraArgs[21]=--auth-logging=true" \
    --set "extraArgs[22]=--request-logging=true" \
    --set "extraArgs[23]=--cookie-refresh=1h" \
    --set "extraArgs[24]=--cookie-expire=8h" \
    --set "extraArgs[25]=--set-xauthrequest=true" \
    --set "extraArgs[26]=--skip-jwt-bearer-tokens=true" \
    --values "${MOUNT_PATH}"/kubernetes/install_k8s/oauth2-proxy/values.yaml \
    --create-namespace
  gok patch ingress oauth2proxy-oauth2-proxy oauth2 letsencrypt $(defaultSubdomain)
  waitForServiceAvailable oauth2
}

waitForServiceAvailable(){
  NS=$1
  echo "Waiting for services to be up!!!!"
  kubectl --timeout=120s wait --for=condition=Ready pods --all --namespace "$NS"
  [[ $? -eq 0 ]] && echoSuccess "Service is now up!" || echoFailed "Service timed-out, please check!!"
}

resolveDns() {
  local domain=$1
  if [ -z "$domain" ]; then
    echo "Usage: resolveDns <domain>"
    return 1
  fi

  kubectl run -it --rm dnsutils --image=busybox:1.28 --restart=Never -- nslookup "$domain"
}

ttydReset() {
  helm uninstall ttyd -n ttyd
  kubectl delete ns ttyd
  echo "ttyd has been uninstalled and the namespace deleted."
}

ttydInst() {
  # Create namespace for ttyd
  kubectl create namespace ttyd || echo "Namespace ttyd already exists"

  # Add Helm repo if needed (ttyd uses Docker image directly, so no repo needed)
  # Prepare a minimal values.yaml if you want customization

  # Install ttyd using a local chart directory or a published chart
  helm install ttyd ${MOUNT_PATH}/kubernetes/install_k8s/ttyd/chart \
    --namespace ttyd \
    --set ingress.enabled=true \
    --set ingress.host="ttyd.$(rootDomain)"

  gok patch ingress ttyd ttyd letsencrypt ttyd
  patchTtydWithOauth

  echo "Waiting for ttyd to be ready..."
  kubectl --namespace ttyd wait --for=condition=Ready pods --all --timeout=120s
  if [[ $? -eq 0 ]]; then
    echoSuccess "ttyd is now up! Access it at: https://ttyd.$(rootDomain)"
  else
    echoFailed "ttyd setup timed out. Please check the logs."
    return 1
  fi
}


enableJenkins(){
  USERNAME=$(promptUserInput "Enter Jenkins Username (skmaji1): " "skmaji1")
  PASSWORD=$(promptSecret "Enter Jenkins Password: ")

  docker exec -it halyard hal config ci jenkins enable

  docker exec -it halyard hal config ci jenkins master add gok-jenkins-master \
    --address https://jenkins.gokcloud.com \
    --username ${USERNAME} \
    --password ${PASSWORD}

  docker exec -it halyard hal config ci jenkins master edit gok-jenkins-master --csrf true

  docker exec -it halyard hal deploy apply
}

spinnakerInst(){
  CLIENT_SECRET=$(dataFromSecret oauth-secrets kube-system OIDC_CLIENT_SECRET)
  CLIENT_ID=$(dataFromSecret oauth-secrets kube-system OIDC_CLIENT_ID)
  OIDC_ISSUE_URL=$(dataFromSecret oauth-secrets kube-system OIDC_ISSUE_URL)

  useradd -m -g root spinnaker
  mkdir -p /home/spinnaker/.hal && chmod -R 777 /home/spinnaker/.hal
  mkdir -p /home/spinnaker/.kube/ && cp ~/.kube/config /home/spinnaker/.kube/config && chmod 755 /home/spinnaker/.kube/config
  docker run --name halyard -v /home/spinnaker/.hal:/home/spinnaker/.hal -v /home/spinnaker/.kube/config:/home/spinnaker/.kube/config \
    -d gcr.io/spinnaker-marketplace/halyard:stable

  echo "Waiting for the service to be up for 30 Seconds"
  sleep 30
  docker exec -it halyard kubectl cluster-info
  docker exec -it halyard hal config provider kubernetes enable
  docker exec -it halyard hal config provider kubernetes account add gok-k8s --provider-version v2 \
    --context $(kubectl config current-context)
  docker exec -it halyard hal config features edit --artifacts true
  docker exec -it halyard hal config deploy edit --type distributed --account-name gok-k8s
  kubectl create ns spinnaker
  helm repo add stable https://charts.helm.sh/stable
  helm install minio --namespace spinnaker --set accessKey="myaccesskey" --set secretKey="mysecretkey" \
    --set persistence.enabled=false stable/minio
  docker exec -it halyard bash -c 'mkdir /home/spinnaker/.hal/default/profiles'
  docker exec -it halyard echo "spinnaker.s3.versioning: false" > /home/spinnaker/.hal/default/profiles/front50-local.yml

  cat <<EOF > /home/spinnaker/.hal/default/profiles/front50-local.yml
kubernetes:
  volumes:
  - id: internal-trust-store
    mountPath: /etc/ssl/certs/java
    type: secret
EOF

  ##https://github.com/spinnaker/spinnaker/issues/6498
  cat <<EOF > /home/spinnaker/.hal/permission.yml
users:
 - username: skmaji1
   roles:
   - admin
EOF

  cat <<EOF > /home/spinnaker/.hal/default/profiles/spinnaker-local.yml
logging:
  level:
    # Enable debug logging by changing level to DEBUG
    root: TRACE  # default
EOF

  #https://stackoverflow.com/questions/78087469/how-to-apply-custom-profiles-setting-to-spinnaker-to-make-it-deploy-with-one-com
  cat <<EOF > /home/spinnaker/.hal/default/profiles/fiat-local.yml
fiat.restrictApplicationCreation: true #Allows to restrict permissions
auth.permissions.provider.application: aggregate
auth.permissions.source.application.prefix: #Allows to work with
  enabled: true                          # applications prefixes
  prefixes:
  - prefix: "*" # All applications
    permissions:
      READ:
      - "administrators"
      - "admin"
      WRITE:
      - "administrators"
      - "admin"
      EXECUTE:
      - "administrators"
      - "admin"
      CREATE:
      - "administrators"
      - "admin"
EOF

  docker exec -it halyard hal config storage s3 edit --endpoint http://minio:9000 --access-key-id "myaccesskey" --secret-access-key "mysecretkey"
  docker exec -it halyard hal config storage s3 edit --path-style-access true
  docker exec -it halyard hal config storage edit --type s3
  docker exec -it halyard hal version list
  docker exec -it halyard hal config version edit --version 1.34.2

  cat <<EOF > /home/spinnaker/.hal/default/profiles/gate-local.yml
kubernetes:
  volumes:
  - id: internal-trust-store
    mountPath: /etc/ssl/certs
    type: secret
server:
  tomcat:
    protocolHeader: X-Forwarded-Proto
    remoteIpHeader: X-Forwarded-For
    internalProxies: .*
    httpsServerPort: X-Forwarded-Port
security:
  oauth2:
    enabled: true
    client:
      clientId: gok-developers-client
      clientSecret: $CLIENT_SECRET
      userAuthorizationUri: $OIDC_ISSUE_URL/protocol/openid-connect/auth
      accessTokenUri: $OIDC_ISSUE_URL/protocol/openid-connect/token
      scope: openid,email,profile,groups
      preEstablishedRedirectUri: https://spin-gate.gokcloud.com/login
      clientAuthenticationScheme: form
    resource:
      userInfoUri: $OIDC_ISSUE_URL/protocol/openid-connect/userinfo
    userInfoMapping:
      email: email
      firstName: given_name
      lastName: family_name
      username: preferred_username
      roles: groups
EOF

  cat <<EOF | kubectl apply -f -
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  annotations:
    kubernetes.io/ingress.allow-http: "false"
    nginx.ingress.kubernetes.io/ssl-redirect: "true"
    ingress.kubernetes.io/ssl-passthrough: "true"
    nginx.ingress.kubernetes.io/backend-protocol: "HTTP"
  name: spin-deck
  namespace: spinnaker
spec:
  ingressClassName: nginx
  rules:
  - host: spinnaker.cloud.com
    http:
      paths:
      - backend:
          service:
            name: spin-deck
            port:
              number: 9000
        path: /
        pathType: ImplementationSpecific
  tls:
    - secretName: appingress-certificate
      hosts:
        - spinnaker.cloud.com
---
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  annotations:
    kubernetes.io/ingress.allow-http: "false"
    nginx.ingress.kubernetes.io/ssl-redirect: "true"
    ingress.kubernetes.io/ssl-passthrough: "true"
    nginx.ingress.kubernetes.io/backend-protocol: "HTTP"
  name: spin-gate
  namespace: spinnaker
spec:
  ingressClassName: nginx
  rules:
  - host: spin-gate.cloud.com
    http:
      paths:
      - backend:
          service:
            name: spin-gate
            port:
              number: 8084
        path: /
        pathType: ImplementationSpecific
  tls:
    - secretName: appingress-certificate
      hosts:
        - spin-gate.cloud.com
EOF

  gok patch ingress spin-deck spinnaker letsencrypt spinnaker
  gok patch ingress spin-gate spinnaker letsencrypt spin-gate

  docker exec -it halyard hal config security ui edit --override-base-url "https://spinnaker.gokcloud.com"
  docker exec -it halyard hal config security api edit --override-base-url "https://spin-gate.gokcloud.com"
  docker exec -it halyard hal config security api edit --cors-access-pattern "https://spinnaker.gokcloud.com"

  #Authentication
  docker exec -it halyard hal config security authn oauth2 edit --client-id $CLIENT_ID --client-secret $CLIENT_SECRET --provider OTHER --pre-established-redirect-uri https://spin-gate.gokcloud.com/login
  docker exec -it halyard hal config security authn oauth2 enable

  #Authorization
  #Roles will be fetched from keycloak
  #https://github.com/spinnaker/spinnaker/issues/6498
  #docker exec -it halyard hal config security authz file edit --file-path /home/spinnaker/.hal/permission.yml
  docker exec -it halyard hal config security authz enable

  docker exec -it halyard hal config artifact github account add sumitmaji

  kubectl get secrets -n cert-manager gokselfsign-ca -o json | jq -r '.data["tls.crt"]' | base64 -d > /home/spinnaker/.hal/issuer.crt
  kubectl get secrets -n keycloak keycloak-gokcloud-com -o json | jq -r '.data["tls.crt"]' | base64 -d > /home/spinnaker/.hal/keycloak.crt
  kubectl get secrets -n jenkins jenkins-gokcloud-com -o json | jq -r '.data["tls.crt"]' | base64 -d > /home/spinnaker/.hal/jenkins.crt
  
  docker exec -it halyard  cp /etc/ssl/certs/java/cacerts /home/spinnaker/.hal/
  chmod +w /home/spinnaker/.hal/cacerts
  #password is changeit
  docker exec -it halyard keytool -import -noprompt -trustcacerts -alias ca -file /home/spinnaker/.hal/issuer.crt -keystore /home/spinnaker/.hal/cacerts
  docker exec -it halyard keytool -import -noprompt -trustcacerts -alias keycloak -file /home/spinnaker/.hal/keycloak.crt -keystore /home/spinnaker/.hal/cacerts
  docker exec -it halyard keytool -import -noprompt -trustcacerts -alias jenkins -file /home/spinnaker/.hal/jenkins.crt -keystore /home/spinnaker/.hal/cacerts

  kubectl create secret generic -n spinnaker internal-trust-store \
  	--from-file /home/spinnaker/.hal/cacerts

  #https://spinnaker.io/docs/reference/halyard/custom/
  mkdir -p /home/spinnaker/.hal/default/service-settings/
  cat <<EOF > /home/spinnaker/.hal/default/service-settings/gate.yml
kubernetes:
  volumes:
  - id: internal-trust-store
    mountPath: /etc/ssl/certs/java
    type: secret
EOF

  cat <<EOF > /home/spinnaker/.hal/default/service-settings/igor.yml
kubernetes:
  volumes:
  - id: internal-trust-store
    mountPath: /etc/ssl/certs/java
    type: secret
EOF

  docker exec -it halyard hal config artifact github enable
  docker exec -it halyard hal config artifact github account add gok-github

  docker exec -it halyard hal config artifact helm enable
  docker exec -it halyard hal config artifact helm account add gok-helm \
    --repository https://chart.gokcloud.com --username sumit --password abcdef


  docker exec -it halyard hal deploy apply
}

rabbitmqInst(){
  echo "Installing RabbitMQ using Cluster Operator..."
  
  kubectl create namespace rabbitmq || true
  createLocalStorageClassAndPV "rabbitmq-storage" "rabbitmq-pv" "/data/volumes/rabbitmq"
  
  # Install RabbitMQ Cluster Operator
  kubectl apply -f "https://github.com/rabbitmq/cluster-operator/releases/latest/download/cluster-operator.yml"
  
  # Wait for operator to be ready
  kubectl wait --for=condition=available deployment/rabbitmq-cluster-operator --timeout=300s -n rabbitmq-system
  
  # Create RabbitMQ cluster
  cat <<EOF | kubectl apply -f -
apiVersion: rabbitmq.com/v1beta1
kind: RabbitmqCluster
metadata:
  name: rabbitmq
  namespace: rabbitmq
spec:
  replicas: 1
  image: rabbitmq:3.12-management
  resources:
    requests:
      cpu: 256m
      memory: 1Gi
    limits:
      cpu: 1
      memory: 2Gi
  rabbitmq:
    additionalConfig: |
      log.console.level = info
      channel_max = 1700
      default_user_tags.administrator = true
      management.tcp.port = 15672
  persistence:
    storageClassName: rabbitmq-storage
    storage: 10Gi
  service:
    type: ClusterIP
  override:
    statefulSet:
      spec:
        template:
          spec:
            containers:
            - name: rabbitmq
              ports:
              - containerPort: 5672
                name: amqp
              - containerPort: 15672
                name: management
EOF

  # Create ingress for management UI
  cat <<EOF | kubectl apply -f -
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: rabbitmq-management
  namespace: rabbitmq
  annotations:
    nginx.ingress.kubernetes.io/rewrite-target: /
spec:
  ingressClassName: nginx
  rules:
  - host: rabbitmq.$(rootDomain)
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: rabbitmq
            port:
              number: 15672
EOF

  # Wait for RabbitMQ to be ready - use the correct condition
  echo "Waiting for RabbitMQ cluster to be ready..."
  kubectl wait --for=condition=AllReplicasReady rabbitmqcluster/rabbitmq -n rabbitmq --timeout=600s
  
  # Alternative: Wait for the StatefulSet to be ready as well
  kubectl wait --for=condition=ready pod -l app.kubernetes.io/name=rabbitmq -n rabbitmq --timeout=300s
  
  # Get default credentials
  echo "RabbitMQ default credentials:"
  echo "Username: $(kubectl get secret rabbitmq-default-user -n rabbitmq -o jsonpath='{.data.username}' | base64 --decode)"
  echo "Password: $(kubectl get secret rabbitmq-default-user -n rabbitmq -o jsonpath='{.data.password}' | base64 --decode)"
  
  gok patch ingress rabbitmq-management rabbitmq letsencrypt rabbitmq
}

rabbitmqReset(){
  # Delete RabbitMQ cluster
  kubectl delete rabbitmqcluster rabbitmq -n rabbitmq --ignore-not-found=true
  
  # Delete operator (optional - might affect other clusters)
  kubectl delete -f "https://github.com/rabbitmq/cluster-operator/releases/latest/download/cluster-operator.yml"
  
  # Clean up namespace and storage
  kubectl delete namespace rabbitmq --ignore-not-found=true
  emptyLocalFsStorage "RabbitMQ" "rabbitmq-pv" "rabbitmq-storage" "/data/volumes/rabbitmq"
}


rabbitmqResetLegacy(){
  kubectl delete all --all -n rabbitmq
  kubectl delete ns rabbitmq
  emptyLocalFsStorage "RabbitMQ" "rabbitmq-pv" "rabbitmq-storage" "/data/volumes/rabbitmq" "rabbitmq"
}

rabbitmqInstLegacy(){
  helm repo add bitnami https://charts.bitnami.com/bitnami
  helm repo update

  kubectl create namespace rabbitmq || true
  createLocalStorageClassAndPV "rabbitmq-storage" "rabbitmq-pv" "/data/volumes/rabbitmq"
  
  helm install rabbitmq bitnami/rabbitmq \
    --namespace rabbitmq \
    --create-namespace \
    --values "${MOUNT_PATH}"/kubernetes/install_k8s/rabbitmq/values.yaml

  kubectl get secret --namespace rabbitmq rabbitmq -o jsonpath="{.data.rabbitmq-password}" | base64 --decode
  echo
}


argocdReset(){
  helm uninstall argocd -n argocd
  kubectl delete ns argocd
  # emptyLocalFsStorage "ArgoCD" "argocd-pv" "argocd-storage" "/data/volumes/argocd" "argocd"
}

argocdInst(){
  CLIENT_SECRET=$(dataFromSecret oauth-secrets kube-system OIDC_CLIENT_SECRET)
  CLIENT_ID=$(dataFromSecret oauth-secrets kube-system OIDC_CLIENT_ID)
  REALM_NAME=$(dataFromSecret oauth-secrets kube-system OAUTH_REALM)
  OAUTH2_HOST="$(fullKeycloakUrl)"
  OAUTH2_OIDC_ISSUE_URL="$(dataFromSecret oauth-secrets kube-system OIDC_ISSUE_URL)"

  kubectl create ns argocd
  helm repo add argo https://argoproj.github.io/argo-helm
  helm repo update
  helm install argocd argo/argo-cd --namespace argocd \
    --set configs.secret.extra."oidc\.keycloak\.clientSecret"="${CLIENT_SECRET}" \
    --set configs.secret.argocdServerAdminPassword="sumit" \
    --values "${MOUNT_PATH}"/kubernetes/install_k8s/argocd/values.yaml

  gok patch ingress argocd-server argocd letsencrypt $(argocdSubdomain)
 
  #kubectl create secret generic argocd-secret -n argocd \
  #--from-literal=oidc.clientId=$CLIENT_ID \
  #--from-literal=oidc.clientSecret=$CLIENT_SECRET \
  #--from-literal=oidc.issuerUrl=$OAUTH2_HOST \
  #--from-literal=oidc.scopes="openid email profile groups" \
  #--from-literal=oidc.usernameClaim=email \
  #--from-literal=oidc.groupsClaim=groups
}

getUserInfo(){
  curl https://keycloak.gokcloud.com/realms/GokDevelopers/protocol/openid-connect/auth?client_id=gok-developers-client&redirect_uri=https://localhost/login&response_type=code&scope=openid%20email%20profile&state=eLUwT2

  TOKEN=$(curl --location --request POST 'https://keycloak.gokcloud.com/realms/GokDevelopers/protocol/openid-connect/token' \
     --header 'Content-Type: application/x-www-form-urlencoded' \
     --data-urlencode 'grant_type=authorization_code' \
     --data-urlencode 'client_id=gok-developers-client' \
     --data-urlencode 'client_secret=sC1hHm9c2qHjwYtfumcQnEwyH7NOqkaV' \
     --data-urlencode 'redirect_uri=https://localhost/login' \
     --data-urlencode 'scope=roles email profile openid' \
     --data-urlencode 'code=2a34daeb-4bba-48fb-9531-69f55104ab62.69c4c286-f2c9-4455-8368-7a8723ad60cf.f79ef798-e122-403f-8626-ad3793bf3f44' | jq -r '.["access_token"]')

  curl -kv 'https://keycloak.gokcloud.com/realms/GokDevelopers/protocol/openid-connect/userinfo' \
  --header 'Content-Type: application/x-www-form-urlencoded' \
  --header "Authorization: Bearer $TOKEN"

}

spinnakerReset(){
  kubectl delete all --all -n spinnaker
  kubectl delete ns spinnaker
  docker stop halyard
  docker rm halyard
  rm -rf /home/spinnaker/.hal
  rm -rf /home/spinnaker/.kube
}

patchLocalTls() {
  NAME=$1
  NS=$2
  kubectl patch ing "$NAME" --patch "$(
    cat <<EOF
spec:
  tls:
    - hosts:
        - $APP_HOST
      secretName: appingress-certificate
EOF
  )" -n "$NS"
  kubectl patch ing "$NAME" --type=json -p='[{"op": "replace", "path": "/spec/rules/0/host", "value":"master.cloud.com"}]' -n "$NS"
  kubectl patch ing "$NAME" --type=json -p='[{"op": "add", "path": "/metadata/annotations", "value":{"nginx.ingress.kubernetes.io/rewrite-target": "/", "kubernetes.io/ingress.class": "nginx"}}]' -n "$NS"
}

#Enable debug logs for Api Server and Kubelet
debugLog(){
  kcd default
  cat << EOT | kubectl apply -f -
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: edit-debug-flags-v
rules:
- apiGroups:
  - ""
  resources:
  - nodes/proxy
  verbs:
  - update
- nonResourceURLs:
  - /debug/flags/v
  verbs:
  - put
EOT

  cat << EOT | kubectl apply -f -
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: edit-debug-flags-v
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: edit-debug-flags-v
subjects:
- kind: ServiceAccount
  name: default
  namespace: default
EOT
  TOKEN=$(kubectl create token default)

  # Change the log level to 5 (trace) for Api Server
  APISERVER=$(apiUrl)
  curl -s -X PUT -d '5' $APISERVER/debug/flags/v --header "Authorization: Bearer $TOKEN" -k

  # Change the log level to 5 (trace) for Kubelet : Currently not working
  docker exec kind-control-plane curl -s -X PUT -d '5' https://localhost:10250/debug/flags/v --header "Authorization: Bearer $TOKEN" -k

}

kubeloginInst(){
  useradd -m -g root linuxbrew
  cd /home/linuxbrew
  su - linuxbrew
  /bin/bash -c "$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)"
  echo 'eval "$(/home/linuxbrew/.linuxbrew/bin/brew shellenv)"' >> /home/linuxbrew/.profile
  eval "$(/home/linuxbrew/.linuxbrew/bin/brew shellenv)"
  brew install kubelogin
  echo 'export PATH="/home/linuxbrew/.linuxbrew/bin:$PATH"' >> /home/linuxbrew/.profile
  echo 'export PATH="/home/linuxbrew/.linuxbrew/sbin:$PATH"' >> /home/linuxbrew/.profile
  brew doctor
  brew install Azure/kubelogin/kubelogin
  # upgrade
  brew update
  brew upgrade Azure/kubelogin/kubelogin
}

kyvernoInst(){
  helm repo add kyverno https://kyverno.github.io/kyverno/
  helm repo update
  helm install kyverno --namespace kyverno kyverno/kyverno --create-namespace
  waitForServiceAvailable kyverno
  sleep 10

  #grants the cluster-admin role to two specific ServiceAccounts
cat <<EOF | kubectl apply -f -
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: kyverno:cloud-cluster-admin
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: cluster-admin
subjects:
- kind: ServiceAccount
  name: kyverno-admission-controller
  namespace: kyverno
- kind: ServiceAccount
  name: kyverno-background-controller
  namespace: kyverno
EOF


cat <<EOF | kubectl apply -f -
apiVersion: kyverno.io/v1
kind: ClusterPolicy
metadata:
  name: sync-secrets
spec:
  rules:
  - name: sync-image-pull-secret
    match:
      any:
      - resources:
          kinds:
          - Namespace
    generate:
      apiVersion: v1
      kind: Secret
      name: regcred
      namespace: "{{request.object.metadata.name}}"
      synchronize: true
      clone:
        namespace: kube-system
        name: regcred
  - name: sync-oauth-secrets
    match:
      any:
      - resources:
          kinds:
          - Namespace
    generate:
      apiVersion: v1
      kind: Secret
      name: oauth-secrets
      namespace: "{{request.object.metadata.name}}"
      synchronize: true
      clone:
        namespace: kube-system
        name: oauth-secrets
  - name: sync-opensearch-secrets
    match:
      any:
      - resources:
          kinds:
          - Namespace
    generate:
      apiVersion: v1
      kind: Secret
      name: opensearch-secrets
      namespace: "{{request.object.metadata.name}}"
      synchronize: true
      clone:
        namespace: kube-system
        name: opensearch-secrets
EOF
}

kyvernoReset(){
  helm uninstall kyverno -n kyverno
  kubectl delete ns kyverno
  kubectl delete clusterpolicy --all
}

help(){
  IFS='' read -r -d '' HELP <<"EOF"
gok install kubernetes => Installing Kubernetes
gok install ingress => Installing Ingress
gok install dashboard => Installing Dashboard
gok install cert-manager => Installing Cert-Manager
gok install keycloak => Installing Keycloak
gok install registry => Install registry
python3 keycloak-setup.py all => Setup keycloak
helmTemplateDebug $char_name $char_repo = Debug helm template
apiUrl => view url for kubernetes api server
oauthUser => Configure Kubeconfig for OAuth
oauthDev => Create developers role for oauth
oauthAdmin => Create administrators role for oauth

kcd $namespace => switch namespace
k => short form for kubectl
current => Show current namespace
pods => show pods in current namespace
secrets => show secrets in current namespace
viewcert => View certificate in secret in current namespace
decode => Base64 decode content in secret in current namespace
ns => Show all namespaces
edit $resouce => Edit resource in current namespace
logs => View logs of pod in current namespace
bash => Get terminal in pod
all => Show all resource in current namespace
kctl => Execute kubectl command with oauth authentication, create dev and admin role
EOF
echo "$HELP"
}

function bashCmd() {
  pod=$(getpod)
  echo "Opening terminal on $pod"
  kubectl exec -it "$pod" -- /bin/bash
}

function helpCmd() {
    log_header "GOK Platform - Kubernetes Operations Toolkit" "Your Complete Cloud-Native Development & Operations Platform"
    
    echo -e "${COLOR_BRIGHT_BLUE}${COLOR_BOLD}🎯 WHAT IS GOK?${COLOR_RESET}"
    echo -e "${COLOR_CYAN}GOK is an enterprise-grade, comprehensive toolkit for managing Kubernetes clusters"
    echo -e "and deploying cloud-native applications with integrated security, monitoring, and DevOps tools."
    echo -e "It provides a unified interface to deploy and manage 35+ components across your infrastructure.${COLOR_RESET}"
    echo
    
    echo -e "${COLOR_BRIGHT_GREEN}${COLOR_BOLD}📋 CORE COMMANDS${COLOR_RESET}"
    echo
    echo -e "${COLOR_YELLOW}${COLOR_BOLD}Lifecycle Management:${COLOR_RESET}"
    echo -e "  ${COLOR_GREEN}install${COLOR_RESET} <component>     ${COLOR_CYAN}Install and configure 35+ components${COLOR_RESET}"
    echo -e "  ${COLOR_GREEN}reset${COLOR_RESET} <component>       ${COLOR_CYAN}Reset and uninstall components safely${COLOR_RESET}"
    echo -e "  ${COLOR_GREEN}start${COLOR_RESET} <component>       ${COLOR_CYAN}Start system services and processes${COLOR_RESET}"
    echo -e "  ${COLOR_GREEN}deploy${COLOR_RESET} <component>      ${COLOR_CYAN}Deploy applications and services${COLOR_RESET}"
    echo
    echo -e "${COLOR_YELLOW}${COLOR_BOLD}Resource Management:${COLOR_RESET}"
    echo -e "  ${COLOR_GREEN}create${COLOR_RESET} <resource>       ${COLOR_CYAN}Create Kubernetes resources and configurations${COLOR_RESET}"
    echo -e "  ${COLOR_GREEN}generate${COLOR_RESET} <type>         ${COLOR_CYAN}Generate microservice templates and code${COLOR_RESET}"
    echo -e "  ${COLOR_GREEN}patch${COLOR_RESET} <resource>        ${COLOR_CYAN}Patch and modify existing resources${COLOR_RESET}"
    echo
    echo -e "${COLOR_YELLOW}${COLOR_BOLD}Cluster Operations:${COLOR_RESET}"
    echo -e "  ${COLOR_GREEN}bash${COLOR_RESET}                    ${COLOR_CYAN}Open interactive terminal in pods${COLOR_RESET}"
    echo -e "  ${COLOR_GREEN}desc${COLOR_RESET} [methods]          ${COLOR_CYAN}Describe pod details or GOK methods${COLOR_RESET}"
    echo -e "  ${COLOR_GREEN}logs${COLOR_RESET}                    ${COLOR_CYAN}View pod logs and diagnostics${COLOR_RESET}"
    echo -e "  ${COLOR_GREEN}status${COLOR_RESET}                  ${COLOR_CYAN}Check service and Helm release status${COLOR_RESET}"
    echo -e "  ${COLOR_GREEN}taint-node${COLOR_RESET}              ${COLOR_CYAN}Configure node taints and scheduling${COLOR_RESET}"
    echo -e "  ${COLOR_GREEN}completion${COLOR_RESET}              ${COLOR_CYAN}Setup bash tab completion for GOK commands${COLOR_RESET}"
    echo
    
    echo -e "${COLOR_BRIGHT_YELLOW}${COLOR_BOLD}🚀 QUICK START EXAMPLES${COLOR_RESET}"
    echo
    echo -e "${COLOR_MAGENTA}${COLOR_BOLD}Complete Production Cluster Setup:${COLOR_RESET}"
    echo -e "${COLOR_CYAN}  gok install kubernetes        ${COLOR_DIM}# Core Kubernetes cluster with HA${COLOR_RESET}"
    echo -e "${COLOR_CYAN}  gok install cert-manager      ${COLOR_DIM}# Automated TLS certificate management${COLOR_RESET}"
    echo -e "${COLOR_CYAN}  gok install ingress           ${COLOR_DIM}# NGINX ingress controller${COLOR_RESET}"
    echo -e "${COLOR_CYAN}  gok install monitoring        ${COLOR_DIM}# Prometheus + Grafana stack${COLOR_RESET}"
    echo
    echo -e "${COLOR_MAGENTA}${COLOR_BOLD}Security & Identity Management:${COLOR_RESET}"
    echo -e "${COLOR_CYAN}  gok install vault             ${COLOR_DIM}# HashiCorp Vault for secrets${COLOR_RESET}"
    echo -e "${COLOR_CYAN}  gok install keycloak          ${COLOR_DIM}# Identity and access management${COLOR_RESET}"
    echo -e "${COLOR_CYAN}  gok install oauth2            ${COLOR_DIM}# OAuth2 proxy for authentication${COLOR_RESET}"
    echo
    echo -e "${COLOR_MAGENTA}${COLOR_BOLD}Development & DevOps:${COLOR_RESET}"
    echo -e "${COLOR_CYAN}  gok install argocd            ${COLOR_DIM}# GitOps continuous delivery${COLOR_RESET}"
    echo -e "${COLOR_CYAN}  gok install jenkins           ${COLOR_DIM}# CI/CD automation server${COLOR_RESET}"
    echo -e "${COLOR_CYAN}  gok install jupyter           ${COLOR_DIM}# Data science development${COLOR_RESET}"
    echo
    echo -e "${COLOR_MAGENTA}${COLOR_BOLD}Microservice Generation:${COLOR_RESET}"
    echo -e "${COLOR_CYAN}  gok generate python-api user-service    ${COLOR_DIM}# Python REST API template${COLOR_RESET}"
    echo -e "${COLOR_CYAN}  gok generate python-reactjs webapp      ${COLOR_DIM}# Full-stack React+Python app${COLOR_RESET}"
    echo
    echo -e "${COLOR_MAGENTA}${COLOR_BOLD}Resource Creation:${COLOR_RESET}"
    echo -e "${COLOR_CYAN}  gok create certificate production       ${COLOR_DIM}# Production TLS certificates${COLOR_RESET}"
    echo -e "${COLOR_CYAN}  gok create kubeconfig developer         ${COLOR_DIM}# Developer access configuration${COLOR_RESET}"
    echo -e "${COLOR_CYAN}  gok create secret apphost               ${COLOR_DIM}# Application host secrets${COLOR_RESET}"
    echo
    
    echo -e "${COLOR_BRIGHT_MAGENTA}${COLOR_BOLD}📚 DETAILED HELP & DOCUMENTATION${COLOR_RESET}"
    echo
    echo -e "${COLOR_YELLOW}For comprehensive information about any command, use:${COLOR_RESET}"
    echo
    echo -e "  ${COLOR_GREEN}${COLOR_BOLD}gok install help${COLOR_RESET}        ${COLOR_CYAN}View all 35+ installable components with descriptions${COLOR_RESET}"
    echo -e "  ${COLOR_GREEN}${COLOR_BOLD}gok reset help${COLOR_RESET}          ${COLOR_CYAN}View all 32+ resettable components${COLOR_RESET}"
    echo -e "  ${COLOR_GREEN}${COLOR_BOLD}gok create help${COLOR_RESET}         ${COLOR_CYAN}View resource creation options and templates${COLOR_RESET}"
    echo -e "  ${COLOR_GREEN}${COLOR_BOLD}gok generate help${COLOR_RESET}       ${COLOR_CYAN}View microservice template options${COLOR_RESET}"
    echo -e "  ${COLOR_GREEN}${COLOR_BOLD}gok status${COLOR_RESET}              ${COLOR_CYAN}View complete platform status overview${COLOR_RESET}"
    echo
    
    echo -e "${COLOR_BRIGHT_CYAN}${COLOR_BOLD}⭐ PLATFORM FEATURES & CAPABILITIES${COLOR_RESET}"
    echo
    echo -e "${COLOR_GREEN}${EMOJI_SUCCESS} ${COLOR_BOLD}Core Infrastructure:${COLOR_RESET}"
    echo -e "  ${COLOR_CYAN}• Complete Kubernetes cluster management with HA${COLOR_RESET}"
    echo -e "  ${COLOR_CYAN}• Automated TLS certificate management (Let's Encrypt + Custom CA)${COLOR_RESET}"
    echo -e "  ${COLOR_CYAN}• Production-ready ingress with NGINX${COLOR_RESET}"
    echo -e "  ${COLOR_CYAN}• Comprehensive monitoring (Prometheus/Grafana/AlertManager)${COLOR_RESET}"
    echo
    echo -e "${COLOR_GREEN}${EMOJI_SHIELD} ${COLOR_BOLD}Security & Identity:${COLOR_RESET}"
    echo -e "  ${COLOR_CYAN}• Enterprise identity management (Keycloak/OAuth2)${COLOR_RESET}"
    echo -e "  ${COLOR_CYAN}• Secrets management (HashiCorp Vault integration)${COLOR_RESET}"
    echo -e "  ${COLOR_CYAN}• LDAP directory services${COLOR_RESET}"
    echo -e "  ${COLOR_CYAN}• RBAC and policy enforcement (Kyverno)${COLOR_RESET}"
    echo
    echo -e "${COLOR_GREEN}${EMOJI_TOOLS} ${COLOR_BOLD}Development & DevOps:${COLOR_RESET}"
    echo -e "  ${COLOR_CYAN}• GitOps deployment (ArgoCD)${COLOR_RESET}"
    echo -e "  ${COLOR_CYAN}• CI/CD automation (Jenkins/Spinnaker)${COLOR_RESET}"
    echo -e "  ${COLOR_CYAN}• Development environments (JupyterHub/Eclipse Che)${COLOR_RESET}"
    echo -e "  ${COLOR_CYAN}• Microservice code generation${COLOR_RESET}"
    echo
    echo -e "${COLOR_GREEN}${EMOJI_NETWORK} ${COLOR_BOLD}Service Mesh & Networking:${COLOR_RESET}"
    echo -e "  ${COLOR_CYAN}• Service mesh ready (Istio integration)${COLOR_RESET}"
    echo -e "  ${COLOR_CYAN}• Message broker (RabbitMQ)${COLOR_RESET}"
    echo -e "  ${COLOR_CYAN}• Container registry${COLOR_RESET}"
    echo -e "  ${COLOR_CYAN}• Advanced networking and traffic management${COLOR_RESET}"
    echo
    
    echo -e "${COLOR_BRIGHT_BLUE}${COLOR_BOLD}🌐 SUPPORTED PLATFORMS & REQUIREMENTS${COLOR_RESET}"
    echo
    echo -e "${COLOR_YELLOW}${COLOR_BOLD}Operating Systems:${COLOR_RESET}   ${COLOR_CYAN}Ubuntu 18.04+, Debian 9+, CentOS 7+${COLOR_RESET}"
    echo -e "${COLOR_YELLOW}${COLOR_BOLD}Cloud Providers:${COLOR_RESET}     ${COLOR_CYAN}AWS, GCP, Azure, On-premises, Edge computing${COLOR_RESET}"
    echo -e "${COLOR_YELLOW}${COLOR_BOLD}Container Runtime:${COLOR_RESET}   ${COLOR_CYAN}Docker, containerd, CRI-O${COLOR_RESET}"
    echo -e "${COLOR_YELLOW}${COLOR_BOLD}Kubernetes:${COLOR_RESET}          ${COLOR_CYAN}1.20+ (supports latest versions)${COLOR_RESET}"
    echo -e "${COLOR_YELLOW}${COLOR_BOLD}Hardware:${COLOR_RESET}            ${COLOR_CYAN}Minimum 4GB RAM, 2 CPU cores (8GB+ recommended)${COLOR_RESET}"
    echo
    
    echo -e "${COLOR_BRIGHT_GREEN}${COLOR_BOLD}🎯 COMPLETE SETUP GUIDE${COLOR_RESET}"
    echo
    echo -e "${COLOR_MAGENTA}${COLOR_BOLD}Phase 1 - Core Infrastructure:${COLOR_RESET}"
    echo -e "${COLOR_CYAN}  1. ${COLOR_BOLD}gok install docker${COLOR_RESET}              ${COLOR_DIM}# Container runtime${COLOR_RESET}"
    echo -e "${COLOR_CYAN}  2. ${COLOR_BOLD}gok install kubernetes${COLOR_RESET}          ${COLOR_DIM}# Core K8s cluster${COLOR_RESET}"
    echo -e "${COLOR_CYAN}  3. ${COLOR_BOLD}gok install cert-manager${COLOR_RESET}        ${COLOR_DIM}# Certificate management${COLOR_RESET}"
    echo -e "${COLOR_CYAN}  4. ${COLOR_BOLD}gok install ingress${COLOR_RESET}             ${COLOR_DIM}# Traffic routing${COLOR_RESET}"
    echo
    echo -e "${COLOR_MAGENTA}${COLOR_BOLD}Phase 2 - Monitoring & Security:${COLOR_RESET}"
    echo -e "${COLOR_CYAN}  5. ${COLOR_BOLD}gok install monitoring${COLOR_RESET}          ${COLOR_DIM}# Observability stack${COLOR_RESET}"
    echo -e "${COLOR_CYAN}  6. ${COLOR_BOLD}gok install vault${COLOR_RESET}               ${COLOR_DIM}# Secrets management${COLOR_RESET}"
    echo -e "${COLOR_CYAN}  7. ${COLOR_BOLD}gok install keycloak${COLOR_RESET}            ${COLOR_DIM}# Identity management${COLOR_RESET}"
    echo
    echo -e "${COLOR_MAGENTA}${COLOR_BOLD}Phase 3 - Development & DevOps:${COLOR_RESET}"
    echo -e "${COLOR_CYAN}  8. ${COLOR_BOLD}gok install argocd${COLOR_RESET}              ${COLOR_DIM}# GitOps deployment${COLOR_RESET}"
    echo -e "${COLOR_CYAN}  9. ${COLOR_BOLD}gok install jupyter${COLOR_RESET}             ${COLOR_DIM}# Development environment${COLOR_RESET}"
    echo -e "${COLOR_CYAN} 10. ${COLOR_BOLD}gok install gok-controller${COLOR_RESET}      ${COLOR_DIM}# GOK platform services${COLOR_RESET}"
    echo
    
    log_next_steps "After Installation" \
        "Check platform status: ${COLOR_BOLD}gok status${COLOR_RESET}" \
        "Access services via ingress URLs (check gok status for URLs)" \
        "Configure authentication and RBAC as needed" \
        "Generate your first microservice: ${COLOR_BOLD}gok generate python-api my-service${COLOR_RESET}" \
        "Deploy applications using ArgoCD or direct kubectl"
    
    echo -e "${COLOR_BRIGHT_CYAN}${COLOR_BOLD}📞 SUPPORT & RESOURCES${COLOR_RESET}"
    echo -e "${COLOR_CYAN}• Documentation: ${COLOR_BOLD}https://github.com/sumitmaji/kubernetes${COLOR_RESET}"
    echo -e "${COLOR_CYAN}• Issues & Support: ${COLOR_BOLD}https://github.com/sumitmaji/kubernetes/issues${COLOR_RESET}"
    echo -e "${COLOR_CYAN}• Community: Join our Kubernetes community for help and discussions${COLOR_RESET}"
    echo
    
    echo -e "${COLOR_BRIGHT_YELLOW}${COLOR_BOLD}💡 PRO TIPS${COLOR_RESET}"
    echo -e "${COLOR_YELLOW}• Use ${COLOR_BOLD}gok completion setup${COLOR_RESET} for tab completion${COLOR_RESET}"
    echo -e "${COLOR_YELLOW}• Run ${COLOR_BOLD}gok status${COLOR_RESET} regularly to monitor your platform${COLOR_RESET}"
    echo -e "${COLOR_YELLOW}• Install components in the recommended order for best results${COLOR_RESET}"
    echo -e "${COLOR_YELLOW}• Use ${COLOR_BOLD}gok logs${COLOR_RESET} for troubleshooting issues${COLOR_RESET}"
    echo
}

function descCmd() {
  TARGET=$1
  
  if [ "$TARGET" == "methods" ] || [ "$TARGET" == "functions" ] || [ "$TARGET" == "all" ]; then
    describeAllMethods
  elif [ "$TARGET" == "help" ] || [ "$TARGET" == "--help" ]; then
    echo "gok desc - Describe pods or GOK methods"
    echo ""
    echo "Usage:"
    echo "  gok desc              # Describe a selected pod (interactive)"
    echo "  gok desc methods      # Describe all GOK methods and functions"
    echo "  gok desc functions    # Same as methods"
    echo "  gok desc all          # Same as methods"
    echo ""
    echo "Examples:"
    echo "  gok desc              # Interactive pod description"
    echo "  gok desc methods      # Technical method documentation"
    return 0
  else
    # Traditional pod description functionality
    pod=$(getpod)
    echo "Describing pod $pod"
    kubectl describe po "$pod"
  fi
}

function describeAllMethods() {
  echo "GOK - Method Reference Documentation"
  echo "===================================="
  echo ""
  echo "This document describes all methods, functions, and their parameters"
  echo "available in the GOK Kubernetes Operations Toolkit."
  echo ""
  echo "COMMAND INTERFACE METHODS:"
  echo "========================="
  echo ""
  
  echo "helpCmd()"
  echo "├─ Purpose: Display main help and documentation"
  echo "├─ Parameters: None"
  echo "├─ Usage: gok help | gok --help | gok -h"
  echo "└─ Returns: Comprehensive toolkit overview and usage guide"
  echo ""
  
  echo "installCmd(\$COMPONENT)"
  echo "├─ Purpose: Install and configure Kubernetes components"
  echo "├─ Parameters:"
  echo "│  └─ \$COMPONENT: Component name to install"
  echo "├─ Supported Components: 35+ including:"
  echo "│  ├─ Core: docker, kubernetes, kubernetes-worker, cert-manager, ingress, dashboard"
  echo "│  ├─ Monitoring: monitoring, fluentd, opensearch"
  echo "│  ├─ Security: keycloak, oauth2, vault, ldap"
  echo "│  ├─ Development: jupyter, devworkspace, workspace, che, ttyd, cloudshell, console"
  echo "│  ├─ CI/CD: argocd, jenkins, spinnaker, registry"
  echo "│  ├─ Networking: istio, rabbitmq"
  echo "│  ├─ Policy: kyverno"
  echo "│  ├─ GOK Platform: gok-agent, gok-controller, controller, gok-login, chart"
  echo "│  └─ Solutions: base, base-services"
  echo "├─ Usage: gok install <component>"
  echo "└─ Returns: Installation status and configuration"
  echo ""
  
  echo "resetCmd(\$COMPONENT)"
  echo "├─ Purpose: Reset and uninstall Kubernetes components"
  echo "├─ Parameters:"
  echo "│  └─ \$COMPONENT: Component name to reset/uninstall"
  echo "├─ Supported Components: 32+ (same as install minus docker)"
  echo "├─ Usage: gok reset <component>"
  echo "├─ Warning: Destructive operation - permanent data loss"
  echo "└─ Returns: Reset status and cleanup confirmation"
  echo ""
  
  echo "createCmd(\$RESOURCE, \$NAME, \$ADDITIONAL)"
  echo "├─ Purpose: Create Kubernetes resources and configurations"
  echo "├─ Parameters:"
  echo "│  ├─ \$RESOURCE: Resource type (secret, certificate, kubeconfig)"
  echo "│  ├─ \$NAME: Resource name or identifier"
  echo "│  └─ \$ADDITIONAL: Optional additional parameters"
  echo "├─ Resource Types:"
  echo "│  ├─ secret: Creates Kubernetes secrets (apphost supported)"
  echo "│  ├─ certificate: Creates TLS certificates for namespaces"
  echo "│  └─ kubeconfig: Creates kubeconfig files for users"
  echo "├─ Usage: gok create <resource> <name> [additional]"
  echo "└─ Returns: Resource creation status and details"
  echo ""
  
  echo "generateCmd(\$SERVICE_TYPE, \$SERVICE_NAME, \$SERVICE_DESC, \$SERVICE_NS, \$SERVICE_HOST)"
  echo "├─ Purpose: Generate microservice templates from predefined patterns"
  echo "├─ Parameters:"
  echo "│  ├─ \$SERVICE_TYPE: Template type (python-api, python-reactjs)"
  echo "│  ├─ \$SERVICE_NAME: Name of the service to generate"
  echo "│  ├─ \$SERVICE_DESC: Optional service description"
  echo "│  ├─ \$SERVICE_NS: Optional Kubernetes namespace"
  echo "│  └─ \$SERVICE_HOST: Optional ingress hostname"
  echo "├─ Service Types:"
  echo "│  ├─ python-api: Python Flask REST API (backend-only)"
  echo "│  └─ python-reactjs: Python Flask + React.js (full-stack)"
  echo "├─ Generated Features: OAuth2, RBAC, TLS, health checks, API docs"
  echo "├─ Usage: gok generate <type> <name> [description] [namespace] [hostname]"
  echo "└─ Returns: Complete microservice with deployment configuration"
  echo ""
  
  echo "startCmd(\$COMPONENT)"
  echo "├─ Purpose: Start system services and components"
  echo "├─ Parameters:"
  echo "│  └─ \$COMPONENT: Service name (kubernetes, proxy, kubelet)"
  echo "├─ Usage: gok start <component>"
  echo "└─ Returns: Service startup status"
  echo ""
  
  echo "deployCmd(\$COMPONENT)"
  echo "├─ Purpose: Deploy applications and services"
  echo "├─ Parameters:"
  echo "│  └─ \$COMPONENT: Application name (currently supports: app1)"
  echo "├─ Usage: gok deploy <component>"
  echo "└─ Returns: Deployment status and application details"
  echo ""
  
  echo "patchCmd(\$RESOURCE, \$NAME, \$NAMESPACE, \$OPTIONS, \$SUBDOMAIN)"
  echo "├─ Purpose: Patch and modify existing Kubernetes resources"
  echo "├─ Parameters:"
  echo "│  ├─ \$RESOURCE: Resource type (currently supports: ingress)"
  echo "│  ├─ \$NAME: Resource name"
  echo "│  ├─ \$NAMESPACE: Kubernetes namespace"
  echo "│  ├─ \$OPTIONS: Patch options (letsencrypt, ldap, localtls)"
  echo "│  └─ \$SUBDOMAIN: Optional subdomain configuration"
  echo "├─ Usage: gok patch <resource> <name> <namespace> <options> [subdomain]"
  echo "└─ Returns: Patch operation status"
  echo ""
  
  echo "descCmd([\$TARGET])"
  echo "├─ Purpose: Describe pods or GOK methods"
  echo "├─ Parameters:"
  echo "│  └─ \$TARGET: Optional target (methods, functions, all, or empty for pod)"
  echo "├─ Usage: gok desc [methods|functions|all]"
  echo "└─ Returns: Pod description or method documentation"
  echo ""
  
  echo "logsCmd()"
  echo "├─ Purpose: View pod logs and diagnostics"
  echo "├─ Parameters: None (interactive pod selection)"
  echo "├─ Usage: gok logs"
  echo "└─ Returns: Pod log output"
  echo ""
  
  echo "bashCmd()"
  echo "├─ Purpose: Open interactive terminal in pods"
  echo "├─ Parameters: None (interactive pod selection)"
  echo "├─ Usage: gok bash"
  echo "└─ Returns: Interactive shell session"
  echo ""
  
  echo "statusCmd()"
  echo "├─ Purpose: Check Helm release status"
  echo "├─ Parameters: Uses global \$release variable"
  echo "├─ Usage: gok status"
  echo "└─ Returns: Helm release status information"
  echo ""
  
  echo "taintNodeCmd()"
  echo "├─ Purpose: Configure node taints and scheduling"
  echo "├─ Parameters: None (interactive configuration)"
  echo "├─ Usage: gok taint-node"
  echo "└─ Returns: Node taint configuration status"
  echo ""
  
  echo ""
  echo "INTERNAL UTILITY FUNCTIONS:"
  echo "==========================="
  echo ""
  
  echo "updateSys()"
  echo "├─ Purpose: Update system packages and dependencies"
  echo "├─ Called by: installCmd()"
  echo "└─ Returns: System update status"
  echo ""
  
  echo "installDeps()"
  echo "├─ Purpose: Install required system dependencies"
  echo "├─ Called by: installCmd()"
  echo "└─ Returns: Dependency installation status"
  echo ""
  
  echo "getpod()"
  echo "├─ Purpose: Interactive pod selection utility"
  echo "├─ Called by: descCmd(), logsCmd(), bashCmd()"
  echo "└─ Returns: Selected pod name"
  echo ""
  
  echo "taintNode()"
  echo "├─ Purpose: Apply node taints for scheduling control"
  echo "├─ Called by: taintNodeCmd(), installCmd()"
  echo "└─ Returns: Node taint application status"
  echo ""
  
  echo ""
  echo "COMPONENT-SPECIFIC INSTALLATION FUNCTIONS:"
  echo "=========================================="
  echo ""
  
  echo "dockrInst() - Docker installation and configuration"
  echo "k8sInst(\$TYPE) - Kubernetes cluster setup (\$TYPE: kubernetes|kubernetes-worker)"
  echo "haInst() - High availability proxy installation"
  echo "helmInst() - Helm package manager installation"
  echo "calicoInst() - Calico network plugin installation"
  echo "certmanagerInst() - Certificate manager installation"
  echo "setupCertiIssuers() - Certificate issuers configuration"
  echo "ingressInst() - NGINX ingress controller installation"
  echo "installPrometheusGrafanaWithCertMgr() - Monitoring stack installation"
  echo "installKeycloakWithCertMgr() - Identity management installation"
  echo "installRegistryWithCertMgr() - Container registry installation"
  echo "installDashboardwithCertManager() - Kubernetes dashboard installation"
  echo "jupyterHubInst() - JupyterHub installation"
  echo "argocdInst() - ArgoCD GitOps installation"
  echo "jenkinsInst() - Jenkins CI/CD installation"
  echo "spinnakerInst() - Spinnaker deployment platform installation"
  echo "fluentdInst() - Fluentd logging installation"
  echo "opensearchInst() - OpenSearch analytics installation"
  echo "opensearchDashInst() - OpenSearch dashboard installation"
  echo "vaultInstall() - HashiCorp Vault installation"
  echo "oauth2ProxyInst() - OAuth2 proxy installation"
  echo "istioInst() - Istio service mesh installation"
  echo "kyvernoInst() - Kyverno policy engine installation"
  echo "rabbitmqInst() - RabbitMQ message broker installation"
  echo "eclipseCheInst() - Eclipse Che IDE installation"
  echo "ttydInst() - Terminal over HTTP installation"
  echo "cloudshellInst() - Cloud shell installation"
  echo "consoleInst() - Web console installation"
  echo "createDevWorkspace() - Legacy developer workspace creation"
  echo "createDevWorkspaceV2() - Enhanced developer workspace creation"
  echo "gokAgentInstall() - GOK agent component installation"
  echo "gokControllerInstall() - GOK controller component installation"
  echo "gokLoginInst() - GOK authentication service installation"
  echo "chartInst() - Helm chart repository setup"
  echo "baseInst() - Base system components installation"
  echo "installBaseServices() - Complete base services stack installation"
  echo "installLdap() - LDAP directory service installation"
  echo ""
  
  echo ""
  echo "COMPONENT-SPECIFIC RESET FUNCTIONS:"
  echo "===================================="
  echo ""
  
  echo "prometheusGrafanaResetv2() - Monitoring stack reset"
  echo "dashboardReset() - Kubernetes dashboard reset"
  echo "keycloakReset() - Keycloak identity management reset"
  echo "vaultReset() - Vault secrets management reset"
  echo "ingressUnInst() - Ingress controller uninstallation"
  echo "resetChart() - Helm chart repository reset"
  echo "ldapReset() - LDAP service reset"
  echo "gokAgentReset() - GOK agent reset"
  echo "gokControllerReset() - GOK controller reset"
  echo "argocdReset() - ArgoCD reset"
  echo "deleteDevWorkspace() - Legacy workspace deletion"
  echo "deleteDevWorkspaceV2() - Enhanced workspace deletion"
  echo "resetDockerRegistry() - Container registry reset"
  echo "fluentdReset() - Fluentd logging reset"
  echo "jupyterHubReset() - JupyterHub reset"
  echo "rabbitmqReset() - RabbitMQ reset"
  echo "resetEclipseChe() - Eclipse Che reset"
  echo "ttydReset() - Terminal service reset"
  echo "cloudshellReset() - Cloud shell reset"
  echo "consoleReset() - Web console reset"
  echo "opensearchReset() - OpenSearch reset"
  echo "opensearchDashReset() - OpenSearch dashboard reset"
  echo "jenkinsReset() - Jenkins reset"
  echo "spinnakerReset() - Spinnaker reset"
  echo "oauth2ProxyReset() - OAuth2 proxy reset"
  echo "certManagerReset() - Certificate manager reset"
  echo "istioReset() - Istio service mesh reset"
  echo "kyvernoReset() - Kyverno policy engine reset"
  echo "resetBaseServices() - Complete base services reset"
  echo ""
  
  echo ""
  echo "CONFIGURATION AND UTILITY FUNCTIONS:"
  echo "====================================="
  echo ""
  
  echo "getOAuth0Config() - Auth0 OAuth configuration"
  echo "getKeycloakConfig() - Keycloak OAuth configuration"
  echo "waitForServiceAvailable(\$NAMESPACE) - Service availability checker"
  echo "dnsUtils() - DNS utilities installation"
  echo "customDns() - Custom DNS configuration"
  echo "kcurl() - Kubernetes curl utility installation"
  echo "oauthAdmin() - OAuth admin configuration"
  echo "disableSwap() - System swap disabling"
  echo "startHa() - High availability service startup"
  echo "startKubelet() - Kubelet service startup"
  echo "createApp1() - Sample application deployment"
  echo "createLocalStorageClassAndPV() - Local storage configuration"
  echo "createExampleJenkinsPipeline() - Jenkins pipeline examples"
  echo "createJenkinsPipeline() - Jenkins pipeline creation"
  echo "createCertificate() - Certificate creation utilities"
  echo "createClientCertificate() - Client certificate creation"
  echo "createKubeConfig(\$NAME) - Kubeconfig file creation"
  echo "certificateRequestForNs(\$NAME, \$ADDITIONAL) - Namespace certificate requests"
  echo "hostSecret() - Application host secret creation"
  echo "createVaultSecretStore() - Vault secret store configuration"
  echo "create_sub_scope() - OAuth scope creation"
  echo "createUserGroup() - LDAP user and group creation"
  echo "emptyLocalFsStorage() - Local filesystem storage cleanup"
  echo "collectUserInputs() - Interactive user input collection"
  echo "promptUserInput() - User input prompting utility"
  echo "promptSecret() - Secure password input utility"
  echo ""
  
  echo "GLOBAL VARIABLES:"
  echo "================="
  echo ""
  echo "WORKING_DIR - Base working directory (\$MOUNT_PATH/kubernetes/install_k8s)"
  echo "release - Helm release name for status operations"
  echo "CMD - Current command being executed (\$1)"
  echo "MOUNT_PATH - Base mount path for GOK installation"
  echo "GOK_ROOT_DOMAIN - Root domain for GOK services"
  echo ""
  
  echo "CONFIGURATION FILES:"
  echo "===================="
  echo ""
  echo "config - Main GOK configuration file"
  echo "root_config - Root-level system configuration"
  echo "sample_service_config.yaml - Service generation template configuration"
  echo "/etc/bash.bashrc - System bash configuration (modified by GOK)"
  echo "/etc/rc.local - System startup script (created by GOK)"
  echo ""
  
  echo "For detailed usage of any command, use: gok <command> help"
  echo "For interactive pod operations, GOK provides automatic pod selection."
  echo "Most installation functions support both standalone and integrated deployment modes."
}

function logsCmd() {
  pod=$(getpod)
  echo "Viewing logs of pod $pod"
  kubectl logs "$pod"
}

# Service status check functions
check_service_status() {
  local service_name="$1"
  local namespace="$2"
  local helm_release="$3"
  local check_type="${4:-helm}"  # helm, kubectl, or both
  
  # Quick connectivity check - if cluster is not accessible, return ❌ for kubectl-based checks
  if [[ "$check_type" == "kubectl" ]] || [[ "$check_type" == "both" ]]; then
    if ! check_cluster_connectivity; then
      echo "❌"
      return
    fi
  fi
  
  case "$check_type" in
    "helm")
      if helm list -n "$namespace" 2>/dev/null | grep -q "^$helm_release"; then
        local status=$(helm list -n "$namespace" 2>/dev/null | grep "^$helm_release" | awk '{print $8}')
        if [[ "$status" == "deployed" ]]; then
          echo "✅"
        else
          echo "⚠️"
        fi
      else
        echo "❌"
      fi
      ;;
    "kubectl")
      if kubectl get deployment "$service_name" -n "$namespace" &>/dev/null; then
        local ready=$(kubectl get deployment "$service_name" -n "$namespace" -o jsonpath='{.status.readyReplicas}' 2>/dev/null)
        local desired=$(kubectl get deployment "$service_name" -n "$namespace" -o jsonpath='{.spec.replicas}' 2>/dev/null)
        if [[ "$ready" == "$desired" ]] && [[ "$ready" -gt 0 ]]; then
          echo "✅"
        else
          echo "⚠️"
        fi
      else
        echo "❌"
      fi
      ;;
    "both")
      local helm_status=$(check_service_status "$service_name" "$namespace" "$helm_release" "helm")
      local kubectl_status=$(check_service_status "$service_name" "$namespace" "$helm_release" "kubectl")
      if [[ "$helm_status" == "✅" ]] && [[ "$kubectl_status" == "✅" ]]; then
        echo "✅"
      elif [[ "$helm_status" == "❌" ]] && [[ "$kubectl_status" == "❌" ]]; then
        echo "❌"
      else
        echo "⚠️"
      fi
      ;;
  esac
}

check_ingress_status() {
  check_service_status "ingress-nginx-controller" "ingress-nginx" "ingress-nginx" "both"
}

check_certmanager_status() {
  check_service_status "cert-manager" "cert-manager" "cert-manager" "both"
}

check_registry_status() {
  # Check for docker registry deployment
  if kubectl get deployment registry-docker-registry -n registry &>/dev/null; then
    check_service_status "registry-docker-registry" "registry" "" "kubectl"
  else
    echo "❌"
  fi
}

check_kubernetes_status() {
  # Use kubectl cluster-info to check Kubernetes cluster status
  if kubectl cluster-info &>/dev/null; then
    # Check if core components are running
    local cluster_info_output=$(kubectl cluster-info 2>/dev/null)
    local api_server=$(echo "$cluster_info_output" | grep -i "kubernetes control plane\|kubernetes master" | wc -l)
    
    if [[ "$api_server" -ge 1 ]]; then
      # Additional checks for cluster health
      local ready_nodes=$(kubectl get nodes --no-headers 2>/dev/null | grep " Ready " | wc -l)
      local not_ready_nodes=$(kubectl get nodes --no-headers 2>/dev/null | grep " NotReady " | wc -l)
      
      if [[ "$ready_nodes" -gt 0 ]] && [[ "$not_ready_nodes" -eq 0 ]]; then
        # All nodes are ready
        echo "✅"
      elif [[ "$ready_nodes" -gt 0 ]] && [[ "$not_ready_nodes" -gt 0 ]]; then
        # Some nodes ready, some not ready
        echo "⚠️"
      elif [[ "$ready_nodes" -eq 0 ]]; then
        # No ready nodes
        echo "❌"
      else
        echo "✅"
      fi
    else
      echo "❌"
    fi
  else
    # kubectl cluster-info failed - cluster not accessible
    echo "❌"
  fi
}

check_cluster_connectivity() {
  # Quick check if kubectl can connect to cluster
  kubectl cluster-info &>/dev/null
}

check_base_status() {
  # Base is typically a custom image/deployment, check for common base components
  if kubectl get deployment base-services -n default &>/dev/null || \
     kubectl get pods -l app=base &>/dev/null | grep -q Running; then
    echo "✅"
  else
    echo "❌"
  fi
}

check_kyverno_status() {
  check_service_status "kyverno-admission-controller" "kyverno" "kyverno" "both"
}

check_ldap_status() {
  check_service_status "ldap" "ldap" "" "kubectl"
}

check_keycloak_status() {
  # Keycloak uses StatefulSet instead of Deployment
  if ! check_cluster_connectivity; then
    echo "❌"
    return
  fi
  
  # Check Helm release first
  local helm_status="❌"
  if helm list -n keycloak 2>/dev/null | grep -q "^keycloak"; then
    local status=$(helm list -n keycloak 2>/dev/null | grep "^keycloak" | awk '{print $8}')
    if [[ "$status" == "deployed" ]]; then
      helm_status="✅"
    else
      helm_status="⚠️"
    fi
  fi
  
  # Check StatefulSet status
  local kubectl_status="❌"
  if kubectl get statefulset keycloak -n keycloak &>/dev/null; then
    local ready=$(kubectl get statefulset keycloak -n keycloak -o jsonpath='{.status.readyReplicas}' 2>/dev/null)
    local desired=$(kubectl get statefulset keycloak -n keycloak -o jsonpath='{.spec.replicas}' 2>/dev/null)
    if [[ "$ready" == "$desired" ]] && [[ "$ready" -gt 0 ]]; then
      kubectl_status="✅"
    else
      kubectl_status="⚠️"
    fi
  fi
  
  # Combine statuses
  if [[ "$helm_status" == "✅" ]] && [[ "$kubectl_status" == "✅" ]]; then
    echo "✅"
  elif [[ "$helm_status" == "❌" ]] && [[ "$kubectl_status" == "❌" ]]; then
    echo "❌"
  else
    echo "⚠️"
  fi
}

check_oauth2_status() {
  check_service_status "oauth2proxy-oauth2-proxy" "oauth2" "oauth2proxy" "both"
}

check_service_status_statefulset() {
  local service_name="$1"
  local namespace="$2" 
  local helm_release="$3"
  local resource_type="${4:-statefulset}"  # statefulset or deployment
  
  if ! check_cluster_connectivity; then
    echo "❌"
    return
  fi
  
  # Check Helm release first
  local helm_status="❌"
  if helm list -n "$namespace" 2>/dev/null | grep -q "^$helm_release"; then
    local status=$(helm list -n "$namespace" 2>/dev/null | grep "^$helm_release" | awk '{print $8}')
    if [[ "$status" == "deployed" ]]; then
      helm_status="✅"
    else
      helm_status="⚠️"
    fi
  fi
  
  # Check Kubernetes resource status
  local kubectl_status="❌"
  if kubectl get "$resource_type" "$service_name" -n "$namespace" &>/dev/null; then
    local ready=$(kubectl get "$resource_type" "$service_name" -n "$namespace" -o jsonpath='{.status.readyReplicas}' 2>/dev/null)
    local desired=$(kubectl get "$resource_type" "$service_name" -n "$namespace" -o jsonpath='{.spec.replicas}' 2>/dev/null)
    if [[ "$ready" == "$desired" ]] && [[ "$ready" -gt 0 ]]; then
      kubectl_status="✅"
    else
      kubectl_status="⚠️"
    fi
  fi
  
  # Combine statuses
  if [[ "$helm_status" == "✅" ]] && [[ "$kubectl_status" == "✅" ]]; then
    echo "✅"
  elif [[ "$helm_status" == "❌" ]] && [[ "$kubectl_status" == "❌" ]]; then
    echo "❌"
  else
    echo "⚠️"
  fi
}

check_rabbitmq_status() {
  # RabbitMQ typically uses StatefulSet for clustering
  check_service_status_statefulset "rabbitmq" "rabbitmq" "rabbitmq" "statefulset"
}

check_vault_status() {
  # Vault uses StatefulSet in HA mode
  check_service_status_statefulset "vault" "vault" "vault" "statefulset"
}

check_goklogin_status() {
  check_service_status "gok-login" "gok-login" "gok-login" "both"
}

function statusCmd() {
  if [[ -n "$release" ]]; then
    # If a specific release is provided, show helm status for that release
    helm status "$release"
    return
  fi

  echo "🚀 GOK Platform Status Overview"
  echo "=============================="
  echo ""
  
  # Core Infrastructure Services (Priority Order)
  echo "📋 Core Infrastructure Services:"
  echo ""
  
  local index=1
  
  # Priority services in specified order
  printf "  %2d. %-15s %s  %s\n" $index "ingress" "$(check_ingress_status)" "NGINX Ingress Controller"
  ((index++))
  
  printf "  %2d. %-15s %s  %s\n" $index "cert-manager" "$(check_certmanager_status)" "TLS Certificate Management"
  ((index++))
  
  printf "  %2d. %-15s %s  %s\n" $index "registry" "$(check_registry_status)" "Docker Registry"
  ((index++))
  
  printf "  %2d. %-15s %s  %s\n" $index "base" "$(check_base_status)" "Base System Components"
  ((index++))
  
  printf "  %2d. %-15s %s  %s\n" $index "kyverno" "$(check_kyverno_status)" "Policy Engine"
  ((index++))
  
  printf "  %2d. %-15s %s  %s\n" $index "ldap" "$(check_ldap_status)" "LDAP Directory Service"
  ((index++))
  
  printf "  %2d. %-15s %s  %s\n" $index "keycloak" "$(check_keycloak_status)" "Identity & Access Management"
  ((index++))
  
  printf "  %2d. %-15s %s  %s\n" $index "oauth2" "$(check_oauth2_status)" "OAuth2 Proxy"
  ((index++))
  
  printf "  %2d. %-15s %s  %s\n" $index "rabbitmq" "$(check_rabbitmq_status)" "Message Broker"
  ((index++))
  
  printf "  %2d. %-15s %s  %s\n" $index "vault" "$(check_vault_status)" "Secrets Management"
  ((index++))
  
  printf "  %2d. %-15s %s  %s\n" $index "gok-login" "$(check_goklogin_status)" "GOK Authentication Service"
  ((index++))
  
  echo ""
  echo "📊 Additional Services:"
  echo ""
  
  # Monitoring & Observability Services
  echo "🔍 Monitoring & Observability:"
  local monitoring_services=(
    "monitoring:prometheus-operator:monitoring:Prometheus/Grafana Monitoring:deployment"
    "opensearch:opensearch-cluster-master:opensearch:OpenSearch Logging:statefulset"
    "fluentd:fluentd:fluentd:Log Collection:deployment"
  )
  
  for service_info in "${monitoring_services[@]}"; do
    IFS=':' read -r service_name deployment_name namespace description resource_type <<< "$service_info"
    if [[ "$resource_type" == "statefulset" ]]; then
      status=$(check_service_status_statefulset "$deployment_name" "$namespace" "$service_name" "statefulset")
    else
      status=$(check_service_status "$deployment_name" "$namespace" "$service_name" "kubectl")
    fi
    printf "  %2d. %-15s %s  %s\n" $index "$service_name" "$status" "$description"
    ((index++))
  done
  
  echo ""
  echo "🚀 Development & CI/CD:"
  local devops_services=(
    "jenkins:jenkins:jenkins:CI/CD Pipeline"
    "jupyter:jupyterhub:jupyterhub:JupyterHub Development"
    "argocd:argocd-server:argocd:GitOps Deployment"
    "spinnaker:spin-deck:spinnaker:Multi-cloud Deployment Platform"
  )
  
  for service_info in "${devops_services[@]}"; do
    IFS=':' read -r service_name deployment_name namespace description <<< "$service_info"
    printf "  %2d. %-15s %s  %s\n" $index "$service_name" "$(check_service_status "$deployment_name" "$namespace" "$service_name" "kubectl")" "$description"
    ((index++))
  done
  
  echo ""
  echo "💻 Developer Tools & IDEs:"
  local dev_tools=(
    "che:che:eclipse-che:Eclipse Che IDE"
    "devworkspace:devworkspace:devworkspace:Developer Workspace (Legacy)"
    "workspace:workspace-v2:workspace:Enhanced Developer Workspace"
    "ttyd:ttyd:ttyd:Terminal over HTTP"
    "cloudshell:cloudshell:cloudshell:Cloud-based Terminal"
    "console:console:console:Web-based Console"
  )
  
  for service_info in "${dev_tools[@]}"; do
    IFS=':' read -r service_name deployment_name namespace description <<< "$service_info"
    printf "  %2d. %-15s %s  %s\n" $index "$service_name" "$(check_service_status "$deployment_name" "$namespace" "$service_name" "kubectl")" "$description"
    ((index++))
  done
  
  echo ""
  echo "🌐 Service Mesh & Management:"
  local mesh_services=(
    "istio:istiod:istio-system:Service Mesh"
    "dashboard:kubernetes-dashboard:kubernetes-dashboard:K8s Dashboard"
    "chart:chartmuseum:chartmuseum:Helm Chart Repository"
  )
  
  for service_info in "${mesh_services[@]}"; do
    IFS=':' read -r service_name deployment_name namespace description <<< "$service_info"
    printf "  %2d. %-15s %s  %s\n" $index "$service_name" "$(check_service_status "$deployment_name" "$namespace" "$service_name" "kubectl")" "$description"
    ((index++))
  done
  
  echo ""
  echo "🏗️ Platform & Infrastructure:"
  local platform_services=(
    "kubernetes:kube-apiserver:kube-system:Kubernetes Control Plane"
    "docker:::Docker Container Runtime"
    "gok-agent:gok-agent:gok-agent:GOK Distributed System Agent"
    "gok-controller:gok-controller:gok-controller:GOK Controller"
    "base-services:::Complete Base Services Stack"
  )
  
  for service_info in "${platform_services[@]}"; do
    IFS=':' read -r service_name deployment_name namespace description <<< "$service_info"
    if [[ "$service_name" == "kubernetes" ]]; then
      # Use kubectl cluster-info for Kubernetes cluster status
      status="$(check_kubernetes_status)"
    elif [[ "$service_name" == "docker" ]]; then
      # Special check for Docker
      if systemctl is-active --quiet docker; then
        status="✅"
      else
        status="❌"
      fi
    elif [[ "$service_name" == "base-services" ]]; then
      # Special check for base-services (combination check)
      if [[ "$(check_base_status)" == "✅" ]] && [[ "$(check_ingress_status)" == "✅" ]] && [[ "$(check_certmanager_status)" == "✅" ]]; then
        status="✅"
      elif [[ "$(check_base_status)" == "❌" ]] && [[ "$(check_ingress_status)" == "❌" ]] && [[ "$(check_certmanager_status)" == "❌" ]]; then
        status="❌"
      else
        status="⚠️"
      fi
    else
      status="$(check_service_status "$deployment_name" "$namespace" "$service_name" "kubectl")"
    fi
    printf "  %2d. %-15s %s  %s\n" $index "$service_name" "$status" "$description"
    ((index++))
  done
  
  echo ""
  echo "📍 Status Legend:"
  echo "   ✅ Installed and Running"
  echo "   ⚠️  Installed but Issues Detected"
  echo "   ❌ Not Installed"
  echo ""
  echo "💡 Usage:"
  echo "   gok status                    # Show all services status"
  echo "   gok status <helm-release>     # Show specific Helm release status"
  echo ""
}

function taintNodeCmd() {
  taintNode
}

collectUserInputs(){
  INPUT_FILE="/root/base_services_inputs"
  
  echo "# Base Services Installation Inputs" > "$INPUT_FILE"

  # Collect Docker credentials
  echo "=== Docker Registry Credentials ==="
  DOCKER_USERNAME=$(promptUserInput "Please enter docker user id: ")
  DOCKER_PASSWORD=$(promptSecret "Please enter your docker password: ")
  {
    echo "export DOCKER_USERNAME='$DOCKER_USERNAME'"
    echo "export DOCKER_PASSWORD='$DOCKER_PASSWORD'"
  } >> "$INPUT_FILE"
  
  # Collect inputs for LDAP
  echo "=== LDAP Configuration ==="
  LDAP_PASSWORD=$(promptSecret "Please enter LDAP password for admin: ")
  KERBEROS_PASSWORD=$(promptSecret "Please enter Kerberos password: ")
  KERBEROS_KDC_PASSWORD=$(promptSecret "Please enter Kerberos kdc password: ")
  KERBEROS_ADM_PASSWORD=$(promptSecret "Please enter Kerberos adm password: ")

  {
    echo "export LDAP_PASSWORD='$LDAP_PASSWORD'"
    echo "export KERBEROS_PASSWORD='$KERBEROS_PASSWORD'"
    echo "export KERBEROS_KDC_PASSWORD='$KERBEROS_KDC_PASSWORD'"
    echo "export KERBEROS_ADM_PASSWORD='$KERBEROS_ADM_PASSWORD'"
  } >> "$INPUT_FILE"

  # Collect inputs for Keycloak
  echo "=== Keycloak Configuration ==="
  KEYCLOAK_ADMIN_USERNAME=$(promptUserInput "Please enter keycloak admin username (admin): " "admin")
  KEYCLOAK_ADMIN_PASSWORD=$(promptSecret "Please enter keycloak admin password: ")
  echo "export KEYCLOAK_ADMIN_USERNAME='$KEYCLOAK_ADMIN_USERNAME'" >> "$INPUT_FILE"
  echo "export KEYCLOAK_ADMIN_PASSWORD='$KEYCLOAK_ADMIN_PASSWORD'" >> "$INPUT_FILE"

  # Collect inputs for PostgreSQL
  POSTGRESQL_USERNAME=$(promptUserInput "Please enter postgresql username (postgres): " "postgres")
  POSTGRESQL_PASSWORD=$(promptSecret "Please enter postgresql password: ")
  echo "export POSTGRESQL_USERNAME='$POSTGRESQL_USERNAME'" >> "$INPUT_FILE"
  echo "export POSTGRESQL_PASSWORD='$POSTGRESQL_PASSWORD'" >> "$INPUT_FILE"

  # Collect inputs for OIDC
  OIDC_CLIENT_ID=$(promptUserInput "Please enter OIDC client id (${OIDC_CLIENT_ID}): " "${OIDC_CLIENT_ID}")
  REALM=$(promptUserInput "Please enter realm name (${REALM}): " "${REALM}")
  echo "export OIDC_CLIENT_ID='$OIDC_CLIENT_ID'" >> "$INPUT_FILE"
  echo "export REALM='$REALM'" >> "$INPUT_FILE"
  
  SAMPLE_USER_PASSWORD=$(promptSecret "Please enter sample user password (for user: skmaji1): ")
  echo "export SAMPLE_USER_PASSWORD='$SAMPLE_USER_PASSWORD'" >> "$INPUT_FILE"

  # Collect OAuth2 client secret
  ACTIVE_PROFILE=$(promptUserInput "Enter Active Profile (keycloak): " "keycloak")
  OIDC_ISSUE_URL=$(promptUserInput "Enter OIDC Issue URL (https://keycloak.gokcloud.com/realms/$REALM): " "https://keycloak.gokcloud.com/realms/$REALM")
  OIDC_USERNAME_CLAIM=$(promptUserInput "Enter OIDC Username Claim (${OIDC_USERNAME_CLAIM}): " "${OIDC_USERNAME_CLAIM}")
  OIDC_GROUPS_CLAIM=$(promptUserInput "Enter OIDC Groups Claim (${OIDC_GROUPS_CLAIM}): " "${OIDC_GROUPS_CLAIM}")
  AUTH0_DOMAIN=$(promptUserInput "Enter Auth0 Domain (${AUTH0_DOMAIN}): " "${AUTH0_DOMAIN}")
  APP_HOST=$(promptUserInput "Enter App Host (${APP_HOST}): " "${APP_HOST}")
  JWKS_URL=$(promptUserInput "Enter JWKS URL (${JWKS_URL}): " "${JWKS_URL}")
  {
    echo "export ACTIVE_PROFILE='$ACTIVE_PROFILE'"
    echo "export OIDC_ISSUE_URL='$OIDC_ISSUE_URL'"
    echo "export OIDC_USERNAME_CLAIM='$OIDC_USERNAME_CLAIM'"
    echo "export OIDC_GROUPS_CLAIM='$OIDC_GROUPS_CLAIM'"
    echo "export AUTH0_DOMAIN='$AUTH0_DOMAIN'"
    echo "export APP_HOST='$APP_HOST'"
    echo "JWKS_URL='$JWKS_URL'"
  } >> "$INPUT_FILE"
  
  # Collect Vault inputs if needed
  # Add other inputs as needed...
  
  echo "All inputs collected and saved to $INPUT_FILE"
}

function installBaseServices(){
  STATE_FILE="/root/base_services_install_state"
  INPUT_FILE="/root/base_services_inputs"
  
  # Check current state
  if [ -f "$STATE_FILE" ]; then
    STATE=$(cat "$STATE_FILE")
    case "$STATE" in
      "post_reboot")
        echo "Continuing base services installation after reboot..."
        postRebootBaseServices
        return
        ;;
      "completed")
        echo "Base services installation already completed"
        return
        ;;
    esac
  fi
  
  # Pre-collect all user inputs
  echo "Collecting required inputs for base services installation..."
  collectUserInputs
  
  # Pre-reboot services
  echo "Installing pre-reboot base services..."
  echo "starting_install" > "$STATE_FILE"
  
  gok install ingress
  gok install cert-manager
  
  # Check if reboot is needed
  if [[ "$CERTMANAGER_CHALANGE_TYPE" == "selfsigned" ]]; then
    echo "post_reboot" > "$STATE_FILE"
    
    # Create a separate script file instead of inline bash
    cat > /root/post-reboot-script.sh <<'SCRIPT_EOF'
#!/bin/bash
# Wait for Kubernetes API to be available before proceeding
export MOUNT_PATH=/root
source $MOUNT_PATH/kubernetes/install_k8s/util
source $MOUNT_PATH/kubernetes/install_k8s/gok
export KUBECONFIG=/root/.kube/config

for i in {1..30}; do
  if kubectl get nodes &>/dev/null; then
    echo "Kubernetes is up and running."
    break
  else
    kubectl get nodes
    echo "Waiting for Kubernetes API to be available... ($i/30)"
    sleep 10
  fi
done

if ! kubectl get nodes &>/dev/null; then
  echo "Kubernetes API is not available after waiting. Exiting."
  exit 1
fi

installBaseServices
SCRIPT_EOF
    
    chmod +x /root/post-reboot-script.sh
    
    # Create systemd service for post-reboot continuation
    cat > /etc/systemd/system/base-services-post-reboot-1.service <<EOF
[Unit]
Description=Base Services Post Reboot Setup
After=multi-user.target

[Service]
Type=simple
ExecStart=/root/post-reboot-script.sh
Restart=no
StandardOutput=journal
StandardError=journal

[Install]
WantedBy=multi-user.target
EOF

    systemctl enable base-services-post-reboot-1.service
    
    echoWarning "Self-signed certificate added to root ca, rebooting system for it to take effect"
    sudo reboot
  else
    postRebootBaseServices
  fi
}

baseServicesInstallLogs(){
  journalctl -u base-services-post-reboot-1.service --no-pager -f
}

postRebootBaseServices(){
  echo "Installing post-reboot base services..."
  
  # Load saved inputs
  INPUT_FILE="/root/base_services_inputs"
  if [ -f "$INPUT_FILE" ]; then
    source "$INPUT_FILE"
    echo "Loaded saved user inputs"
  else
    echoFailed "Input file not found! Cannot continue without user inputs."
    return 1
  fi
  
  # Clean up systemd service and script
  if [ -f "/etc/systemd/system/base-services-post-reboot-1.service" ]; then
    systemctl disable base-services-post-reboot-1.service
    rm -f /etc/systemd/system/base-services-post-reboot-1.service
    rm -f /root/post-reboot-script.sh
    systemctl daemon-reload
  fi
  
  # Install services using saved inputs
  gok install kyverno
  gok install registry
  gok install base
  gok install ldap
  gok install keycloak
  gok install oauth2
  gok install rabibtmq
  gok install vault
  
  # Clean up input file
  rm -f "$INPUT_FILE"
  
  echo "completed" > "/root/base_services_install_state"
  echoSuccess "Base services installation completed."
}

function resetBaseServices(){
  echo "Resetting base services..."
  echo "This will reset the following components: vault, rabbitmq, oauth2, keycloak, ldap, registry, kyverno, cert-manager, ingress."
  sleep 5
  gok reset vault
  echoSuccess "Vault reset completed."
  gok reset rabbitmq
  echoSuccess "RabbitMQ reset completed."
  gok reset oauth2
  echoSuccess "OAuth2 reset completed."
  gok reset keycloak
  echoSuccess "Keycloak reset completed."
  gok reset ldap
  echoSuccess "LDAP reset completed."
  gok reset registry
  echoSuccess "Registry reset completed."
  gok reset kyverno
  echoSuccess "Kyverno reset completed."
  gok reset cert-manager
  echoSuccess "Cert-Manager reset completed."
  gok reset ingress
  echoSuccess "Ingress reset completed."
}


function installCmd() {
  COMPONENT=$1
  
  if [ -z "$COMPONENT" ] || [ "$COMPONENT" == "help" ] || [ "$COMPONENT" == "--help" ]; then
    echo "gok install - Install and configure Kubernetes components and services"
    echo ""
    echo "Usage: gok install <component> [--verbose|-v]"
    echo ""
    echo "Options:"
    echo "  --verbose, -v     Show detailed installation output (default: progress bars)"
    echo "  GOK_VERBOSE=true  Environment variable to enable verbose mode globally"
    echo ""
    echo "Core Infrastructure:"
    echo "  docker            Docker container runtime"
    echo "  helm              Helm package manager for Kubernetes"
    echo "  haproxy           HA proxy container for load balancing (aliases: ha-proxy, ha)"
    echo "  kubernetes        Complete Kubernetes cluster with HA"
    echo "  kubernetes-worker Kubernetes worker node"
    echo "  cert-manager      Certificate management and TLS automation"
    echo "  ingress           NGINX ingress controller"
    echo "  dashboard         Kubernetes web dashboard"
    echo ""
    echo "Monitoring & Logging:"
    echo "  monitoring        Prometheus and Grafana stack"
    echo "  fluentd           Log collection and forwarding"
    echo "  opensearch        Search and analytics engine with dashboard"
    echo ""
    echo "Security & Identity:"
    echo "  keycloak          Identity and access management"
    echo "  oauth2            OAuth2 proxy for authentication"
    echo "  vault             Secrets management"
    echo "  ldap              LDAP directory service"
    echo ""
    echo "Development Tools:"
    echo "  jupyter           JupyterHub for data science"
    echo "  devworkspace      Developer workspace (legacy)"
    echo "  workspace         Enhanced developer workspace"
    echo "  che               Eclipse Che IDE"
    echo "  ttyd              Terminal over HTTP"
    echo "  cloudshell        Cloud-based terminal"
    echo "  console           Web-based console"
    echo ""
    echo "CI/CD & DevOps:"
    echo "  argocd            GitOps continuous delivery"
    echo "  jenkins           CI/CD automation server"
    echo "  spinnaker         Multi-cloud deployment platform"
    echo "  registry          Container image registry"
    echo ""
    echo "Service Mesh & Networking:"
    echo "  istio             Service mesh for microservices"
    echo "  rabbitmq          Message broker"
    echo ""
    echo "Governance & Policy:"
    echo "  kyverno           Kubernetes policy engine"
    echo ""
    echo "GOK Platform:"
    echo "  gok-agent         GOK distributed system agent"
    echo "  gok-controller    GOK distributed system controller"
    echo "  controller        Install both gok-agent and gok-controller"
    echo "  gok-login         GOK authentication service"
    echo "  chart             Helm chart repository"
    echo ""
    echo "Complete Solutions:"
    echo "  base              Base system components"
    echo "  base-services     Complete base services stack"
    echo ""
    echo "Examples:"
    echo "  gok install kubernetes        # Install complete K8s cluster"
    echo "  gok install cert-manager      # Install certificate management"
    echo "  gok install monitoring        # Install Prometheus & Grafana"
    echo "  gok install base-services     # Install complete base stack"
    echo ""
    echo "Installation Features:"
    echo "  ✅ Automated dependency resolution"
    echo "  ✅ High availability configuration"
    echo "  ✅ TLS/SSL certificate automation"
    echo "  ✅ RBAC and security hardening"
    echo "  ✅ Production-ready configurations"
    echo "  ✅ Integrated monitoring and logging"
    echo "  ✅ Service mesh ready"
    echo ""
    echo "Prerequisites:"
    echo "  - Ubuntu/Debian-based system"
    echo "  - Root or sudo access"
    echo "  - Internet connectivity"
    echo "  - Minimum 4GB RAM, 2 CPU cores"
    return 0
  fi
  
  # Check for interactive mode
  if [ "$COMPONENT" == "interactive" ] || [ "$COMPONENT" == "wizard" ]; then
    interactive_installation
    return 0
  fi
  
  # Initialize component tracking
  init_component_tracking "$COMPONENT" "Installing $COMPONENT component"
  
  # Check for verbose flag in all arguments
  local verbose_flag=""
  for arg in "$@"; do
    if [[ "$arg" == "--verbose" ]] || [[ "$arg" == "-v" ]]; then
      verbose_flag="--verbose"
      export GOK_VERBOSE="true"
      log_info "Verbose logging enabled"
      break
    fi
  done
  
  # Also check environment variable
  if [[ "$GOK_VERBOSE" == "true" ]] && [[ "$verbose_flag" == "" ]]; then
    verbose_flag="--verbose"
    log_info "Verbose logging enabled via GOK_VERBOSE"
  fi
  
  # Start component installation with enhanced logging
  start_component "$COMPONENT" "Installing $COMPONENT component"
  
  # Run system updates with progress bars (unless verbose)
  if ! updateSys "$verbose_flag"; then
    fail_component "$COMPONENT" "System update failed"
    return 1
  fi
  
  if ! installDeps "$verbose_flag"; then
    fail_component "$COMPONENT" "Dependency installation failed"
    return 1
  fi
  
  # Enhanced installation with validation
  if [ "$COMPONENT" == "docker" ]; then
    if dockrInst; then
      if validate_component_installation "docker" 120; then
        complete_component "docker" "Docker installation completed and validated"
        show_component_next_steps "docker"
      else
        complete_component "docker" "Docker installed but validation had warnings"
      fi
    else
      fail_component "docker" "Docker installation failed"
      return 1
    fi
  elif [ "$COMPONENT" == "helm" ]; then
    if helmInst; then
      if validate_component_installation "helm" 60; then
        complete_component "helm" "Helm installation completed and validated"
        show_component_next_steps "helm"
      else
        complete_component "helm" "Helm installed but validation had warnings"
      fi
    else
      fail_component "helm" "Helm installation failed"
      return 1
    fi
  elif [ "$COMPONENT" == "cert-manager" ]; then
    if certmanagerInst && setupCertiIssuers; then
      if validate_component_installation "cert-manager" 300; then
        complete_component "cert-manager" "Cert-manager installation completed and validated"
        show_component_next_steps "cert-manager"
      else
        complete_component "cert-manager" "Cert-manager installed but validation had warnings"
      fi
    else
      fail_component "cert-manager" "Cert-manager installation failed"
      return 1
    fi
  elif [ "$COMPONENT" == "monitoring" ]; then
    if installPrometheusGrafanaWithCertMgr; then
      if validate_component_installation "monitoring" 600; then
        complete_component "monitoring" "Monitoring stack installation completed and validated"
        show_component_next_steps "monitoring"
      else
        complete_component "monitoring" "Monitoring stack installed but validation had warnings"
      fi
    else
      fail_component "monitoring" "Monitoring stack installation failed"
      return 1
    fi
  elif [ "$COMPONENT" == "ldap" ]; then
    installLdap
  elif [ "$COMPONENT" == "dashboard" ]; then
    installDashboardwithCertManager
  elif [ "$COMPONENT" == "jupyter" ]; then
    jupyterHubInst
  elif [ "$COMPONENT" == "devworkspace" ]; then
    createDevWorkspace
  elif [ "$COMPONENT" == "workspace" ]; then
    createDevWorkspaceV2
  elif [ "$COMPONENT" == "che" ]; then
    eclipseCheInst
  elif [ "$COMPONENT" == "ttyd" ]; then
    ttydInst
  elif [ "$COMPONENT" == "cloudshell" ]; then
    cloudshellInst
  elif [ "$COMPONENT" == "console" ]; then
    consoleInst
  elif [ "$COMPONENT" == "argocd" ]; then
    if argocdInst; then
      if validate_component_installation "argocd" 300; then
        complete_component "argocd" "ArgoCD installation completed and validated"
        show_component_next_steps "argocd"
      else
        complete_component "argocd" "ArgoCD installed but validation had warnings"
      fi
    else
      fail_component "argocd" "ArgoCD installation failed"
      return 1
    fi
  elif [ "$COMPONENT" == "gok-agent" ]; then
    if gokAgentInstall; then
      if validate_component_installation "gok-agent" 180; then
        complete_component "gok-agent" "GOK Agent installation completed and validated"
        show_component_next_steps "gok-agent"
      else
        complete_component "gok-agent" "GOK Agent installed but validation had warnings"
      fi
    else
      fail_component "gok-agent" "GOK Agent installation failed"
      return 1
    fi
  elif [ "$COMPONENT" == "gok-controller" ]; then
    if gokControllerInstall; then
      if validate_component_installation "gok-controller" 180; then
        complete_component "gok-controller" "GOK Controller installation completed and validated"
        show_component_next_steps "gok-controller"
      else
        complete_component "gok-controller" "GOK Controller installed but validation had warnings"
      fi
    else
      fail_component "gok-controller" "GOK Controller installation failed"
      return 1
    fi
  elif [ "$COMPONENT" == "controller" ]; then
    log_info "Installing GOK Platform (Agent + Controller)"
    if gok install gok-agent && gok install gok-controller; then
      complete_component "controller" "GOK Platform installation completed"
      show_component_next_steps "controller"
    else
      fail_component "controller" "GOK Platform installation failed"
      return 1
    fi
  elif [ "$COMPONENT" == "chart" ]; then
    chartInst
  elif [ "$COMPONENT" == "rabbitmq" ]; then
    rabbitmqInst
  elif [ "$COMPONENT" == "vault" ]; then
    if vaultInstall; then
      if validate_component_installation "vault" 600; then
        complete_component "vault" "Vault installation completed and validated"
        show_component_next_steps "vault"
      else
        complete_component "vault" "Vault installed but requires manual unsealing"
      fi
    else
      fail_component "vault" "Vault installation failed"
      return 1
    fi
  elif [ "$COMPONENT" == "gok-login" ]; then
    if gokLoginInst; then
      complete_component "gok-login" "GOK Login service installation completed"
      show_component_next_steps "gok-login"
    else
      fail_component "gok-login" "GOK Login service installation failed"
      return 1
    fi
  elif [ "$COMPONENT" == "keycloak" ]; then
    if installKeycloakWithCertMgr; then
      if validate_component_installation "keycloak" 300; then
        complete_component "keycloak" "Keycloak installation completed and validated"
        show_component_next_steps "keycloak"
      else
        complete_component "keycloak" "Keycloak installed but validation had warnings"
      fi
    else
      fail_component "keycloak" "Keycloak installation failed"
      return 1
    fi
  elif [ "$COMPONENT" == "ingress" ]; then
    if ingressInst; then
      if validate_component_installation "ingress" 300; then
        complete_component "ingress" "Ingress controller installation completed and validated"
        show_component_next_steps "ingress"
      else
        complete_component "ingress" "Ingress controller installed but validation had warnings"
      fi
    else
      fail_component "ingress" "Ingress controller installation failed"
      return 1
    fi
  elif [ "$COMPONENT" == "registry" ]; then
    installRegistryWithCertMgr
  elif [ "$COMPONENT" == "fluentd" ]; then
    fluentdInst
  elif [ "$COMPONENT" == "opensearch" ]; then
    opensearchInst
    opensearchDashInst
  elif [ "$COMPONENT" == "jenkins" ]; then
    jenkinsInst
  elif [ "$COMPONENT" == "spinnaker" ]; then
    spinnakerInst
  elif [ "$COMPONENT" == "oauth2" ]; then
    oauth2ProxyInst
  elif [ "$COMPONENT" == "istio" ]; then
    istioInst
  elif [ "$COMPONENT" == "kyverno" ]; then
    kyvernoInst
  elif [ "$COMPONENT" == "base" ]; then
    baseInst
  elif [ "$COMPONENT" == "kubernetes-worker" ]; then
    dockrInst $verbose_flag
    k8sInst "kubernetes-worker" "$verbose_flag"
  elif [ "$COMPONENT" == "base-services" ]; then
    installBaseServices
  elif [ "$COMPONENT" == "haproxy" ] || [ "$COMPONENT" == "ha-proxy" ] || [ "$COMPONENT" == "ha" ]; then
    if haInst; then
      if validate_haproxy_installation; then
        complete_component "haproxy" "HA proxy installation completed and validated"
        show_haproxy_next_steps
      else
        complete_component "haproxy" "HA proxy installed but validation had warnings"
      fi
    else
      fail_component "haproxy" "HA proxy installation failed"
      return 1
    fi
  elif [ "$COMPONENT" == "kubernetes" ]; then
    # Enhanced Kubernetes installation with comprehensive logging and validation
    
    # Step 1: Install Docker first (prerequisite)
    log_info "Ensuring Docker is installed and running..."
    if ! dockrInst; then
      log_error "Docker installation failed - required for Kubernetes"
      fail_component "kubernetes" "Docker prerequisite installation failed"
      return 1
    fi
    log_success "Docker prerequisite satisfied"
    
    # Step 2: Handle HA proxy setup if needed
    if [[ -n "$API_SERVERS" ]] && [[ "$API_SERVERS" == *","* ]]; then
      log_info "Multiple API servers detected - installing HA proxy for high availability"
      if haInst; then
        log_success "HA proxy installed for multi-master setup"
      else
        log_warning "HA proxy installation failed - proceeding with single-node setup"
      fi
    else
      log_info "Single API server configuration - skipping HA proxy"
    fi
    
    # Step 3: Install Kubernetes cluster
    if k8sInst "kubernetes" "$verbose_flag"; then
      log_success "Kubernetes master node installation completed"
      
      # Validate Kubernetes installation before proceeding
      if validate_component_installation "kubernetes" 300; then
        log_success "Kubernetes cluster validation passed"
      else
        log_warning "Kubernetes cluster validation had issues"
      fi
    else
      log_error "Kubernetes installation failed"
      fail_component "kubernetes" "Kubernetes cluster installation failed"
      return 1
    fi
    
    # Step 4: Install Helm package manager
    log_info "Installing Helm package manager..."
    if helmInst; then
      log_success "Helm package manager installed"
    else
      log_warning "Helm installation failed - some deployments may not work"
    fi
    
    # Step 5: Configure master node (remove taints for single-node)
    log_info "Configuring master node scheduling..."
    if taintNode; then
      log_success "Master node configured for pod scheduling"
    else
      log_warning "Master node taint removal failed"
    fi
    
    # Step 6: Install network plugin (Calico)
    log_info "Installing Calico network plugin..."
    if calicoInst $verbose_flag; then
      log_success "Calico network plugin installed"
    else
      log_error "Calico installation failed - cluster networking may not work"
      log_error "Please install a network plugin manually"
    fi
    
    # Step 7: Wait for system services to be ready
    log_info "Waiting for Kubernetes system services..."
    if waitForServiceAvailable kube-system; then
      log_success "Kubernetes system services are ready"
    else
      log_warning "Some system services may not be ready yet"
    fi
    
    # Step 8: Install additional utilities
    log_info "Installing cluster utilities..."
    dnsUtils >/dev/null 2>&1 && log_success "DNS utilities installed" || log_warning "DNS utilities installation failed"
    customDns >/dev/null 2>&1 && log_success "Custom DNS configured" || log_warning "Custom DNS configuration failed"
    kcurl >/dev/null 2>&1 && log_success "kubectl curl utility installed" || log_warning "kubectl curl utility failed"
    oauthAdmin >/dev/null 2>&1 && log_success "OAuth admin configured" || log_warning "OAuth admin configuration failed"
    
    # Step 9: Setup system integration
    log_info "Setting up system integration..."
    {
      echo "source $MOUNT_PATH/kubernetes/install_k8s/util" >> /etc/bash.bashrc
      echo "source $MOUNT_PATH/kubernetes/install_k8s/gok" >> /etc/bash.bashrc
      ln -sf $MOUNT_PATH/kubernetes/install_k8s/gok /bin/gok
      
      cat <<EOF > /etc/rc.local
#!/bin/bash
/bin/gok start kubernetes
exit 0
EOF
      chmod +x /etc/rc.local
      systemctl enable rc-local >/dev/null 2>&1
      source /etc/bash.bashrc
    } >/dev/null 2>&1
    
    log_success "System integration configured"
    
    # Final validation and next steps
    if validate_component_installation "kubernetes" 180; then
      complete_component "kubernetes" "Kubernetes cluster installation completed and validated"
      show_kubernetes_complete_next_steps
    else
      complete_component "kubernetes" "Kubernetes cluster installed but validation had warnings"
      show_kubernetes_complete_next_steps
    fi
  else
    echo "Error: Unsupported component: $COMPONENT"
    echo ""
    echo "Supported components:"
    echo "Core: docker, kubernetes, kubernetes-worker, cert-manager, ingress, dashboard"
    echo "Monitoring: monitoring, fluentd, opensearch"
    echo "Security: keycloak, oauth2, vault, ldap"
    echo "Development: jupyter, devworkspace, workspace, che, ttyd, cloudshell, console"
    echo "CI/CD: argocd, jenkins, spinnaker, registry"
    echo "Networking: istio, rabbitmq"
    echo "Policy: kyverno"
    echo "GOK: gok-agent, gok-controller, controller, gok-login, chart"
    echo "Solutions: base, base-services"
    echo ""
    echo "Run 'gok install help' for detailed information"
    return 1
  fi
}

function fixCmd() {
  COMPONENT=$1
  
  if [ -z "$COMPONENT" ] || [ "$COMPONENT" == "help" ] || [ "$COMPONENT" == "--help" ]; then
    echo "gok fix - Fix common repository and installation issues"
    echo ""
    echo "Usage: gok fix <issue-type>"
    echo ""
    echo "Available Fix Options:"
    echo "  helm-repository    Fix Helm repository 404 errors and conflicts"
    echo "  repositories       Fix all package repository issues"
    echo "  package-conflicts  Resolve package manager conflicts"
    echo ""
    echo "Examples:"
    echo "  gok fix helm-repository    # Fix Helm 404 errors from baltocdn"
    echo "  gok fix repositories       # Fix all repository issues"
    echo ""
    echo "Common Issues Fixed:"
    echo "  ✅ Helm repository 404 errors (baltocdn)"
    echo "  ✅ Deprecated APT key warnings"
    echo "  ✅ Mixed installation method conflicts"
    echo "  ✅ Broken package repository configurations"
    echo "  ✅ GPG key verification issues"
    echo ""
    echo "Prevention Tips:"
    echo "  • Use snap for Helm: sudo snap install helm --classic"
    echo "  • Avoid mixing installation methods"
    echo "  • Regular repository cleanup"
    return 0
  fi
  
  case "$COMPONENT" in
    "helm-repository"|"helm-repo"|"helm")
      fix_helm_repository_errors
      ;;
    "repositories"|"repos"|"repository")  
      fix_helm_repository_errors
      # Can be extended for other repository fixes
      ;;
    "package-conflicts"|"packages")
      log_info "Package conflict resolution not yet implemented"
      echo "Available: helm-repository, repositories"
      return 1
      ;;
    *)
      echo "Error: Unknown fix type: $COMPONENT"
      echo ""
      echo "Available fix options:"
      echo "  helm-repository    Fix Helm repository 404 errors"
      echo "  repositories       Fix all repository issues"
      echo ""
      echo "Run 'gok fix help' for detailed information"
      return 1
      ;;
  esac
}

function startCmd() {
  COMPONENT=$1
  if [ "$COMPONENT" == "kubernetes" ]; then
    disableSwap
    startHa
    startKubelet
  elif [ "$COMPONENT" == "proxy" ]; then
    startHa
  elif [ "$COMPONET" == "kubelet" ]; then
    startKubelet
  fi
}

# Enhanced user confirmation with detailed information
confirm_reset() {
    local component="$1"
    local description="$2"
    local warning="$3"
    
    echo
    log_section "🔄 Reset Confirmation" "${EMOJI_WARNING}"
    
    echo -e "${COLOR_CYAN}Component:${COLOR_RESET} ${COLOR_BOLD}$component${COLOR_RESET}"
    echo -e "${COLOR_CYAN}Description:${COLOR_RESET} $description"
    
    if [ -n "$warning" ]; then
        echo
        log_warning "$warning"
    fi
    
    echo
    echo -e "${COLOR_RED}${COLOR_BOLD}⚠️  WARNING: This operation is destructive and irreversible!${COLOR_RESET}"
    echo -e "${COLOR_RED}   • All data and configurations will be permanently lost${COLOR_RESET}"
    echo -e "${COLOR_RED}   • Backup your important data before proceeding${COLOR_RESET}"
    echo -e "${COLOR_RED}   • Some operations require cluster admin privileges${COLOR_RESET}"
    echo
    
    while true; do
        echo -n -e "${COLOR_YELLOW}Do you want to continue? [y/N]: ${COLOR_RESET}"
        read -r response
        case $response in
            [Yy]|[Yy][Ee][Ss]) 
                log_info "User confirmed reset operation"
                return 0
                ;;
            [Nn]|[Nn][Oo]|"") 
                log_info "Reset operation cancelled by user"
                echo -e "${COLOR_GREEN}Reset cancelled. No changes made.${COLOR_RESET}"
                return 1
                ;;
            *) 
                echo -e "${COLOR_RED}Please answer yes or no.${COLOR_RESET}"
                ;;
        esac
    done
}

# Enhanced cleanup function for Kubernetes files and directories
cleanup_kubernetes_files() {
    local verbose_mode="$1"
    local is_verbose=false
    
    if [[ "$verbose_mode" == "--verbose" ]] || [[ "$GOK_VERBOSE" == "true" ]]; then
        is_verbose=true
    fi
    
    log_step 1 "Cleaning up Kubernetes configuration files"
    
    # Kubernetes config directories
    local k8s_dirs=(
        "/etc/kubernetes"
        "/var/lib/kubelet"
        "/var/lib/kube-proxy"
        "/var/lib/kube-scheduler" 
        "/var/lib/kube-controller-manager"
        "/var/lib/etcd"
        "/opt/cni/bin"
        "/etc/cni/net.d"
        "/var/lib/cni"
        "/var/run/kubernetes"
        "/etc/systemd/system/kubelet.service.d"
    )
    
    # User kubeconfig files
    local user_configs=(
        "$HOME/.kube"
        "/root/.kube"
    )
    
    # Container runtime directories
    local container_dirs=(
        "/var/lib/docker/containers"
        "/var/lib/containerd"
        "/run/containerd"
        "/var/lib/dockershim"
    )
    
    # Network configuration
    local network_files=(
        "/etc/cni/net.d/*"
        "/opt/cni/bin/*"
        "/var/lib/calico"
        "/var/lib/canal"
        "/var/lib/weave"
    )
    
    log_info "Stopping Kubernetes services..."
    
    # Stop services gracefully
    local services=("kubelet" "kube-proxy" "docker" "containerd")
    for service in "${services[@]}"; do
        if systemctl is-active --quiet "$service" 2>/dev/null; then
            if [[ "$is_verbose" == "true" ]]; then
                log_substep "Stopping $service service"
            fi
            systemctl stop "$service" 2>/dev/null || log_warning "Failed to stop $service"
        fi
    done
    
    # Remove Kubernetes directories
    log_info "Removing Kubernetes directories..."
    for dir in "${k8s_dirs[@]}"; do
        if [ -d "$dir" ]; then
            if [[ "$is_verbose" == "true" ]]; then
                log_substep "Removing directory: $dir"
            fi
            rm -rf "$dir" 2>/dev/null || log_warning "Failed to remove $dir"
        fi
    done
    
    # Handle user kubeconfig files
    log_info "Cleaning up kubeconfig files..."
    for config in "${user_configs[@]}"; do
        if [ -d "$config" ]; then
            if [[ "$is_verbose" == "true" ]]; then
                log_substep "Backing up and removing: $config"
            fi
            # Create backup before removing
            if [ -f "$config/config" ]; then
                cp "$config/config" "$config/config.backup.$(date +%Y%m%d_%H%M%S)" 2>/dev/null
                if [[ "$is_verbose" == "true" ]]; then
                    log_substep "Created backup: $config/config.backup.$(date +%Y%m%d_%H%M%S)"
                fi
            fi
            rm -rf "$config" 2>/dev/null || log_warning "Failed to remove $config"
        fi
    done
    
    # Clean network configurations
    log_info "Cleaning up network configurations..."
    for net_path in "${network_files[@]}"; do
        if ls $net_path 1> /dev/null 2>&1; then
            if [[ "$is_verbose" == "true" ]]; then
                log_substep "Removing network files: $net_path"
            fi
            rm -rf $net_path 2>/dev/null || log_warning "Failed to remove $net_path"
        fi
    done
    
    # Clean container runtime (always do full cleanup for kubernetes reset)
    log_info "Performing container runtime cleanup..."
        
        # Stop and remove all containers
        if command -v docker &> /dev/null; then
            if [[ "$is_verbose" == "true" ]]; then
                log_substep "Stopping and removing Docker containers"
            fi
            docker stop $(docker ps -aq) 2>/dev/null || true
            docker rm $(docker ps -aq) 2>/dev/null || true
            docker system prune -af 2>/dev/null || true
        fi
        
        # Clean containerd
        if command -v ctr &> /dev/null; then
            if [[ "$is_verbose" == "true" ]]; then
                log_substep "Cleaning containerd containers and images"
            fi
            ctr -n k8s.io containers rm $(ctr -n k8s.io containers list -q) 2>/dev/null || true
            ctr -n k8s.io images rm $(ctr -n k8s.io images list -q) 2>/dev/null || true
        fi
        
        # Clean container directories
        for dir in "${container_dirs[@]}"; do
            if [ -d "$dir" ]; then
                if [[ "$is_verbose" == "true" ]]; then
                    log_substep "Cleaning container directory: $dir"
                fi
                find "$dir" -type f -name "*.pid" -delete 2>/dev/null || true
                find "$dir" -type f -name "*.lock" -delete 2>/dev/null || true
            fi
        done
    
    # Clean systemd files
    log_info "Cleaning up systemd service files..."
    local systemd_files=(
        "/etc/systemd/system/kubelet.service"
        "/etc/systemd/system/kubelet.service.d"
        "/etc/systemd/system/kube-proxy.service"
        "/lib/systemd/system/kubelet.service"
    )
    
    for file in "${systemd_files[@]}"; do
        if [ -e "$file" ]; then
            if [[ "$is_verbose" == "true" ]]; then
                log_substep "Removing systemd file: $file"
            fi
            rm -rf "$file" 2>/dev/null || log_warning "Failed to remove $file"
        fi
    done
    
    # Reload systemd daemon
    if [[ "$is_verbose" == "true" ]]; then
        log_substep "Reloading systemd daemon"
    fi
    systemctl daemon-reload 2>/dev/null || log_warning "Failed to reload systemd daemon"
    
    # Clean up iptables rules (optional)
    log_info "Cleaning up iptables rules..."
    iptables -F 2>/dev/null || log_warning "Failed to flush iptables rules"
    iptables -t nat -F 2>/dev/null || log_warning "Failed to flush NAT rules"
    iptables -t mangle -F 2>/dev/null || log_warning "Failed to flush mangle rules"
    
    # Clean up network interfaces
    log_info "Cleaning up network interfaces..."
    local interfaces=$(ip link show | grep -E "(cni|flannel|calico|weave)" | awk -F: '{print $2}' | tr -d ' ')
    for iface in $interfaces; do
        if [ -n "$iface" ]; then
            if [[ "$is_verbose" == "true" ]]; then
                log_substep "Removing network interface: $iface"
            fi
            ip link delete "$iface" 2>/dev/null || log_warning "Failed to remove interface $iface"
        fi
    done
    
    log_success "Kubernetes file cleanup completed"
}

# Enhanced progress tracking for reset operations
track_reset_progress() {
    local message="$1"
    local timestamp=$(date '+%Y-%m-%d %H:%M:%S')
    
    echo -e "${COLOR_BRIGHT_BLUE}[${timestamp}]${COLOR_RESET} ${COLOR_CYAN}${message}${COLOR_RESET}"
}

function resetCmd() {
  COMPONENT=$1
  
  if [ -z "$COMPONENT" ] || [ "$COMPONENT" == "help" ] || [ "$COMPONENT" == "--help" ]; then
    echo "gok reset - Reset and uninstall Kubernetes components and services"
    echo ""
    echo "Usage: gok reset <component> [--verbose|-v]"
    echo ""
    echo "Options:"
    echo "  --verbose, -v     Show detailed reset output (default: progress indicators)"
    echo "  GOK_VERBOSE=true  Environment variable to enable verbose mode globally"
    echo ""
    echo "Core Infrastructure:"
    echo "  kubernetes        Complete Kubernetes cluster reset"
    echo "  kubernetes-worker Kubernetes worker node reset"
    echo "  cert-manager      Certificate management system"
    echo "  ingress           NGINX ingress controller"
    echo ""
    echo "Monitoring & Logging:"
    echo "  monitoring        Prometheus and Grafana stack"
    echo "  fluentd           Log collection and forwarding"
    echo "  opensearch        Search and analytics engine with dashboard"
    echo ""
    echo "Security & Identity:"
    echo "  keycloak          Identity and access management"
    echo "  oauth2            OAuth2 proxy for authentication"
    echo "  vault             Secrets management"
    echo "  ldap              LDAP directory service"
    echo ""
    echo "Development Tools:"
    echo "  jupyter           JupyterHub for data science"
    echo "  devworkspace      Developer workspace (legacy)"
    echo "  workspace         Enhanced developer workspace"
    echo "  che               Eclipse Che IDE"
    echo "  ttyd              Terminal over HTTP"
    echo "  cloudshell        Cloud-based terminal"
    echo "  console           Web-based console"
    echo "  dashboard         Kubernetes web dashboard"
    echo ""
    echo "CI/CD & DevOps:"
    echo "  argocd            GitOps continuous delivery"
    echo "  jenkins           CI/CD automation server"
    echo "  spinnaker         Multi-cloud deployment platform"
    echo "  registry          Container image registry"
    echo ""
    echo "Service Mesh & Networking:"
    echo "  istio             Service mesh for microservices"
    echo "  rabbitmq          Message broker"
    echo ""
    echo "Governance & Policy:"
    echo "  kyverno           Kubernetes policy engine"
    echo ""
    echo "GOK Platform:"
    echo "  gok-agent         GOK distributed system agent"
    echo "  gok-controller    GOK distributed system controller"
    echo "  controller        Reset both gok-agent and gok-controller"
    echo "  gok-login         GOK authentication service"
    echo "  chart             Helm chart repository"
    echo ""
    echo "Complete Solutions:"
    echo "  base-services     Complete base services stack"
    echo ""
    echo "Examples:"
    echo "  gok reset monitoring          # Remove Prometheus & Grafana"
    echo "  gok reset keycloak            # Remove identity management"
    echo "  gok reset kubernetes          # Reset entire cluster"
    echo "  gok reset base-services       # Remove complete base stack"
    echo ""
    echo "Reset Operations:"
    echo "  ✅ Clean uninstallation of Helm releases"
    echo "  ✅ Removal of Kubernetes namespaces and resources"
    echo "  ✅ Cleanup of persistent volumes and storage"
    echo "  ✅ Reset of system configurations"
    echo "  ✅ Removal of certificates and secrets"
    echo "  ✅ Network policy and ingress cleanup"
    echo "  ✅ Service mesh configuration reset"
    echo ""
    echo "⚠️  Warning:"
    echo "  - Reset operations are destructive and irreversible"
    echo "  - All data and configurations will be permanently lost"
    echo "  - Ensure you have backups before proceeding"
    echo "  - Some operations require cluster admin privileges"
    return 0
  fi
  
  # Check for verbose flag in all arguments
  local verbose_flag=""
  for arg in "$@"; do
    if [[ "$arg" == "--verbose" ]] || [[ "$arg" == "-v" ]]; then
      verbose_flag="--verbose"
      export GOK_VERBOSE="true"
      log_info "Verbose logging enabled for reset operation"
      break
    fi
  done
  
  # Also check environment variable
  if [[ "$GOK_VERBOSE" == "true" ]] && [[ "$verbose_flag" == "" ]]; then
    verbose_flag="--verbose"
    log_info "Verbose logging enabled for reset operation via GOK_VERBOSE"
  fi
  
  if [ "$COMPONENT" == "kubernetes" ]; then
    track_reset_progress "Starting Kubernetes cluster reset..."
    
    # Get user confirmation
    if ! confirm_reset "Kubernetes cluster" "This will completely remove the Kubernetes cluster and all data"; then
      echo "Kubernetes reset cancelled by user."
      return 0
    fi
    
    track_reset_progress "Performing kubeadm reset..."
    if [[ "$verbose_flag" == "--verbose" ]]; then
      kubeadm reset <<EOF
y
EOF
    else
      kubeadm reset <<EOF 2>/dev/null || { log_error "kubeadm reset failed"; return 1; }
y
EOF
    fi
    
    track_reset_progress "Cleaning up Kubernetes files and configurations..."
    cleanup_kubernetes_files "$verbose_flag"
    
    track_reset_progress "Kubernetes reset completed successfully."
  elif [ "$COMPONENT" == "monitoring" ]; then
    prometheusGrafanaResetv2

    emptyLocalFsStorage "Monitoring" "prometheus-pv" "prometheus-storage" "/data/volumes/pv1"
    emptyLocalFsStorage "Monitoring" "alertmanager-pv" "alertmanager-storage" "/data/volumes/pv2"
  elif [ "$COMPONENT" == "dashboard" ]; then
    dashboardReset
  elif [ "$COMPONENT" == "keycloak" ]; then
    keycloakReset
  elif [ "$COMPONENT" == "vault" ]; then
    vaultReset
  elif [ "$COMPONENT" == "ingress" ]; then
    ingressUnInst
  elif [ "$COMPONENT" == "chart" ]; then
    resetChart
  elif [ "$COMPONENT" == "gok-login" ]; then
    helm uninstall gok-login -n gok-login
    kubectl delete ns gok-login
  elif [ "$COMPONENT" == "ldap" ]; then
    ldapReset
  elif [ "$COMPONENT" == "gok-agent" ]; then
    gokAgentReset
  elif [ "$COMPONENT" == "gok-controller" ]; then
    gokControllerReset
  elif [ "$COMPONENT" == "controller" ]; then
    gok reset gok-agent
    gok reset gok-controller
  elif [ "$COMPONENT" == "argocd" ]; then
    argocdReset
  elif [ "$COMPONENT" == "devworkspace" ]; then
    deleteDevWorkspace
  elif [ "$COMPONENT" == "workspace" ]; then
    deleteDevWorkspaceV2
  elif [ "$COMPONENT" == "registry" ]; then
    resetDockerRegistry
  elif [ "$COMPONENT" == "fluentd" ]; then
    fluentdReset
  elif [ "$COMPONENT" == "jupyter" ]; then
    jupyterHubReset
  elif [ "$COMPONENT" == "rabbitmq" ]; then
    rabbitmqReset
  elif [ "$COMPONENT" == "che" ]; then
    resetEclipseChe
  elif [ "$COMPONENT" == "ttyd" ]; then
    ttydReset
  elif [ "$COMPONENT" == "cloudshell" ]; then
    cloudshellReset
  elif [ "$COMPONENT" == "console" ]; then
    consoleReset
  elif [ "$COMPONENT" == "opensearch" ]; then
    opensearchDashReset
    opensearchReset
  elif [ "$COMPONENT" == "jenkins" ]; then
    jenkinsReset
  elif [ "$COMPONENT" == "spinnaker" ]; then
    spinnakerReset
  elif [ "$COMPONENT" == "oauth2" ]; then
    oauth2ProxyReset
  elif [ "$COMPONENT" == "cert-manager" ]; then
    certManagerReset
  elif [ "$COMPONENT" == "istio" ]; then
    istioReset
  elif [ "$COMPONENT" == "kyverno" ]; then
    kyvernoReset
  elif [ "$COMPONENT" == "base-services" ]; then
    resetBaseServices
  elif [ "$COMPONENT" == "kubernetes-worker" ]; then
    kubeadm reset <<EOF
y
EOF
  else
    echo "Error: Unsupported component: $COMPONENT"
    echo ""
    echo "Supported components:"
    echo "Core: kubernetes, kubernetes-worker, cert-manager, ingress, dashboard"
    echo "Monitoring: monitoring, fluentd, opensearch"
    echo "Security: keycloak, oauth2, vault, ldap"
    echo "Development: jupyter, devworkspace, workspace, che, ttyd, cloudshell, console"
    echo "CI/CD: argocd, jenkins, spinnaker, registry"
    echo "Networking: istio, rabbitmq"
    echo "Policy: kyverno"
    echo "GOK: gok-agent, gok-controller, controller, gok-login, chart"
    echo "Solutions: base-services"
    echo ""
    echo "Run 'gok reset help' for detailed information"
    return 1
  fi
}

function deployCmd() {
  COMPONENT=$2
  if [ "$COMPONENT" == "app1" ]; then
    createApp1
  fi
}

function patchCmd() {
  RESOURCE=$1
  NAME=$2
  NS=$3
  OPTIONS=$4
  SUBDOMAIN=$5
  if [ "$RESOURCE" == "ingress" ]; then
    if [ "$OPTIONS" == "letsencrypt" ]; then
      patchLetsEncrypt "$NAME" "$NS" "$SUBDOMAIN"
    elif [ "$OPTIONS" == "ldap" ]; then
      patchLdapSecure "$NAME" "$NS"
    elif [ "$OPTIONS" == "localtls" ]; then
      patchLocalTls "$NAME" "$NS"
    fi
  fi
}

function createCmd() {
  RESOURCE=$1
  NAME=$2
  ADDITIONAL=$3
  
  if [ -z "$RESOURCE" ] || [ "$RESOURCE" == "help" ] || [ "$RESOURCE" == "--help" ]; then
    echo "gok create - Create Kubernetes resources and configurations"
    echo ""
    echo "Usage: gok create <resource> <name> [additional]"
    echo ""
    echo "Resource Types:"
    echo "  secret            Create Kubernetes secrets"
    echo "  certificate       Create TLS certificates for namespaces"
    echo "  kubeconfig        Create kubeconfig files for users"
    echo ""
    echo "Parameters:"
    echo "  <resource>        Resource type (required)"
    echo "  <name>            Resource name (required)"
    echo "  [additional]      Additional parameters (depends on resource type)"
    echo ""
    echo "Secret Resources:"
    echo "  apphost           Create application host secret"
    echo ""
    echo "Certificate Resources:"
    echo "  <namespace>       Create certificate request for specified namespace"
    echo "  [domain]          Custom domain name (optional)"
    echo ""
    echo "Kubeconfig Resources:"
    echo "  <username>        Create kubeconfig for specified user"
    echo ""
    echo "Examples:"
    echo "  gok create secret apphost"
    echo "  gok create certificate production"
    echo "  gok create certificate staging custom.domain.com"
    echo "  gok create kubeconfig developer-user"
    echo ""
    echo "Generated Resources:"
    echo "  ✅ Kubernetes secrets with proper encoding"
    echo "  ✅ TLS certificates with proper CA signing"
    echo "  ✅ Kubeconfig with cluster access configuration"
    echo "  ✅ Namespace-scoped certificate requests"
    echo "  ✅ Proper RBAC and security configurations"
    return 0
  fi
  
  if [ -z "$NAME" ]; then
    echo "Error: Resource name is required"
    echo "Usage: gok create <resource> <name> [additional]"
    echo "Run 'gok create help' for detailed usage information"
    return 1
  fi
  
  if [ "$RESOURCE" == "secret" ]; then
    if [ "$NAME" == "apphost" ]; then
      hostSecret
    else
      echo "Error: Unsupported secret type: $NAME"
      echo "Supported secret types: apphost"
      echo "Run 'gok create help' for detailed usage information"
      return 1
    fi
  elif [ "$RESOURCE" == "certificate" ]; then
    certificateRequestForNs "$NAME" "$ADDITIONAL"
  elif [ "$RESOURCE" == "kubeconfig" ]; then
    createKubeConfig "$NAME"
  else
    echo "Error: Unsupported resource type: $RESOURCE"
    echo "Supported resource types: secret, certificate, kubeconfig"
    echo "Run 'gok create help' for detailed usage information"
    return 1
  fi
}

function generateCmd() {
  SERVICE_TYPE=$1
  SERVICE_NAME=$2
  SERVICE_DESC=$3
  SERVICE_NS=$4
  SERVICE_HOST=$5
  
  if [ -z "$SERVICE_TYPE" ] || [ "$SERVICE_TYPE" == "help" ] || [ "$SERVICE_TYPE" == "--help" ]; then
    echo "gok generate - Generate microservices from templates"
    echo ""
    echo "Usage: gok generate <type> <name> [description] [namespace] [hostname]"
    echo ""
    echo "Service Types:"
    echo "  python-api        Generate Python Flask REST API (backend-only)"
    echo "  python-reactjs    Generate Python Flask + React.js (full-stack)"
    echo ""
    echo "Parameters:"
    echo "  <type>            Service type (required)"
    echo "  <name>            Service name (required)"
    echo "  [description]     Service description (optional)"
    echo "  [namespace]       Kubernetes namespace (optional, default: service name)"
    echo "  [hostname]        Ingress hostname (optional, default: <name>.gokcloud.com)"
    echo ""
    echo "Examples:"
    echo "  gok generate python-api user-service"
    echo "  gok generate python-api payment-api 'Payment processing API'"
    echo "  gok generate python-reactjs webapp 'Customer portal' production portal.company.com"
    echo ""
    echo "Generated Features:"
    echo "  ✅ Production-ready Docker containers"
    echo "  ✅ Kubernetes Helm charts with RBAC"
    echo "  ✅ OAuth2/OIDC authentication"
    echo "  ✅ HTTPS/TLS encryption"
    echo "  ✅ Health check endpoints"
    echo "  ✅ API documentation (Swagger UI)"
    echo "  ✅ Local development setup"
    return 0
  fi
  
  if [ -z "$SERVICE_NAME" ]; then
    echo "Error: Service name is required"
    echo "Usage: gok generate <type> <name> [description] [namespace] [hostname]"
    echo "Run 'gok generate help' for detailed usage information"
    return 1
  fi
  
  # Navigate to service generator directory
  SERVICE_GENERATOR_DIR="$WORKING_DIR/service-generator"
  
  # Fallback to current directory structure if WORKING_DIR not set
  if [ ! -d "$SERVICE_GENERATOR_DIR" ]; then
    # Try relative path from gok script location
    SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
    SERVICE_GENERATOR_DIR="$SCRIPT_DIR/service-generator"
    
    # If still not found, try current working directory
    if [ ! -d "$SERVICE_GENERATOR_DIR" ]; then
      SERVICE_GENERATOR_DIR="$(pwd)/service-generator"
    fi
    
    # Final check - see if we're already in service-generator directory
    if [ ! -d "$SERVICE_GENERATOR_DIR" ] && [ -f "generate_service.py" ]; then
      SERVICE_GENERATOR_DIR="$(pwd)"
    fi
  fi
  
  if [ ! -d "$SERVICE_GENERATOR_DIR" ] && [ ! -f "$SERVICE_GENERATOR_DIR/generate_service.py" ]; then
    echo "Error: Service generator not found"
    echo "Tried locations:"
    echo "  - $WORKING_DIR/service-generator"
    echo "  - $SCRIPT_DIR/service-generator" 
    echo "  - $(pwd)/service-generator"
    echo "  - $(pwd)"
    return 1
  fi
  
  cd "$SERVICE_GENERATOR_DIR"
  
  # Build command arguments
  GENERATE_ARGS=""
  
  # Add description if provided
  if [ ! -z "$SERVICE_DESC" ]; then
    GENERATE_ARGS="$GENERATE_ARGS --description \"$SERVICE_DESC\""
  fi
  
  # Add namespace if provided
  if [ ! -z "$SERVICE_NS" ]; then
    GENERATE_ARGS="$GENERATE_ARGS --namespace $SERVICE_NS"
  fi
  
  # Add ingress hostname if provided
  if [ ! -z "$SERVICE_HOST" ]; then
    GENERATE_ARGS="$GENERATE_ARGS --ingress-host $SERVICE_HOST"
  fi
  
  echo "🚀 Generating $SERVICE_TYPE service: $SERVICE_NAME"
  echo "📁 Working directory: $SERVICE_GENERATOR_DIR"
  
  if [ "$SERVICE_TYPE" == "python-api" ]; then
    echo "🐍 Creating Python Flask API service..."
    eval "python3 generate_service.py --python-api $SERVICE_NAME $GENERATE_ARGS"
    
  elif [ "$SERVICE_TYPE" == "python-reactjs" ]; then
    echo "🐍⚛️  Creating Python Flask + React.js full-stack service..."
    eval "python3 generate_service.py --python-reactjs $SERVICE_NAME $GENERATE_ARGS"
    
  else
    echo "Error: Unsupported service type: $SERVICE_TYPE"
    echo "Supported types: python-api, python-reactjs"
    return 1
  fi
  
  if [ $? -eq 0 ]; then
    echo ""
    echo "✅ Service generation completed successfully!"
    echo "📁 Location: $SERVICE_GENERATOR_DIR/generated_services/$SERVICE_NAME"
    echo ""
    echo "🚀 Next steps:"
    echo "1. cd $SERVICE_GENERATOR_DIR/generated_services/$SERVICE_NAME"
    echo "2. Review and customize the generated files"
    echo "3. Build container: ./build.sh"
    echo "4. Push to registry: ./tag_push.sh"
    echo "5. Deploy to Kubernetes: helm install $SERVICE_NAME ./chart"
    
    if [ ! -z "$SERVICE_HOST" ]; then
      echo "6. Access your service at: https://$SERVICE_HOST"
    else
      echo "6. Access your service at: https://$SERVICE_NAME.gokcloud.com"
    fi
  else
    echo "❌ Service generation failed!"
    return 1
  fi
}

# Bash completion function for GOK
_gok_completion() {
  local cur prev opts base
  COMPREPLY=()
  cur="${COMP_WORDS[COMP_CWORD]}"
  prev="${COMP_WORDS[COMP_CWORD-1]}"
  
  # Main commands
  local commands="install fix reset start deploy patch create generate bash desc logs status taint-node completion help"
  
  # Install components
  local install_components="docker helm kubernetes kubernetes-worker cert-manager ingress dashboard monitoring fluentd opensearch keycloak oauth2 vault ldap jupyter devworkspace workspace che ttyd cloudshell console argocd jenkins spinnaker registry istio rabbitmq kyverno gok-agent gok-controller controller gok-login chart base base-services"
  
  # Reset components  
  local reset_components="kubernetes kubernetes-worker cert-manager ingress dashboard monitoring fluentd opensearch keycloak oauth2 vault ldap jupyter devworkspace workspace che ttyd cloudshell console argocd jenkins spinnaker registry istio rabbitmq kyverno gok-agent gok-controller controller gok-login chart base-services"
  
  # Start components
  local start_components="kubernetes proxy kubelet"
  
  # Deploy components
  local deploy_components="app1"
  
  # Fix options
  local fix_options="helm-repository repositories package-conflicts"
  
  # Create resources
  local create_resources="secret certificate kubeconfig"
  local create_secret_types="apphost"
  
  # Generate service types
  local generate_types="python-api python-reactjs"
  
  # Patch resources
  local patch_resources="ingress"
  local patch_options="letsencrypt ldap localtls"
  
  case ${COMP_CWORD} in
    1)
      # First argument - complete main commands
      COMPREPLY=($(compgen -W "${commands}" -- ${cur}))
      return 0
      ;;
    2)
      # Second argument - complete based on first command
      case ${prev} in
        install)
          COMPREPLY=($(compgen -W "${install_components} --verbose -v" -- ${cur}))
          return 0
          ;;
        reset)
          COMPREPLY=($(compgen -W "${reset_components} --verbose -v" -- ${cur}))
          return 0
          ;;
        fix)
          COMPREPLY=($(compgen -W "${fix_options}" -- ${cur}))
          return 0
          ;;
        start)
          COMPREPLY=($(compgen -W "${start_components}" -- ${cur}))
          return 0
          ;;
        deploy)
          COMPREPLY=($(compgen -W "${deploy_components}" -- ${cur}))
          return 0
          ;;
        create)
          COMPREPLY=($(compgen -W "${create_resources}" -- ${cur}))
          return 0
          ;;
        generate)
          COMPREPLY=($(compgen -W "${generate_types}" -- ${cur}))
          return 0
          ;;
        patch)
          COMPREPLY=($(compgen -W "${patch_resources}" -- ${cur}))
          return 0
          ;;
        desc)
          COMPREPLY=($(compgen -W "methods" -- ${cur}))
          return 0
          ;;
        completion)
          COMPREPLY=($(compgen -W "enable setup" -- ${cur}))
          return 0
          ;;
      esac
      ;;
    3)
      # Third argument - complete based on first two commands
      local prev2="${COMP_WORDS[COMP_CWORD-2]}"
      case ${prev2} in
        install|reset)
          # If previous word is a component, suggest verbose flags
          if [[ "${install_components}" =~ ${prev} ]]; then
            COMPREPLY=($(compgen -W "--verbose -v" -- ${cur}))
            return 0
          fi
          ;;
        create)
          case ${prev} in
            secret)
              COMPREPLY=($(compgen -W "${create_secret_types}" -- ${cur}))
              return 0
              ;;
            certificate|kubeconfig)
              # For certificate and kubeconfig, suggest common names
              COMPREPLY=($(compgen -W "production staging development test" -- ${cur}))
              return 0
              ;;
          esac
          ;;
        generate)
          case ${prev} in
            python-api|python-reactjs)
              # Suggest common service names
              COMPREPLY=($(compgen -W "user-service auth-service api-gateway webapp frontend backend microservice" -- ${cur}))
              return 0
              ;;
          esac
          ;;
      esac
      ;;
    4)
      # Fourth argument - complete based on context
      local prev3="${COMP_WORDS[COMP_CWORD-3]}"
      local prev2="${COMP_WORDS[COMP_CWORD-2]}"
      case ${prev3} in
        patch)
          case ${prev2} in
            ingress)
              # Complete ingress names - could be dynamic from kubectl
              COMPREPLY=($(compgen -W "default production staging" -- ${cur}))
              return 0
              ;;
          esac
          ;;
      esac
      ;;
    5)
      # Fifth argument - namespaces for patch command
      local prev4="${COMP_WORDS[COMP_CWORD-4]}"
      local prev3="${COMP_WORDS[COMP_CWORD-3]}"
      case ${prev4} in
        patch)
          case ${prev3} in
            ingress)
              # Complete namespaces - could be dynamic from kubectl  
              COMPREPLY=($(compgen -W "default kube-system ingress-nginx monitoring" -- ${cur}))
              return 0
              ;;
          esac
          ;;
      esac
      ;;
    6)
      # Sixth argument - patch options
      local prev5="${COMP_WORDS[COMP_CWORD-5]}"
      case ${prev5} in
        patch)
          COMPREPLY=($(compgen -W "${patch_options}" -- ${cur}))
          return 0
          ;;
      esac
      ;;
  esac
  
  return 0
}

# Function to enable completion
enable_gok_completion() {
  complete -F _gok_completion gok
  echo "✅ GOK tab completion enabled!"
  echo "💡 You can now use tab completion with gok commands:"
  echo "   gok install <TAB>     # Shows all installable components"
  echo "   gok reset <TAB>       # Shows all resettable components"  
  echo "   gok create <TAB>      # Shows resource types"
  echo "   gok generate <TAB>    # Shows service types"
}

# Function to setup completion permanently
setup_gok_completion() {
  local completion_dir="/etc/bash_completion.d"
  local completion_file="$completion_dir/gok"
  local user_bashrc="$HOME/.bashrc"
  
  echo "Setting up GOK tab completion..."
  
  # Extract just the completion function to a separate file
  cat > /tmp/gok_completion << 'COMPLETION_EOF'
# GOK Bash Completion
_gok_completion() {
  local cur prev opts base
  COMPREPLY=()
  cur="${COMP_WORDS[COMP_CWORD]}"
  prev="${COMP_WORDS[COMP_CWORD-1]}"
  
  local commands="install fix reset start deploy patch create generate bash desc logs status taint-node completion help"
  local install_components="interactive wizard docker helm haproxy ha-proxy ha kubernetes kubernetes-worker cert-manager ingress dashboard monitoring fluentd opensearch keycloak oauth2 vault ldap jupyter devworkspace workspace che ttyd cloudshell console argocd jenkins spinnaker registry istio rabbitmq kyverno gok-agent gok-controller controller gok-login chart base base-services"
  local reset_components="kubernetes kubernetes-worker cert-manager ingress dashboard monitoring fluentd opensearch keycloak oauth2 vault ldap jupyter devworkspace workspace che ttyd cloudshell console argocd jenkins spinnaker registry istio rabbitmq kyverno gok-agent gok-controller controller gok-login chart base-services"
  local start_components="kubernetes proxy kubelet"
  local deploy_components="app1"
  local fix_options="helm-repository repositories package-conflicts"
  local create_resources="secret certificate kubeconfig"
  local create_secret_types="apphost"
  local generate_types="python-api python-reactjs"
  local patch_resources="ingress"
  local patch_options="letsencrypt ldap localtls"
  
  case ${COMP_CWORD} in
    1) COMPREPLY=($(compgen -W "${commands}" -- ${cur})) ;;
    2) case ${prev} in
         install) COMPREPLY=($(compgen -W "${install_components}" -- ${cur})) ;;
         fix) COMPREPLY=($(compgen -W "${fix_options}" -- ${cur})) ;;
         reset) COMPREPLY=($(compgen -W "${reset_components}" -- ${cur})) ;;
         start) COMPREPLY=($(compgen -W "${start_components}" -- ${cur})) ;;
         deploy) COMPREPLY=($(compgen -W "${deploy_components}" -- ${cur})) ;;
         create) COMPREPLY=($(compgen -W "${create_resources}" -- ${cur})) ;;
         generate) COMPREPLY=($(compgen -W "${generate_types}" -- ${cur})) ;;
         patch) COMPREPLY=($(compgen -W "${patch_resources}" -- ${cur})) ;;
         desc) COMPREPLY=($(compgen -W "methods" -- ${cur})) ;;
         remote) COMPREPLY=($(compgen -W "setup setup-ssh setup-keys copy-key setup-sudo passwordless-sudo add list show test-connection exec status copy install-gok" -- ${cur})) ;;
         completion) COMPREPLY=($(compgen -W "enable setup" -- ${cur})) ;;
       esac ;;
    3) local prev2="${COMP_WORDS[COMP_CWORD-2]}"
       case ${prev2} in
         create) case ${prev} in
                   secret) COMPREPLY=($(compgen -W "${create_secret_types}" -- ${cur})) ;;
                   certificate|kubeconfig) COMPREPLY=($(compgen -W "production staging development test" -- ${cur})) ;;
                 esac ;;
         generate) case ${prev} in
                     python-api|python-reactjs) COMPREPLY=($(compgen -W "user-service auth-service api-gateway webapp frontend backend microservice" -- ${cur})) ;;
                   esac ;;
       esac ;;
  esac
  return 0
}

complete -F _gok_completion gok
COMPLETION_EOF

  # Try to install system-wide first (requires sudo)
  if [[ -w "$completion_dir" ]] || sudo -n true 2>/dev/null; then
    if sudo cp /tmp/gok_completion "$completion_file" 2>/dev/null; then
      echo "✅ System-wide completion installed at $completion_file"
    else
      echo "⚠️  Could not install system-wide completion"
    fi
  fi
  
  # Install user-specific completion
  if ! grep -q "source.*gok_completion\|_gok_completion" "$user_bashrc" 2>/dev/null; then
    echo "" >> "$user_bashrc"
    echo "# GOK tab completion" >> "$user_bashrc"
    echo "source /tmp/gok_completion 2>/dev/null || true" >> "$user_bashrc"
    echo "✅ User completion added to $user_bashrc"
  else
    echo "ℹ️  Completion already configured in $user_bashrc"
  fi
  
  # Enable for current session
  source /tmp/gok_completion
  echo "✅ Tab completion enabled for current session!"
  echo ""
  echo "💡 Usage examples:"
  echo "   gok install <TAB><TAB>     # Shows all installable components"
  echo "   gok install ku<TAB>        # Completes to 'kubernetes'"
  echo "   gok create cert<TAB>       # Completes to 'certificate'"
  echo "   gok generate py<TAB>       # Shows python-api, python-reactjs"
  echo ""
  echo "🔄 Restart your terminal or run 'source ~/.bashrc' to enable permanently"
  
  rm -f /tmp/gok_completion
}

# Auto-enable completion if this script is being sourced
if [[ "${BASH_SOURCE[0]}" != "${0}" ]]; then
  complete -F _gok_completion gok 2>/dev/null || true
fi

# Call the appropriate function based on the command
if [ -z "$CMD" ] || [ "$CMD" == "help" ] || [ "$CMD" == "--help" ] || [ "$CMD" == "-h" ]; then
  helpCmd
elif [ "$CMD" == "bash" ]; then
  bashCmd
elif [ "$CMD" == "desc" ]; then
  descCmd "$2"
elif [ "$CMD" == "logs" ]; then
  logsCmd
elif [ "$CMD" == "status" ]; then
  statusCmd
elif [ "$CMD" == "taint-node" ]; then
  taintNodeCmd
elif [ "$CMD" == "install" ]; then
  installCmd "$2" "$3"
elif [ "$CMD" == "fix" ]; then
  fixCmd "$2"
elif [ "$CMD" == "start" ]; then
  startCmd "$2"
elif [ "$CMD" == "reset" ]; then
  resetCmd "$2" "$3"
elif [ "$CMD" == "deploy" ]; then
  deployCmd "$2"
elif [ "$CMD" == "patch" ]; then
  patchCmd "$2" "$3" "$4" "$5" "$6"
elif [ "$CMD" == "create" ]; then
  createCmd "$2" "$3" "$4"
elif [ "$CMD" == "generate" ]; then
  generateCmd "$2" "$3" "$4" "$5" "$6"
elif [ "$CMD" == "remote" ]; then
  case "$2" in
    setup)
      if [[ -z "$3" || -z "$4" ]]; then
        echo "Usage: gok remote setup <host> <user> [key_file]"
        echo ""
        echo "Set up default remote host for simple execution."
        echo ""
        echo "Examples:"
        echo "  gok remote setup 10.0.0.244 sumit"
        echo "  gok remote setup 192.168.1.100 ubuntu ~/.ssh/custom_key"
        echo ""
        echo "After setup, you can simply use:"
        echo "  gok remote exec \"kubectl get pods\""
        exit 1
      fi
      
      local host="$3"
      local user="$4"
      local key_file="${5:-$HOME/.ssh/id_rsa}"
      
      log_info "Setting up default remote host: $user@$host"
      
      # Setup SSH keys if needed
      setup_ssh_keys "$key_file"
      
      # Copy SSH key if needed
      log_info "Ensuring SSH access to $user@$host..."
      ssh -i "$key_file" -o StrictHostKeyChecking=no -o ConnectTimeout=5 \
          "$user@$host" "echo 'test'" >/dev/null 2>&1
      
      if [ $? -ne 0 ]; then
        log_info "Setting up SSH key access..."
        copy_ssh_key "$host" "$user" "$key_file"
        if [ $? -ne 0 ]; then
          log_error "Failed to setup SSH access"
          exit 1
        fi
      else
        log_info "SSH access already working"
      fi
      
      # Setup passwordless sudo
      log_info "Setting up passwordless sudo for root commands..."
      if ! setup_passwordless_sudo "$host" "$user" "$key_file"; then
        log_warning "Passwordless sudo setup failed, you'll need to use passwords for root commands"
        echo ""
        echo "You can retry passwordless sudo setup later with:"
        echo "  gok remote exec \"echo 'your_password' | sudo -S bash -c 'echo \\\"$user ALL=(ALL) NOPASSWD: ALL\\\" > /etc/sudoers.d/$user && chmod 440 /etc/sudoers.d/$user'\""
      fi
      
      # Save as default
      save_default_remote_config "$host" "$user" "$key_file"
      
      # Test the configuration
      log_info "Testing default remote configuration..."
      if simple_remote_exec "echo 'Remote setup successful! Host:' \$(hostname)"; then
        echo ""
        log_success "✨ Default remote setup complete!"
        echo ""
        echo "You can now use simple commands like:"
        echo "  gok remote exec \"kubectl get pods\""
        echo "  gok remote exec \"docker ps\""
        echo "  gok remote exec \"systemctl status nginx\""
      else
        log_error "Setup test failed"
        exit 1
      fi
      ;;
    add)
      if [[ -z "$3" || -z "$4" ]]; then
        echo "Usage: gok remote add <alias> <host> [user] [key_file]"
        echo ""
        echo "Examples:"
        echo "  gok remote add master 192.168.1.100 ubuntu"
        echo "  gok remote add node1 192.168.1.101 root ~/.ssh/id_rsa"
        exit 1
      fi
      configure_remote_host "$3" "$4" "${5:-root}" "${6:-$HOME/.ssh/id_rsa}"
      ;;
    list|show)
      show_remote_hosts
      ;;
    exec)
      if [[ -z "$3" ]]; then
        echo "Usage: gok remote exec [target] <command>"
        echo ""
        echo "Simple usage (after 'gok remote setup'):"
        echo "  gok remote exec \"kubectl get pods\"                         # Use default remote host"
        echo "  gok remote exec \"docker ps\"                                # Use default remote host"
        echo ""
        echo "Advanced usage with target:"
        echo "  gok remote exec master 'kubectl get nodes'                    # Use configured alias"
        echo "  gok remote exec 10.0.0.244:sumit 'kubectl get pods'          # Auto-setup sumit@10.0.0.244"
        echo "  gok remote exec 192.168.1.100 'docker ps'                    # Auto-setup root@192.168.1.100"
        echo "  gok remote exec all 'systemctl status docker'                # Execute on all hosts"
        echo ""
        echo "First-time setup:"
        echo "  gok remote setup <host> <user>                               # One-time configuration"
        echo ""
        exit 1
      fi
      
      # Check if we have a simple command (no target specified)
      # If $3 doesn't contain spaces and $4 exists, then $3 is likely a target
      # If $3 contains spaces or $4 is empty, then $3 is likely the command
      if [[ "$3" != *" "* ]] && [[ -n "$4" ]] && [[ "$3" != *":"* ]] && [[ "$3" =~ ^[a-zA-Z0-9._-]+$ ]]; then
        # Advanced usage: gok remote exec <target> <command>
        local target="$3"
        shift 3
        smart_remote_exec "$target" "$*"
      else
        # Simple usage: gok remote exec <command>
        shift 2
        simple_remote_exec "$*"
      fi
      ;;
    status)
      remote_status "${3:-all}"
      ;;
    install-gok)
      remote_install_gok "${3:-all}"
      ;;
    copy)
      if [[ -z "$3" || -z "$4" || -z "$5" ]]; then
        echo "Usage: gok remote copy <alias> <local_file> <remote_path>"
        echo ""
        echo "Examples:"
        echo "  gok remote copy master ./script.sh /tmp/script.sh"
        echo "  gok remote copy node1 ./config.yaml /etc/app/config.yaml"
        exit 1
      fi
      remote_copy "$3" "$4" "$5"
      ;;
    setup-ssh|setup-keys)
      local key_file="${3:-$HOME/.ssh/id_rsa}"
      setup_ssh_keys "$key_file"
      ;;
    copy-key)
      if [[ -z "$3" || -z "$4" ]]; then
        echo "Usage: gok remote copy-key <host> <user> [key_file]"
        echo ""
        echo "Examples:"
        echo "  gok remote copy-key 10.0.0.244 sumit"
        echo "  gok remote copy-key 192.168.1.100 ubuntu ~/.ssh/id_rsa"
        exit 1
      fi
      copy_ssh_key "$3" "$4" "${5:-$HOME/.ssh/id_rsa}"
      ;;
    setup-sudo|passwordless-sudo)
      if [[ -z "$3" || -z "$4" ]]; then
        echo "Usage: gok remote setup-sudo <host> <user> [key_file] [password]"
        echo ""
        echo "Configure passwordless sudo for root commands."
        echo ""
        echo "Examples:"
        echo "  gok remote setup-sudo 10.0.0.244 sumit"
        echo "  gok remote setup-sudo 192.168.1.100 ubuntu ~/.ssh/id_rsa mypassword"
        echo ""
        echo "Note: If password is not provided, you'll be prompted for it."
        exit 1
      fi
      setup_passwordless_sudo "$3" "$4" "${5:-$HOME/.ssh/id_rsa}" "$6"
      ;;
    test-connection)
      if [[ -z "$3" ]]; then
        echo "Usage: gok remote test-connection <alias>"
        echo ""
        echo "Examples:"
        echo "  gok remote test-connection master"
        echo "  gok remote test-connection node1"
        exit 1
      fi
      if [[ -z "${REMOTE_HOSTS[$3]}" ]]; then
        log_error "Remote host '$3' not configured"
        log_info "Available hosts: ${!REMOTE_HOSTS[*]}"
        exit 1
      fi
      local host="${REMOTE_HOSTS[$3]}"
      local user="${REMOTE_USERS[$3]}"
      local key_file="${REMOTE_KEYS[$3]}"
      
      log_info "Testing connection to $3 ($user@$host)..."
      ssh -i "$key_file" -o StrictHostKeyChecking=no -o ConnectTimeout=10 \
          "$user@$host" "echo 'SSH connection successful'; hostname; date"
      if [ $? -eq 0 ]; then
        log_success "Connection test successful for $3"
      else
        log_error "Connection test failed for $3"
      fi
      ;;
    *)
      echo "gok remote - Remote execution management"
      echo ""
      echo "🎯 SIMPLE 2-STEP PROCESS:"
      echo ""
      echo "Step 1 - One-time setup:"
      echo "  gok remote setup <host> <user> [key_file]         # Configure default remote"
      echo ""
      echo "Step 2 - Execute commands:"
      echo "  gok remote exec \"<command>\"                       # Run on default remote"
      echo ""
      echo "✨ Example workflow:"
      echo "  gok remote setup 10.0.0.244 sumit                # Setup once"
      echo "  gok remote exec \"kubectl get pods\"               # Use anytime"
      echo "  gok remote exec \"docker ps\"                      # Use anytime"
      echo "  gok remote exec \"systemctl status nginx\"        # Use anytime"
      echo ""
      echo "📋 All Commands:"
      echo "  gok remote setup <host> <user> [key_file]         # Setup default remote (includes passwordless sudo)"
      echo "  gok remote exec \"<command>\"                       # Execute on default remote"
      echo "  gok remote setup-ssh [key_file]                   # Generate SSH keys"
      echo "  gok remote copy-key <host> <user> [key_file]      # Copy SSH key to host"
      echo "  gok remote setup-sudo <host> <user> [key_file]    # Setup passwordless sudo"
      echo "  gok remote add <alias> <host> [user] [key_file]   # Add remote host alias"
      echo "  gok remote list                                   # Show configured hosts"
      echo "  gok remote test-connection <alias>               # Test SSH connection"
      echo "  gok remote exec <alias> \"<command>\"              # Execute on specific alias"  
      echo "  gok remote status [alias]                        # Show system status"
      echo "  gok remote copy <alias> <local_file> <remote_path> # Copy file"
      echo "  gok remote install-gok [alias]                   # Install GOK remotely"
      echo ""
      echo "📋 Manual Setup Examples:"
      echo "  gok remote setup-ssh                             # Generate SSH keys"
      echo "  gok remote copy-key 10.0.0.244 sumit            # Copy key to host"
      echo "  gok remote add debug 10.0.0.244 sumit           # Add debug host"
      echo "  gok remote test-connection debug                 # Test connection"
      echo ""
      echo "🔧 Advanced Examples:"
      echo "  gok remote add master 192.168.1.100 ubuntu       # Add master node"
      echo "  gok remote add node1 192.168.1.101 ubuntu        # Add worker node"
      echo "  gok remote list                                   # Show all hosts"
      echo "  gok remote exec master 'kubectl get nodes'       # Run kubectl on master"
      echo "  gok remote exec all 'docker ps'                  # Run docker on all hosts"
      echo "  gok remote status                                 # Check all host status"
      echo "  gok remote copy master ./script.sh /tmp/         # Copy script to master"
      echo ""
      echo "💡 Auto-Setup Features:"
      echo "  • SSH keys generated automatically if missing"
      echo "  • SSH keys copied to hosts automatically"
      echo "  • Passwordless sudo configured automatically"
      echo "  • Remote hosts configured automatically"
      echo "  • No manual setup steps required!"
      echo "  • Execute any root command without passwords!"
      ;;
  esac
elif [ "$CMD" == "completion" ]; then
  case "$2" in
    enable)
      enable_gok_completion
      ;;
    setup)
      setup_gok_completion
      ;;
    *)
      echo "gok completion - Bash completion management"
      echo ""
      echo "Usage:"
      echo "  gok completion enable    # Enable completion for current session"
      echo "  gok completion setup     # Setup completion permanently"
      echo ""
      echo "Examples:"
      echo "  gok completion enable    # Quick enable for testing"
      echo "  gok completion setup     # Permanent installation"
      ;;
  esac
fi