#!/bin/bash

: ${WORKING_DIR:=$MOUNT_PATH/kubernetes/install_k8s}
source "$WORKING_DIR"/config

release=$2
CMD=$1

createApp1() {
  cat <<EOF | kubectl apply -f -
apiVersion: apps/v1
kind: Deployment
metadata:
  name: app1
  namespace: default
spec:
  replicas: 1
  selector:
    matchLabels:
      app: app1
  template:
    metadata:
      labels:
        app: app1
    spec:
      containers:
      - name: app1
        image: dockersamples/static-site
        env:
        - name: AUTHOR
          value: app1
        ports:
        - containerPort: 80
---
apiVersion: v1
kind: Service
metadata:
  name: appsvc1
  namespace: default
spec:
  ports:
  - port: 80
    protocol: TCP
    targetPort: 80
  selector:
    app: app1
---
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  annotations:
    nginx.ingress.kubernetes.io/rewrite-target: /
    kubernetes.io/ingress.class: "nginx"
  name: app-ingress
  namespace: default
spec:
  rules:
  - host: master.cloud.com
    http:
      paths:
      - backend:
          service:
            name: appsvc1
            port:
              number: 80
        path: /app1
        pathType: Prefix
EOF
}

getpod() {
  pod=$(kubectl get po -l app="$release" 2>/dev/null | awk "/${release}/" | awk '{print $1}' | head -n 1)
  echo "$pod"
}

updateSys() {
  apt-get update
}

installDeps() {
  #Install network tools
  apt-get install net-tools
  apt-get install jq -y

  #Installing python
  apt-get install python3 -y
  apt-get install python3-pip -y

}

ingressUnInst() {
  output=$(kubectl get po -n ingress-nginx -l app.kubernetes.io/component=controller -o json | jq '.items | length')
  if [ "$output" == "1" ]; then
    helm uninstall ingress-nginx
    kubectl delete ns ingress-nginx
  fi
}

ingressInst() {
  helm repo add ingress-nginx https://kubernetes.github.io/ingress-nginx
  helm repo update
  helm install \
    ingress-nginx ingress-nginx/ingress-nginx \
    --namespace ingress-nginx \
    --create-namespace \
    --set controller.service.nodePorts.http=80 \
    --set controller.service.nodePorts.https=443 \
    --set controller.service.type=NodePort \
    --set defaultBackend.enabled=true
  #    -f charts/values.yaml

  output=$(kubectl get po -n ingress-nginx -l app.kubernetes.io/component=controller -ojsonpath='{.items[0].status.containerStatuses[0].ready}')
  while [ "$output" != "true" ]; do
    echo "Ingress controller service is not up, will check again after 5seconds"
    sleep 5
    output=$(kubectl get po -n ingress-nginx -l app.kubernetes.io/component=controller -ojsonpath='{.items[0].status.containerStatuses[0].ready}')
  done
}

dockrInst() {

  echo "Installing docker"
  apt-get install -y \
    apt-transport-https \
    ca-certificates \
    curl \
    software-properties-common
  curl -fsSL https://download.docker.com/linux/ubuntu/gpg | apt-key add -
  sudo install -m 0755 -d /etc/apt/keyrings
  sudo curl -fsSL https://download.docker.com/linux/ubuntu/gpg -o /etc/apt/keyrings/docker.asc
  sudo chmod a+r /etc/apt/keyrings/docker.asc

  # Add the repository to Apt sources:
  echo \
    "deb [arch=$(dpkg --print-architecture) signed-by=/etc/apt/keyrings/docker.asc] https://download.docker.com/linux/ubuntu \
    $(. /etc/os-release && echo "$VERSION_CODENAME") stable" | \
    sudo tee /etc/apt/sources.list.d/docker.list > /dev/null
  sudo apt-get update

  apt-get update && apt-get install -y \
    docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin

  # Create required directories
  sudo mkdir -p /etc/systemd/system/docker.service.d

  # Create daemon json config file
  sudo tee /etc/docker/daemon.json <<EOF
{
  "exec-opts": ["native.cgroupdriver=systemd"],
  "log-driver": "json-file",
  "log-opts": {
    "max-size": "100m"
  },
  "storage-driver": "overlay2"
}
EOF

  # Start and enable Services
  sudo systemctl daemon-reload
  sudo systemctl restart docker
  sudo systemctl enable docker

  # Start and enable containerd services
  sudo systemctl enable containerd
  sudo systemctl start containerd
  sudo rm /etc/containerd/config.toml
  sudo systemctl restart containerd
}

customDns() {
  #Adding custom dns server
  cat <<EOF | kubectl apply -f -
apiVersion: v1
data:
  Corefile: |
    .:53 {
        errors
        health {
           lameduck 5s
        }
        ready
        kubernetes cloud.uat in-addr.arpa ip6.arpa {
           pods insecure
           fallthrough in-addr.arpa ip6.arpa
           ttl 30
        }
        prometheus :9153
        forward . /etc/resolv.conf {
           max_concurrent 1000
        }
        cache 30
        loop
        reload
        loadbalance
    }
    cloud.com:53 {
        errors
        cache 30
        forward . 11.0.0.1
    }
kind: ConfigMap
metadata:
  name: coredns
  namespace: kube-system
EOF

  kubectl delete pod --namespace kube-system -l k8s-app=kube-dns
}

taintNode() {
  echo "Going to taint node for scheduling in master"
  : ${IP:=$(ifconfig eth2 2>/dev/null | awk '/inet / {print $2}' | sed 's/addr://')}
  if [ -z "$IP" ]; then
    : ${IP:=$(ifconfig enp0s3 2>/dev/null | awk '/inet / {print $2}' | sed 's/addr://')}
  fi
  JSONPATH="{.items[?(@.status.addresses[0].address == \"${IP}\")].metadata.name}"
  NODE_NAME="$(kubectl get nodes -o jsonpath="$JSONPATH")"
  kubectl taint node "${NODE_NAME}" node-role.kubernetes.io/control-plane:NoSchedule-

}

k8sInst() {
  # Enable kernel modules
  sudo modprobe overlay
  sudo modprobe br_netfilter

  # Add some settings to sysctl
  sudo tee /etc/sysctl.d/kubernetes.conf <<EOF
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
net.ipv4.ip_forward = 1
EOF

  # Reload sysctl
  sudo sysctl --system
  sudo tee /etc/modules-load.d/containerd.conf <<EOF
overlay
br_netfilter
EOF
  sysctl --system
  mkdir -p /etc/containerd
  containerd config default>/etc/containerd/config.toml

  #https://github.com/containerd/containerd/blob/main/docs/cri/config.md#cgroup-driver
  sed -i 's/SystemdCgroup = false/SystemdCgroup = true/g' /etc/containerd/config.toml
  systemctl restart containerd
  systemctl enable containerd


  echo "Installing Kubernetes"
  apt-get update && apt-get install -y apt-transport-https
  sudo apt-get update
  # apt-transport-https may be a dummy package; if so, you can skip that package
  sudo apt-get install -y apt-transport-https ca-certificates curl

  # If the folder `/etc/apt/keyrings` does not exist, it should be created before the curl command, read the note below.
  # sudo mkdir -p -m 755 /etc/apt/keyrings
  curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.30/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg
  sudo chmod 644 /etc/apt/keyrings/kubernetes-apt-keyring.gpg # allow unprivileged APT programs to read this keyring
  # This overwrites any existing configuration in /etc/apt/sources.list.d/kubernetes.list
  echo 'deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.30/deb/ /' | sudo tee /etc/apt/sources.list.d/kubernetes.list
  sudo chmod 644 /etc/apt/sources.list.d/kubernetes.list   # helps tools such as command-not-found to work correctly

  sudo apt-get update
  sudo apt-get install -y kubectl kubeadm kubelet

  kubeadm version
  kubeadm config images pull

  sudo sed -i '/ swap / s/^\(.*\)$/#\1/g' /etc/fstab
  sudo swapoff -a

  sudo systemctl enable kubelet

  envsubst <"$WORKING_DIR"/cluster-config-master.yaml >"$WORKING_DIR"/config.yaml
  kubeadm init --config="$WORKING_DIR"/config.yaml
  export KUBECONFIG=/etc/kubernetes/admin.conf

  mkdir -p "$HOME"/.kube
  sudo cp -i /etc/kubernetes/admin.conf "$HOME"/.kube/config

  # shellcheck disable=SC2181
  if [ $? -ne 0 ]; then
    echo "Kubectl command execution failed, please check!!!!!"
    exit 1
  fi

}

calicoInst(){
  kubectl create -f https://raw.githubusercontent.com/projectcalico/calico/v3.25.1/manifests/calico.yaml

  if [ $? -ne 0 ]; then
    echo "Calico installation failed, please check!!!!!"
    exit 1
  fi
}

helmInst() {
  #Installing helm
  curl https://baltocdn.com/helm/signing.asc | apt-key add - &&
    apt-get install apt-transport-https --yes &&
    echo "deb https://baltocdn.com/helm/stable/debian/ all main" | tee /etc/apt/sources.list.d/helm-stable-debian.list &&
    apt-get update &&
    apt-get install helm &&
    helm version --short &&
    helm repo add stable https://charts.helm.sh/stable

}

certmanagerInst() {
  helm repo add jetstack https://charts.jetstack.io
  helm repo update
  kubectl apply -f https://github.com/cert-manager/cert-manager/releases/download/v1.14.5/cert-manager.crds.yaml

  helm install \
    cert-manager jetstack/cert-manager \
    --namespace cert-manager \
    --create-namespace \
    --version v1.14.5
}

certificateRequestForNs() {
NS=$1
if [ -z $2 ]; then
  SUBDOMAIN=kube
else
  SUBDOMAIN=$2
fi

  cat <<EOF | kubectl apply -f -
apiVersion: cert-manager.io/v1
kind: Certificate
metadata:
  name: ${SUBDOMAIN}-gokcloud-com-tls
  namespace: ${NS}
spec:
  secretName: ${SUBDOMAIN}-gokcloud-com
  issuerRef:
    name: letsencrypt-prod
    kind: ClusterIssuer
  commonName: ${SUBDOMAIN}.gokcloud.com
  dnsNames:
    - ${SUBDOMAIN}.gokcloud.com
  issuerRef:
    name: letsencrypt-prod
    kind: ClusterIssuer
EOF
echo "Certificate request for NS $NS created, executing below command to know current status"
echo "kubectl get Issuers,ClusterIssuers,Certificates,CertificateRequests,Orders,Challenges --all-namespaces"
kubectl get Issuers,ClusterIssuers,Certificates,CertificateRequests,Orders,Challenges --all-namespaces
}

generateHttpSolverCertificate() {

  cat <<EOF | kubectl apply -f -
apiVersion: cert-manager.io/v1
kind: ClusterIssuer
metadata:
  name: letsencrypt-prod
spec:
  acme:
    email: majisumitkumar@gmail.com
    #server: https://acme-v02.api.letsencrypt.org/directory
    server: https://acme-staging-v02.api.letsencrypt.org/directory
    privateKeySecretRef:
      name: letsencrypt-prod
    solvers:
    - http01:
        ingress:
          ingressClassName: nginx
EOF

  cat <<EOF | kubectl apply -f -
apiVersion: cert-manager.io/v1
kind: Certificate
metadata:
  name: gokcloud-com-tls
  namespace: default
spec:
  secretName: gokcloud-com
  issuerRef:
    name: letsencrypt-prod
    kind: ClusterIssuer
  commonName: kube.gokcloud.com
  dnsNames:
    - kube.gokcloud.com
  issuerRef:
    name: letsencrypt-prod
    kind: ClusterIssuer
EOF

}

certManagerReset() {
  kubectl delete Issuers,ClusterIssuers,Certificates,CertificateRequests,Orders,Challenges --all --all-namespaces
  helm --namespace cert-manager delete cert-manager
  kubectl delete namespace cert-manager
  kubectl delete -f https://github.com/cert-manager/cert-manager/releases/download/v1.8.2/cert-manager.crds.yaml
}

#Godday api calls are disabled, hence going to remove this call.
godaddyWebhook() {
  kubectl apply -f https://github.com/sumitmaji/kubernetes/raw/q1/install_k8s/godaddy-cert-webhook/webhook-all.yml --validate=false
  echo "Provide godaddy apikey and secret <API_KEY:SECRET>"
  # shellcheck disable=SC2162
  read API_KEY

  cat <<EOF | kubectl apply -f -
apiVersion: v1
kind: Secret
metadata:
  name: godaddy-api-key-secret
  namespace: cert-manager
type: Opaque
stringData:
  api-key: ${API_KEY}
EOF

  cat <<EOF | kubectl apply -f -
apiVersion: cert-manager.io/v1
kind: ClusterIssuer
metadata:
  name: letsencrypt-prod
spec:
  acme:
    email: majisumitkumar@gmail.com
    server: https://acme-v02.api.letsencrypt.org/directory
    privateKeySecretRef:
      name: letsencrypt-prod
    solvers:
    - dns01:
        webhook:
          config:
            apiKeySecretRef:
              name: godaddy-api-key-secret
              key: api-key
            production: true
            ttl: 600
          groupName: gokcloud.com
          solverName: godaddy
      selector:
       dnsNames:
       - 'kube.gokcloud.com'
       - '*.gokcloud.com'
EOF

  cat <<EOF | kubectl apply -f -
apiVersion: cert-manager.io/v1
kind: Certificate
metadata:
  name: gokcloud-com-tls
  namespace: default
spec:
  secretName: gokcloud-com
  issuerRef:
    name: letsencrypt-prod
    kind: ClusterIssuer
  commonName: kube.gokcloud.com
  dnsNames:
    - kube.gokcloud.com
  issuerRef:
    name: letsencrypt-prod
    kind: ClusterIssuer
EOF

}

godaddyWebhookReset() {
  kubectl delete -f https://github.com/sumitmaji/kubernetes/raw/q1/install_k8s/godaddy-cert-webhook/webhook-all.yml
  kubectl delete secret godaddy-api-key-secret -n cert-manager
}

haInst() {
  docker stop master-proxy
  docker rm master-proxy
  cat <<EOF >/opt/haproxy.cfg
global
        log 127.0.0.1 local0
        log 127.0.0.1 local1 notice
        maxconn 4096
        maxpipes 1024
        daemon
defaults
        log global
        mode tcp
        option tcplog
        option dontlognull
        option redispatch
        option http-server-close
        retries 3
        timeout connect 5000
        timeout client 50000
        timeout server 50000
        frontend default_frontend
        bind *:$HA_PROXY_PORT
        default_backend master-cluster
backend master-cluster
$(#Install master nodes
    IFS=','
    counter=0
    cluster=""
    for worker in $API_SERVERS; do
      oifs=$IFS
      IFS=':'
      read -r ip node <<<"$worker"
      if [ -z "$cluster" ]; then
        cluster="$ip:6443"
      else
        cluster="$cluster,http://$ip:4001"
      fi
      counter=$((counter + 1))
      IFS=$oifs
      echo "        server master-$counter ${cluster} check"
      cluster=""
    done
    unset IFS
  )
EOF

  docker run -d --name master-proxy \
    -v /opt/haproxy.cfg:/usr/local/etc/haproxy/haproxy.cfg:ro \
    --net=host haproxy
}

startKubelet() {
  systemctl stop kubelet
  systemctl start kubelet
}

startHa() {
  docker stop master-proxy
  docker rm master-proxy
  docker run -d --name master-proxy \
    -v /opt/haproxy.cfg:/usr/local/etc/haproxy/haproxy.cfg:ro \
    --net=host haproxy
}

disableSwap() {
  sudo sed -i '/ swap / s/^\(.*\)$/#\1/g' /etc/fstab
  sudo swapoff -a
}

hostSecret() {
  openssl genrsa -out ${APP_HOST}.key 4096
  openssl req -new -key ${APP_HOST}.key -out ${APP_HOST}.csr -subj "/CN=${APP_HOST}" \
    -addext "subjectAltName = DNS:${APP_HOST}"
  openssl x509 -req -in ${APP_HOST}.csr -CA /etc/kubernetes/pki/ca.crt -CAkey /etc/kubernetes/pki/ca.key -CAcreateserial -out ${APP_HOST}.crt -days 7200

  kubectl create secret tls appingress-certificate --key ${APP_HOST}.key --cert ${APP_HOST}.crt -n default
}

dashboardInst() {
  helm repo add kubernetes-dashboard https://kubernetes.github.io/dashboard/
  kubectl delete ns kubernetes-dashboard
  helm install kubernetes-dashboard \
    kubernetes-dashboard/kubernetes-dashboard \
    --namespace kubernetes-dashboard \
    --create-namespace \
    -f https://github.com/sumitmaji/kubernetes/raw/q1/install_k8s/dashboard/values2.yaml
}

prometheusGrafanaReset() {
  helm -n monitoring delete monitoring
  kubectl delete ns monitoring
  helm -n db delete postgres
  kubectl delete ns db
}

emptyLocalFsStorageKeycloak(){
  kubectl delete pv keycloak-pv
  kubectl delete sc keycloak-storage
  rm -rf /data/volumes/pv3
}

localFsStorageClassKeycloakCreate(){
  cat << EOF | kubectl apply -f -
kind: StorageClass
apiVersion: storage.k8s.io/v1
metadata:
  name: keycloak-storage
provisioner: kubernetes.io/no-provisioner
volumeBindingMode: WaitForFirstConsumer
EOF

  mkdir -p /data/volumes/pv3
  chmod 777 /data/volumes/pv3

  cat << EOF | kubectl apply -f -
apiVersion: v1
kind: PersistentVolume
metadata:
  name: keycloak-pv
spec:
  capacity:
    storage: 10Gi
  accessModes:
  - ReadWriteOnce
  persistentVolumeReclaimPolicy: Retain
  storageClassName: keycloak-storage
  local:
    path: /data/volumes/pv3
  nodeAffinity:
    required:
      nodeSelectorTerms:
      - matchExpressions:
        - key: kubernetes.io/hostname
          operator: In
          values:
          - master.cloud.com
EOF
}



emptyLocalFsStorageMonotring(){
  kubectl delete pv prometheus-pv
  kubectl delete pv alertmanager-pv
  kubectl delete sc prometheus-storage
  kubectl delete sc alertmanager-storage
  rm -rf /data/volumes/{pv1,pv2}
}

localFsStorageClassMonitoringCreate(){
  cat << EOF | kubectl apply -f -
kind: StorageClass
apiVersion: storage.k8s.io/v1
metadata:
  name: prometheus-storage
provisioner: kubernetes.io/no-provisioner
volumeBindingMode: WaitForFirstConsumer
EOF

  cat << EOF | kubectl apply -f -
kind: StorageClass
apiVersion: storage.k8s.io/v1
metadata:
  name: alertmanager-storage
provisioner: kubernetes.io/no-provisioner
volumeBindingMode: WaitForFirstConsumer
EOF

  mkdir -p /data/volumes/pv1
  chmod 777 /data/volumes/pv1
  mkdir -p /data/volumes/pv2
  chmod 777 /data/volumes/pv2

  cat << EOF | kubectl apply -f -
apiVersion: v1
kind: PersistentVolume
metadata:
  name: prometheus-pv
spec:
  capacity:
    storage: 10Gi
  accessModes:
  - ReadWriteOnce
  persistentVolumeReclaimPolicy: Retain
  storageClassName: prometheus-storage
  local:
    path: /data/volumes/pv1
  nodeAffinity:
    required:
      nodeSelectorTerms:
      - matchExpressions:
        - key: kubernetes.io/hostname
          operator: In
          values:
          - master.cloud.com
EOF

  cat << EOF | kubectl apply -f -
apiVersion: v1
kind: PersistentVolume
metadata:
  name: alertmanager-pv
spec:
  capacity:
    storage: 10Gi
  accessModes:
  - ReadWriteOnce
  persistentVolumeReclaimPolicy: Retain
  storageClassName: alertmanager-storage
  local:
    path: /data/volumes/pv2
  nodeAffinity:
    required:
      nodeSelectorTerms:
      - matchExpressions:
        - key: kubernetes.io/hostname
          operator: In
          values:
          - master.cloud.com
EOF

}

prometheusGrafanaResetv2(){
  helm -n monitoring delete prometheus
  helm -n monitoring delete grafana
  kubectl delete ns monitoring

}

prometheusGrafanaInstv2(){
  helm repo add prometheus-community https://prometheus-community.github.io/helm-charts
  helm repo add grafana https://grafana.github.io/helm-charts
  helm repo update
  helm install prometheus prometheus-community/prometheus \
    --values https://github.com/sumitmaji/kubernetes/raw/q1/install_k8s/prometheus-grafana/prometheus-values.yaml \
    --namespace monitoring \
    --create-namespace

  helm install grafana grafana/grafana \
    --values https://github.com/sumitmaji/kubernetes/raw/q1/install_k8s/prometheus-grafana/grafana-values.yaml \
    --namespace monitoring

  kubectl -n kube-system get cm kube-proxy -o yaml | sed 's/metricsBindAddress: ""/metricsBindAddress: 0.0.0.0:10249/' | kubectl apply -f -

  kubectl -n kube-system patch ds kube-proxy -p \
      '{"spec":{"template":{"metadata":{"labels":{"updateTime":"'`date +'%s'`'"}}}}}'


}

prometheusGrafanaInst() {

  helm repo add prometheus-community \
    https://prometheus-community.github.io/helm-charts
  helm repo update
  helm install monitoring \
    prometheus-community/kube-prometheus-stack \
    --values https://github.com/sumitmaji/kubernetes/raw/master/install_k8s/prometheus-grafana/values.yaml \
    --version 39.6.0 \
    --namespace monitoring \
    --create-namespace

#  kubectl -n kube-system get cm kube-proxy-config -o yaml | sed \
#    's/metricsBindAddress: 127.0.0.1:10249/metricsBindAddress: 0.0.0.0:10249' \
#    kubectl apply -f -

  kubectl -n kube-system get cm kube-proxy -o yaml | sed 's/metricsBindAddress: ""/metricsBindAddress: 0.0.0.0:10249/' | kubectl apply -f -

  kubectl -n kube-system patch ds kube-proxy -p \
    '{"spec":{"template":{"metadata":{"labels":{"updateTime":"'`date +'%s'`'"}}}}}'

  helm repo add bitnami https://charts.bitnami.com/bitnami
  helm repo update
  helm install postgres \
    bitnami/postgresql \
    --values https://github.com/sumitmaji/kubernetes/raw/master/install_k8s/prometheus-grafana/postgres-values.yaml \
    --version 11.7.1 \
    --namespace db \
    --create-namespace

}

dashboardReset() {
  helm uninstall kubernetes-dashboard -n kubernetes-dashboard
}

adminRole() {
  WC=$(kubectl get clusterrolebinding cloud-cluster-admin 2>/dev/null | wc -l)
  if [ "$WC" == "0" ]; then

    cat <<EOF | kubectl apply -f -
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: cloud-cluster-admin
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: cluster-admin
subjects:
- apiGroup: rbac.authorization.k8s.io
  kind: Group
  name: cloud:masters
EOF
  fi

}

createKubeConfig() {
  USERNAME=$1
  : ${IP:=$(ifconfig eth0 2>/dev/null | awk '/inet / {print $2}' | sed 's/addr://')}
  if [ -z "$IP" ]; then
    : ${IP:=$(ifconfig enp0s3 2>/dev/null | awk '/inet / {print $2}' | sed 's/addr://')}
  fi
  adminRole
  echo + Creating private key: ${USERNAME}.key
  openssl genrsa -out ${USERNAME}.key 4096

  echo + Creating signing request: ${USERNAME}.csr
  openssl req -new -key ${USERNAME}.key -out ${USERNAME}.csr -subj "/CN=${USERNAME}/O=cloud:masters"
  WC=$(kubectl get csr ${USERNAME}-csr 2>/dev/null | wc -l)
  if [ "$WC" != "0" ]; then
    kubectl delete csr ${USERNAME}-csr
  fi
  echo + Creating signing request in kubernetes
  cat <<EOF | kubectl create -f -
apiVersion: certificates.k8s.io/v1
kind: CertificateSigningRequest
metadata:
  name: ${USERNAME}-csr
spec:
  groups:
    - system:authenticated
    - cloud:masters
  request: $(cat ${USERNAME}.csr | base64 | tr -d '\n')
  signerName: kubernetes.io/kube-apiserver-client
  usages:
    - digital signature
    - key encipherment
    - client auth
EOF

  kubectl certificate approve ${USERNAME}-csr

  kubectl get csr ${USERNAME}-csr -o jsonpath='{.status.certificate}' | base64 -d >${USERNAME}.crt

  echo "======Kubeconfig file user ${USERNAME}.conf generated"

  kubectl config set-cluster cloud.com --certificate-authority=/etc/kubernetes/pki/ca.crt --embed-certs=true --server=https://${IP}:6443 --kubeconfig=/root/${USERNAME}.conf
  kubectl config set-credentials ${USERNAME} --client-key=${USERNAME}.key --client-certificate=${USERNAME}.crt --embed-certs=true --kubeconfig=/root/${USERNAME}.conf
  kubectl config --kubeconfig=/root/${USERNAME}.conf set-context ${USERNAME}@cloud.com --cluster=cloud.com --user=${USERNAME}
  kubectl config --kubeconfig=/root/${USERNAME}.conf use-context ${USERNAME}@cloud.com

  rm ${USERNAME}.key ${USERNAME}.csr ${USERNAME}.crt

}

patchLdapSecure() {
  kubectl patch ing "$NAME" --patch "$(
    cat <<EOF
metadata:
  annotations:
    nginx.ingress.kubernetes.io/auth-signin: https://kube.gokcloud.com/authenticate
    nginx.ingress.kubernetes.io/auth-url: https://kube.gokcloud.com/check
EOF
  )" -n "$NS"
}

patchLetsEncrypt() {
  NAME=$1
  NS=$2
  if [ -z $3 ]; then
    SUBDOMAIN=kube
  else
    SUBDOMAIN=$3
  fi
  kubectl patch ing "$NAME" --patch "$(
    cat <<EOF
metadata:
  annotations:
    certmanager.k8s.io/cluster-issuer: letsencrypt-prod
    nginx.ingress.kubernetes.io/ssl-redirect: "true"
spec:
  tls:
    - hosts:
        - ${SUBDOMAIN}.gokcloud.com
      secretName: ${SUBDOMAIN}-gokcloud-com
EOF
  )" -n "$NS"
  kubectl patch ing "$NAME" --type=json -p='[{"op": "replace", "path": "/spec/rules/0/host", "value":"kube.gokcloud.com"}]' -n "$NS"

}

kcd(){
  alias kcd='kubectl config set-context $(kubectl config current-context) --namespace'
}

helmTemplateDebug(){
  CHART_NAME=$1
  CHART_REPO=$2
  helm install $1 $2  \
    --dry-run --debug | less
}

keycloakReset(){
  helm uninstall keycloak -n keycloak
  kubectl delete pvc --all -n keycloak
}

k(){
  alias k='kubectl'
}

keycloakInst(){

  helm install keycloak oci://registry-1.docker.io/bitnamicharts/keycloak \
        --namespace keycloak \
        --create-namespace \
        -f https://github.com/sumitmaji/kubernetes/raw/q1/install_k8s/keycloak/values2.yaml
}

installKeycloakWithCertMgr(){
  ./gok install keycloak
  ./gok create certificate keycloak keycloak
  ./gok patch ingress keycloak keycloak letsencrypt keycloak
}

installPrometheusGrafanaWithCertMgr(){
  ./gok install monitoring
  ./gok create certificate monitoring kube
  ./gok patch ingress prometheus-server monitoring letsencrypt kube
  ./gok patch ingress grafana monitoring letsencrypt kube
}

installDashboardwithCertManager(){
  ./gok install dashboard
  ./gok create certificate kubernetes-dashboard kube
  ./gok patch ingress kubernetes-dashboard kubernetes-dashboard letsencrypt kube
}

decodeSecret(){
  SECRET=$1
  NS=$2
  kubectl get secret -n $NS $SECRET -o json | jq -r '.data."tls.crt"' | base64 -d | openssl x509 -noout -text
}

patchLocalTls() {
  NAME=$1
  NS=$2
  kubectl patch ing "$NAME" --patch "$(
    cat <<EOF
spec:
  tls:
    - hosts:
        - $APP_HOST
      secretName: appingress-certificate
EOF
  )" -n "$NS"
  kubectl patch ing "$NAME" --type=json -p='[{"op": "replace", "path": "/spec/rules/0/host", "value":"master.cloud.com"}]' -n "$NS"
  kubectl patch ing "$NAME" --type=json -p='[{"op": "add", "path": "/metadata/annotations", "value":{"nginx.ingress.kubernetes.io/rewrite-target": "/", "kubernetes.io/ingress.class": "nginx"}}]' -n "$NS"
}

if [ "$CMD" == "bash" ]; then
  pod=$(getpod)
  echo "Opening terminal on $pod"
  kubectl exec -it "$pod" -- /bin/bash
elif [ "$CMD" == "desc" ]; then
  pod=$(getpod)
  echo "Describing pod $pod"
  kubectl describe po "$pod"
elif [ "$CMD" == "logs" ]; then
  pod=$(getpod)
  echo "Viewing logs of pod $pod"
  kubectl logs "$pod"
elif [ "$CMD" == "status" ]; then
  helm status "$release"
elif [ "$CMD" == "taint-node" ]; then
  taintNode
elif [ "$CMD" == "install" ]; then
  updateSys
  installDeps
  COMPONENT=$2
  if [ "$COMPONENT" == "docker" ]; then
    dockrInst
  elif [ "$COMPONENT" == "cert-manager" ]; then
    certmanagerInst
    generateHttpSolverCertificate
    #godaddyWebhook
  elif [ "$COMPONENT" == "monitoring" ]; then
    #prometheusGrafanaInst
    localFsStorageClassMonitoringCreate
    prometheusGrafanaInstv2
  elif [ "$COMPONENT" == "dashboard" ]; then
    dashboardInst
  elif [ "$COMPONENT" == "keycloak" ]; then
      keycloakInst
      localFsStorageClassKeycloakCreate
  elif [ "$COMPONENT" == "ingress" ]; then
    ingressInst
  elif [ "$COMPONENT" == "kubernetes" ]; then
    dockrInst
    haInst
    k8sInst
    helmInst
    taintNode
    calicoInst
    customDns
    cat <<EOF
\______   |  |   ____ _____    ______ ____   __  _  ______  |___/  |_  _/ _______________  /_   |   _____ |__| ____
 |     ___|  | _/ __ \\__  \  /  ____/ __ \  \ \/ \/ \__  \ |  \   __\ \   __/  _ \_  __ \  |   |  /     \|  |/    \
 |    |   |  |_\  ___/ / __ \_\___ \\  ___/   \     / / __ \|  ||  |    |  |(  <_> |  | \/  |   | |  Y Y  |  |   |  \
 |____|   |____/\___  (____  /____  >\___  >   \/\_/ (____  |__||__|    |__| \____/|__|     |___| |__|_|  |__|___|  / /\
                    \/     \/     \/     \/               \/                                            \/        \/  \/
EOF
  fi
elif [ "$CMD" == "start" ]; then
  COMPONENT=$2
  if [ "$COMPONENT" == "kubernetes" ]; then
    disableSwap
    startHa
    startKubelet
  elif [ "$COMPONENT" == "proxy" ]; then
    startHa
  elif [ "$COMPONET" == "kubelet" ]; then
    startKubelet
  fi
elif [ "$CMD" == "reset" ]; then
  COMPONENT=$2
  if [ "$COMPONENT" == "kubernetes" ]; then
    kubeadm reset <<EOF
y
EOF
  elif [ "$COMPONENT" == "monitoring" ]; then
    #prometheusGrafanaReset
    prometheusGrafanaResetv2
    emptyLocalFsStorageMonotring
  elif [ "$COMPONENT" == "dashboard" ]; then
    dashboardReset
  elif [ "$COMPONENT" == "keycloak" ]; then
        keycloakReset
        emptyLocalFsStorageKeycloak
  elif [ "$COMPONENT" == "ingress" ]; then
    ingressUnInst
  elif [ "$COMPONENT" == "cert-manager" ]; then
    #godaddyWebhookReset
    certManagerReset
  fi
elif [ "$CMD" == "deploy" ]; then
  COMPONENT=$2
  if [ "$COMPONENT" == "app1" ]; then
    createApp1
  fi
elif [ "$CMD" == "patch" ]; then
  RESOURCE=$2
  NAME=$3
  NS=$4
  OPTIONS=$5
  SUBDOMAIN=$6
  if [ "$RESOURCE" == "ingress" ]; then
    if [ "$OPTIONS" == "letsencrypt" ]; then
      patchLetsEncrypt "$NAME" "$NS" "$SUBDOMAIN"
    elif [ "$OPTIONS" == "ldap" ]; then
      patchLdapSecure "$NAME" "$NS"
    elif [ "$OPTIONS" == "localtls" ]; then
      patchLocalTls "$NAME" "$NS"
    fi
  fi
elif [ "$CMD" == "create" ]; then
  RESOURCE=$2
  NAME=$3
  ADDITIONAL=$4
  if [ "$RESOURCE" == "secret" ]; then
    if [ "$NAME" == "apphost" ]; then
      hostSecret
    fi
  elif [ "$RESOURCE" == "certificate" ]; then
    certificateRequestForNs "$NAME" "$ADDITIONAL"
  elif [ "$RESOURCE" == "kubeconfig" ]; then
    createKubeConfig "$NAME"
  fi
fi
